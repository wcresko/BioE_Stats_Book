{
  "hash": "26dc689902f8423b8ef50974dfe38f92",
  "result": {
    "engine": "knitr",
    "markdown": "# Simple Linear Regression {#sec-linear-regression}\n\n\n::: {.cell}\n\n:::\n\n\n## From Correlation to Prediction\n\nCorrelation tells us that two variables are related, but it does not allow us to predict one from the other or to describe the nature of that relationship. Linear regression goes further—it models the relationship between variables, allowing us to make predictions and to quantify how changes in one variable are associated with changes in another.\n\nIn simple linear regression, we model a response variable Y as a linear function of a predictor variable X:\n\n$$y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i$$\n\nHere $\\beta_0$ is the intercept (the expected value of Y when X equals zero), $\\beta_1$ is the slope (how much Y changes for a one-unit change in X), and $\\epsilon_i$ represents the random error—the part of Y not explained by X.\n\n![](../images/images_4b.008.jpeg){fig-align=\"center\"}\n\n## Origins of the Term \"Regression\"\n\nThe term \"regression\" comes from Francis Galton's studies of heredity in the 1880s. He observed that tall parents tended to have children who were tall, but not as extremely tall as the parents—children's heights \"regressed\" toward the population mean. This phenomenon, now called regression to the mean, is a statistical artifact that appears whenever two variables are imperfectly correlated.\n\n![](../images/images_4b.002.jpeg){fig-align=\"center\"}\n\n## The Regression Fallacy\n\nUnderstanding regression to the mean is crucial because ignoring it leads to a common error in reasoning called the **regression fallacy**. This occurs when we attribute regression to the mean to some other cause—typically claiming credit for improvement that was simply statistical regression.\n\nThe most famous example is the \"Sophomore Slump\" in baseball. The player who wins Rookie of the Year typically performs worse in their second season. Sportswriters often blame this on pressure, complacency, or teams \"figuring out\" the player. But much of this decline is simply regression to the mean.\n\nConsider why: to win Rookie of the Year, a player typically needs exceptional performance—often their personal best. This outstanding season likely involved skill plus some good luck (favorable conditions, timely hits, etc.). In the following season, luck averages out, and performance regresses toward the player's true ability level.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Simulating the Sophomore Slump\nset.seed(42)\nn_players <- 500\n\n# True talent level for each player (varies between players)\ntrue_talent <- rnorm(n_players, mean = 0.265, sd = 0.020)\n\n# Season 1: observed performance = true talent + luck\nluck_season1 <- rnorm(n_players, mean = 0, sd = 0.025)\nbatting_avg_yr1 <- true_talent + luck_season1\n\n# Season 2: observed performance = same true talent + different luck\nluck_season2 <- rnorm(n_players, mean = 0, sd = 0.025)\nbatting_avg_yr2 <- true_talent + luck_season2\n\n# Find the \"Rookie of the Year\" - best performer in year 1\nroy_idx <- which.max(batting_avg_yr1)\n\n# Look at top 10 performers in year 1\ntop_10 <- order(batting_avg_yr1, decreasing = TRUE)[1:10]\n\ncat(\"Top 10 performers in Year 1 vs Year 2:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTop 10 performers in Year 1 vs Year 2:\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"========================================\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n========================================\n```\n\n\n:::\n\n```{.r .cell-code}\nfor (i in 1:10) {\n  idx <- top_10[i]\n  change <- batting_avg_yr2[idx] - batting_avg_yr1[idx]\n  cat(sprintf(\"Player %d: Yr1 = %.3f, Yr2 = %.3f, Change = %+.3f\\n\",\n              i, batting_avg_yr1[idx], batting_avg_yr2[idx], change))\n}\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nPlayer 1: Yr1 = 0.384, Yr2 = 0.291, Change = -0.093\nPlayer 2: Yr1 = 0.366, Yr2 = 0.272, Change = -0.094\nPlayer 3: Yr1 = 0.357, Yr2 = 0.298, Change = -0.059\nPlayer 4: Yr1 = 0.352, Yr2 = 0.240, Change = -0.112\nPlayer 5: Yr1 = 0.349, Yr2 = 0.328, Change = -0.021\nPlayer 6: Yr1 = 0.343, Yr2 = 0.301, Change = -0.042\nPlayer 7: Yr1 = 0.337, Yr2 = 0.299, Change = -0.038\nPlayer 8: Yr1 = 0.334, Yr2 = 0.325, Change = -0.009\nPlayer 9: Yr1 = 0.329, Yr2 = 0.242, Change = -0.087\nPlayer 10: Yr1 = 0.329, Yr2 = 0.283, Change = -0.046\n```\n\n\n:::\n\n```{.r .cell-code}\n# How many of top 10 declined?\ndeclines <- sum(batting_avg_yr2[top_10] < batting_avg_yr1[top_10])\ncat(sprintf(\"\\n%d of top 10 performers showed a decline (the 'slump')\\n\", declines))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n10 of top 10 performers showed a decline (the 'slump')\n```\n\n\n:::\n:::\n\n\nNotice that nearly all of the top performers declined—not because of anything about being a sophomore, but because extreme initial performance tends to be followed by more average performance.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Visualize regression to the mean\nplot(batting_avg_yr1, batting_avg_yr2,\n     pch = 19, col = rgb(0, 0, 0, 0.3),\n     xlab = \"Year 1 Batting Average\",\n     ylab = \"Year 2 Batting Average\",\n     main = \"Regression to the Mean: The Sophomore Slump\",\n     xlim = c(0.20, 0.35), ylim = c(0.20, 0.35))\n\n# Add y = x line (what we'd see with no regression)\nabline(a = 0, b = 1, col = \"gray\", lty = 2, lwd = 2)\n\n# Add regression line\nreg_line <- lm(batting_avg_yr2 ~ batting_avg_yr1)\nabline(reg_line, col = \"red\", lwd = 2)\n\n# Highlight top performers\npoints(batting_avg_yr1[top_10], batting_avg_yr2[top_10],\n       pch = 19, col = \"blue\", cex = 1.5)\n\n# Mark the \"Rookie of the Year\"\npoints(batting_avg_yr1[roy_idx], batting_avg_yr2[roy_idx],\n       pch = 17, col = \"red\", cex = 2)\n\nlegend(\"topleft\",\n       legend = c(\"All players\", \"Top 10 Year 1\", \"Best performer\",\n                  \"No regression (y=x)\", \"Regression line\"),\n       pch = c(19, 19, 17, NA, NA),\n       lty = c(NA, NA, NA, 2, 1),\n       col = c(rgb(0,0,0,0.3), \"blue\", \"red\", \"gray\", \"red\"),\n       lwd = c(NA, NA, NA, 2, 2))\n```\n\n::: {.cell-output-display}\n![](16-simple-linear-regression_files/figure-html/unnamed-chunk-3-1.png){width=768}\n:::\n:::\n\n\nThe dashed line shows what we would see if Year 1 perfectly predicted Year 2 (no regression to the mean). The red regression line shows reality—it's flatter, meaning extreme Year 1 performers tend to move toward the center in Year 2.\n\n::: {.callout-warning}\n## Avoiding the Regression Fallacy\n\nThe regression fallacy appears in many contexts:\n\n- **Medical treatments**: Patients seek treatment when symptoms are worst; subsequent improvement may be regression, not treatment effect\n- **Performance management**: Workers reprimanded for poor performance often improve; those praised for good performance often decline—both may be regression\n- **Educational interventions**: Students identified as struggling (tested at their worst) often improve regardless of intervention\n\nWhen evaluating any intervention applied to extreme cases, always consider whether observed changes might simply be regression to the mean.\n:::\n\n## Fitting the Model: Ordinary Least Squares\n\nThe most common method for fitting a regression line is **ordinary least squares (OLS)**. OLS finds the line that minimizes the sum of squared residuals—the squared vertical distances between observed points and the fitted line.\n\n![](../images/images_4b.009.jpeg){fig-align=\"center\"}\n\nThe OLS estimates for the slope and intercept are:\n\n$$\\hat{\\beta}_1 = \\frac{\\sum(x_i - \\bar{x})(y_i - \\bar{y})}{\\sum(x_i - \\bar{x})^2} = r \\frac{s_y}{s_x}$$\n\n$$\\hat{\\beta}_0 = \\bar{y} - \\hat{\\beta}_1 \\bar{x}$$\n\nNotice that the slope equals the correlation coefficient times the ratio of standard deviations. This makes clear the connection between correlation and regression.\n\n## Linear Regression in R\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Example: zebrafish size data\nset.seed(42)\nn <- 100\nlength_cm <- runif(n, 0.5, 3.5)\nweight_mg <- 15 * length_cm^2 + rnorm(n, sd = 10)\n\n# Fit the model\nfish_lm <- lm(weight_mg ~ length_cm)\nsummary(fish_lm)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = weight_mg ~ length_cm)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-39.277  -9.033  -0.432   9.998  29.934 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -51.429      3.579  -14.37   <2e-16 ***\nlength_cm     61.660      1.583   38.95   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 14.27 on 98 degrees of freedom\nMultiple R-squared:  0.9393,\tAdjusted R-squared:  0.9387 \nF-statistic:  1517 on 1 and 98 DF,  p-value: < 2.2e-16\n```\n\n\n:::\n:::\n\n\nThe output shows the estimated coefficients with their standard errors, t-statistics, and p-values. The Multiple R-squared indicates how much of the variance in Y is explained by X.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Visualize the fit\nplot(length_cm, weight_mg, pch = 19, col = \"blue\",\n     xlab = \"Length (cm)\", ylab = \"Weight (mg)\",\n     main = \"Linear Regression: Weight vs Length\")\nabline(fish_lm, col = \"red\", lwd = 2)\n```\n\n::: {.cell-output-display}\n![](16-simple-linear-regression_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\n## Interpretation of Coefficients\n\nThe **intercept** $\\hat{\\beta}_0$ is the predicted value of Y when X equals zero. This may or may not be meaningful depending on whether X = 0 makes sense in your context.\n\nThe **slope** $\\hat{\\beta}_1$ is the predicted change in Y for a one-unit increase in X. If the slope is 15, then each additional unit of X is associated with 15 more units of Y on average.\n\n## Hypothesis Testing in Regression\n\nThe hypothesis test for the slope asks whether there is evidence of a relationship between X and Y:\n\n$$H_0: \\beta_1 = 0 \\quad \\text{(no relationship)}$$\n$$H_A: \\beta_1 \\neq 0 \\quad \\text{(relationship exists)}$$\n\nThe test uses a t-statistic, comparing the estimated slope to its standard error. The p-value indicates the probability of observing a slope this far from zero if the true slope were zero.\n\n![](../images/images_4b.011.jpeg){fig-align=\"center\"}\n\n## R-Squared: Measuring Model Fit\n\n**R-squared** ($R^2$) measures the proportion of variance in Y explained by the model:\n\n$$R^2 = 1 - \\frac{SS_{error}}{SS_{total}} = \\frac{SS_{model}}{SS_{total}}$$\n\nIn simple linear regression, $R^2$ equals the square of the correlation coefficient. An $R^2$ of 0.7 means the model explains 70% of the variance in Y; the remaining 30% is unexplained.\n\nBe cautious with $R^2$—it always increases when you add predictors, even useless ones. Adjusted $R^2$ penalizes for model complexity.\n\n## Making Predictions\n\nOnce you have a fitted model, you can predict Y for new values of X:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Predict weight for new lengths\nnew_lengths <- data.frame(length_cm = c(1.0, 2.0, 3.0))\npredict(fish_lm, newdata = new_lengths, interval = \"confidence\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n        fit        lwr       upr\n1  10.23108   5.828292  14.63387\n2  71.89135  69.050838  74.73186\n3 133.55162 129.491293 137.61194\n```\n\n\n:::\n:::\n\n\nThe confidence interval indicates uncertainty about the mean Y at each X value. A prediction interval (using `interval = \"prediction\"`) would be wider, accounting for individual variability around that mean.\n\n## Model Assumptions\n\nLinear regression assumptions include:\n\n1. **Linearity**: The relationship between X and Y is linear\n2. **Independence**: Observations are independent of each other\n3. **Normality**: Residuals are normally distributed\n4. **Homoscedasticity**: Residuals have constant variance across X\n\nThese assumptions should be checked through residual analysis.\n\n## Residual Analysis\n\n**Residuals** are the differences between observed and fitted values: $e_i = y_i - \\hat{y}_i$. Examining residuals reveals whether model assumptions are satisfied.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Residual diagnostic plots\npar(mfrow = c(2, 2))\nplot(fish_lm)\n```\n\n::: {.cell-output-display}\n![](16-simple-linear-regression_files/figure-html/unnamed-chunk-7-1.png){width=768}\n:::\n:::\n\n\nKey diagnostic plots:\n\n1. **Residuals vs Fitted**: Should show random scatter around zero. Patterns suggest non-linearity or heteroscedasticity.\n\n2. **Q-Q Plot**: Residuals should fall along the diagonal line if normally distributed. Deviations at the tails indicate non-normality.\n\n3. **Scale-Location**: Should show constant spread. A funnel shape indicates heteroscedasticity.\n\n4. **Residuals vs Leverage**: Identifies influential points. Points with high leverage and large residuals may unduly influence the fit.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Example of problematic residuals\nset.seed(123)\nx_prob <- seq(1, 10, length.out = 50)\ny_prob <- x_prob^2 + rnorm(50, sd = 5)  # Quadratic relationship\n\nlm_prob <- lm(y_prob ~ x_prob)\n\npar(mfrow = c(1, 2))\nplot(x_prob, y_prob, pch = 19, main = \"Data with Non-linear Pattern\")\nabline(lm_prob, col = \"red\", lwd = 2)\n\nplot(fitted(lm_prob), residuals(lm_prob), pch = 19,\n     xlab = \"Fitted values\", ylab = \"Residuals\",\n     main = \"Residuals Show Clear Pattern\")\nabline(h = 0, col = \"red\", lty = 2)\n```\n\n::: {.cell-output-display}\n![](16-simple-linear-regression_files/figure-html/unnamed-chunk-8-1.png){width=768}\n:::\n:::\n\n\nThe curved pattern in the residuals reveals that a linear model is inappropriate—the true relationship is non-linear.\n\n## Model I vs Model II Regression\n\nStandard OLS regression (Model I) assumes X is measured without error and minimizes vertical distances to the line. This is appropriate when:\n\n- X is fixed by the experimenter (controlled variable)\n- X is measured with negligible error compared to Y\n- The goal is prediction of Y from X\n\nWhen both variables are measured with error (common in observational studies), **Model II regression** may be more appropriate. Model II methods include:\n\n- **Major Axis (MA) regression**: Minimizes perpendicular distances to the line\n- **Reduced Major Axis (RMA)**: Often preferred when both variables have similar measurement error\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Model I slope estimate\nslope_model1 <- coef(fish_lm)[2]\n\n# Reduced Major Axis slope estimate (ratio of standard deviations)\nslope_rma <- sd(weight_mg) / sd(length_cm) * sign(cor(length_cm, weight_mg))\n\ncat(\"Model I (OLS) slope:\", round(slope_model1, 3), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nModel I (OLS) slope: 61.66 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Model II (RMA) slope:\", round(slope_rma, 3), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nModel II (RMA) slope: 63.62 \n```\n\n\n:::\n:::\n\n\n::: {.callout-note}\n## When to Use Model II Regression\n\nUse Model II regression when:\n- Both X and Y are random variables measured with error\n- You want to describe the relationship rather than predict Y from X\n- You need to compare slopes across groups (e.g., allometric scaling)\n\nThe `lmodel2` package in R provides Model II regression methods.\n:::\n\n## Extrapolation Warning\n\nRegression models should only be used to make predictions within the range of observed X values. **Extrapolation**—predicting beyond this range—is risky because the linear relationship may not hold.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Danger of extrapolation\nplot(length_cm, weight_mg, pch = 19, col = \"blue\",\n     xlim = c(0, 6), ylim = c(-50, 600),\n     xlab = \"Length (cm)\", ylab = \"Weight (mg)\",\n     main = \"Extrapolation Risk\")\nabline(fish_lm, col = \"red\", lwd = 2)\nabline(v = c(min(length_cm), max(length_cm)), col = \"gray\", lty = 2)\n\n# Mark extrapolation zone\nrect(max(length_cm), -50, 6, 600, col = rgb(1, 0, 0, 0.1), border = NA)\ntext(5, 100, \"Extrapolation\\nzone\", col = \"red\")\n```\n\n::: {.cell-output-display}\n![](16-simple-linear-regression_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n\n## Summary\n\nSimple linear regression models the relationship between a predictor and response variable:\n\n- OLS finds the line minimizing squared residuals\n- The slope indicates how Y changes per unit change in X\n- R-squared measures proportion of variance explained\n- Residual analysis checks model assumptions\n- Model II regression is appropriate when both variables have measurement error\n- Avoid extrapolating beyond the range of observed data\n\n## Practice Exercises\n\nFor hands-on practice with regression concepts, see @sec-ex-linear in the Practice Exercises appendix. The exercises include:\n\n- Fitting and interpreting linear models\n- Visualizing regression lines and residuals\n- Checking model assumptions\n- ANOVA for comparing group means\n\n## Additional Resources\n\n- @james2023islr - Excellent introduction to regression in the context of statistical learning\n- @logan2010biostatistical - Detailed coverage of regression with biological applications\n",
    "supporting": [
      "16-simple-linear-regression_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}