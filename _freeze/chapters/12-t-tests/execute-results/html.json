{
  "hash": "dbb3a1b6460fca840ea8d5a3cc85379b",
  "result": {
    "engine": "knitr",
    "markdown": "# T-Tests {#sec-t-tests}\n\n\n::: {.cell}\n\n:::\n\n\n## Comparing Means\n\nOne of the most common questions in data analysis is whether two groups differ. Is the mean expression level different between treatment and control? Does the new material have different strength than the standard? Do patients on drug A have different outcomes than patients on drug B?\n\nThe t-test is the classic method for comparing means. It compares the observed difference between groups to the variability expected by chance, producing a test statistic that follows a t-distribution under the null hypothesis of no difference.\n\n## The T-Distribution\n\nThe t-distribution, discovered by William Sealy Gosset (who published under the pseudonym \"Student\"), resembles the normal distribution but has heavier tails. This accounts for the extra uncertainty that comes from estimating the population standard deviation from sample data.\n\nThe t-distribution is characterized by its degrees of freedom (df). As df increases, the t-distribution approaches the normal distribution. For small samples, the heavier tails mean that extreme values are more likely, leading to wider confidence intervals and more conservative tests.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Compare t-distributions with different df\nx <- seq(-4, 4, length.out = 200)\nplot(x, dnorm(x), type = \"l\", lwd = 2, col = \"black\",\n     xlab = \"x\", ylab = \"Density\",\n     main = \"T-distributions vs. Normal\")\nlines(x, dt(x, df = 3), lwd = 2, col = \"red\")\nlines(x, dt(x, df = 10), lwd = 2, col = \"blue\")\nlegend(\"topright\",\n       legend = c(\"Normal\", \"t (df=3)\", \"t (df=10)\"),\n       col = c(\"black\", \"red\", \"blue\"), lwd = 2)\n```\n\n::: {.cell-output-display}\n![](12-t-tests_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\n### Why Heavier Tails Matter in Practice\n\nThe heavier tails of the t-distribution have real practical consequences. When you estimate the standard deviation from a small sample, you might underestimate or overestimate the true value. The t-distribution accounts for this uncertainty by assigning more probability to extreme values.\n\nConsider this concrete example: suppose you're estimating voter support from a poll of 25 likely voters. With the true population proportion unknown and estimated from the sample, how wide should your confidence interval be?\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Demonstrate the practical difference between normal and t-based intervals\nset.seed(2016)\n\n# Simulate: true support is 48.5%, poll 25 people\ntrue_support <- 0.485\nn_poll <- 25\n\n# One poll result\npoll_result <- rbinom(1, n_poll, true_support) / n_poll\npoll_se <- sqrt(poll_result * (1 - poll_result) / n_poll)\n\n# Compare critical values\nz_crit <- qnorm(0.975)      # Normal: 1.96\nt_crit <- qt(0.975, df = n_poll - 1)  # t with 24 df: 2.06\n\n# Calculate intervals\nnormal_ci <- c(poll_result - z_crit * poll_se, poll_result + z_crit * poll_se)\nt_ci <- c(poll_result - t_crit * poll_se, poll_result + t_crit * poll_se)\n\ncat(\"Poll result:\", round(poll_result * 100, 1), \"%\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nPoll result: 40 %\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Standard error:\", round(poll_se * 100, 1), \"%\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nStandard error: 9.8 %\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nNormal-based 95% CI: [\", round(normal_ci[1]*100, 1), \"%, \",\n    round(normal_ci[2]*100, 1), \"%]\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nNormal-based 95% CI: [ 20.8 %,  59.2 %]\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"t-based 95% CI:      [\", round(t_ci[1]*100, 1), \"%, \",\n    round(t_ci[2]*100, 1), \"%]\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nt-based 95% CI:      [ 19.8 %,  60.2 %]\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nDifference in width:\", round((t_ci[2] - t_ci[1] - (normal_ci[2] - normal_ci[1]))*100, 2),\n    \"percentage points\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nDifference in width: 2.04 percentage points\n```\n\n\n:::\n:::\n\n\nWith only 25 observations, the t-distribution gives a critical value of about 2.06 instead of 1.96. This ~5% wider interval provides better coverage when the sample standard deviation might deviate substantially from the population value.\n\nThe difference matters most in the tails. For extreme values (like being 2.5+ standard errors away from the mean), the t-distribution assigns noticeably more probability:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Probability of being more than 2.5 SE from the mean\nprob_extreme_normal <- 2 * pnorm(-2.5)\nprob_extreme_t <- 2 * pt(-2.5, df = 24)\n\ncat(\"P(|Z| > 2.5) with normal distribution:\", round(prob_extreme_normal, 4), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nP(|Z| > 2.5) with normal distribution: 0.0124 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"P(|T| > 2.5) with t(df=24):\", round(prob_extreme_t, 4), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nP(|T| > 2.5) with t(df=24): 0.0197 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"The t-distribution gives\", round(prob_extreme_t/prob_extreme_normal, 1),\n    \"times higher probability to extreme values\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nThe t-distribution gives 1.6 times higher probability to extreme values\n```\n\n\n:::\n:::\n\n\nThis is why using the normal distribution instead of the t-distribution for small samples leads to confidence intervals that are too narrow and p-values that are too small—both resulting in overconfident conclusions.\n\n## One-Sample T-Test\n\nThe one-sample t-test compares a sample mean to a hypothesized population value. The null hypothesis is that the population mean equals the specified value: $H_0: \\mu = \\mu_0$.\n\nThe test statistic is:\n\n$$t = \\frac{\\bar{x} - \\mu_0}{s / \\sqrt{n}}$$\n\nThis is the difference between the sample mean and hypothesized value, divided by the standard error of the mean. Under the null hypothesis, this statistic follows a t-distribution with $n-1$ degrees of freedom.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# One-sample t-test example\n# Does this sample come from a population with mean = 100?\nset.seed(42)\nsample_data <- rnorm(25, mean = 105, sd = 15)\n\nt.test(sample_data, mu = 100)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tOne Sample t-test\n\ndata:  sample_data\nt = 1.9936, df = 24, p-value = 0.05768\nalternative hypothesis: true mean is not equal to 100\n95 percent confidence interval:\n  99.72443 115.90166\nsample estimates:\nmean of x \n  107.813 \n```\n\n\n:::\n:::\n\n\nThe output shows the t-statistic, degrees of freedom, p-value, confidence interval, and sample mean. The small p-value indicates evidence that the true mean differs from 100.\n\n## Two-Sample T-Test\n\nThe two-sample (independent samples) t-test compares means from two independent groups. The null hypothesis is that the population means are equal: $H_0: \\mu_1 = \\mu_2$.\n\nThe test assumes:\n- Independence of observations within and between groups\n- Normally distributed populations (or large samples)\n- Equal variances in both groups (for the standard version)\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Two-sample t-test example\nset.seed(518)\ntreatment <- rnorm(n = 30, mean = 12, sd = 3)\ncontrol <- rnorm(n = 30, mean = 10, sd = 3)\n\n# Visualize the data\npar(mfrow = c(1, 2))\nboxplot(treatment, control, names = c(\"Treatment\", \"Control\"),\n        col = c(\"lightblue\", \"lightgreen\"), main = \"Boxplot\")\n        \n# Combined histogram\nhist(treatment, col = rgb(0, 0, 1, 0.5), xlim = c(0, 20),\n     main = \"Histograms\", xlab = \"Value\")\nhist(control, col = rgb(0, 1, 0, 0.5), add = TRUE)\nlegend(\"topright\", legend = c(\"Treatment\", \"Control\"),\n       fill = c(rgb(0, 0, 1, 0.5), rgb(0, 1, 0, 0.5)))\n```\n\n::: {.cell-output-display}\n![](12-t-tests_files/figure-html/unnamed-chunk-6-1.png){width=768}\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Perform the t-test\nt.test(treatment, control)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tWelch Two Sample t-test\n\ndata:  treatment and control\nt = 1.3224, df = 57.98, p-value = 0.1912\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -0.5256045  2.5718411\nsample estimates:\nmean of x mean of y \n 11.08437  10.06125 \n```\n\n\n:::\n:::\n\n\n## Welch's T-Test\n\nThe classic two-sample t-test assumes equal variances. When this assumption is violated, Welch's t-test provides a better alternative. It adjusts the degrees of freedom to account for unequal variances.\n\nR's `t.test()` function uses Welch's test by default. To use the equal-variance version, set `var.equal = TRUE`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# When variances are unequal\nset.seed(42)\ngroup1 <- rnorm(30, mean = 50, sd = 5)\ngroup2 <- rnorm(30, mean = 52, sd = 15)\n\n# Welch's test (default)\nt.test(group1, group2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tWelch Two Sample t-test\n\ndata:  group1 and group2\nt = 0.055423, df = 37.98, p-value = 0.9561\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -6.095093  6.438216\nsample estimates:\nmean of x mean of y \n 50.34293  50.17137 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Equal variance assumed\nt.test(group1, group2, var.equal = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tTwo Sample t-test\n\ndata:  group1 and group2\nt = 0.055423, df = 58, p-value = 0.956\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -6.024786  6.367910\nsample estimates:\nmean of x mean of y \n 50.34293  50.17137 \n```\n\n\n:::\n:::\n\n\n## Paired T-Test\n\nWhen observations in two groups are naturally paired—the same subjects measured twice, matched pairs, or before-and-after measurements—the paired t-test is more appropriate. It tests whether the mean difference within pairs is zero.\n\nThe paired t-test is more powerful than the two-sample test when pairs are positively correlated, because it removes between-subject variability.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Paired t-test example: before and after treatment\nset.seed(123)\nn <- 20\nbefore <- rnorm(n, mean = 100, sd = 15)\n# After measurements are correlated with before\nafter <- before + rnorm(n, mean = 5, sd = 5)\n\n# Paired test (correct for this data)\nt.test(after, before, paired = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tPaired t-test\n\ndata:  after and before\nt = 5.1123, df = 19, p-value = 6.19e-05\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n 2.801598 6.685830\nsample estimates:\nmean difference \n       4.743714 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Compare to unpaired (less power)\nt.test(after, before, paired = FALSE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tWelch Two Sample t-test\n\ndata:  after and before\nt = 1.0209, df = 37.992, p-value = 0.3138\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -4.663231 14.150660\nsample estimates:\nmean of x mean of y \n 106.8681  102.1244 \n```\n\n\n:::\n:::\n\n\nNotice that the paired test produces a smaller p-value because it accounts for the correlation between measurements on the same subject.\n\n## One-Tailed vs. Two-Tailed Tests\n\nBy default, `t.test()` performs a two-tailed test. For a one-tailed test, specify the alternative hypothesis:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Two-tailed (default): H_A: treatment ≠ control\nt.test(treatment, control, alternative = \"two.sided\")$p.value\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.1912327\n```\n\n\n:::\n\n```{.r .cell-code}\n# One-tailed: H_A: treatment > control\nt.test(treatment, control, alternative = \"greater\")$p.value\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.09561633\n```\n\n\n:::\n\n```{.r .cell-code}\n# One-tailed: H_A: treatment < control\nt.test(treatment, control, alternative = \"less\")$p.value\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.9043837\n```\n\n\n:::\n:::\n\n\nUse one-tailed tests only when you have a strong prior reason to expect an effect in a specific direction and would not act on an effect in the opposite direction.\n\n## Checking Assumptions\n\nT-tests assume normally distributed data (or large samples) and, for the standard two-sample test, equal variances. Check these assumptions before interpreting results.\n\n**Normality**: Use histograms, Q-Q plots, or formal tests like Shapiro-Wilk.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Check normality with Q-Q plot\nqqnorm(treatment)\nqqline(treatment, col = \"red\")\n```\n\n::: {.cell-output-display}\n![](12-t-tests_files/figure-html/unnamed-chunk-11-1.png){width=576}\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Shapiro-Wilk test for normality\nshapiro.test(treatment)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tShapiro-Wilk normality test\n\ndata:  treatment\nW = 0.9115, p-value = 0.01624\n```\n\n\n:::\n:::\n\n\nA non-significant Shapiro-Wilk test suggests the data are consistent with normality. However, this test has low power for small samples and may reject normality for trivial deviations with large samples.\n\n**Equal variances**: Compare standard deviations or use Levene's test.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Compare standard deviations\nsd(treatment)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 3.024138\n```\n\n\n:::\n\n```{.r .cell-code}\nsd(control)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 2.968592\n```\n\n\n:::\n\n```{.r .cell-code}\n# Levene's test (from car package)\n# car::leveneTest(c(treatment, control), \n#                 factor(rep(c(\"treatment\", \"control\"), each = 30)))\n```\n:::\n\n\n## Effect Size: Cohen's d\n\nStatistical significance does not tell you how large an effect is. **Cohen's d** measures effect size as the standardized difference between means:\n\n$$d = \\frac{\\bar{x}_1 - \\bar{x}_2}{s_{pooled}}$$\n\nwhere $s_{pooled}$ is the pooled standard deviation.\n\nConventional interpretations: $|d| = 0.2$ is small, $|d| = 0.5$ is medium, $|d| = 0.8$ is large. However, context matters—a small d might be practically important in some fields.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Calculate Cohen's d\nmean_diff <- mean(treatment) - mean(control)\ns_pooled <- sqrt((var(treatment) + var(control)) / 2)\ncohens_d <- mean_diff / s_pooled\n\ncat(\"Cohen's d:\", round(cohens_d, 2), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nCohen's d: 0.34 \n```\n\n\n:::\n:::\n\n\n## Practical Example\n\nLet's work through a complete analysis comparing two groups:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Simulated drug trial data\nset.seed(999)\ndrug <- rnorm(40, mean = 75, sd = 12)\nplacebo <- rnorm(40, mean = 70, sd = 12)\n\n# Step 1: Visualize\npar(mfrow = c(2, 2))\nboxplot(drug, placebo, names = c(\"Drug\", \"Placebo\"), \n        col = c(\"coral\", \"lightblue\"), main = \"Response by Group\")\n\n# Step 2: Check normality\nqqnorm(drug, main = \"Q-Q Plot: Drug\")\nqqline(drug, col = \"red\")\nqqnorm(placebo, main = \"Q-Q Plot: Placebo\")\nqqline(placebo, col = \"red\")\n\n# Combined histogram\nhist(drug, col = rgb(1, 0.5, 0.5, 0.5), xlim = c(40, 110),\n     main = \"Distribution Comparison\", xlab = \"Response\")\nhist(placebo, col = rgb(0.5, 0.5, 1, 0.5), add = TRUE)\n```\n\n::: {.cell-output-display}\n![](12-t-tests_files/figure-html/unnamed-chunk-15-1.png){width=768}\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Step 3: Perform t-test\nresult <- t.test(drug, placebo)\nprint(result)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tWelch Two Sample t-test\n\ndata:  drug and placebo\nt = 1.2147, df = 75.923, p-value = 0.2282\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -1.990367  8.213525\nsample estimates:\nmean of x mean of y \n 72.26982  69.15824 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Step 4: Calculate effect size\ncohens_d <- (mean(drug) - mean(placebo)) / \n            sqrt((var(drug) + var(placebo)) / 2)\ncat(\"\\nCohen's d:\", round(cohens_d, 2), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCohen's d: 0.27 \n```\n\n\n:::\n:::\n\n\nThe t-test shows a significant difference (p < 0.05), and Cohen's d indicates a medium effect size. We can conclude that the drug group shows higher response than the placebo group, with the mean difference being about 0.4 standard deviations.\n\n## Randomization Tests as an Alternative\n\nWhen normality assumptions are questionable and sample sizes are small, randomization (permutation) tests provide a non-parametric alternative to the t-test. The logic is elegant: if there is no difference between groups, then the group labels are arbitrary and could be shuffled without affecting the distribution of the test statistic.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Randomization test example\nset.seed(42)\ngroup_A <- c(23, 25, 28, 31, 35, 29)\ngroup_B <- c(18, 20, 22, 19, 21, 23)\n\n# Observed difference\nobs_diff <- mean(group_A) - mean(group_B)\n\n# Combine all observations\nall_data <- c(group_A, group_B)\nn_A <- length(group_A)\nn_B <- length(group_B)\n\n# Generate null distribution by permutation\nn_perms <- 10000\nperm_diffs <- numeric(n_perms)\n\nfor (i in 1:n_perms) {\n  shuffled <- sample(all_data)\n  perm_diffs[i] <- mean(shuffled[1:n_A]) - mean(shuffled[(n_A+1):(n_A+n_B)])\n}\n\n# Plot null distribution\nhist(perm_diffs, breaks = 50, col = \"lightblue\",\n     main = \"Randomization Null Distribution\",\n     xlab = \"Difference in Means\")\nabline(v = obs_diff, col = \"red\", lwd = 2)\nabline(v = -obs_diff, col = \"red\", lwd = 2, lty = 2)\n```\n\n::: {.cell-output-display}\n![](12-t-tests_files/figure-html/unnamed-chunk-17-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# Two-tailed p-value\np_value <- mean(abs(perm_diffs) >= abs(obs_diff))\ncat(\"Observed difference:\", round(obs_diff, 2), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nObserved difference: 8 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Permutation p-value:\", p_value, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nPermutation p-value: 0.0039 \n```\n\n\n:::\n:::\n\n\nThe randomization test makes no assumptions about the underlying distribution—it only assumes that observations are exchangeable under the null hypothesis. This makes it robust to non-normality and outliers.\n\n::: {.callout-tip}\n## When to Use Randomization Tests\n\n- Sample sizes are small (n < 20 per group)\n- Data are clearly non-normal or contain outliers\n- You want to avoid distributional assumptions\n- As a sensitivity analysis to complement parametric results\n:::\n\n## Choosing the Right T-Test\n\n| Scenario | Test | R Function |\n|:---------|:-----|:-----------|\n| Compare sample mean to known value | One-sample | `t.test(x, mu = value)` |\n| Compare two independent groups | Two-sample (Welch's) | `t.test(x, y)` |\n| Compare two independent groups (equal variance) | Two-sample (Student's) | `t.test(x, y, var.equal = TRUE)` |\n| Compare paired measurements | Paired | `t.test(x, y, paired = TRUE)` |\n\n**Decision guidelines:**\n\n1. If comparing to a fixed, known value: **one-sample t-test**\n2. If observations in groups are naturally paired: **paired t-test**\n3. If groups are independent with potentially unequal variances: **Welch's t-test** (the default)\n4. If groups are independent and you have strong evidence of equal variances: **Student's t-test**\n\nWhen in doubt, use Welch's t-test—it performs nearly as well as Student's t-test when variances are equal and much better when they are not.\n\n## Summary\n\nThe t-test family provides essential tools for comparing means:\n\n- One-sample tests compare a sample to a hypothesized value\n- Two-sample tests compare independent groups\n- Paired tests compare matched or repeated measurements\n- Welch's version handles unequal variances (recommended default)\n- Randomization tests provide a distribution-free alternative\n\nAlways visualize your data, check assumptions, and report effect sizes alongside p-values. A statistically significant result is only meaningful if the underlying assumptions are reasonable and the effect size is practically relevant.\n\n## Practice Exercises\n\n### Exercise H.1: One-Sample t-test\n\n1. Generate a sample of 30 observations from a normal distribution with mean 105 and SD 15\n2. Test whether the mean differs significantly from 100\n3. Interpret the p-value and confidence interval\n4. What happens to the p-value when you increase the sample size?\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(42)\nsample_data <- rnorm(30, mean = 105, sd = 15)\nt.test(sample_data, mu = 100)\n```\n:::\n\n\n### Exercise H.2: Two-Sample t-test\n\nCreate a dummy dataset with one continuous and one categorical variable:\n\n1. Draw samples of 100 observations from two normal distributions with slightly different means but equal standard deviations\n2. Perform a two-sample t-test\n3. Visualize the data with a boxplot\n4. Repeat with sample sizes of 10, 100, and 1000—how does sample size affect the results?\n5. What happens when you make the means more different?\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(42)\ngroup_a <- rnorm(100, mean = 10, sd = 2)\ngroup_b <- rnorm(100, mean = 11, sd = 2)\n\n# Combine into data frame\ndata <- data.frame(\n  value = c(group_a, group_b),\n  group = rep(c(\"A\", \"B\"), each = 100)\n)\n\n# t-test\nt.test(value ~ group, data = data)\n\n# Visualization\nboxplot(value ~ group, data = data)\n```\n:::\n\n\n### Exercise H.3: Chi-Square Test for Hardy-Weinberg Equilibrium\n\nTest whether a population is in Hardy-Weinberg equilibrium:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Observed genotype counts\nAA_counts <- 50\nAa_counts <- 40\naa_counts <- 10\n\n# Calculate allele frequencies\ntotal <- AA_counts + Aa_counts + aa_counts\np <- (2*AA_counts + Aa_counts) / (2*total)\nq <- 1 - p\n\n# Expected counts under HWE\nexpected <- c(p^2, 2*p*q, q^2) * total\n\n# Chi-square test\nobserved <- c(AA_counts, Aa_counts, aa_counts)\nchisq.test(observed, p = c(p^2, 2*p*q, q^2))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tChi-squared test for given probabilities\n\ndata:  observed\nX-squared = 0.22676, df = 2, p-value = 0.8928\n```\n\n\n:::\n:::\n\n\n1. Modify the observed counts and see how it affects the test result\n2. What genotype frequencies would indicate strong departure from HWE?\n\n### Exercise H.4: Effect Size and Power\n\n1. Using the two-sample t-test from Exercise H.2, calculate Cohen's d effect size\n2. How does effect size change when you increase the difference between means?\n3. How does effect size change when you increase the standard deviation?\n\n## Additional Resources\n\n- @logan2010biostatistical - Detailed coverage of t-tests with biological examples\n- @irizarry2019introduction - Excellent treatment of randomization and permutation methods\n",
    "supporting": [
      "12-t-tests_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}