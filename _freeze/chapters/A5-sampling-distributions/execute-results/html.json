{
  "hash": "4f5b4f8aee3002b59fdf2e9449ba2514",
  "result": {
    "engine": "knitr",
    "markdown": "# Sampling Distributions in Hypothesis Testing {#sec-sampling-distributions}\n\n\n::: {.cell}\n\n:::\n\n\nThis appendix provides a reference for the statistical distributions used in hypothesis testing. While @sec-probability-distributions covers probability distributions for modeling data, this appendix focuses on **sampling distributions**—the theoretical distributions that test statistics follow under the null hypothesis.\n\n## Why Sampling Distributions Matter\n\nWhen we conduct a hypothesis test, we calculate a test statistic from our sample data. To determine whether this statistic is \"unusual,\" we need to know what values to expect if the null hypothesis were true. The **sampling distribution** tells us exactly this—it's the distribution of the test statistic across all possible samples.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Demonstrate: sampling distribution of the mean\nset.seed(42)\npopulation <- rnorm(100000, mean = 100, sd = 15)\n\n# Take many samples and compute means\nsample_means <- replicate(5000, mean(sample(population, 30)))\n\nhist(sample_means, breaks = 40, col = \"lightblue\",\n     main = \"Sampling Distribution of the Mean\",\n     xlab = \"Sample Mean (n = 30)\",\n     probability = TRUE)\n\n# Overlay theoretical normal\nx <- seq(min(sample_means), max(sample_means), length.out = 100)\nlines(x, dnorm(x, mean = 100, sd = 15/sqrt(30)), col = \"red\", lwd = 2)\n\nlegend(\"topright\",\n       legend = c(\"Simulated\", \"Theoretical\"),\n       fill = c(\"lightblue\", NA),\n       border = c(\"black\", NA),\n       lty = c(NA, 1), lwd = c(NA, 2),\n       col = c(NA, \"red\"))\n```\n\n::: {.cell-output-display}\n![](A5-sampling-distributions_files/figure-html/unnamed-chunk-2-1.png){width=768}\n:::\n:::\n\n\n## The Standard Normal (Z) Distribution\n\n### When It's Used\n\nThe standard normal distribution is used when:\n\n- Testing means with **known** population variance\n- Large samples (n > 30) where CLT applies\n- Testing proportions with large samples\n\n### The Distribution\n\n$$Z = \\frac{\\bar{X} - \\mu}{\\sigma / \\sqrt{n}}$$\n\nUnder $H_0$, $Z \\sim N(0, 1)$\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx <- seq(-4, 4, length.out = 200)\ny <- dnorm(x)\n\nplot(x, y, type = \"l\", lwd = 2,\n     xlab = \"z\", ylab = \"Density\",\n     main = \"Standard Normal Distribution\")\n\n# Shade rejection regions (two-tailed, α = 0.05)\nx_left <- seq(-4, -1.96, length.out = 50)\nx_right <- seq(1.96, 4, length.out = 50)\n\npolygon(c(-4, x_left, -1.96), c(0, dnorm(x_left), 0),\n        col = rgb(1, 0, 0, 0.3), border = NA)\npolygon(c(1.96, x_right, 4), c(0, dnorm(x_right), 0),\n        col = rgb(1, 0, 0, 0.3), border = NA)\n\nabline(v = c(-1.96, 1.96), lty = 2, col = \"red\")\ntext(0, 0.15, \"95%\\nAcceptance\\nRegion\", cex = 0.9)\ntext(-2.8, 0.05, \"2.5%\", col = \"red\")\ntext(2.8, 0.05, \"2.5%\", col = \"red\")\n```\n\n::: {.cell-output-display}\n![](A5-sampling-distributions_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\n### Critical Values\n\n| Confidence Level | Two-tailed α | Critical Z |\n|:-----------------|:-------------|:-----------|\n| 90% | 0.10 | ±1.645 |\n| 95% | 0.05 | ±1.960 |\n| 99% | 0.01 | ±2.576 |\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# R functions for Z distribution\nqnorm(0.975)  # 97.5th percentile (for two-tailed 95% CI)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1.959964\n```\n\n\n:::\n\n```{.r .cell-code}\npnorm(1.96)   # Probability below z = 1.96\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.9750021\n```\n\n\n:::\n:::\n\n\n## Student's t-Distribution\n\n### When It's Used\n\nThe t-distribution is used when:\n\n- Testing means with **unknown** population variance (estimated from sample)\n- Comparing two means (two-sample t-test)\n- Testing regression coefficients\n- Small to moderate sample sizes\n\n### The Distribution\n\n$$t = \\frac{\\bar{X} - \\mu}{s / \\sqrt{n}}$$\n\nUnder $H_0$, $t \\sim t_{df}$ where $df = n - 1$ for one-sample tests.\n\n### Effect of Degrees of Freedom\n\nThe t-distribution has heavier tails than the normal, reflecting additional uncertainty from estimating variance. As df increases, t approaches normal:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx <- seq(-4, 4, length.out = 200)\n\nplot(x, dnorm(x), type = \"l\", lwd = 2, col = \"black\",\n     xlab = \"t\", ylab = \"Density\",\n     main = \"t-Distribution: Effect of Degrees of Freedom\")\nlines(x, dt(x, df = 3), lwd = 2, col = \"red\")\nlines(x, dt(x, df = 10), lwd = 2, col = \"blue\")\nlines(x, dt(x, df = 30), lwd = 2, col = \"darkgreen\")\n\nlegend(\"topright\",\n       legend = c(\"Normal (df = ∞)\", \"t (df = 3)\", \"t (df = 10)\", \"t (df = 30)\"),\n       col = c(\"black\", \"red\", \"blue\", \"darkgreen\"),\n       lwd = 2)\n```\n\n::: {.cell-output-display}\n![](A5-sampling-distributions_files/figure-html/unnamed-chunk-5-1.png){width=768}\n:::\n:::\n\n\nNotice how df = 3 has much heavier tails (more extreme values expected), while df = 30 is nearly indistinguishable from the normal.\n\n### Critical Values Change with df\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Critical t-values for 95% CI (two-tailed)\ndfs <- c(5, 10, 20, 30, 50, 100, Inf)\nt_crits <- qt(0.975, df = dfs)\n\ndata.frame(\n  df = dfs,\n  critical_t = round(t_crits, 3)\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   df critical_t\n1   5      2.571\n2  10      2.228\n3  20      2.086\n4  30      2.042\n5  50      2.009\n6 100      1.984\n7 Inf      1.960\n```\n\n\n:::\n:::\n\n\n### Practical Implications\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# How confidence interval width depends on sample size\nn_values <- seq(5, 100, by = 5)\nci_multipliers <- qt(0.975, df = n_values - 1)\n\nplot(n_values, ci_multipliers, type = \"b\", pch = 19, col = \"blue\",\n     xlab = \"Sample Size (n)\", ylab = \"t Critical Value (α = 0.05)\",\n     main = \"Why Larger Samples Give Narrower CIs\")\nabline(h = 1.96, lty = 2, col = \"red\")\ntext(80, 2.05, \"Z = 1.96 (infinite df)\", col = \"red\")\n```\n\n::: {.cell-output-display}\n![](A5-sampling-distributions_files/figure-html/unnamed-chunk-7-1.png){width=768}\n:::\n:::\n\n\nWith small samples, we need a larger critical value to achieve the same confidence level, making confidence intervals wider.\n\n### R Functions\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# t-distribution functions\nqt(0.975, df = 10)    # Critical value for 95% CI with df = 10\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 2.228139\n```\n\n\n:::\n\n```{.r .cell-code}\npt(2.228, df = 10)    # Probability below t = 2.228\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.9749941\n```\n\n\n:::\n\n```{.r .cell-code}\ndt(0, df = 10)        # Density at t = 0\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.3891084\n```\n\n\n:::\n:::\n\n\n## Chi-Square (χ²) Distribution\n\n### When It's Used\n\nThe chi-square distribution is used for:\n\n- Goodness of fit tests (observed vs. expected frequencies)\n- Tests of independence (contingency tables)\n- Testing variance (one population)\n- Model fit in regression (deviance tests)\n\n### The Distribution\n\nThe chi-square distribution is the sum of squared standard normal variables:\n\n$$\\chi^2 = \\sum_{i=1}^{k} Z_i^2$$\n\nThe distribution is always positive and right-skewed. As df increases, it becomes more symmetric and approaches normality.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx <- seq(0, 30, length.out = 200)\n\nplot(x, dchisq(x, df = 2), type = \"l\", lwd = 2, col = \"red\",\n     xlab = expression(chi^2), ylab = \"Density\",\n     main = expression(paste(chi^2, \" Distribution\")),\n     ylim = c(0, 0.3))\nlines(x, dchisq(x, df = 5), lwd = 2, col = \"blue\")\nlines(x, dchisq(x, df = 10), lwd = 2, col = \"darkgreen\")\nlines(x, dchisq(x, df = 20), lwd = 2, col = \"purple\")\n\nlegend(\"topright\",\n       legend = c(\"df = 2\", \"df = 5\", \"df = 10\", \"df = 20\"),\n       col = c(\"red\", \"blue\", \"darkgreen\", \"purple\"),\n       lwd = 2)\n```\n\n::: {.cell-output-display}\n![](A5-sampling-distributions_files/figure-html/unnamed-chunk-9-1.png){width=768}\n:::\n:::\n\n\n### Properties\n\n- **Mean**: $E[\\chi^2] = df$\n- **Variance**: $Var(\\chi^2) = 2 \\times df$\n- Always positive (sums of squares)\n- Right-skewed, especially for small df\n\n### Critical Values for Common Tests\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Chi-square critical values (right-tail, α = 0.05)\ndfs <- c(1, 2, 3, 5, 10, 20)\nchi_crits <- qchisq(0.95, df = dfs)\n\ndata.frame(\n  df = dfs,\n  critical_chi_sq = round(chi_crits, 3),\n  mean = dfs  # Note: critical value is close to df + 2*sqrt(2*df)\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  df critical_chi_sq mean\n1  1           3.841    1\n2  2           5.991    2\n3  3           7.815    3\n4  5          11.070    5\n5 10          18.307   10\n6 20          31.410   20\n```\n\n\n:::\n:::\n\n\n### R Functions\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Chi-square distribution functions\nqchisq(0.95, df = 5)      # Critical value (right-tail α = 0.05)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 11.0705\n```\n\n\n:::\n\n```{.r .cell-code}\n1 - pchisq(11.07, df = 5) # p-value for chi-square = 11.07\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.05000962\n```\n\n\n:::\n\n```{.r .cell-code}\ndchisq(5, df = 5)         # Density at chi-square = 5\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.1220415\n```\n\n\n:::\n:::\n\n\n## F Distribution\n\n### When It's Used\n\nThe F distribution is used for:\n\n- Comparing two variances (F-test)\n- ANOVA (comparing means of multiple groups)\n- Testing overall significance in regression\n- Comparing nested models\n\n### The Distribution\n\nThe F distribution is the ratio of two chi-square distributions:\n\n$$F = \\frac{\\chi^2_1 / df_1}{\\chi^2_2 / df_2}$$\n\n- $df_1$: numerator degrees of freedom (between-groups)\n- $df_2$: denominator degrees of freedom (within-groups or error)\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx <- seq(0, 5, length.out = 200)\n\nplot(x, df(x, df1 = 1, df2 = 10), type = \"l\", lwd = 2, col = \"red\",\n     xlab = \"F\", ylab = \"Density\",\n     main = \"F Distribution\",\n     ylim = c(0, 1))\nlines(x, df(x, df1 = 5, df2 = 10), lwd = 2, col = \"blue\")\nlines(x, df(x, df1 = 10, df2 = 10), lwd = 2, col = \"darkgreen\")\nlines(x, df(x, df1 = 10, df2 = 50), lwd = 2, col = \"purple\")\n\nlegend(\"topright\",\n       legend = c(\"F(1,10)\", \"F(5,10)\", \"F(10,10)\", \"F(10,50)\"),\n       col = c(\"red\", \"blue\", \"darkgreen\", \"purple\"),\n       lwd = 2)\n```\n\n::: {.cell-output-display}\n![](A5-sampling-distributions_files/figure-html/unnamed-chunk-12-1.png){width=768}\n:::\n:::\n\n\n### Understanding F in ANOVA\n\nIn ANOVA, F is the ratio of between-group variance to within-group variance:\n\n$$F = \\frac{MS_{between}}{MS_{within}} = \\frac{\\text{Signal}}{\\text{Noise}}$$\n\n- Large F: Groups differ more than expected from random variation\n- F ≈ 1: Group differences are similar to within-group variation\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Visualize rejection region for ANOVA\nx <- seq(0, 6, length.out = 200)\ny <- df(x, df1 = 3, df2 = 20)  # 4 groups, total n = 24\n\nplot(x, y, type = \"l\", lwd = 2,\n     xlab = \"F\", ylab = \"Density\",\n     main = \"F(3, 20) Distribution for One-Way ANOVA\")\n\n# Critical value and rejection region\nf_crit <- qf(0.95, df1 = 3, df2 = 20)\nx_reject <- seq(f_crit, 6, length.out = 50)\npolygon(c(f_crit, x_reject, 6), c(0, df(x_reject, 3, 20), 0),\n        col = rgb(1, 0, 0, 0.3), border = NA)\n\nabline(v = f_crit, lty = 2, col = \"red\")\ntext(f_crit + 0.3, 0.3, paste(\"F* =\", round(f_crit, 2)), col = \"red\")\ntext(4.5, 0.05, \"Rejection\\nRegion\\n(α = 0.05)\", col = \"red\")\n```\n\n::: {.cell-output-display}\n![](A5-sampling-distributions_files/figure-html/unnamed-chunk-13-1.png){width=672}\n:::\n:::\n\n\n### Critical Values Table\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# F critical values for α = 0.05 (common ANOVA scenarios)\n# Rows: numerator df (groups - 1)\n# Columns: denominator df (total n - groups)\n\ndf1_vals <- c(1, 2, 3, 4, 5)\ndf2_vals <- c(10, 20, 30, 60, 120)\n\nf_table <- outer(df1_vals, df2_vals,\n                 function(d1, d2) round(qf(0.95, d1, d2), 2))\nrownames(f_table) <- paste(\"df1 =\", df1_vals)\ncolnames(f_table) <- paste(\"df2 =\", df2_vals)\n\ncat(\"F Critical Values (α = 0.05)\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nF Critical Values (α = 0.05)\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(f_table)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n        df2 = 10 df2 = 20 df2 = 30 df2 = 60 df2 = 120\ndf1 = 1     4.96     4.35     4.17     4.00      3.92\ndf1 = 2     4.10     3.49     3.32     3.15      3.07\ndf1 = 3     3.71     3.10     2.92     2.76      2.68\ndf1 = 4     3.48     2.87     2.69     2.53      2.45\ndf1 = 5     3.33     2.71     2.53     2.37      2.29\n```\n\n\n:::\n:::\n\n\n### R Functions\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# F distribution functions\nqf(0.95, df1 = 3, df2 = 20)  # Critical F for ANOVA\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 3.098391\n```\n\n\n:::\n\n```{.r .cell-code}\n1 - pf(3.5, df1 = 3, df2 = 20)  # p-value for F = 3.5\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.0344931\n```\n\n\n:::\n:::\n\n\n## Relationships Between Distributions\n\nThese distributions are mathematically related:\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](A5-sampling-distributions_files/figure-html/unnamed-chunk-16-1.png){width=864}\n:::\n:::\n\n\nKey relationships:\n\n1. **t² = F(1, df)**: A squared t-statistic follows an F distribution with 1 numerator df\n2. **χ² → Normal**: As df increases, chi-square approaches normality\n3. **t → Z**: As df → ∞, t-distribution becomes standard normal\n4. **F(1, ∞) = χ²(1)**: Limiting case of F distribution\n\n## Choosing the Right Distribution\n\n| Test | Distribution | Degrees of Freedom |\n|:-----|:-------------|:-------------------|\n| Z-test (known σ) | Normal | N/A |\n| One-sample t-test | t | n - 1 |\n| Two-sample t-test | t | n₁ + n₂ - 2 (pooled) |\n| Paired t-test | t | n - 1 |\n| Chi-square GOF | χ² | k - 1 |\n| Chi-square independence | χ² | (r-1)(c-1) |\n| One-way ANOVA | F | k-1, N-k |\n| Regression F-test | F | p, n-p-1 |\n| Regression coefficient | t | n - p - 1 |\n\nwhere k = number of groups/categories, n = sample size, p = number of predictors\n\n## Degrees of Freedom: Intuition\n\n**Degrees of freedom** represent the number of independent pieces of information available for estimation. They decrease when we estimate parameters from the data:\n\n- **Sample mean**: Uses 1 df → leaves n-1 for variance estimation\n- **Two groups**: Estimate 2 means → lose 2 df from total\n- **Regression**: Estimate p+1 coefficients → leaves n-p-1 error df\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Demonstration: Why df matters\n# Sampling distribution of sample variance with different n\n\nset.seed(42)\ntrue_variance <- 100\n\nsimulate_s2 <- function(n, reps = 5000) {\n  replicate(reps, var(rnorm(n, mean = 0, sd = 10)))\n}\n\ns2_small <- simulate_s2(5)    # df = 4\ns2_medium <- simulate_s2(20)  # df = 19\ns2_large <- simulate_s2(50)   # df = 49\n\npar(mfrow = c(1, 3))\nhist(s2_small, breaks = 30, main = \"n = 5 (df = 4)\",\n     xlab = \"Sample Variance\", col = \"lightblue\", xlim = c(0, 400))\nabline(v = 100, col = \"red\", lwd = 2)\n\nhist(s2_medium, breaks = 30, main = \"n = 20 (df = 19)\",\n     xlab = \"Sample Variance\", col = \"lightblue\", xlim = c(0, 400))\nabline(v = 100, col = \"red\", lwd = 2)\n\nhist(s2_large, breaks = 30, main = \"n = 50 (df = 49)\",\n     xlab = \"Sample Variance\", col = \"lightblue\", xlim = c(0, 400))\nabline(v = 100, col = \"red\", lwd = 2)\n```\n\n::: {.cell-output-display}\n![](A5-sampling-distributions_files/figure-html/unnamed-chunk-17-1.png){width=768}\n:::\n:::\n\n\nWith more degrees of freedom:\n- Variance estimates are more precise (narrower distribution)\n- More likely to be close to the true value\n- Critical values move closer to their limiting values\n\n## Summary\n\n| Distribution | Parameters | Mean | Use For |\n|:-------------|:-----------|:-----|:--------|\n| Normal (Z) | None | 0 | Means (known σ), proportions |\n| t | df | 0 | Means (unknown σ), regression |\n| Chi-square | df | df | Frequencies, variance, GOF |\n| F | df₁, df₂ | df₂/(df₂-2) | ANOVA, comparing variances |\n\nRemember:\n- More data (higher df) → distributions approach their limits\n- The t approaches Z, χ² becomes symmetric, F becomes more peaked\n- Heavier tails in t and F require larger critical values for small df\n- These distributions assume normality of underlying data (robustness varies)\n",
    "supporting": [
      "A5-sampling-distributions_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}