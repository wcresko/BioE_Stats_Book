{
  "hash": "d8e0e1804bace63dcf82130aa94ff718",
  "result": {
    "engine": "knitr",
    "markdown": "# Statistical Learning {#sec-statistical-learning}\n\n\n::: {.cell}\n\n:::\n\n\n## From Inference to Prediction\n\nTraditional statistics emphasizes inference—understanding relationships, testing hypotheses, and quantifying uncertainty. Statistical learning (or machine learning) shifts focus toward prediction—building models that accurately predict outcomes for new data.\n\nBoth approaches use similar mathematical tools, but the goals differ. In inference, we want to understand the true relationship between variables. In prediction, we want accurate predictions, even if the model does not perfectly capture the underlying mechanism.\n\n## The Overfitting Problem\n\nModels are built to fit training data as closely as possible. A linear regression minimizes squared errors; a logistic regression maximizes likelihood. But models that fit training data too well often predict poorly on new data.\n\n**Overfitting** occurs when a model captures noise specific to the training data rather than the true underlying pattern. Complex models with many parameters are especially susceptible.\n\nThe solution is to evaluate models on data they have not seen—held-out test data or through cross-validation.\n\n## Cross-Validation\n\nCross-validation estimates how well a model will generalize to new data.\n\n**K-fold cross-validation**:\n1. Split data into k roughly equal parts (folds)\n2. For each fold: train on k-1 folds, test on the held-out fold\n3. Average performance across all folds\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Simple CV example with linear regression\nlibrary(boot)\n\n# Generate data\nset.seed(42)\nx <- rnorm(100)\ny <- 2 + 3*x + rnorm(100)\ndata <- data.frame(x, y)\n\n# Fit model and perform CV\nmodel <- glm(y ~ x, data = data)\n\n# 10-fold cross-validation\ncv_result <- cv.glm(data, model, K = 10)\ncat(\"CV estimate of prediction error:\", round(cv_result$delta[1], 3), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nCV estimate of prediction error: 0.846 \n```\n\n\n:::\n:::\n\n\n**Leave-one-out cross-validation (LOOCV)** is k-fold with k = n: each observation is held out once. More computationally expensive but lower variance.\n\n## Bias-Variance Tradeoff\n\nPrediction error has two components:\n\n**Bias**: Error from approximating a complex reality with a simpler model. Simple models have high bias—they may miss important patterns.\n\n**Variance**: Error from sensitivity to training data. Complex models have high variance—they change substantially with different training samples.\n\nThe best predictions come from models that balance bias and variance. As model complexity increases, bias decreases but variance increases. The optimal complexity minimizes total prediction error.\n\n## LOESS: Flexible Non-Parametric Smoothing\n\n**LOESS** (Locally Estimated Scatterplot Smoothing) fits local regressions to subsets of data, weighted by distance from each point.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Compare linear regression and LOESS\nset.seed(123)\nx <- seq(0, 4*pi, length.out = 100)\ny <- sin(x) + rnorm(100, sd = 0.3)\n\nplot(x, y, pch = 16, col = \"gray60\", main = \"Linear vs LOESS\")\nabline(lm(y ~ x), col = \"red\", lwd = 2)\nlines(x, predict(loess(y ~ x, span = 0.3)), col = \"blue\", lwd = 2)\nlegend(\"topright\", c(\"Linear\", \"LOESS\"), col = c(\"red\", \"blue\"), lwd = 2)\n```\n\n::: {.cell-output-display}\n![](22-statistical-learning_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\nThe **span** parameter controls smoothness: smaller values fit more locally (more flexible), larger values average more broadly (smoother).\n\n## Classification\n\nWhen the response is categorical, we have a classification problem rather than regression. The goal is to predict which category an observation belongs to.\n\n**Logistic regression** produces probabilities that can be converted to class predictions.\n\n**Decision trees** recursively partition the feature space based on simple rules.\n\n**Random forests** combine many decision trees for more robust predictions.\n\n## Confusion Matrices\n\nClassification performance is evaluated with a **confusion matrix**:\n\n|  | Predicted Positive | Predicted Negative |\n|:--|:--:|:--:|\n| Actual Positive | True Positive (TP) | False Negative (FN) |\n| Actual Negative | False Positive (FP) | True Negative (TN) |\n\nKey metrics:\n- **Accuracy**: (TP + TN) / Total\n- **Sensitivity** (Recall): TP / (TP + FN) — how many positives were caught\n- **Specificity**: TN / (TN + FP) — how many negatives were correctly identified\n- **Precision**: TP / (TP + FP) — among positive predictions, how many were correct\n\n## Decision Trees\n\nDecision trees make predictions by asking a series of yes/no questions about the features:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(rpart)\nlibrary(rpart.plot)\n\n# Build a simple decision tree\ndata(iris)\ntree_model <- rpart(Species ~ Sepal.Length + Sepal.Width, data = iris)\nrpart.plot(tree_model)\n```\n\n::: {.cell-output-display}\n![](22-statistical-learning_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\nTrees are interpretable but prone to overfitting. Pruning (removing branches) or using ensembles helps.\n\n## Random Forests\n\nRandom forests improve on single trees by:\n1. Building many trees on bootstrap samples (bagging)\n2. Using a random subset of features at each split\n3. Averaging predictions across all trees\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(randomForest)\n\nrf_model <- randomForest(Species ~ ., data = iris, ntree = 100)\nrf_model\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\n randomForest(formula = Species ~ ., data = iris, ntree = 100) \n               Type of random forest: classification\n                     Number of trees: 100\nNo. of variables tried at each split: 2\n\n        OOB estimate of  error rate: 6%\nConfusion matrix:\n           setosa versicolor virginica class.error\nsetosa         50          0         0        0.00\nversicolor      0         47         3        0.06\nvirginica       0          6        44        0.12\n```\n\n\n:::\n\n```{.r .cell-code}\n# Variable importance\nvarImpPlot(rf_model)\n```\n\n::: {.cell-output-display}\n![](22-statistical-learning_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\n## Practical Workflow\n\nA typical statistical learning workflow:\n\n1. **Split data** into training and test sets\n2. **Explore** the training data\n3. **Build candidate models** with different algorithms or parameters\n4. **Evaluate** using cross-validation on training data\n5. **Select** the best model\n6. **Final evaluation** on held-out test data\n7. **Report** honest estimates of performance\n\nNever use test data for model building or selection—that defeats the purpose of holding it out.\n\n## When to Use Statistical Learning\n\nStatistical learning excels when:\n- Prediction is the primary goal\n- Relationships are complex or non-linear\n- You have substantial data\n- Interpretability is less critical\n\nTraditional statistical methods may be preferable when:\n- Understanding relationships matters more than prediction\n- Sample sizes are small\n- You need confidence intervals and hypothesis tests\n- Interpretability is essential\n\n## Connection to Dimensionality Reduction\n\nHigh-dimensional data often benefit from dimensionality reduction before applying statistical learning methods. Techniques like PCA, clustering, and discriminant analysis are covered in detail in @sec-dimensionality-reduction.\n\n## Summary\n\nStatistical learning provides powerful tools for prediction and pattern discovery:\n\n- Overfitting is the central challenge—models that fit training data too well predict poorly\n- Cross-validation provides honest estimates of predictive performance\n- The bias-variance tradeoff governs model complexity choices\n- LOESS offers flexible non-parametric smoothing\n- Classification methods (decision trees, random forests) handle categorical outcomes\n- Confusion matrices summarize classification performance\n- The choice between traditional statistics and machine learning depends on goals\n\n## Additional Resources\n\n- @james2023islr - The standard introduction to statistical learning\n- @thulin2025msr - Modern perspectives on statistics with R\n- @crawley2007r - Practical statistical methods in R\n",
    "supporting": [
      "22-statistical-learning_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}