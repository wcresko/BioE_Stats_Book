{
  "hash": "dcc3eab6527465337e79968329395980",
  "result": {
    "engine": "knitr",
    "markdown": "# Dimensionality Reduction and Multivariate Methods {#sec-dimensionality-reduction}\n\n\n::: {.cell}\n\n:::\n\n\n## The Challenge of High-Dimensional Data\n\nModern biology generates datasets with many variables: gene expression across thousands of genes, metabolomic profiles with hundreds of compounds, morphological measurements on many traits. When datasets have many variables, visualization becomes impossible and statistical analysis becomes complicated.\n\n**Dimensionality reduction** creates a smaller set of new variables that capture most of the information in the original data. These techniques help us:\n\n- Visualize high-dimensional data in 2D or 3D\n- Identify patterns and clusters\n- Remove noise and redundancy\n- Create composite variables for downstream analysis\n\n## Principal Component Analysis (PCA)\n\n**Principal Component Analysis (PCA)** is the most widely used dimensionality reduction technique. It finds new variables (principal components) that are linear combinations of the originals, chosen to capture maximum variance.\n\n### The Eigenanalysis Foundation\n\nThe key insight involves **eigenanalysis**: decomposing the covariance (or correlation) matrix of variables to find directions of maximum variation.\n\nGiven a covariance matrix $\\Sigma$, eigenanalysis finds:\n\n- **Eigenvectors**: The directions of the principal components\n- **Eigenvalues**: The variance explained by each component\n\nThe first principal component points in the direction of maximum variance. Each subsequent component is orthogonal (uncorrelated) and captures remaining variance in decreasing order.\n\n![](../images/Images_7a.006.jpeg){fig-align=\"center\"}\n\n### PCA in R\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# PCA on iris data\niris_pca <- prcomp(iris[, 1:4], scale. = TRUE)\n\n# Variance explained\nsummary(iris_pca)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nImportance of components:\n                          PC1    PC2     PC3     PC4\nStandard deviation     1.7084 0.9560 0.38309 0.14393\nProportion of Variance 0.7296 0.2285 0.03669 0.00518\nCumulative Proportion  0.7296 0.9581 0.99482 1.00000\n```\n\n\n:::\n\n```{.r .cell-code}\n# Scree plot\npar(mfrow = c(1, 2))\nplot(iris_pca, type = \"l\", main = \"Scree Plot\")\n\n# PC scores colored by species\nplot(iris_pca$x[, 1:2],\n     col = c(\"red\", \"green\", \"blue\")[iris$Species],\n     pch = 19, xlab = \"PC1\", ylab = \"PC2\",\n     main = \"PCA of Iris Data\")\nlegend(\"topright\", levels(iris$Species),\n       col = c(\"red\", \"green\", \"blue\"), pch = 19)\n```\n\n::: {.cell-output-display}\n![](23-dimensionality-reduction_files/figure-html/unnamed-chunk-2-1.png){width=768}\n:::\n:::\n\n\n### Loadings: What Variables Drive Each PC?\n\nEach principal component is defined by its **loadings**—the coefficients showing how much each original variable contributes:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Loadings (rotation matrix)\niris_pca$rotation\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                    PC1         PC2        PC3        PC4\nSepal.Length  0.5210659 -0.37741762  0.7195664  0.2612863\nSepal.Width  -0.2693474 -0.92329566 -0.2443818 -0.1235096\nPetal.Length  0.5804131 -0.02449161 -0.1421264 -0.8014492\nPetal.Width   0.5648565 -0.06694199 -0.6342727  0.5235971\n```\n\n\n:::\n:::\n\n\nLarge absolute loadings indicate that a variable strongly influences that component. The sign indicates the direction of the relationship.\n\n### Interpreting PCA Results\n\nKey elements of PCA output:\n\n- **Eigenvalues**: Variance explained by each component (shown in scree plot)\n- **Proportion of variance**: How much of total variance each PC captures\n- **Loadings**: Coefficients relating original variables to PCs\n- **Scores**: Values of the new variables for each observation\n\nThe first few PCs often capture most of the meaningful variation, allowing you to reduce many variables to just 2-3 for visualization and analysis.\n\n::: {.callout-tip}\n## How Many Components to Keep?\n\nCommon approaches:\n\n- Keep components with eigenvalues > 1 (Kaiser criterion)\n- Keep enough to explain 80-90% of variance\n- Look for an \"elbow\" in the scree plot\n- Use cross-validation if using PCs for prediction\n:::\n\n### Biplot Visualization\n\nA **biplot** shows both observations (scores) and variables (loadings) on the same plot:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbiplot(iris_pca, col = c(\"gray50\", \"red\"), cex = 0.7,\n       main = \"PCA Biplot of Iris Data\")\n```\n\n::: {.cell-output-display}\n![](23-dimensionality-reduction_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\nArrows show variable loadings—their direction and length indicate how each variable relates to the principal components.\n\n## Principal Coordinate Analysis (PCoA)\n\nWhile PCA uses correlations among variables, **Principal Coordinate Analysis (PCoA)** (also called Metric Multidimensional Scaling) starts with a dissimilarity matrix among observations. This is valuable when:\n\n- You have a meaningful distance metric (e.g., genetic distances)\n- Variables are mixed types or non-numeric\n- The data are counts (e.g., microbiome data)\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# PCoA example using Euclidean distances\ndist_matrix <- dist(iris[, 1:4])\npcoa_result <- cmdscale(dist_matrix, k = 2, eig = TRUE)\n\n# Proportion of variance explained\neig_vals <- pcoa_result$eig[pcoa_result$eig > 0]\nvar_explained <- eig_vals / sum(eig_vals)\ncat(\"Variance explained by first two axes:\",\n    round(sum(var_explained[1:2]) * 100, 1), \"%\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nVariance explained by first two axes: 97.8 %\n```\n\n\n:::\n\n```{.r .cell-code}\n# Plot\nplot(pcoa_result$points,\n     col = c(\"red\", \"green\", \"blue\")[iris$Species],\n     pch = 19,\n     xlab = paste0(\"PCoA1 (\", round(var_explained[1]*100, 1), \"%)\"),\n     ylab = paste0(\"PCoA2 (\", round(var_explained[2]*100, 1), \"%)\"),\n     main = \"PCoA of Iris Data\")\nlegend(\"topright\", levels(iris$Species),\n       col = c(\"red\", \"green\", \"blue\"), pch = 19)\n```\n\n::: {.cell-output-display}\n![](23-dimensionality-reduction_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\n### When to Use PCoA vs. PCA\n\n- **PCA**: Variables are measured on a common scale; interested in variable contributions\n- **PCoA**: Have a distance matrix; want to preserve distances among samples\n- For Euclidean distances, PCA and PCoA give equivalent results\n\n## Non-Metric Multidimensional Scaling (NMDS)\n\n**NMDS** is an ordination technique that preserves rank-order of distances rather than exact distances. It's widely used in ecology because it makes no assumptions about the data distribution.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# NMDS example\nlibrary(vegan)\nnmds_result <- metaMDS(iris[, 1:4], k = 2, trymax = 100, trace = FALSE)\n\n# Stress value indicates fit (< 0.1 is good, < 0.2 is acceptable)\ncat(\"Stress:\", round(nmds_result$stress, 3), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nStress: 0.038 \n```\n\n\n:::\n\n```{.r .cell-code}\nplot(nmds_result$points,\n     col = c(\"red\", \"green\", \"blue\")[iris$Species],\n     pch = 19, xlab = \"NMDS1\", ylab = \"NMDS2\",\n     main = \"NMDS of Iris Data\")\nlegend(\"topright\", levels(iris$Species),\n       col = c(\"red\", \"green\", \"blue\"), pch = 19)\n```\n\n::: {.cell-output-display}\n![](23-dimensionality-reduction_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n\n::: {.callout-warning}\n## Metric vs. Non-Metric Methods\n\nPCA and metric PCoA produce scores on a ratio scale—differences between scores are meaningful. These can be used directly in linear models.\n\nNon-metric multidimensional scaling (NMDS) produces ordinal rankings only. NMDS scores should **not** be used in parametric analyses like ANOVA or regression.\n:::\n\n## Cluster Analysis\n\n**Cluster analysis** groups observations based on their similarity. Unlike PCA, which creates new continuous variables, clustering assigns observations to discrete groups.\n\n### Hierarchical Clustering\n\n**Hierarchical clustering** builds a tree (dendrogram) of nested clusters. At each step, it either combines the most similar observations/clusters (agglomerative) or splits the most dissimilar (divisive).\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Hierarchical clustering\niris_scaled <- scale(iris[, 1:4])\nhc <- hclust(dist(iris_scaled), method = \"complete\")\nplot(hc, labels = FALSE, main = \"Hierarchical Clustering of Iris\")\nrect.hclust(hc, k = 3, border = \"red\")\n```\n\n::: {.cell-output-display}\n![](23-dimensionality-reduction_files/figure-html/unnamed-chunk-7-1.png){width=768}\n:::\n:::\n\n\nDifferent **linkage methods** define how cluster distances are calculated:\n\n- **Complete**: Maximum distance between points in different clusters\n- **Single**: Minimum distance (tends to chain)\n- **Average**: Mean distance between all pairs\n- **Ward's**: Minimizes within-cluster variance\n\n### K-Means Clustering\n\n**K-means** partitions data into K groups by minimizing within-cluster variance. It requires specifying K in advance.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(42)\nkm <- kmeans(iris_scaled, centers = 3, nstart = 20)\n\n# Compare to true species\ntable(Cluster = km$cluster, Species = iris$Species)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n       Species\nCluster setosa versicolor virginica\n      1     50          0         0\n      2      0         39        14\n      3      0         11        36\n```\n\n\n:::\n:::\n\n\nThe `nstart` parameter runs the algorithm multiple times with different starting points, reducing sensitivity to initialization.\n\n### Choosing the Number of Clusters\n\nSeveral methods help determine the optimal number of clusters:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Elbow method: look for bend in within-cluster sum of squares\nwss <- sapply(1:10, function(k) {\n  kmeans(iris_scaled, centers = k, nstart = 20)$tot.withinss\n})\n\npar(mfrow = c(1, 2))\nplot(1:10, wss, type = \"b\", pch = 19,\n     xlab = \"Number of Clusters\", ylab = \"Within-cluster SS\",\n     main = \"Elbow Method\")\n\n# Silhouette method\nlibrary(cluster)\nsil_width <- sapply(2:10, function(k) {\n  km <- kmeans(iris_scaled, centers = k, nstart = 20)\n  mean(silhouette(km$cluster, dist(iris_scaled))[, \"sil_width\"])\n})\nplot(2:10, sil_width, type = \"b\", pch = 19,\n     xlab = \"Number of Clusters\", ylab = \"Average Silhouette Width\",\n     main = \"Silhouette Method\")\n```\n\n::: {.cell-output-display}\n![](23-dimensionality-reduction_files/figure-html/unnamed-chunk-9-1.png){width=768}\n:::\n:::\n\n\n## MANOVA: Multivariate Analysis of Variance\n\nWhen you have multiple response variables and want to test for group differences, **MANOVA** (Multivariate Analysis of Variance) is the appropriate technique. It extends ANOVA to multiple dependent variables simultaneously.\n\n### Why Not Multiple ANOVAs?\n\nRunning separate ANOVAs on each variable:\n\n- Ignores correlations among response variables\n- Inflates Type I error rate with multiple tests\n- May miss differences only apparent when variables are considered together\n\nMANOVA tests whether group centroids differ in multivariate space.\n\n### The MANOVA Framework\n\nMANOVA decomposes the total multivariate variation:\n\n$$\\mathbf{T} = \\mathbf{H} + \\mathbf{E}$$\n\nwhere:\n\n- **T**: Total sum of squares and cross-products matrix\n- **H**: Hypothesis (between-groups) matrix\n- **E**: Error (within-groups) matrix\n\nThese are matrices because we have multiple response variables.\n\n![](../images/Images_7a.011.jpeg){fig-align=\"center\"}\n\n### Test Statistics\n\nSeveral test statistics exist for MANOVA, each a function of the eigenvalues of $\\mathbf{HE}^{-1}$:\n\n| Statistic | Description |\n|:----------|:------------|\n| **Wilks' Lambda (Λ)** | Product of 1/(1+λᵢ); most commonly used |\n| **Hotelling-Lawley Trace** | Sum of eigenvalues |\n| **Pillai's Trace** | Sum of λᵢ/(1+λᵢ); most robust |\n| **Roy's Largest Root** | Maximum eigenvalue; most powerful but sensitive |\n\n**Pillai's Trace** is generally recommended because it's most robust to violations of assumptions.\n\n### MANOVA in R\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# MANOVA on iris data\nmanova_model <- manova(cbind(Sepal.Length, Sepal.Width,\n                              Petal.Length, Petal.Width) ~ Species,\n                       data = iris)\n\n# Summary with different test statistics\nsummary(manova_model, test = \"Pillai\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n           Df Pillai approx F num Df den Df    Pr(>F)    \nSpecies     2 1.1919   53.466      8    290 < 2.2e-16 ***\nResiduals 147                                            \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n\n```{.r .cell-code}\nsummary(manova_model, test = \"Wilks\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n           Df    Wilks approx F num Df den Df    Pr(>F)    \nSpecies     2 0.023439   199.15      8    288 < 2.2e-16 ***\nResiduals 147                                              \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\n\nThe significant result tells us that species differ in their multivariate centroid—the combination of all four measurements.\n\n### Follow-Up Analyses\n\nA significant MANOVA should be followed by:\n\n1. **Univariate ANOVAs** to see which variables differ\n2. **Discriminant Function Analysis** to understand how groups differ\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Univariate follow-ups\nsummary.aov(manova_model)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n Response Sepal.Length :\n             Df Sum Sq Mean Sq F value    Pr(>F)    \nSpecies       2 63.212  31.606  119.26 < 2.2e-16 ***\nResiduals   147 38.956   0.265                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n Response Sepal.Width :\n             Df Sum Sq Mean Sq F value    Pr(>F)    \nSpecies       2 11.345  5.6725   49.16 < 2.2e-16 ***\nResiduals   147 16.962  0.1154                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n Response Petal.Length :\n             Df Sum Sq Mean Sq F value    Pr(>F)    \nSpecies       2 437.10 218.551  1180.2 < 2.2e-16 ***\nResiduals   147  27.22   0.185                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n Response Petal.Width :\n             Df Sum Sq Mean Sq F value    Pr(>F)    \nSpecies       2 80.413  40.207  960.01 < 2.2e-16 ***\nResiduals   147  6.157   0.042                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\n\n### MANOVA Assumptions\n\nMANOVA assumes:\n\n1. **Multivariate normality** within groups\n2. **Homogeneity of covariance matrices** across groups\n3. **Independence** of observations\n4. **No multicollinearity** among response variables\n\nTest homogeneity of covariance matrices with Box's M test (though it's sensitive to non-normality):\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Box's M test (requires biotools package)\n# library(biotools)\n# boxM(iris[, 1:4], iris$Species)\n```\n:::\n\n\n## Discriminant Function Analysis (DFA)\n\n**Discriminant Function Analysis** (DFA, also called Linear Discriminant Analysis or LDA) finds linear combinations of variables that best separate groups. It complements MANOVA by showing *how* groups differ.\n\n### The Goal of DFA\n\nDFA finds discriminant functions—weighted combinations of original variables—that maximize separation between groups while minimizing variation within groups.\n\nThe first discriminant function captures the most separation, the second captures remaining separation orthogonal to the first, and so on.\n\n![](../images/Images_7a.012.jpeg){fig-align=\"center\"}\n\n### DFA in R\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Linear Discriminant Analysis\nlda_model <- lda(Species ~ ., data = iris)\n\n# View the model\nlda_model\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nCall:\nlda(Species ~ ., data = iris)\n\nPrior probabilities of groups:\n    setosa versicolor  virginica \n 0.3333333  0.3333333  0.3333333 \n\nGroup means:\n           Sepal.Length Sepal.Width Petal.Length Petal.Width\nsetosa            5.006       3.428        1.462       0.246\nversicolor        5.936       2.770        4.260       1.326\nvirginica         6.588       2.974        5.552       2.026\n\nCoefficients of linear discriminants:\n                    LD1         LD2\nSepal.Length  0.8293776 -0.02410215\nSepal.Width   1.5344731 -2.16452123\nPetal.Length -2.2012117  0.93192121\nPetal.Width  -2.8104603 -2.83918785\n\nProportion of trace:\n   LD1    LD2 \n0.9912 0.0088 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Discriminant scores\nlda_scores <- predict(lda_model)$x\n\n# Plot\nplot(lda_scores,\n     col = c(\"red\", \"green\", \"blue\")[iris$Species],\n     pch = 19,\n     main = \"Discriminant Function Scores\",\n     xlab = \"LD1\", ylab = \"LD2\")\nlegend(\"topright\", levels(iris$Species),\n       col = c(\"red\", \"green\", \"blue\"), pch = 19)\n```\n\n::: {.cell-output-display}\n![](23-dimensionality-reduction_files/figure-html/unnamed-chunk-13-1.png){width=672}\n:::\n:::\n\n\n### Interpreting DFA Output\n\nKey components:\n\n- **Coefficients of linear discriminants**: Weights for creating discriminant scores\n- **Proportion of trace**: Variance explained by each discriminant function\n- **Group means**: Average score on each discriminant function for each group\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Coefficients (loadings)\nlda_model$scaling\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                    LD1         LD2\nSepal.Length  0.8293776 -0.02410215\nSepal.Width   1.5344731 -2.16452123\nPetal.Length -2.2012117  0.93192121\nPetal.Width  -2.8104603 -2.83918785\n```\n\n\n:::\n\n```{.r .cell-code}\n# Proportion of separation explained\nlda_model$svd^2 / sum(lda_model$svd^2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.991212605 0.008787395\n```\n\n\n:::\n:::\n\n\n### Using DFA for Prediction\n\nDFA can classify new observations into groups based on their discriminant scores:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Classification accuracy\npredictions <- predict(lda_model)$class\ntable(Predicted = predictions, Actual = iris$Species)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n            Actual\nPredicted    setosa versicolor virginica\n  setosa         50          0         0\n  versicolor      0         48         1\n  virginica       0          2        49\n```\n\n\n:::\n\n```{.r .cell-code}\n# Classification accuracy\nmean(predictions == iris$Species)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.98\n```\n\n\n:::\n:::\n\n\n### Cross-Validated Classification\n\nFor honest estimates of classification accuracy, use leave-one-out cross-validation:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Cross-validated LDA\nlda_cv <- lda(Species ~ ., data = iris, CV = TRUE)\n\n# Cross-validated classification table\ntable(Predicted = lda_cv$class, Actual = iris$Species)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n            Actual\nPredicted    setosa versicolor virginica\n  setosa         50          0         0\n  versicolor      0         48         1\n  virginica       0          2        49\n```\n\n\n:::\n\n```{.r .cell-code}\n# Cross-validated accuracy\nmean(lda_cv$class == iris$Species)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.98\n```\n\n\n:::\n:::\n\n\n### DFA for Biomarker Discovery\n\nDFA is valuable for identifying which variables best distinguish groups—useful in biomarker discovery:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Which variables contribute most to separation?\nscaling_df <- data.frame(\n  Variable = rownames(lda_model$scaling),\n  LD1 = abs(lda_model$scaling[, 1]),\n  LD2 = abs(lda_model$scaling[, 2])\n)\n\nbarplot(scaling_df$LD1, names.arg = scaling_df$Variable,\n        main = \"Variable Contributions to LD1\",\n        ylab = \"Absolute Coefficient\",\n        col = \"steelblue\")\n```\n\n::: {.cell-output-display}\n![](23-dimensionality-reduction_files/figure-html/unnamed-chunk-17-1.png){width=672}\n:::\n:::\n\n\n## Comparing Methods\n\n| Method | Input | Output | Supervision | Best For |\n|:-------|:------|:-------|:------------|:---------|\n| PCA | Variables | Continuous scores | None | Reducing correlated variables |\n| PCoA | Distance matrix | Continuous scores | None | Preserving sample distances |\n| NMDS | Distance matrix | Ordinal scores | None | Ecological community data |\n| Cluster Analysis | Variables or distances | Group assignments | None | Finding natural groupings |\n| MANOVA | Variables + groups | Test statistics | Groups known | Testing group differences |\n| DFA | Variables + groups | Discriminant scores | Groups known | Classifying observations |\n\n## Using Ordination Scores in Further Analyses\n\nPC scores and discriminant scores are legitimate new variables that can be used in downstream analysis:\n\n- Regression of scores on other continuous variables\n- ANOVA comparing groups on ordination scores\n- Correlation of scores with environmental gradients\n\nThis is valuable when you have many correlated variables and want to reduce dimensionality before hypothesis testing.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Use PC scores in ANOVA\npc_scores <- data.frame(\n  PC1 = iris_pca$x[, 1],\n  PC2 = iris_pca$x[, 2],\n  Species = iris$Species\n)\n\nsummary(aov(PC1 ~ Species, data = pc_scores))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n             Df Sum Sq Mean Sq F value Pr(>F)    \nSpecies       2  406.4  203.21    1051 <2e-16 ***\nResiduals   147   28.4    0.19                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\n\n## Practical Workflow\n\n1. **Explore data**: Check for outliers, missing values, scaling issues\n\n2. **Standardize if needed**: Especially important when variables are on different scales\n\n3. **Choose appropriate method**: Based on your data type and question\n\n4. **Examine output**: Scree plots, loadings, clustering diagnostics\n\n5. **Validate**: Cross-validation for classification; permutation tests for significance\n\n6. **Interpret biologically**: What do the patterns mean in your system?\n\n## Summary\n\n- Dimensionality reduction creates fewer variables that capture most information\n- PCA finds linear combinations maximizing variance; useful for correlated variables\n- PCoA works from distance matrices; useful for ecological and genetic data\n- Cluster analysis groups similar observations together\n- MANOVA tests whether groups differ on multiple response variables simultaneously\n- DFA finds combinations that best discriminate known groups\n- These methods can be combined: use PCA to reduce dimensions, then cluster or classify\n\n## Additional Resources\n\n- @james2023islr - Modern treatment of dimensionality reduction and clustering\n- @logan2010biostatistical - MANOVA and DFA in biological research contexts\n- Borcard, D., Gillet, F., & Legendre, P. (2018). *Numerical Ecology with R* - Comprehensive ordination methods\n",
    "supporting": [
      "23-dimensionality-reduction_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}