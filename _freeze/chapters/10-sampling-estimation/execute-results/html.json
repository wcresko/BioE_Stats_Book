{
  "hash": "dec6c0e48165fd27147d6157e75d8f84",
  "result": {
    "engine": "knitr",
    "markdown": "# Sampling and Parameter Estimation {#sec-sampling-estimation}\n\n\n::: {.cell}\n\n:::\n\n\n## The Problem of Inference\n\nScience often works by measuring samples to learn about populations. We cannot measure every protein in a cell, every patient with a disease, or every fish in the ocean. Instead, we take samples and use statistical inference to draw conclusions about the larger populations from which they came.\n\nThis creates a fundamental challenge: sample statistics vary from sample to sample, even when samples come from the same population. If you take two different random samples from a population and calculate their means, you will almost certainly get two different values. How, then, can we say anything reliable about the population?\n\nThe answer lies in understanding the sampling distribution—the distribution of a statistic across all possible samples of a given size.\n\n## Parameters and Statistics\n\nA **parameter** is a numerical characteristic of a population—the true population mean $\\mu$, the true population standard deviation $\\sigma$, the true proportion $p$. Parameters are typically fixed but unknown.\n\nA **statistic** is a numerical characteristic of a sample—the sample mean $\\bar{x}$, the sample standard deviation $s$, the sample proportion $\\hat{p}$. Statistics are calculated from data and vary from sample to sample.\n\nWe use statistics to estimate parameters. The sample mean $\\bar{x}$ estimates the population mean $\\mu$. The sample standard deviation $s$ estimates the population standard deviation $\\sigma$. These estimates will rarely equal the true parameter values exactly, but we can quantify how close they are likely to be.\n\n## Point Estimates\n\nA **point estimate** is a single number used as our best guess for a parameter. The sample mean is a natural point estimate for the population mean:\n\n$$\\bar{x} = \\frac{1}{n} \\sum_{i=1}^{n} x_i$$\n\nWhat makes a good estimator? Ideally, an estimator should be:\n\n**Unbiased**: On average, across many samples, the estimator equals the true parameter. The sample mean is an unbiased estimator of the population mean.\n\n**Efficient**: Among unbiased estimators, it has the smallest variance. The sample mean is the most efficient estimator of a normal mean.\n\n**Consistent**: As sample size increases, the estimator converges to the true parameter value.\n\n## The Sampling Distribution of the Mean\n\nImagine drawing all possible samples of size $n$ from a population and calculating the mean of each. The distribution of these means is the sampling distribution of the mean.\n\nThe sampling distribution has remarkable properties:\n\n1. Its mean equals the population mean: $E[\\bar{X}] = \\mu$\n2. Its standard deviation (the **standard error**) equals: $SE = \\frac{\\sigma}{\\sqrt{n}}$\n3. For large samples, it is approximately normal (Central Limit Theorem)\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Demonstrate sampling distribution\nset.seed(32)\n\n# Create a population\ntrue_pop <- rpois(n = 10000, lambda = 3)\npop_mean <- mean(true_pop)\npop_sd <- sd(true_pop)\n\n# Take many samples and compute their means\nsample_sizes <- c(5, 20, 50, 200)\npar(mfrow = c(2, 2))\n\nfor (n in sample_sizes) {\n  sample_means <- replicate(1000, mean(sample(true_pop, n)))\n  hist(sample_means, breaks = 30, main = paste(\"n =\", n),\n       xlab = \"Sample Mean\", col = \"steelblue\",\n       xlim = c(1, 5))\n  abline(v = pop_mean, col = \"red\", lwd = 2)\n}\n```\n\n::: {.cell-output-display}\n![](10-sampling-estimation_files/figure-html/unnamed-chunk-2-1.png){width=768}\n:::\n:::\n\n\nAs sample size increases, the sampling distribution becomes narrower (smaller standard error) and more normal in shape. This is why larger samples give more precise estimates.\n\n## Standard Error\n\nThe **standard error** (SE) measures the variability of a statistic across samples. For the sample mean:\n\n$$SE_{\\bar{x}} = \\frac{\\sigma}{\\sqrt{n}}$$\n\nSince we usually do not know $\\sigma$, we estimate the standard error using the sample standard deviation:\n\n$$\\widehat{SE}_{\\bar{x}} = \\frac{s}{\\sqrt{n}}$$\n\nThe standard error shrinks as sample size increases, but following a square root relationship. To halve the standard error, you need to quadruple the sample size.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Demonstrate how SE changes with sample size\nset.seed(32)\ntrue_pop <- rpois(n = 1000, lambda = 5)\n\n# Sample size of 5\nsamps_5 <- replicate(n = 50, sample(true_pop, size = 5))\nmeans_5 <- apply(samps_5, 2, mean)\nse_5 <- sd(means_5)\n\n# Sample size of 50\nsamps_50 <- replicate(n = 50, sample(true_pop, size = 50))\nmeans_50 <- apply(samps_50, 2, mean)\nse_50 <- sd(means_50)\n\ncat(\"Standard error with n=5:\", round(se_5, 3), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nStandard error with n=5: 0.919 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Standard error with n=50:\", round(se_50, 3), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nStandard error with n=50: 0.305 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Ratio:\", round(se_5/se_50, 2), \"(theoretical: √10 =\", round(sqrt(10), 2), \")\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRatio: 3.01 (theoretical: √10 = 3.16 )\n```\n\n\n:::\n:::\n\n\n![](../images/week_2.026.jpeg){fig-align=\"center\"}\n\n## Confidence Intervals\n\nA point estimate tells us our best guess, but not how uncertain we are. A **confidence interval** provides a range of plausible values for the parameter along with a measure of confidence.\n\nA 95% confidence interval for the population mean, when the population is normally distributed or the sample is large, is:\n\n$$\\bar{x} \\pm t_{\\alpha/2} \\times \\frac{s}{\\sqrt{n}}$$\n\nwhere $t_{\\alpha/2}$ is the critical value from the t-distribution with $n-1$ degrees of freedom.\n\n![](../images/week_2.027.jpeg){fig-align=\"center\"}\n\nThe interpretation requires care: a 95% confidence interval means that if we repeated this procedure many times, 95% of the resulting intervals would contain the true parameter. Any particular interval either does or does not contain the true value—we just don't know which.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Calculate a confidence interval\nset.seed(42)\nsample_data <- rnorm(30, mean = 100, sd = 15)\n\nsample_mean <- mean(sample_data)\nsample_se <- sd(sample_data) / sqrt(length(sample_data))\nt_crit <- qt(0.975, df = length(sample_data) - 1)\n\nlower <- sample_mean - t_crit * sample_se\nupper <- sample_mean + t_crit * sample_se\n\ncat(\"Sample mean:\", round(sample_mean, 2), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSample mean: 101.03 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"95% CI: [\", round(lower, 2), \",\", round(upper, 2), \"]\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n95% CI: [ 94 , 108.06 ]\n```\n\n\n:::\n\n```{.r .cell-code}\n# Or use t.test directly\nt.test(sample_data)$conf.int\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1]  93.99927 108.05833\nattr(,\"conf.level\")\n[1] 0.95\n```\n\n\n:::\n:::\n\n\n![](../images/week_2.028.jpeg){fig-align=\"center\"}\n\n## Coefficient of Variation\n\nWhen comparing variability across groups with different means, the standard deviation alone can be misleading. The **coefficient of variation** (CV) standardizes variability relative to the mean:\n\n$$CV = \\frac{s}{\\bar{x}} \\times 100\\%$$\n\nA CV of 10% means the standard deviation is 10% of the mean. This allows meaningful comparisons between groups or measurements on different scales.\n\n## Percentiles and Quantiles\n\n**Percentiles** describe the relative position of values within a distribution. The $p$th percentile is the value below which $p$% of the data falls. The 50th percentile is the median, the 25th percentile is the first quartile, and the 75th percentile is the third quartile.\n\n**Quantiles** divide data into equal parts. Quartiles divide into four parts, deciles into ten parts, percentiles into one hundred parts.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Calculate percentiles\ndata <- c(12, 15, 18, 22, 25, 28, 32, 35, 40, 45)\n\nquantile(data, probs = c(0.25, 0.5, 0.75))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  25%   50%   75% \n19.00 26.50 34.25 \n```\n\n\n:::\n\n```{.r .cell-code}\nsummary(data)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  12.00   19.00   26.50   27.20   34.25   45.00 \n```\n\n\n:::\n:::\n\n\nQuantiles form the basis for many statistical procedures, including constructing confidence intervals and calculating p-values.\n\n## Bias and Variability\n\nTwo distinct types of error affect estimates:\n\n**Bias** is systematic error—the tendency for an estimator to consistently over- or underestimate the true parameter. An unbiased estimator has zero bias: its average value across all possible samples equals the true parameter.\n\n**Variability** is random error—the spread of estimates around their average value. Low variability means estimates cluster tightly together.\n\nThe ideal estimator has both low bias and low variability. Sometimes there is a tradeoff: a slightly biased estimator might have much lower variability, resulting in estimates that are closer to the truth on average.\n\nThe **mean squared error** (MSE) combines both sources of error:\n\n$$MSE = Bias^2 + Variance$$\n\n## Key Takeaways\n\nUnderstanding sampling distributions and estimation is fundamental to statistical inference. Key points to remember:\n\n1. Statistics vary from sample to sample; this variability is quantified by the standard error\n2. Larger samples give more precise estimates (smaller standard errors)\n3. Confidence intervals quantify uncertainty about parameter estimates\n4. The Central Limit Theorem explains why the normal distribution appears so frequently\n5. Both bias and variability affect the quality of estimates\n\nThese concepts provide the foundation for hypothesis testing and the statistical inference methods we develop in subsequent chapters.\n",
    "supporting": [
      "10-sampling-estimation_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}