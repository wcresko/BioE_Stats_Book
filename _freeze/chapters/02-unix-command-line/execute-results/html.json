{
  "hash": "30f884212335807141aa13870e0362d1",
  "result": {
    "engine": "knitr",
    "markdown": "# Unix and the Command Line {#sec-unix}\n\n\n::: {.cell}\n\n:::\n\n\n![](../images/w1_code.jpeg){fig-align=\"center\"}\n\n## What is Unix?\n\nUnix is a family of operating systems that originated at Bell Labs in 1969 and was released publicly in 1973. Its design philosophy emphasizes modularity—small programs that do one thing well and can be combined to accomplish complex tasks. This approach has proven remarkably durable, and Unix-based systems remain dominant in scientific computing, web servers, and high-performance computing environments.\n\nLinux is an open-source implementation of Unix that runs on everything from embedded devices to the world's fastest supercomputers. MacOS is built on a Unix foundation, which means Mac users have native access to Unix commands. Windows historically used a different approach, but recent versions include the Windows Subsystem for Linux (WSL), allowing Windows users to run Linux environments alongside their Windows applications.\n\nUnderstanding Unix is essential for modern data science. You will need it to access remote computing resources like supercomputer clusters, to run bioinformatics software that is only available through the command line, and to automate repetitive tasks. The skills you develop here will transfer across platforms and remain relevant throughout your career.\n\n## The Shell and Terminal\n\nThe shell is a program that interprets your commands and communicates with the operating system. When you type a command, the shell parses it, figures out what you want to do, and tells the operating system to do it. The results are then displayed back to you.\n\nBash (Bourne Again SHell) is the most common shell on Linux systems and was the default on MacOS until recently (MacOS now defaults to zsh, which is very similar). The shell runs inside a terminal application, which provides the window where you type commands and see output.\n\n![](../images/w1_shell.jpeg){fig-align=\"center\"}\n\nOn Mac, you can access the terminal by opening the Terminal app or a third-party alternative like iTerm2. On Linux, look for a Terminal application in your system menus. Windows users should install the Windows Subsystem for Linux following Microsoft's documentation, then access it through the Ubuntu app or similar.\n\nRStudio also includes a terminal pane, which can be convenient when you want shell access without leaving your R development environment.\n\n## Anatomy of a Shell Command\n\nShell commands follow a consistent structure. You type a command name, possibly followed by options that modify its behavior, and arguments that specify what the command should operate on. The shell waits at a prompt—typically `$` for regular users or `#` for administrators—indicating it is ready to accept input.\n\n![](../images/w1_shell_command.jpeg){fig-align=\"center\"}\n\nConsider the command `ls -l Documents`. Here, `ls` is the command (list directory contents), `-l` is an option (use long format), and `Documents` is the argument (the directory to list). Options usually begin with a dash and can often be combined: `ls -la` combines the `-l` (long format) and `-a` (show hidden files) options.\n\n## File System Organization\n\nUnix organizes files in a hierarchical structure of directories (folders) and files. The root directory, represented by a single forward slash `/`, sits at the top of this hierarchy and contains all other directories.\n\n![](../images/w1_file_structure.jpeg){fig-align=\"center\"}\n\nYour home directory is your personal workspace, typically located at `/Users/yourusername` on Mac or `/home/yourusername` on Linux. The tilde character `~` serves as a shorthand for your home directory, so `~/Documents` refers to the Documents folder in your home directory.\n\nEvery file and directory has a path—a specification of its location in the file system. Absolute paths start from the root directory and give the complete location, like `/Users/wcresko/Documents/data.csv`. Relative paths specify location relative to your current directory, so if you are in your home directory, `Documents/data.csv` refers to the same file.\n\n## Navigation Commands\n\nThe most fundamental navigation command is `pwd` (print working directory), which tells you where you currently are in the file system. This is often the first thing you type when opening a terminal to orient yourself.\n\n![](../images/w1_navigation_1.jpeg){fig-align=\"center\"}\n\n```bash\npwd\n```\n\nThe `ls` command lists the contents of a directory. Without arguments, it lists the current directory. With a path argument, it lists that location.\n\n```bash\nls                  # list current directory\nls Documents        # list the Documents folder\nls -l               # long format with details\nls -a               # include hidden files (starting with .)\nls -la              # combine long format and hidden files\nls -lS              # long format, sorted by size\n```\n\n![](../images/w1_navigation_2.jpeg){fig-align=\"center\"}\n\nThe `cd` command (change directory) moves you to a different location.\n\n```bash\ncd Documents        # move into Documents\ncd ..               # move up one level (parent directory)\ncd ~                # move to home directory\ncd /                # move to root directory\ncd -                # move to previous location\n```\n\n![](../images/w1_navigate.jpeg){fig-align=\"center\"}\n\n## Working with Files and Directories\n\nCreating new directories uses the `mkdir` command.\n\n```bash\nmkdir project_data\nmkdir -p analysis/results/figures  # create nested directories\n```\n\nThe `-p` flag tells `mkdir` to create parent directories as needed, which is useful for creating nested folder structures in one command.\n\nTo remove an empty directory, use `rmdir`.\n\n```bash\nrmdir empty_folder                 # remove an empty directory\n```\n\nNote that `rmdir` only works on empty directories. For directories with contents, you need `rm -r` (discussed below).\n\nCreating a new, empty file uses the `touch` command.\n\n```bash\ntouch newfile.txt                  # create an empty file\ntouch notes.md data.csv            # create multiple files\n```\n\nThe `touch` command is also useful for updating the modification timestamp of existing files without changing their contents.\n\nMoving and renaming files uses the `mv` command.\n\n```bash\nmv old_name.txt new_name.txt       # rename a file\nmv file.txt Documents/             # move file to Documents\nmv file.txt Documents/newname.txt  # move and rename\n```\n\nCopying files uses `cp`.\n\n```bash\ncp original.txt copy.txt           # copy a file\ncp -r folder/ backup/              # copy a directory recursively\n```\n\nRemoving files uses `rm`. Be careful with this command—there is no trash can or undo in the shell.\n\n```bash\nrm unwanted_file.txt               # remove a file\nrm -r unwanted_folder/             # remove a directory and contents\nrm -i file.txt                     # ask for confirmation before removing\n```\n\n::: {.callout-tip}\n## Interrupting Commands\nIf you need to stop a running command—perhaps you started a process that is taking too long or realize you made a mistake—press `Ctrl-C`. This sends an interrupt signal that terminates most running processes and returns you to the command prompt.\n:::\n\n## Viewing File Contents\n\nSeveral commands let you examine file contents without opening them in an editor.\n\nThe `cat` command displays the entire contents of a file.\n\n```bash\ncat data.txt\n```\n\nFor longer files, `head` and `tail` show the beginning and end.\n\n```bash\nhead data.csv          # first 10 lines\nhead -n 20 data.csv    # first 20 lines\ntail data.csv          # last 10 lines\ntail -f logfile.txt    # follow a file as it grows\n```\n\nThe `less` command opens an interactive viewer that lets you scroll through large files.\n\n```bash\nless large_data.txt\n```\n\nInside `less`, use arrow keys to scroll, `/` to search, and `q` to quit.\n\nThe `wc` command counts lines, words, and characters.\n\n```bash\nwc data.txt            # lines, words, characters\nwc -l data.txt         # just lines\n```\n\n## Getting Help\n\nUnix provides documentation through manual pages, accessible with the `man` command.\n\n```bash\nman ls                 # manual page for ls command\n```\n\nManual pages can be dense, but they are comprehensive. Use the spacebar to page through, `/` to search, and `q` to exit. Many commands also accept a `--help` flag that provides a shorter summary.\n\n```bash\nls --help\n```\n\nOf course, the internet provides extensive resources. When you encounter an unfamiliar command or error message, searching online often leads to helpful explanations and examples.\n\n## Pipes and Redirection\n\nOne of Unix's most powerful features is the ability to combine simple commands into complex pipelines. The pipe operator `|` sends the output of one command to another command as input.\n\n```bash\nls -l | head -n 5           # list files, show only first 5\ncat data.txt | wc -l        # count lines in file\n```\n\nRedirection operators send output to files instead of the screen.\n\n```bash\nls -l > file_list.txt       # write output to file (overwrite)\nls -l >> file_list.txt      # append output to file\n```\n\nThese features enable powerful text processing. Combined with tools like `grep` (search for patterns), `sort`, and `cut` (extract columns), you can accomplish sophisticated data manipulation with compact commands.\n\n```bash\ngrep \"gene\" data.txt                    # find lines containing \"gene\"\ngrep -c \"gene\" data.txt                 # count matching lines\nsort data.txt                           # sort lines alphabetically\nsort -n numbers.txt                     # sort numerically\ncut -f1,3 data.tsv                      # extract columns 1 and 3 from tab-separated file\n```\n\n## Advanced Text Processing\n\nThe basic commands above are just the beginning. Unix provides powerful tools for searching, manipulating, and transforming text files—skills that are invaluable when working with biological data.\n\n### Pattern Matching with grep\n\nThe `grep` command becomes even more powerful when you use special characters to define patterns. These patterns, called regular expressions, allow you to search for complex text structures.\n\nCommon special characters in `grep` patterns:\n\n- `^` matches the beginning of a line\n- `$` matches the end of a line\n- `.` matches any single character (except newline)\n- `*` matches zero or more of the preceding character\n- `\\s` matches any whitespace character\n\n```bash\ngrep \"^embryo\" data.tsv          # lines starting with \"embryo\"\ngrep \"gene$\" data.tsv            # lines ending with \"gene\"\ngrep \"sample.*control\" data.tsv  # lines with \"sample\" followed by anything then \"control\"\ngrep \"^embryo_10\\s\" data.tsv     # lines starting with \"embryo_10\" followed by whitespace\n```\n\nUseful `grep` flags include:\n\n- `-c` counts matching lines instead of displaying them\n- `-v` returns lines that do NOT match the pattern (inverse match)\n- `-n` includes line numbers in the output\n- `-i` performs case-insensitive matching\n\n```bash\ngrep -v \"^#\" data.tsv            # exclude comment lines starting with #\ngrep -n \"error\" logfile.txt      # show line numbers for matches\ngrep -c \"ATCG\" sequences.fasta   # count lines containing this sequence\n```\n\n### Search and Replace with sed\n\nThe `sed` (stream editor) command is commonly used for search-and-replace operations. The basic syntax uses slashes to separate the pattern, replacement, and flags:\n\n```bash\nsed 's/old/new/' file.txt        # replace first occurrence on each line\nsed 's/old/new/g' file.txt       # replace all occurrences (global)\nsed 's/\\t/,/g' file.tsv          # convert tabs to commas\nsed 's/^/prefix_/' file.txt      # add prefix to beginning of each line\n```\n\nBy default, `sed` prints the modified text to the terminal. To modify a file in place, use the `-i` flag (use with caution):\n\n```bash\nsed -i 's/old/new/g' file.txt    # modify file in place\n```\n\nA safer approach is to redirect output to a new file:\n\n```bash\nsed 's/old/new/g' input.txt > output.txt\n```\n\n### Column Operations with cut and join\n\nThe `cut` command extracts specific columns from delimited files. By default, it assumes tab-delimited data.\n\n```bash\ncut -f1,2 data.tsv               # extract columns 1 and 2 (tab-delimited)\ncut -f1,3 -d\",\" data.csv         # extract columns 1 and 3 (comma-delimited)\ncut -f2-5 data.tsv               # extract columns 2 through 5\n```\n\nThe `join` command combines two files based on a common field, similar to a database join. Both files should be sorted on the join field.\n\n```bash\njoin file1.txt file2.txt         # join on first field\njoin -1 2 -2 1 file1.txt file2.txt  # join file1's column 2 with file2's column 1\n```\n\n### Sorting with Advanced Options\n\nThe `sort` command has many options for controlling how data is sorted.\n\n```bash\nsort -n data.txt                 # sort numerically\nsort -r data.txt                 # sort in reverse order\nsort -k2,2 data.tsv              # sort by second column\nsort -k2,2 -n data.tsv           # sort by second column numerically\nsort -k2,2 -nr data.tsv          # sort by second column, numerically, in reverse\nsort -u data.txt                 # sort and remove duplicate lines\nsort -t\",\" -k3,3 data.csv        # sort comma-separated file by third column\n```\n\n### Flexible Text Processing with awk\n\nThe `awk` command is an extremely powerful tool for processing structured text. It treats each line as a record and each column as a field, making it ideal for tabular data. Fields are referenced using `$1`, `$2`, etc., where `$0` represents the entire line.\n\n```bash\nawk '{print $1}' data.tsv                    # print first column\nawk '{print $1, $3}' data.tsv                # print columns 1 and 3\nawk -F\",\" '{print $1, $2}' data.csv          # specify comma as delimiter\nawk '{print NR, $0}' data.txt                # print line numbers with each line\n```\n\nOne of `awk`'s strengths is its ability to filter rows based on conditions:\n\n```bash\nawk '$3 > 100 {print $1, $3}' data.tsv       # print columns 1 and 3 where column 3 > 100\nawk '$2 == \"control\" {print $0}' data.tsv    # print lines where column 2 is \"control\"\nawk 'NR > 1 {print $0}' data.tsv             # skip header (print from line 2 onward)\nawk '$4 >= 0.05 {print $1}' results.tsv      # extract IDs where p-value >= 0.05\n```\n\nYou can also perform calculations:\n\n```bash\nawk '{sum += $2} END {print sum}' data.tsv           # sum of column 2\nawk '{sum += $2} END {print sum/NR}' data.tsv        # average of column 2\nawk '{print $1, $2 * 1000}' data.tsv                 # multiply column 2 by 1000\n```\n\n### Combining Commands in Pipelines\n\nThe real power of Unix text processing comes from combining these tools. Here are some examples relevant to biological data analysis:\n\n```bash\n# Count unique gene names in column 1 (skipping header)\ntail -n +2 data.tsv | cut -f1 | sort | uniq | wc -l\n\n# Extract rows with significant p-values and sort by effect size\nawk '$5 < 0.05' results.tsv | sort -k3,3 -nr | head -20\n\n# Convert a file from comma to tab-delimited and extract specific columns\nsed 's/,/\\t/g' data.csv | cut -f1,3,5 > subset.tsv\n\n# Find all unique values in column 2 and count occurrences\ncut -f2 data.tsv | sort | uniq -c | sort -nr\n\n# Process a FASTA file to count sequences per chromosome\ngrep \"^>\" sequences.fasta | cut -d\":\" -f1 | sort | uniq -c\n```\n\n::: {.callout-note}\n## Learning More\nThese tools have many more capabilities than we can cover here. The `man` pages provide comprehensive documentation, and online resources like the GNU Awk User's Guide offer in-depth tutorials. With practice, you will develop intuition for which tool to use for different tasks.\n:::\n\n## Wildcards and Pattern Matching\n\nOne of Unix's most powerful features is **wildcards**—special characters that match multiple files at once. Instead of typing each filename individually, you can specify patterns that match many files simultaneously.\n\nThe asterisk `*` matches any number of any characters (including zero characters):\n\n```bash\nls *.csv              # all CSV files\nls data*              # all files starting with \"data\"\nls *.txt *.md         # all text and markdown files\nrm temp*              # remove all files starting with \"temp\"\n```\n\nThe question mark `?` matches exactly one character:\n\n```bash\nls sample?.txt        # sample1.txt, sample2.txt, etc. (but not sample10.txt)\nls data_??.csv        # data_01.csv, data_AB.csv, etc.\n```\n\nSquare brackets `[]` match any single character from a set:\n\n```bash\nls sample[123].txt    # sample1.txt, sample2.txt, or sample3.txt\nls data_[0-9].csv     # data_0.csv through data_9.csv\nls file[A-Z].txt      # fileA.txt through fileZ.txt\n```\n\n::: {.callout-warning}\n## Wildcard Safety\nCombining `rm` with wildcards can be dangerous. The command `rm *` deletes everything in the current directory without confirmation. Always use `ls` first to see what a wildcard pattern matches before using it with `rm`. Consider using `rm -i` for interactive confirmation when removing files with wildcards.\n:::\n\nWildcards are expanded by the shell before the command runs, so they work with any command:\n\n```bash\n# Count lines in all CSV files\nwc -l *.csv\n\n# Copy all R scripts to a backup folder\ncp *.R backup/\n\n# Search for \"gene\" in all text files\ngrep \"gene\" *.txt\n```\n\n## Environment Variables\n\nUnix maintains settings called **environment variables** that affect how the shell and programs behave. These variables store information about your user session, system configuration, and program preferences. Environment variables are distinguished by a `$` prefix when you reference them.\n\nSeveral important environment variables are set automatically:\n\n```bash\n# Your home directory\necho $HOME\n/home/wcresko\n\n# Your username\necho $USER\nwcresko\n\n# Your current shell program\necho $SHELL\n/bin/bash\n\n# Where Unix looks for executable programs\necho $PATH\n/usr/local/bin:/usr/bin:/bin:/home/wcresko/bin\n\n# Your current working directory\necho $PWD\n/home/wcresko/projects\n```\n\nThe `PATH` variable is particularly important—it contains a colon-separated list of directories where Unix searches for programs. When you type a command like `ls` or `python`, Unix looks through each directory in your `PATH` until it finds a matching executable. You can find where a program is located using `which`:\n\n```bash\nwhich python\n/usr/bin/python\n\nwhich R\n/usr/local/bin/R\n```\n\nYou can set your own environment variables and use them in commands:\n\n```bash\n# Set a variable\nPROJECT_DIR=~/projects/analysis\n\n# Use it (note the $)\ncd $PROJECT_DIR\nls $PROJECT_DIR/data\n```\n\nTo make environment variables available to subprocesses (programs you launch), use `export`:\n\n```bash\nexport PROJECT_DIR=~/projects/analysis\n```\n\nTo see all environment variables currently set, use `env` or `printenv`:\n\n```bash\nenv | head -20       # Show first 20 environment variables\n```\n\n### Shell Configuration Files\n\nYour shell can be customized through configuration files that run when you open a terminal. For Bash, the main configuration files are `~/.bashrc` (for interactive shells) and `~/.bash_profile` (for login shells). For zsh (the default on modern macOS), use `~/.zshrc`.\n\nCommon customizations include:\n\n```bash\n# Add a directory to your PATH\nexport PATH=\"$HOME/bin:$PATH\"\n\n# Create an alias (shortcut) for a common command\nalias ll='ls -la'\nalias rm='rm -i'    # Always ask before deleting\n\n# Set default options for programs\nexport R_LIBS_USER=\"$HOME/R/library\"\n```\n\nAfter editing configuration files, apply the changes by either starting a new terminal or running:\n\n```bash\nsource ~/.bashrc\n```\n\n## File Permissions\n\nUnix is a multi-user system, and every file has permissions controlling who can read, write, or execute it. Understanding permissions is essential for security and for running scripts on shared computing clusters.\n\nWhen you run `ls -l`, you see permission strings at the beginning of each line:\n\n```bash\nls -l\n-rwxr-xr-x 1 wcresko staff 2048 Jan 15 10:30 analysis.sh\ndrwxr-xr-x 3 wcresko staff   96 Jan 15 09:00 data\n```\n\nThe permission string `-rwxr-xr-x` encodes three sets of permissions:\n\n| Position | Meaning |\n|:---------|:--------|\n| 1st character | Type: `-` (file), `d` (directory), `l` (link) |\n| Characters 2-4 | **Owner** permissions |\n| Characters 5-7 | **Group** permissions |\n| Characters 8-10 | **Others** (everyone else) permissions |\n\nWithin each set, the three characters represent:\n\n- `r` (read): View file contents or list directory contents\n- `w` (write): Modify file or add/remove files in directory\n- `x` (execute): Run file as program or enter directory\n- `-` (dash): Permission denied for that operation\n\nSo `-rwxr-xr-x` means: this is a regular file; the owner can read, write, and execute; the group can read and execute; others can read and execute.\n\n### Changing Permissions with chmod\n\nThe `chmod` command changes file permissions. You can use symbolic notation or numeric (octal) notation.\n\n**Symbolic notation** uses letters and operators:\n\n```bash\n# Add execute permission for the owner\nchmod u+x script.sh\n\n# Remove write permission for group and others\nchmod go-w data.csv\n\n# Set exact permissions\nchmod u=rwx,g=rx,o=rx script.sh\n```\n\nThe letters are: `u` (user/owner), `g` (group), `o` (others), `a` (all). The operators are: `+` (add), `-` (remove), `=` (set exactly).\n\n**Octal notation** uses numbers where each permission has a value:\n\n| Permission | Value |\n|:-----------|:------|\n| read (r)   | 4     |\n| write (w)  | 2     |\n| execute (x)| 1     |\n\nAdd the values for each set. For example, `rwx` = 4+2+1 = 7, and `r-x` = 4+0+1 = 5.\n\n```bash\nchmod 755 script.sh    # rwxr-xr-x (executable script)\nchmod 644 data.csv     # rw-r--r-- (readable data file)\nchmod 700 private/     # rwx------ (private directory)\n```\n\nCommon permission patterns:\n\n- `755`: Executable scripts (owner can modify, everyone can run)\n- `644`: Data files (owner can modify, everyone can read)\n- `700`: Private directories (only owner has access)\n- `600`: Private files (only owner can read/write)\n\n### Making Scripts Executable\n\nWhen you write a shell script, you need to make it executable before you can run it directly:\n\n```bash\n# Create a simple script\necho '#!/bin/bash' > myscript.sh\necho 'echo \"Hello, World!\"' >> myscript.sh\n\n# Try to run it (will fail)\n./myscript.sh\n# bash: ./myscript.sh: Permission denied\n\n# Make it executable\nchmod +x myscript.sh\n\n# Now it works\n./myscript.sh\n# Hello, World!\n```\n\nThe `#!/bin/bash` line at the top of the script (called a \"shebang\") tells Unix which program should interpret the script.\n\n## Connecting to Remote Systems\n\nThe `ssh` command (secure shell) lets you connect to remote computers.\n\n```bash\nssh username@server.university.edu\n```\n\nYou will use this to connect to computing clusters like Talapas for computationally intensive work. Once connected, you work in a shell environment on the remote system just as you would locally.\n\nThe `scp` command copies files between your computer and remote systems.\n\n```bash\nscp local_file.txt username@server.edu:~/destination/\nscp username@server.edu:~/remote_file.txt ./local_copy.txt\n```\n\n## Data File Best Practices\n\nWhen working with data files, following consistent practices will save you time and prevent errors. These guidelines apply whether you are using Unix tools, R, or any other analysis software.\n\n### Do\n\n- **Store data in plain text formats** such as tab-separated (.tsv) or comma-separated (.csv) files. These nonproprietary formats can be read by any software and will remain accessible for years to come.\n- **Keep an unedited copy of original data files.** Even when your analysis requires modifications, preserve the raw data separately.\n- **Use descriptive, consistent names** for files and variables. A name like `experiment1_control_measurements.tsv` is far more useful than `data2.txt`.\n- **Maintain metadata** that documents what each variable means, how data were collected, and any processing steps applied.\n- **Add new observations as rows** and new variables as columns to maintain a consistent rectangular structure.\n\n### Don't\n\n- **Don't mix data types within a column.** If a column contains numbers, every entry should be a number (or explicitly missing).\n- **Don't use special characters in file or directory names.** Stick to letters, numbers, underscores, and hyphens. Avoid spaces, which can cause problems with command-line tools.\n- **Don't use delimiter characters in data values.** If your file is comma-delimited, don't use commas within data entries. For example, use `2024-03-08` rather than `March 8, 2024` for dates.\n- **Don't copy data from formatted documents** like Microsoft Word directly into data files. Hidden formatting characters can corrupt your data.\n- **Don't edit data files in spreadsheet programs** that might silently convert values (for example, Excel's tendency to convert gene names like SEPT1 to dates).\n\n::: {.callout-warning}\n## Preserving Raw Data\nPerhaps the most important principle is to never modify your original raw data files. Keep them in a separate directory with restricted write permissions if possible. All data cleaning and transformation should be done programmatically (in scripts that can be re-run), with outputs saved to new files.\n:::\n\n## Practice Exercises\n\nThe best way to learn command-line skills is through practice. Save a digital record of your work so that you can study it later if you need to.\n\n### Exercise U.1: Basic Navigation and File Operations\n\nOpen up a terminal and execute the following using Unix commands:\n\n1. Print your working directory using `pwd`\n2. Navigate to a directory somewhere below your home directory where you want to practice writing files\n3. Make 5 directories called `dir_1`, `dir_2`, `dir_3`, `dir_4`, and `dir_5` using `mkdir`\n4. Within each of those directories, create files called `file_1.txt`, `file_2.txt`, and `file_3.txt` using `touch`\n5. Open `file_1.txt` in `dir_1` using a plain text editor (such as `nano` or `vim`), type a few words, and save it\n6. Print `file_1.txt` in `dir_1` to the terminal using `cat`\n7. Delete all files in `dir_3` using `rm`\n8. List all of the contents of your current directory line-by-line using `ls -l`\n9. Delete `dir_3` using `rmdir`\n\n### Exercise U.2: Working with Data Files\n\nFor this exercise, create a sample tab-separated file or download a GFF file from a genomics database.\n\n1. Navigate to `dir_1`\n2. Copy a data file (using its absolute path) to your current directory\n3. Delete the copy that is in your current directory, then copy it again using a relative path this time\n4. Use at least 3 different Unix commands to examine all or parts of your data file (try `cat`, `head`, `tail`, `less`, and `wc`)\n5. What is the file size? Use `ls -lh` to find out\n6. How many lines does the file have? Use `wc -l`\n7. How many lines begin with a specific pattern (like a chromosome name)? Use `grep -c \"^pattern\"`\n8. How many unique entries are there in a specific column? Use `cut` and `sort -u | wc -l`\n9. Sort the file based on reverse numeric order in a specific field using `sort -k -nr`\n10. Capture specific fields and write to a new file using `cut` and redirection\n11. Replace all instances of one string with another using `sed 's/old/new/g'`\n\n### Exercise U.3: Building Pipelines\n\nPractice combining commands with pipes to answer questions about your data:\n\n1. Count the number of unique values in the third column of a tab-separated file:\n   ```bash\n   cut -f3 data.tsv | sort | uniq | wc -l\n   ```\n\n2. Find all lines containing a pattern, extract specific columns, and sort the results:\n   ```bash\n   grep \"pattern\" data.tsv | cut -f1,2,5 | sort -k3,3 -n\n   ```\n\n3. Create a pipeline that filters rows based on a condition, extracts columns, and saves to a new file\n\n4. Use `awk` to filter rows where a numeric column exceeds a threshold:\n   ```bash\n   awk '$5 > 1000 {print $1, $2, $5}' data.tsv\n   ```\n\n### Exercise U.4: File Permissions and Scripts\n\n1. Create a simple shell script that prints \"Hello, World!\" and the current date\n2. Try to run the script—what error do you get?\n3. Make the script executable using `chmod +x`\n4. Run the script and verify it works\n5. Examine the permissions of various files in your system using `ls -l`\n6. Practice changing permissions using both symbolic notation (`chmod u+x`) and octal notation (`chmod 755`)\n\n## Additional Resources\n\n- [Unix/Linux Command Reference](http://mally.stanford.edu/~sr/computing/basic-unix.html) - A comprehensive cheat sheet of common commands\n- [Unix and Perl Primer for Biologists](http://korflab.ucdavis.edu/Unix_and_Perl/) - An outstanding tutorial by Keith Bradnam and Ian Korf, specifically designed for life scientists\n- [Introduction to Shell for Data Science](https://www.datacamp.com/courses/introduction-to-shell-for-data-science) - DataCamp's interactive tutorial\n- [The GNU Awk User's Guide](https://www.gnu.org/software/gawk/manual/gawk.html) - Comprehensive documentation for mastering `awk`\n- [Software Carpentry Shell Lessons](https://swcarpentry.github.io/shell-novice/) - Excellent tutorials designed for researchers\n",
    "supporting": [
      "02-unix-command-line_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}