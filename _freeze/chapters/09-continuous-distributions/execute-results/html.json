{
  "hash": "eec5e8ff4963f53f957daf7ebcb80e8d",
  "result": {
    "engine": "knitr",
    "markdown": "# Continuous Probability Distributions {#sec-continuous-distributions}\n\n\n::: {.cell}\n\n:::\n\n\n## From Discrete to Continuous\n\nMany quantities we measure—weight, concentration, time, temperature—can take any value within a range, not just discrete counts. These continuous random variables require a different mathematical treatment. Instead of probability mass functions that assign probabilities to specific values, we use probability density functions (PDFs) where probabilities come from integrating over intervals.\n\nFor a continuous random variable, the probability that it falls within an interval $[a, b]$ is:\n\n$$P(a \\leq X \\leq b) = \\int_a^b f(x) \\, dx$$\n\nwhere $f(x)$ is the probability density function. The total area under the density curve must equal 1:\n\n$$\\int_{-\\infty}^{\\infty} f(x) \\, dx = 1$$\n\nNote that for continuous variables, the probability of any exact value is zero—only intervals have non-zero probability.\n\n## The Uniform Distribution\n\nThe simplest continuous distribution is the uniform distribution, where all values in an interval are equally likely. If $X$ is uniformly distributed between $a$ and $b$:\n\n$$f(x) = \\frac{1}{b-a} \\quad \\text{for } a \\leq x \\leq b$$\n\nThe mean is $(a+b)/2$ and the variance is $(b-a)^2/12$.\n\n![](../images/prob.019.jpeg){fig-align=\"center\"}\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Uniform distribution between 0 and 10\nx <- seq(0, 10, length.out = 100)\nplot(x, dunif(x, min = 0, max = 10), type = \"l\", lwd = 2,\n     xlab = \"x\", ylab = \"Density\",\n     main = \"Uniform Distribution (0, 10)\")\n```\n\n::: {.cell-output-display}\n![](09-continuous-distributions_files/figure-html/unnamed-chunk-2-1.png){width=576}\n:::\n:::\n\n\nThe uniform distribution is often used to model random number generation and situations where no outcome is favored over another within a range.\n\n## The Exponential Distribution\n\nThe exponential distribution models waiting times between events in a Poisson process—the time until the next event when events occur randomly at a constant rate $\\lambda$. Its density function is:\n\n$$f(x) = \\lambda e^{-\\lambda x} \\quad \\text{for } x \\geq 0$$\n\nThe mean is $1/\\lambda$ and the variance is $1/\\lambda^2$.\n\n![](../images/prob.020.jpeg){fig-align=\"center\"}\n\nIf a radioactive isotope has a decay rate of $\\lambda = 0.1$ per minute, the time until the next decay follows an exponential distribution with mean 10 minutes.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Exponential distributions with different rates\nx <- seq(0, 30, length.out = 200)\nplot(x, dexp(x, rate = 0.1), type = \"l\", lwd = 2, col = \"blue\",\n     xlab = \"Time\", ylab = \"Density\",\n     main = \"Exponential Distribution (λ = 0.1)\")\n```\n\n::: {.cell-output-display}\n![](09-continuous-distributions_files/figure-html/unnamed-chunk-3-1.png){width=576}\n:::\n:::\n\n\nA key property of the exponential distribution is memorylessness: the probability of waiting another $t$ units does not depend on how long you have already waited.\n\n## The Gamma Distribution\n\nThe gamma distribution generalizes the exponential distribution to model the waiting time until the $r$th event in a Poisson process. Its density function involves two parameters: shape $r$ and rate $\\lambda$:\n\n$$f(x) = \\frac{\\lambda^r x^{r-1} e^{-\\lambda x}}{(r-1)!} \\quad \\text{for } x \\geq 0$$\n\nThe mean is $r/\\lambda$ and the variance is $r/\\lambda^2$.\n\nWhen $r = 1$, the gamma distribution reduces to the exponential. As $r$ increases, the distribution becomes more symmetric and bell-shaped.\n\n## The Normal (Gaussian) Distribution\n\nThe normal distribution is the most important continuous distribution in statistics. Its distinctive bell-shaped curve appears throughout nature, and the Central Limit Theorem explains why: the sum of many independent random effects tends toward normality regardless of the underlying distributions.\n\nThe normal distribution is characterized by two parameters: mean $\\mu$ (center) and standard deviation $\\sigma$ (spread):\n\n$$f(x) = \\frac{1}{\\sigma\\sqrt{2\\pi}} e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}$$\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Normal distributions with different parameters\nx <- seq(-10, 15, length.out = 200)\nplot(x, dnorm(x, mean = 0, sd = 1), type = \"l\", lwd = 2, col = \"blue\",\n     ylim = c(0, 0.5), xlab = \"x\", ylab = \"Density\",\n     main = \"Normal Distributions\")\nlines(x, dnorm(x, mean = 0, sd = 2), lwd = 2, col = \"red\")\nlines(x, dnorm(x, mean = 5, sd = 1), lwd = 2, col = \"darkgreen\")\nlegend(\"topright\", \n       legend = c(\"μ=0, σ=1\", \"μ=0, σ=2\", \"μ=5, σ=1\"),\n       col = c(\"blue\", \"red\", \"darkgreen\"), lwd = 2)\n```\n\n::: {.cell-output-display}\n![](09-continuous-distributions_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\n### Properties of the Normal Distribution\n\nThe normal distribution is symmetric around its mean. The mean, median, and mode are all equal. About 68% of the distribution falls within one standard deviation of the mean, 95% within two standard deviations, and 99.7% within three standard deviations (the \"68-95-99.7 rule\").\n\n![The 68-95-99.7 rule for the normal distribution.](../images/week_2.010.jpeg){#fig-normal-rule fig-align=\"center\"}\n\n### Estimating Normal Parameters\n\nThe mean of a sample provides an estimate of the population mean:\n\n$$\\bar{x} = \\frac{1}{n}\\sum_{i=1}^{n}x_i$$\n\nThe sample variance estimates the population variance:\n\n$$s^2 = \\frac{1}{n-1}\\sum_{i=1}^{n}(x_i - \\bar{x})^2$$\n\nNote the $n-1$ in the denominator (called Bessel's correction), which provides an unbiased estimate of the population variance.\n\n### The Standard Normal Distribution\n\nWhen $\\mu = 0$ and $\\sigma = 1$, we have the standard normal distribution. Any normal variable can be converted to standard normal by subtracting the mean and dividing by the standard deviation:\n\n$$Z = \\frac{X - \\mu}{\\sigma}$$\n\nThis standardization, called computing a z-score, allows us to compare values from different normal distributions and to use tables of standard normal probabilities.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Probability calculations with the normal distribution\n# P(X < 1.96) for standard normal\npnorm(1.96)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.9750021\n```\n\n\n:::\n\n```{.r .cell-code}\n# P(-1.96 < X < 1.96)\npnorm(1.96) - pnorm(-1.96)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.9500042\n```\n\n\n:::\n\n```{.r .cell-code}\n# What value has 97.5% of the distribution below it?\nqnorm(0.975)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1.959964\n```\n\n\n:::\n:::\n\n\nThe values 1.96 and -1.96 are particularly important because they bound the middle 95% of the standard normal distribution, forming the basis for 95% confidence intervals.\n\n### Z-Scores\n\nA z-score is a standardized value that tells us how many standard deviations an observation is from the mean:\n\n$$z_i = \\frac{x_i - \\bar{x}}{s}$$\n\nZ-scores allow us to compare values from different normal distributions on a common scale. This is particularly useful when comparing measurements that have different units or very different magnitudes—for example, comparing the relative leg length of mice versus elephants.\n\n### Why the Normal Distribution is Special in Biology\n\nThe normal distribution appears throughout biology because many biological traits are influenced by numerous factors, each contributing a small effect. This is particularly evident in quantitative genetics.\n\n![The genetic model of complex traits explains why many biological measurements are normally distributed.](../images/week_2.017.jpeg){#fig-genetic-model fig-align=\"center\"}\n\nConsider a trait influenced by multiple genes. If we have many loci, each with a small additive effect, the distribution of trait values in a population will approximate a normal distribution—even if the contribution at each locus follows a simple Mendelian pattern.\n\n![The distribution of genotypes in an F2 cross approaches normality as the number of contributing loci increases.](../images/week_2.018.jpeg){#fig-f2-distribution fig-align=\"center\"}\n\nThis connection between many small independent effects and the normal distribution is formalized by the Central Limit Theorem, which we explore below.\n\n### Checking Normality\n\nMany statistical methods assume normally distributed data. Before applying these methods, you should check whether the assumption is reasonable.\n\nVisual methods include histograms and Q-Q (quantile-quantile) plots:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Generate some data\nset.seed(42)\nnormal_data <- rnorm(200, mean = 50, sd = 10)\nskewed_data <- rexp(200, rate = 0.1)\n\npar(mfrow = c(1, 2))\n\n# Q-Q plot for normal data\nqqnorm(normal_data, main = \"Normal Data\")\nqqline(normal_data, col = \"red\")\n\n# Q-Q plot for skewed data\nqqnorm(skewed_data, main = \"Skewed Data\")\nqqline(skewed_data, col = \"red\")\n```\n\n::: {.cell-output-display}\n![](09-continuous-distributions_files/figure-html/unnamed-chunk-6-1.png){width=768}\n:::\n:::\n\n\nIn a Q-Q plot, normally distributed data should fall approximately along the diagonal line. Systematic deviations indicate non-normality.\n\n## The Central Limit Theorem\n\nThe Central Limit Theorem (CLT) states that the sampling distribution of the mean approaches normality as sample size increases, regardless of the shape of the population distribution. This is why the normal distribution appears so frequently in statistics—we often work with means or other sums of random variables.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Demonstrate CLT with exponential distribution\nset.seed(123)\n\n# Exponential distribution is quite skewed\npar(mfrow = c(2, 2))\n\n# Original distribution\nhist(rexp(10000, rate = 1), breaks = 50, main = \"Original: Exponential\",\n     xlab = \"x\", col = \"lightblue\")\n\n# Means of samples of size 5\nmeans_5 <- replicate(10000, mean(rexp(5, rate = 1)))\nhist(means_5, breaks = 50, main = \"Means of n=5\",\n     xlab = \"Sample Mean\", col = \"lightblue\")\n\n# Means of samples of size 30\nmeans_30 <- replicate(10000, mean(rexp(30, rate = 1)))\nhist(means_30, breaks = 50, main = \"Means of n=30\",\n     xlab = \"Sample Mean\", col = \"lightblue\")\n\n# Means of samples of size 100\nmeans_100 <- replicate(10000, mean(rexp(100, rate = 1)))\nhist(means_100, breaks = 50, main = \"Means of n=100\",\n     xlab = \"Sample Mean\", col = \"lightblue\")\n```\n\n::: {.cell-output-display}\n![](09-continuous-distributions_files/figure-html/unnamed-chunk-7-1.png){width=864}\n:::\n:::\n\n\nEven though the exponential distribution is strongly right-skewed, the distribution of sample means becomes increasingly normal as sample size grows. This is the Central Limit Theorem in action.\n\n## Summary of Distribution Functions in R\n\nR provides consistent functions for all distributions:\n\n| Distribution | d (density) | p (cumulative) | q (quantile) | r (random) |\n|:-------------|:------------|:---------------|:-------------|:-----------|\n| Uniform | `dunif` | `punif` | `qunif` | `runif` |\n| Exponential | `dexp` | `pexp` | `qexp` | `rexp` |\n| Normal | `dnorm` | `pnorm` | `qnorm` | `rnorm` |\n| Gamma | `dgamma` | `pgamma` | `qgamma` | `rgamma` |\n\nUnderstanding these distributions and their properties prepares you for statistical inference, where we use sampling distributions to make probabilistic statements about population parameters.\n\n## Additional Resources\n\n- @irizarry2019introduction - Excellent chapters on probability distributions and the Central Limit Theorem\n- @logan2010biostatistical - Comprehensive treatment of distributions in the context of biological statistics\n",
    "supporting": [
      "09-continuous-distributions_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}