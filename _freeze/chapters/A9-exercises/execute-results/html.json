{
  "hash": "3ccdf73fb8ef1d98ee2601875c0c0987",
  "result": {
    "engine": "knitr",
    "markdown": "# Practice Exercises {#sec-exercises}\n\n\n::: {.cell}\n\n:::\n\n\nThis appendix contains practice exercises organized by chapter. Working through these exercises will help reinforce the concepts covered in the main text and develop your practical skills in data analysis. Each section corresponds to a chapter in the book, with cross-references to help you review relevant material.\n\n::: {.callout-tip}\n## Learning by Doing\nThe best way to learn programming and statistics is through hands-on practice. Don't just read these exercises—work through them at your computer. When you get stuck, refer back to the corresponding chapter, consult the help documentation, and experiment with different approaches.\n:::\n\n## Unix and Command Line Exercises {#sec-ex-unix}\n\nThese exercises correspond to @sec-unix. Save a digital record of your work so that you can study it later if you need to.\n\n### Exercise U.1: Basic Navigation and File Operations\n\nOpen up a terminal and execute the following using Unix commands:\n\n1. Print your working directory using `pwd`\n2. Navigate to a directory somewhere below your home directory where you want to practice writing files\n3. Make 5 directories called `dir_1`, `dir_2`, `dir_3`, `dir_4`, and `dir_5` using `mkdir`\n4. Within each of those directories, create files called `file_1.txt`, `file_2.txt`, and `file_3.txt` using `touch`\n5. Open `file_1.txt` in `dir_1` using a plain text editor (such as `nano` or `vim`), type a few words, and save it\n6. Print `file_1.txt` in `dir_1` to the terminal using `cat`\n7. Delete all files in `dir_3` using `rm`\n8. List all of the contents of your current directory line-by-line using `ls -l`\n9. Delete `dir_3` using `rmdir`\n\n### Exercise U.2: Working with Data Files\n\nFor this exercise, create a sample tab-separated file or download a GFF file from a genomics database.\n\n1. Navigate to `dir_1`\n2. Copy a data file (using its absolute path) to your current directory\n3. Delete the copy that is in your current directory, then copy it again using a relative path this time\n4. Use at least 3 different Unix commands to examine all or parts of your data file (try `cat`, `head`, `tail`, `less`, and `wc`)\n5. What is the file size? Use `ls -lh` to find out\n6. How many lines does the file have? Use `wc -l`\n7. How many lines begin with a specific pattern (like a chromosome name)? Use `grep -c \"^pattern\"`\n8. How many unique entries are there in a specific column? Use `cut` and `sort -u | wc -l`\n9. Sort the file based on reverse numeric order in a specific field using `sort -k -nr`\n10. Capture specific fields and write to a new file using `cut` and redirection\n11. Replace all instances of one string with another using `sed 's/old/new/g'`\n\n### Exercise U.3: Building Pipelines\n\nPractice combining commands with pipes to answer questions about your data:\n\n1. Count the number of unique values in the third column of a tab-separated file:\n   ```bash\n   cut -f3 data.tsv | sort | uniq | wc -l\n   ```\n\n2. Find all lines containing a pattern, extract specific columns, and sort the results:\n   ```bash\n   grep \"pattern\" data.tsv | cut -f1,2,5 | sort -k3,3 -n\n   ```\n\n3. Create a pipeline that filters rows based on a condition, extracts columns, and saves to a new file\n\n4. Use `awk` to filter rows where a numeric column exceeds a threshold:\n   ```bash\n   awk '$5 > 1000 {print $1, $2, $5}' data.tsv\n   ```\n\n### Exercise U.4: File Permissions and Scripts\n\n1. Create a simple shell script that prints \"Hello, World!\" and the current date\n2. Try to run the script—what error do you get?\n3. Make the script executable using `chmod +x`\n4. Run the script and verify it works\n5. Examine the permissions of various files in your system using `ls -l`\n6. Practice changing permissions using both symbolic notation (`chmod u+x`) and octal notation (`chmod 755`)\n\n---\n\n## R and RStudio Exercises {#sec-ex-r}\n\nThese exercises correspond to @sec-r-rstudio.\n\n### Exercise R.1: Exploring RStudio\n\nTake a few minutes to familiarize yourself with the RStudio environment:\n\n1. Locate the four main panes:\n   - The code editor (top left)\n   - The workspace and history (top right)\n   - The plots and files window (bottom right)\n   - The R console (bottom left)\n\n2. In the plots and files window, click on the Packages and Help tabs to see what they offer\n\n3. See what types of new files can be made in RStudio by clicking File → New File\n\n4. Open a new R script and a new R Markdown file to see the difference\n\n### Exercise R.2: Basic Mathematics in R\n\nInsert a code chunk and complete the following tasks:\n\n1. Add and subtract numbers\n2. Multiply and divide numbers\n3. Raise a number to a power using the `^` symbol\n4. Create a more complex equation involving all of these operations to convince yourself that R follows the normal priority of mathematical evaluation (PEMDAS)\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Example:\n(4 + 3 * 2^2) / 5 - 1\n```\n:::\n\n\n### Exercise R.3: Assigning Variables and Functions\n\n1. Assign three variables using basic mathematical operations\n2. Take the log of your three variables using `log()`\n3. Use the `print()` function to display your most complex variable\n4. Use the `c()` (concatenate) function combined with `paste()` to create and print a sentence\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Example:\nx <- 10\ny <- x * 2\nz <- sqrt(x + y)\nprint(paste(\"The value of z is\", z))\n```\n:::\n\n\n### Exercise R.4: Vectors and Factors\n\n1. Create a numeric vector using the `c()` function with at least 5 elements\n2. Create a character vector and convert it to a factor using `as.factor()`\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Example:\nvec1 <- c(\"control\", \"treatment\", \"control\", \"treatment\", \"control\")\nfac1 <- as.factor(vec1)\nprint(fac1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] control   treatment control   treatment control  \nLevels: control treatment\n```\n\n\n:::\n\n```{.r .cell-code}\nlevels(fac1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"control\"   \"treatment\"\n```\n\n\n:::\n:::\n\n\n3. Use `str()` and `class()` to evaluate your variables\n4. What is the difference between a character vector and a factor?\n\n### Exercise R.5: Basic Statistics\n\n1. Create a numeric vector with at least 10 elements\n2. Calculate the `mean()`, `sd()`, `sum()`, `length()`, and `var()` of your vector\n3. Use the `log()` and `sqrt()` functions on your vector\n4. What happens when you try to apply `mean()` to a factor? Try it and explain the result\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Example:\nmy_vector <- c(12, 15, 18, 22, 25, 28, 31, 35, 38, 42)\nmean(my_vector)\nsd(my_vector)\n```\n:::\n\n\n### Exercise R.6: Creating Sequences and Random Sampling\n\nSet the random seed for reproducibility, then:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(42)\n```\n:::\n\n\n1. Create a vector with 100 elements using `seq()` and calculate the mean and standard deviation\n2. Create a variable and `sample()` it with equal probability—experiment with the `size` and `replace` arguments\n3. Create a normally distributed variable of 10000 elements using `rnorm()`, then sample that distribution with and without replacement\n4. Use `hist()` to plot your normally distributed variable\n\n### Exercise R.7: Basic Visualization\n\nCreate visualizations with proper axis labels and colors:\n\n1. Create a sequence variable using `seq()` and make two different plots by changing the `type` argument (`\"p\"` for points, `\"l\"` for lines, `\"b\"` for both)\n\n2. Create a normally distributed variable using `rnorm()` and make histograms with different `breaks` values—what does `breaks` control?\n\n3. Use `par(mfrow = c(2, 2))` to create a 2×2 grid of plots\n\n\n::: {.cell}\n\n```{.r .cell-code}\npar(mfrow = c(2, 2))\nx <- seq(1, 100, by = 1)\nplot(x, type = \"p\", main = \"Points\", col = \"blue\")\nplot(x, type = \"l\", main = \"Lines\", col = \"red\")\ny <- rnorm(1000)\nhist(y, breaks = 10, main = \"10 Breaks\", col = \"lightblue\")\nhist(y, breaks = 50, main = \"50 Breaks\", col = \"lightgreen\")\n```\n:::\n\n\n### Exercise R.8: Creating Data Frames\n\n1. Create a data frame with at least three columns: one character/factor, one numeric, and one logical\n2. Assign row names to your data frame using `rownames()`\n3. Examine your data frame structure using `str()`\n4. Calculate the mean of each numeric variable\n5. Use `head()` and `tail()` to view portions of your data frame\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Example:\ntreatment <- c(\"control\", \"low\", \"medium\", \"high\", \"control\", \"low\")\nresponse <- c(12.3, 15.6, 18.9, 24.2, 11.8, 16.1)\nsignificant <- c(FALSE, TRUE, TRUE, TRUE, FALSE, TRUE)\nmy_data <- data.frame(treatment, response, significant)\nstr(my_data)\n```\n:::\n\n\n### Exercise R.9: Data Import and Indexing\n\n1. Create a simple CSV file or use a built-in dataset like `iris`\n2. Use `read.csv()` to read in your file (or access `iris` directly)\n3. Use `str()` and `head()` to examine the data structure\n4. Use `$` and `[ ]` operators to select different parts of the data frame\n5. Create a plot of two numeric variables\n6. Use `tapply()` to calculate summary statistics grouped by a categorical variable\n7. Export your data frame using `write.csv()`\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Example with iris:\ndata(iris)\nstr(iris)\nhead(iris)\niris$Sepal.Length[1:5]  # First 5 sepal lengths\niris[1:3, ]  # First 3 rows\nplot(iris$Sepal.Length, iris$Petal.Length, col = iris$Species)\ntapply(iris$Sepal.Length, iris$Species, mean)\n```\n:::\n\n\n### Exercise R.10: Understanding Object Types\n\nExplore how R handles different data types:\n\n1. Create variables of different classes: numeric, character, logical, and factor\n2. What happens when you try to perform arithmetic on character data?\n3. Experiment with type coercion using `as.numeric()`, `as.character()`, and `as.factor()`\n4. What happens when you add a character element to a numeric vector?\n\n---\n\n## Markdown and LaTeX Exercises {#sec-ex-markdown}\n\nThese exercises correspond to @sec-markdown.\n\n### Exercise M.1: R Markdown Basics\n\nCreate a new R Markdown document and practice:\n\n1. Creating headers at different levels using `#`, `##`, and `###`\n2. Making text *italic* and **bold**\n3. Creating ordered and unordered lists\n4. Inserting a hyperlink\n5. Creating a code chunk that generates output\n6. Knitting the document to HTML\n\n### Exercise M.2: Code Chunk Options\n\nExperiment with code chunk options:\n\n1. Create a code chunk with `echo=TRUE` and `eval=TRUE` (default behavior)\n2. Create the same code chunk with `echo=FALSE`—what happens?\n3. Try `eval=FALSE`—what happens?\n4. Use `fig.width` and `fig.height` to control plot dimensions\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Example chunk with options\nx <- rnorm(100)\nhist(x, col = \"steelblue\", main = \"Random Normal Data\")\n```\n\n::: {.cell-output-display}\n![](A9-exercises_files/figure-html/unnamed-chunk-10-1.png){width=576}\n:::\n:::\n\n\n### Exercise M.3: LaTeX Mathematical Notation\n\nPractice writing equations in LaTeX:\n\n1. Write the equation for the mean: $\\bar{x} = \\frac{1}{n}\\sum_{i=1}^{n} x_i$\n2. Write the equation for variance\n3. Write the normal distribution probability density function\n4. Use both inline math (`$...$`) and display math (`$$...$$`)\n\n### Exercise M.4: Tables in Markdown\n\nCreate tables using:\n\n1. Basic Markdown table syntax\n2. The `kable()` function from the `knitr` package\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(knitr)\nkable(head(iris, 5))\n```\n:::\n\n\n---\n\n## Tidy Data and Data Wrangling Exercises {#sec-ex-tidy}\n\nThese exercises correspond to @sec-tidy-data.\n\n### Exercise T.1: Tidyverse Basics\n\nLoad the tidyverse and practice with a dataset:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\ndata(mpg)\n```\n:::\n\n\n1. Convert the `mpg` data frame to a tibble using `as_tibble()`\n2. What differences do you notice in how it prints?\n3. Use `glimpse()` to get an overview of the data\n\n### Exercise T.2: The Five dplyr Verbs\n\nUsing the `mpg` dataset, practice each core verb:\n\n1. **filter()**: Select only cars with highway mpg greater than 30\n2. **select()**: Choose only the manufacturer, model, and highway mpg columns\n3. **arrange()**: Sort by highway mpg in descending order\n4. **mutate()**: Create a new column that calculates the ratio of highway to city mpg\n5. **summarize()**: Calculate the mean highway mpg for the entire dataset\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Example solutions:\nmpg |> filter(hwy > 30)\nmpg |> select(manufacturer, model, hwy)\nmpg |> arrange(desc(hwy))\nmpg |> mutate(hwy_city_ratio = hwy / cty)\nmpg |> summarize(mean_hwy = mean(hwy))\n```\n:::\n\n\n### Exercise T.3: Grouping and Summarizing\n\n1. Group the `mpg` data by manufacturer and calculate the mean highway mpg for each\n2. Find the manufacturer with the highest average highway mpg\n3. Count how many models each manufacturer has in the dataset\n4. Calculate both mean and standard deviation of highway mpg by vehicle class\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmpg |>\n  group_by(manufacturer) |>\n  summarize(\n    mean_hwy = mean(hwy),\n    n_models = n()\n  ) |>\n  arrange(desc(mean_hwy))\n```\n:::\n\n\n### Exercise T.4: Data Wrangling Pipeline\n\nConstruct a pipeline that:\n\n1. Filters for a subset of manufacturers (e.g., \"audi\", \"toyota\", \"honda\")\n2. Selects relevant columns\n3. Creates a new calculated column\n4. Groups by a categorical variable\n5. Summarizes with multiple statistics\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmpg |>\n  filter(manufacturer %in% c(\"audi\", \"toyota\", \"honda\")) |>\n  select(manufacturer, model, year, cty, hwy) |>\n  mutate(avg_mpg = (cty + hwy) / 2) |>\n  group_by(manufacturer) |>\n  summarize(\n    mean_mpg = mean(avg_mpg),\n    sd_mpg = sd(avg_mpg),\n    n = n()\n  )\n```\n:::\n\n\n### Exercise T.5: Reshaping Data\n\nPractice with `pivot_longer()` and `pivot_wider()`:\n\n1. Create a wide dataset with measurements across multiple time points\n2. Convert it to long format using `pivot_longer()`\n3. Convert it back to wide format using `pivot_wider()`\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Example wide data\nwide_data <- tibble(\n  sample = c(\"A\", \"B\", \"C\"),\n  time_0 = c(10, 12, 8),\n  time_1 = c(15, 18, 12),\n  time_2 = c(22, 25, 18)\n)\n\n# Convert to long format\nlong_data <- wide_data |>\n  pivot_longer(\n    cols = starts_with(\"time\"),\n    names_to = \"timepoint\",\n    values_to = \"measurement\"\n  )\nprint(long_data)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 9 × 3\n  sample timepoint measurement\n  <chr>  <chr>           <dbl>\n1 A      time_0             10\n2 A      time_1             15\n3 A      time_2             22\n4 B      time_0             12\n5 B      time_1             18\n6 B      time_2             25\n7 C      time_0              8\n8 C      time_1             12\n9 C      time_2             18\n```\n\n\n:::\n:::\n\n\n### Exercise T.6: Joining Data\n\nCreate two related tibbles and practice joins:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Sample data\nexperiments <- tibble(\n  sample_id = c(\"S1\", \"S2\", \"S3\", \"S4\"),\n  treatment = c(\"control\", \"low\", \"medium\", \"high\")\n)\n\nmeasurements <- tibble(\n  sample_id = c(\"S1\", \"S1\", \"S2\", \"S3\"),\n  replicate = c(1, 2, 1, 1),\n  value = c(10.2, 9.8, 15.3, 18.7)\n)\n```\n:::\n\n\n1. Perform a `left_join()` to add treatment information to measurements\n2. Use `anti_join()` to find samples that have no measurements\n3. Explain the difference between `inner_join()` and `full_join()`\n\n---\n\n## Data Visualization Exercises {#sec-ex-viz}\n\nThese exercises correspond to @sec-visualization.\n\n### Exercise V.1: Basic ggplot2\n\nCreate your first ggplot visualizations:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ggplot2)\ndata(mpg)\n\n# Basic scatterplot\nggplot(mpg, aes(x = displ, y = hwy)) +\n  geom_point()\n```\n:::\n\n\n1. Create a scatterplot of engine displacement vs. highway mpg\n2. Add color based on vehicle class\n3. Add appropriate axis labels and a title\n4. Try different themes (`theme_minimal()`, `theme_classic()`, `theme_bw()`)\n\n### Exercise V.2: Geometric Objects\n\nPractice with different geoms:\n\n1. Create a histogram of highway mpg using `geom_histogram()`\n2. Create a boxplot of highway mpg by vehicle class using `geom_boxplot()`\n3. Create a bar chart showing the count of vehicles by manufacturer using `geom_bar()`\n4. Create a line plot (use a time series dataset or create synthetic data)\n\n### Exercise V.3: Aesthetic Mappings\n\nExplore different aesthetic mappings:\n\n1. Map a continuous variable to color in a scatterplot\n2. Map a categorical variable to shape\n3. Set fixed aesthetics (like `size = 3`) outside of `aes()`\n4. What is the difference between mapping a variable to an aesthetic inside `aes()` versus setting a fixed value outside?\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Mapped aesthetic (variable determines color)\nggplot(mpg, aes(x = displ, y = hwy, color = class)) +\n  geom_point(size = 3)\n\n# Fixed aesthetic (all points same color)\nggplot(mpg, aes(x = displ, y = hwy)) +\n  geom_point(color = \"steelblue\", size = 3)\n```\n:::\n\n\n### Exercise V.4: Faceting\n\nPractice creating small multiples:\n\n1. Create a scatterplot faceted by vehicle class using `facet_wrap()`\n2. Create a grid of plots using `facet_grid()` with two variables\n3. Experiment with the `scales` argument to allow different axis scales per facet\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(mpg, aes(x = displ, y = hwy)) +\n  geom_point() +\n  facet_wrap(~ class, nrow = 2)\n```\n:::\n\n\n### Exercise V.5: Combining Layers\n\nBuild complex visualizations by layering:\n\n1. Create a scatterplot with a smoothed trend line\n2. Add both points and a regression line\n3. Use different colors for points and the trend line\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(mpg, aes(x = displ, y = hwy)) +\n  geom_point(aes(color = class), alpha = 0.7) +\n  geom_smooth(method = \"lm\", color = \"black\", se = TRUE) +\n  labs(\n    title = \"Engine Size vs. Fuel Efficiency\",\n    x = \"Engine Displacement (L)\",\n    y = \"Highway MPG\",\n    color = \"Vehicle Class\"\n  ) +\n  theme_minimal()\n```\n:::\n\n\n### Exercise V.6: Publication-Quality Figures\n\nCreate a polished figure suitable for publication:\n\n1. Choose an appropriate chart type for your data\n2. Add informative labels (title, subtitle, caption, axis labels)\n3. Use an appropriate color palette\n4. Adjust theme elements for clarity\n5. Save the figure using `ggsave()` with appropriate dimensions and resolution\n\n---\n\n## Probability Exercises {#sec-ex-prob}\n\nThese exercises correspond to @sec-probability.\n\n### Exercise P.1: Simulating Coin Flips\n\nUse R to simulate coin flips:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(123)\n```\n:::\n\n\n1. Simulate 100 fair coin flips using `rbinom()` or `sample()`\n2. Calculate the proportion of heads\n3. Repeat with 1000 and 10000 flips—how does the proportion change?\n4. Create a histogram of results from many simulations\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Simulate coin flips\nn_flips <- 1000\nflips <- rbinom(n_flips, size = 1, prob = 0.5)\nmean(flips)  # Proportion of heads (1s)\n\n# Or using sample\nflips <- sample(c(\"H\", \"T\"), n_flips, replace = TRUE)\nmean(flips == \"H\")\n```\n:::\n\n\n### Exercise P.2: Binomial Distribution\n\nExplore the binomial distribution:\n\n1. Use `rbinom()` to simulate 1000 experiments, each with 20 coin flips\n2. Create a histogram of the number of heads per experiment\n3. What is the most common outcome? Does this match your expectation?\n4. Change the probability to simulate an unfair coin\n5. How does the distribution change with 200 or 2000 flips per experiment?\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# 1000 experiments, 20 flips each, fair coin\nset.seed(42)\nresults <- rbinom(1000, size = 20, prob = 0.5)\nhist(results, breaks = 20, col = \"steelblue\",\n     main = \"Number of Heads in 20 Flips\",\n     xlab = \"Number of Heads\")\n```\n\n::: {.cell-output-display}\n![](A9-exercises_files/figure-html/unnamed-chunk-24-1.png){width=576}\n:::\n:::\n\n\n### Exercise P.3: The Birthday Problem\n\nUse Monte Carlo simulation to explore the birthday problem:\n\n1. Write a function that simulates whether any two people in a group share a birthday\n2. Estimate the probability for groups of size 10, 23, and 50\n3. Plot the probability as a function of group size\n4. At what group size does the probability exceed 50%?\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Birthday simulation function\nsame_birthday <- function(n, B = 10000) {\n  matches <- replicate(B, {\n    birthdays <- sample(1:365, n, replace = TRUE)\n    any(duplicated(birthdays))\n  })\n  mean(matches)\n}\n\n# Test for different group sizes\nsizes <- 2:50\nprobs <- sapply(sizes, same_birthday)\nplot(sizes, probs, type = \"l\",\n     xlab = \"Group Size\", ylab = \"Probability of Shared Birthday\")\nabline(h = 0.5, col = \"red\", lty = 2)\n```\n:::\n\n\n### Exercise P.4: Conditional Probability\n\nExplore conditional probability with card simulations:\n\n1. Create a virtual deck of 52 cards\n2. Calculate the probability of drawing a King\n3. Given that the first card drawn is a King, what is the probability the second card is also a King?\n4. Use simulation to verify your calculation\n\n### Exercise P.5: The Monty Hall Problem\n\nSimulate the Monty Hall problem:\n\n1. Write a function that simulates one round of the game\n2. Compare the win rate when you stick versus when you switch\n3. Run 10,000 simulations for each strategy\n4. Does switching really double your chances?\n\n---\n\n## Discrete Distributions Exercises {#sec-ex-discrete}\n\nThese exercises correspond to @sec-discrete-distributions.\n\n### Exercise D.1: Binomial Distribution Properties\n\n1. For n = 20 and p = 0.3, calculate the probability of exactly 5 successes using `dbinom()`\n2. Calculate the probability of 5 or fewer successes using `pbinom()`\n3. Generate 1000 random values from this distribution using `rbinom()`\n4. Compare your simulated mean and variance to the theoretical values (np and np(1-p))\n\n\n::: {.cell}\n\n```{.r .cell-code}\nn <- 20\np <- 0.3\n\n# Exact probability of 5 successes\ndbinom(5, size = n, prob = p)\n\n# Cumulative probability (≤5)\npbinom(5, size = n, prob = p)\n\n# Simulation\nset.seed(42)\nsamples <- rbinom(1000, size = n, prob = p)\nmean(samples)  # Compare to n*p\nvar(samples)   # Compare to n*p*(1-p)\n```\n:::\n\n\n### Exercise D.2: Poisson Distribution\n\nThe Poisson distribution models counts of rare events:\n\n1. If a hospital averages 4 emergency admissions per hour, what is the probability of exactly 6 admissions in one hour?\n2. What is the probability of 10 or more admissions?\n3. Simulate 1000 hours and plot the distribution\n4. How does the distribution change when λ is small (0.5) versus large (20)?\n\n### Exercise D.3: Comparing Distributions\n\nGenerate samples from binomial and Poisson distributions with similar means and compare:\n\n1. Create histograms side by side\n2. When does the Poisson approximate the binomial well?\n3. What happens as n increases and p decreases while np stays constant?\n\n---\n\n## Continuous Distributions Exercises {#sec-ex-continuous}\n\nThese exercises correspond to @sec-continuous-distributions.\n\n### Exercise C.1: Normal Distribution\n\nExplore the normal distribution:\n\n1. Generate 10,000 values from a normal distribution with μ = 100 and σ = 15\n2. Calculate the sample mean and standard deviation—how close are they to the parameters?\n3. What proportion of values fall within 1, 2, and 3 standard deviations of the mean?\n4. Compare to the theoretical values (68-95-99.7 rule)\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(42)\nx <- rnorm(10000, mean = 100, sd = 15)\n\n# Sample statistics\nmean(x)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 99.83036\n```\n\n\n:::\n\n```{.r .cell-code}\nsd(x)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 15.09202\n```\n\n\n:::\n\n```{.r .cell-code}\n# Proportions within SDs\nmean(abs(x - mean(x)) < 1*sd(x))  # Within 1 SD\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.6829\n```\n\n\n:::\n\n```{.r .cell-code}\nmean(abs(x - mean(x)) < 2*sd(x))  # Within 2 SDs\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.9553\n```\n\n\n:::\n\n```{.r .cell-code}\nmean(abs(x - mean(x)) < 3*sd(x))  # Within 3 SDs\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.9973\n```\n\n\n:::\n\n```{.r .cell-code}\n# Visualize\nhist(x, breaks = 50, freq = FALSE, col = \"lightblue\",\n     main = \"Normal Distribution (μ=100, σ=15)\")\ncurve(dnorm(x, mean = 100, sd = 15), add = TRUE, col = \"red\", lwd = 2)\n```\n\n::: {.cell-output-display}\n![](A9-exercises_files/figure-html/unnamed-chunk-27-1.png){width=672}\n:::\n:::\n\n\n### Exercise C.2: Standard Normal and Z-Scores\n\n1. Convert a set of raw scores to z-scores\n2. Use `pnorm()` to find the proportion of values below a given z-score\n3. Use `qnorm()` to find the z-score corresponding to a given percentile\n4. What z-score corresponds to the 95th percentile?\n\n### Exercise C.3: Log-Normal Distribution\n\nMany biological measurements follow a log-normal distribution:\n\n1. Generate data from a log-normal distribution using `rlnorm()`\n2. Plot the original data—notice the right skew\n3. Take the log and plot again—it should appear normal\n4. When might you encounter log-normal data in biology?\n\n---\n\n## Hypothesis Testing Exercises {#sec-ex-hypothesis}\n\nThese exercises correspond to @sec-hypothesis-testing and @sec-t-tests.\n\n### Exercise H.1: One-Sample t-test\n\n1. Generate a sample of 30 observations from a normal distribution with mean 105 and SD 15\n2. Test whether the mean differs significantly from 100\n3. Interpret the p-value and confidence interval\n4. What happens to the p-value when you increase the sample size?\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(42)\nsample_data <- rnorm(30, mean = 105, sd = 15)\nt.test(sample_data, mu = 100)\n```\n:::\n\n\n### Exercise H.2: Two-Sample t-test\n\nCreate a dummy dataset with one continuous and one categorical variable:\n\n1. Draw samples of 100 observations from two normal distributions with slightly different means but equal standard deviations\n2. Perform a two-sample t-test\n3. Visualize the data with a boxplot\n4. Repeat with sample sizes of 10, 100, and 1000—how does sample size affect the results?\n5. What happens when you make the means more different?\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(42)\ngroup_a <- rnorm(100, mean = 10, sd = 2)\ngroup_b <- rnorm(100, mean = 11, sd = 2)\n\n# Combine into data frame\ndata <- data.frame(\n  value = c(group_a, group_b),\n  group = rep(c(\"A\", \"B\"), each = 100)\n)\n\n# t-test\nt.test(value ~ group, data = data)\n\n# Visualization\nboxplot(value ~ group, data = data)\n```\n:::\n\n\n### Exercise H.3: Chi-Square Test\n\nTest for Hardy-Weinberg equilibrium:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Observed genotype counts\nAA_counts <- 50\nAa_counts <- 40\naa_counts <- 10\n\n# Calculate allele frequencies\ntotal <- AA_counts + Aa_counts + aa_counts\np <- (2*AA_counts + Aa_counts) / (2*total)\nq <- 1 - p\n\n# Expected counts under HWE\nexpected <- c(p^2, 2*p*q, q^2) * total\n\n# Chi-square test\nobserved <- c(AA_counts, Aa_counts, aa_counts)\nchisq.test(observed, p = c(p^2, 2*p*q, q^2))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tChi-squared test for given probabilities\n\ndata:  observed\nX-squared = 0.22676, df = 2, p-value = 0.8928\n```\n\n\n:::\n:::\n\n\n1. Modify the observed counts and see how it affects the test result\n2. What genotype frequencies would indicate strong departure from HWE?\n\n---\n\n## Linear Models Exercises {#sec-ex-linear}\n\nThese exercises correspond to @sec-correlation, @sec-regression, and @sec-anova.\n\n### Exercise L.1: Correlation\n\n1. Create two correlated variables using simulation\n2. Calculate the Pearson correlation coefficient\n3. Create a scatterplot and add the correlation value\n4. What happens to the correlation when you add outliers?\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(42)\nx <- rnorm(50)\ny <- 0.7*x + rnorm(50, sd = 0.5)  # Correlated with x\n\ncor(x, y)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.8400841\n```\n\n\n:::\n\n```{.r .cell-code}\nplot(x, y, main = paste(\"Correlation:\", round(cor(x, y), 2)))\nabline(lm(y ~ x), col = \"red\")\n```\n\n::: {.cell-output-display}\n![](A9-exercises_files/figure-html/unnamed-chunk-31-1.png){width=576}\n:::\n:::\n\n\n### Exercise L.2: Simple Linear Regression\n\n1. Using a dataset of your choice, fit a linear model with `lm()`\n2. Examine the model summary\n3. Create a scatterplot with the regression line\n4. Plot the residuals—do they appear randomly distributed?\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Example with built-in data\ndata(mtcars)\nmodel <- lm(mpg ~ wt, data = mtcars)\nsummary(model)\n\n# Regression plot\nplot(mpg ~ wt, data = mtcars)\nabline(model, col = \"red\")\n\n# Residual plot\nplot(model$fitted.values, model$residuals)\nabline(h = 0, col = \"red\", lty = 2)\n```\n:::\n\n\n### Exercise L.3: ANOVA\n\nPerform a one-way ANOVA:\n\n1. Using the `iris` dataset, test whether sepal length differs among species\n2. Examine the ANOVA table\n3. If significant, which pairs of species differ? Use a post-hoc test\n4. Visualize the differences with boxplots\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# One-way ANOVA\nmodel <- aov(Sepal.Length ~ Species, data = iris)\nsummary(model)\n\n# Post-hoc test\nTukeyHSD(model)\n\n# Visualization\nboxplot(Sepal.Length ~ Species, data = iris)\n```\n:::\n\n\n### Exercise L.4: Two-Way Factorial ANOVA\n\nFor a dataset with two categorical predictors:\n\n1. Fit a factorial model including the interaction\n2. Interpret the main effects and interaction\n3. Create an interaction plot\n4. What does a significant interaction mean biologically?\n\n---\n\n## Advanced Topics Exercises {#sec-ex-advanced}\n\n### Exercise A.1: Principal Component Analysis\n\nUsing a multivariate dataset:\n\n1. Standardize the variables\n2. Perform PCA using `prcomp()`\n3. Examine the proportion of variance explained by each component\n4. Create a biplot showing samples and variable loadings\n5. How many components would you retain?\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# PCA on iris data (excluding species)\niris_numeric <- iris[, 1:4]\npca <- prcomp(iris_numeric, scale. = TRUE)\n\n# Variance explained\nsummary(pca)\n\n# Biplot\nbiplot(pca)\n```\n:::\n\n\n### Exercise A.2: Logistic Regression\n\nFit a logistic regression model:\n\n1. Create or load a dataset with a binary outcome\n2. Fit a logistic regression model using `glm()` with `family = binomial`\n3. Interpret the coefficients in terms of odds ratios\n4. Calculate predicted probabilities for new observations\n\n### Exercise A.3: Bootstrapping\n\nEstimate confidence intervals using bootstrapping:\n\n1. Draw a sample of 50 observations\n2. Create 1000 bootstrap samples\n3. Calculate the mean for each bootstrap sample\n4. Construct a 95% confidence interval from the bootstrap distribution\n5. Compare to the theoretical confidence interval\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(42)\noriginal_sample <- rnorm(50, mean = 100, sd = 15)\n\n# Bootstrap\nn_bootstrap <- 1000\nbootstrap_means <- replicate(n_bootstrap, {\n  boot_sample <- sample(original_sample, replace = TRUE)\n  mean(boot_sample)\n})\n\n# Confidence interval\nquantile(bootstrap_means, c(0.025, 0.975))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n    2.5%    97.5% \n 94.6165 103.9561 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Compare to theoretical\nt.test(original_sample)$conf.int\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1]  94.55623 104.37361\nattr(,\"conf.level\")\n[1] 0.95\n```\n\n\n:::\n\n```{.r .cell-code}\n# Visualize\nhist(bootstrap_means, breaks = 30, main = \"Bootstrap Distribution of Means\",\n     col = \"lightblue\")\nabline(v = mean(original_sample), col = \"red\", lwd = 2)\n```\n\n::: {.cell-output-display}\n![](A9-exercises_files/figure-html/unnamed-chunk-35-1.png){width=576}\n:::\n:::\n\n\n---\n\n## Version Control with Git {#sec-ex-git}\n\n### Exercise G.1: Git Basics\n\nPractice basic Git operations:\n\n1. Create a new directory and initialize a Git repository with `git init`\n2. Create a new file and check the status with `git status`\n3. Stage the file with `git add`\n4. Create a commit with `git commit -m \"message\"`\n5. Make changes and commit again\n6. View the commit history with `git log`\n\n### Exercise G.2: Working with GitHub\n\n1. Create a GitHub account if you don't have one\n2. Create a new repository on GitHub\n3. Clone it to your local machine with `git clone`\n4. Make changes, commit, and push with `git push`\n5. Practice the pull-commit-push workflow\n\n### Exercise G.3: Collaboration\n\nIf working with a partner:\n\n1. Clone your partner's repository\n2. Make a change and push it\n3. Have your partner pull the changes\n4. Practice resolving a merge conflict\n\n---\n\n## Summary\n\nThese exercises cover the core topics in statistical analysis and data science. As you work through them, remember:\n\n- **Practice regularly**: Skills improve with consistent practice\n- **Experiment**: Try variations on the exercises to deepen understanding\n- **Consult documentation**: Use `?function_name` and online resources\n- **Debug systematically**: When code doesn't work, check each step\n- **Collaborate**: Discuss problems with classmates and colleagues\n\nThe exercises are designed to build skills progressively. Return to earlier exercises as you learn new techniques—you may find more elegant solutions with your growing knowledge.\n",
    "supporting": [
      "A9-exercises_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}