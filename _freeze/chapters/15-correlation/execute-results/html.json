{
  "hash": "fc832e6df9d5f7f243f5e71c8bfe918a",
  "result": {
    "engine": "knitr",
    "markdown": "# Correlation {#sec-correlation}\n\n\n::: {.cell}\n\n:::\n\n\n## Measuring Association\n\nWhen two variables vary together, we say they are correlated. Understanding whether and how variables are related is fundamental to science—it helps us identify potential causal relationships, make predictions, and understand systems.\n\nCorrelation quantifies the strength and direction of the linear relationship between two variables. A positive correlation means that high values of one variable tend to occur with high values of the other. A negative correlation means that high values of one variable tend to occur with low values of the other.\n\n## Covariance\n\nThe **covariance** measures how two variables vary together. If X and Y tend to be above their means at the same time (and below their means at the same time), the covariance is positive. If one tends to be above its mean when the other is below, the covariance is negative.\n\n$$Cov(X, Y) = \\frac{\\sum_{i=1}^{n}(x_i - \\bar{x})(y_i - \\bar{y})}{n-1}$$\n\nThe problem with covariance is that its magnitude depends on the scales of X and Y, making it hard to interpret. Is a covariance of 100 strong or weak? It depends entirely on the units of measurement.\n\n## Pearson's Correlation Coefficient\n\nThe **Pearson correlation coefficient** standardizes covariance by dividing by the product of the standard deviations:\n\n$$r = \\frac{Cov(X, Y)}{s_X s_Y} = \\frac{\\sum_{i=1}^{n}(x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum(x_i - \\bar{x})^2 \\sum(y_i - \\bar{y})^2}}$$\n\nThis produces a value between -1 and +1:\n\n- $r = 1$: perfect positive linear relationship\n- $r = -1$: perfect negative linear relationship  \n- $r = 0$: no linear relationship\n\n![](../images/images_4b.006.jpeg){fig-align=\"center\"}\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Examples of different correlations\nset.seed(42)\nn <- 100\n\npar(mfrow = c(2, 2))\n\n# Strong positive\nx1 <- rnorm(n)\ny1 <- 0.9 * x1 + rnorm(n, sd = 0.4)\nplot(x1, y1, main = paste(\"r =\", round(cor(x1, y1), 2)), pch = 19, col = \"blue\")\n\n# Moderate negative\ny2 <- -0.6 * x1 + rnorm(n, sd = 0.8)\nplot(x1, y2, main = paste(\"r =\", round(cor(x1, y2), 2)), pch = 19, col = \"red\")\n\n# No correlation\ny3 <- rnorm(n)\nplot(x1, y3, main = paste(\"r =\", round(cor(x1, y3), 2)), pch = 19, col = \"gray\")\n\n# Non-linear relationship (correlation misleading)\nx4 <- runif(n, -3, 3)\ny4 <- x4^2 + rnorm(n, sd = 0.5)\nplot(x4, y4, main = paste(\"r =\", round(cor(x4, y4), 2), \"(non-linear!)\"), \n     pch = 19, col = \"purple\")\n```\n\n::: {.cell-output-display}\n![](15-correlation_files/figure-html/unnamed-chunk-2-1.png){width=768}\n:::\n:::\n\n\n## Anscombe's Quartet\n\nFrancis Anscombe created a famous set of four datasets that all have nearly identical statistical properties—same means, variances, correlations, and regression lines—yet look completely different when plotted. This demonstrates why visualization is essential.\n\n![](../images/images_4b.007.jpeg){fig-align=\"center\"}\n\nAlways plot your data before calculating correlations. The correlation coefficient captures only linear relationships and can be misleading for non-linear patterns.\n\n## Testing Correlation\n\nThe `cor.test()` function tests whether a correlation is significantly different from zero:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Example: zebrafish length and weight\nset.seed(123)\nlength <- rnorm(50, mean = 2.5, sd = 0.5)\nweight <- 10 * length^2 + rnorm(50, sd = 5)\n\ncor.test(length, weight)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tPearson's product-moment correlation\n\ndata:  length and weight\nt = 29.857, df = 48, p-value < 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.9546054 0.9853063\nsample estimates:\n     cor \n0.974118 \n```\n\n\n:::\n:::\n\n\nThe null hypothesis is that the population correlation is zero ($H_0: \\rho = 0$). A small p-value indicates evidence of a non-zero correlation.\n\n## Parametric Assumptions\n\nPearson's correlation assumes that both variables are normally distributed (or at least that the relationship is linear and homoscedastic). When these assumptions are violated, nonparametric alternatives may be more appropriate.\n\n## Nonparametric Correlation\n\n**Spearman's rank correlation** replaces values with their ranks before calculating correlation. It measures monotonic (consistently increasing or decreasing) rather than strictly linear relationships and is robust to outliers.\n\n**Kendall's tau** is another rank-based measure that counts concordant and discordant pairs. It is particularly appropriate for small samples or data with many ties.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Compare methods on non-normal data\nset.seed(42)\nx <- rexp(30, rate = 0.1)\ny <- x + rexp(30, rate = 0.2)\n\ncat(\"Pearson:\", round(cor(x, y, method = \"pearson\"), 3), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nPearson: 0.764 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Spearman:\", round(cor(x, y, method = \"spearman\"), 3), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSpearman: 0.808 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Kendall:\", round(cor(x, y, method = \"kendall\"), 3), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nKendall: 0.623 \n```\n\n\n:::\n:::\n\n\n## Correlation Is Not Causation\n\nA correlation between X and Y might arise because X causes Y, because Y causes X, because a third variable Z causes both, or simply by chance. Correlation alone cannot distinguish these possibilities.\n\nTo establish causation, you need experimental manipulation (changing X and observing Y), temporal precedence (X occurs before Y), and ruling out confounding variables. Observational correlations are valuable for generating hypotheses but insufficient for establishing causal relationships.\n",
    "supporting": [
      "15-correlation_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}