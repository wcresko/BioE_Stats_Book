{
  "hash": "32b951bbd05784912fc7da061b9b9e46",
  "result": {
    "engine": "knitr",
    "markdown": "# Correlation {#sec-correlation}\n\n\n::: {.cell}\n\n:::\n\n\n## Measuring Association\n\nWhen two variables vary together, we say they are correlated. Understanding whether and how variables are related is fundamental to science—it helps us identify potential causal relationships, make predictions, and understand systems.\n\nCorrelation quantifies the strength and direction of the linear relationship between two variables. A positive correlation means that high values of one variable tend to occur with high values of the other. A negative correlation means that high values of one variable tend to occur with low values of the other.\n\n## Covariance\n\nThe **covariance** measures how two variables vary together. If X and Y tend to be above their means at the same time (and below their means at the same time), the covariance is positive. If one tends to be above its mean when the other is below, the covariance is negative.\n\n$$Cov(X, Y) = \\frac{\\sum_{i=1}^{n}(x_i - \\bar{x})(y_i - \\bar{y})}{n-1}$$\n\n### Understanding Covariance Visually\n\nThe covariance formula involves products of deviations from the mean. Consider each point in a scatterplot:\n\n- If a point is in the **upper-right quadrant** (both X and Y above their means), the product of deviations is positive\n- If a point is in the **lower-left quadrant** (both below their means), the product is also positive\n- If a point is in the **upper-left** or **lower-right** (one above, one below), the product is negative\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Visualizing covariance with quadrants\nset.seed(42)\nx <- rnorm(50, mean = 10, sd = 2)\ny <- 0.8 * x + rnorm(50, sd = 1.5)\n\n# Calculate means\nmean_x <- mean(x)\nmean_y <- mean(y)\n\n# Plot with quadrants\nplot(x, y, pch = 19, col = \"steelblue\",\n     xlab = \"X\", ylab = \"Y\",\n     main = \"Covariance: Products of Deviations from Means\")\nabline(v = mean_x, h = mean_y, col = \"red\", lty = 2, lwd = 2)\n\n# Add quadrant labels\ntext(max(x) - 0.5, max(y) - 0.5, \"(+)(+) = +\", col = \"darkgreen\", cex = 0.9)\ntext(min(x) + 0.5, min(y) + 0.5, \"(-)(−) = +\", col = \"darkgreen\", cex = 0.9)\ntext(max(x) - 0.5, min(y) + 0.5, \"(+)(−) = −\", col = \"darkred\", cex = 0.9)\ntext(min(x) + 0.5, max(y) - 0.5, \"(−)(+) = −\", col = \"darkred\", cex = 0.9)\n```\n\n::: {.cell-output-display}\n![](15-correlation_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# Report covariance\ncat(\"Covariance:\", round(cov(x, y), 3), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nCovariance: 3.479 \n```\n\n\n:::\n:::\n\n\nWhen points cluster in the positive quadrants (upper-right and lower-left), the covariance is positive. When points cluster in the negative quadrants, the covariance is negative. When points are evenly distributed, covariance is near zero.\n\nThe problem with covariance is that its magnitude depends on the scales of X and Y, making it hard to interpret. Is a covariance of 100 strong or weak? It depends entirely on the units of measurement.\n\n## Pearson's Correlation Coefficient\n\nThe **Pearson correlation coefficient** standardizes covariance by dividing by the product of the standard deviations:\n\n$$r = \\frac{Cov(X, Y)}{s_X s_Y} = \\frac{\\sum_{i=1}^{n}(x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum(x_i - \\bar{x})^2 \\sum(y_i - \\bar{y})^2}}$$\n\nThis produces a value between -1 and +1:\n\n- $r = 1$: perfect positive linear relationship\n- $r = -1$: perfect negative linear relationship  \n- $r = 0$: no linear relationship\n\n![](../images/images_4b.006.jpeg){fig-align=\"center\"}\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Examples of different correlations\nset.seed(42)\nn <- 100\n\npar(mfrow = c(2, 2))\n\n# Strong positive\nx1 <- rnorm(n)\ny1 <- 0.9 * x1 + rnorm(n, sd = 0.4)\nplot(x1, y1, main = paste(\"r =\", round(cor(x1, y1), 2)), pch = 19, col = \"blue\")\n\n# Moderate negative\ny2 <- -0.6 * x1 + rnorm(n, sd = 0.8)\nplot(x1, y2, main = paste(\"r =\", round(cor(x1, y2), 2)), pch = 19, col = \"red\")\n\n# No correlation\ny3 <- rnorm(n)\nplot(x1, y3, main = paste(\"r =\", round(cor(x1, y3), 2)), pch = 19, col = \"gray\")\n\n# Non-linear relationship (correlation misleading)\nx4 <- runif(n, -3, 3)\ny4 <- x4^2 + rnorm(n, sd = 0.5)\nplot(x4, y4, main = paste(\"r =\", round(cor(x4, y4), 2), \"(non-linear!)\"), \n     pch = 19, col = \"purple\")\n```\n\n::: {.cell-output-display}\n![](15-correlation_files/figure-html/unnamed-chunk-3-1.png){width=768}\n:::\n:::\n\n\n## Anscombe's Quartet\n\nFrancis Anscombe created a famous set of four datasets that all have nearly identical statistical properties—same means, variances, correlations, and regression lines—yet look completely different when plotted. This demonstrates why visualization is essential.\n\n![](../images/images_4b.007.jpeg){fig-align=\"center\"}\n\nAlways plot your data before calculating correlations. The correlation coefficient captures only linear relationships and can be misleading for non-linear patterns.\n\n## Testing Correlation\n\nThe `cor.test()` function tests whether a correlation is significantly different from zero:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Example: zebrafish length and weight\nset.seed(123)\nlength <- rnorm(50, mean = 2.5, sd = 0.5)\nweight <- 10 * length^2 + rnorm(50, sd = 5)\n\ncor.test(length, weight)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tPearson's product-moment correlation\n\ndata:  length and weight\nt = 29.857, df = 48, p-value < 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.9546054 0.9853063\nsample estimates:\n     cor \n0.974118 \n```\n\n\n:::\n:::\n\n\nThe null hypothesis is that the population correlation is zero ($H_0: \\rho = 0$). A small p-value indicates evidence of a non-zero correlation.\n\n## Sample Correlation as a Random Variable\n\nJust like the sample mean, the sample correlation coefficient is a **random variable**—it varies from sample to sample. If we could repeatedly draw samples from the same population and compute r for each, we would get a distribution of r values centered around the true population correlation $\\rho$.\n\nThis sampling variability has important implications for interpretation. A sample correlation of r = 0.3 from a small study might arise even when the true correlation is zero (or is actually 0.5). Understanding this uncertainty is essential for proper inference.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Demonstrate sampling variability of correlation\nset.seed(42)\n\n# True population parameters\nrho_true <- 0.5  # True population correlation\nn_small <- 20\nn_large <- 100\nn_reps <- 1000\n\n# Function to generate correlated data\ngenerate_correlated_data <- function(n, rho) {\n  x <- rnorm(n)\n  y <- rho * x + sqrt(1 - rho^2) * rnorm(n)\n  return(cor(x, y))\n}\n\n# Generate sampling distributions for different sample sizes\nr_small <- replicate(n_reps, generate_correlated_data(n_small, rho_true))\nr_large <- replicate(n_reps, generate_correlated_data(n_large, rho_true))\n\n# Plot sampling distributions\npar(mfrow = c(1, 2))\n\nhist(r_small, breaks = 30, col = \"lightblue\",\n     main = paste(\"Sampling Distribution of r\\n(n =\", n_small, \")\"),\n     xlab = \"Sample Correlation\", xlim = c(-0.2, 1))\nabline(v = rho_true, col = \"red\", lwd = 2, lty = 2)\nabline(v = mean(r_small), col = \"blue\", lwd = 2)\nlegend(\"topleft\", legend = c(paste(\"True ρ =\", rho_true),\n                              paste(\"Mean r =\", round(mean(r_small), 3))),\n       col = c(\"red\", \"blue\"), lty = c(2, 1), lwd = 2, cex = 0.8)\n\nhist(r_large, breaks = 30, col = \"lightgreen\",\n     main = paste(\"Sampling Distribution of r\\n(n =\", n_large, \")\"),\n     xlab = \"Sample Correlation\", xlim = c(-0.2, 1))\nabline(v = rho_true, col = \"red\", lwd = 2, lty = 2)\nabline(v = mean(r_large), col = \"blue\", lwd = 2)\nlegend(\"topleft\", legend = c(paste(\"True ρ =\", rho_true),\n                              paste(\"Mean r =\", round(mean(r_large), 3))),\n       col = c(\"red\", \"blue\"), lty = c(2, 1), lwd = 2, cex = 0.8)\n```\n\n::: {.cell-output-display}\n![](15-correlation_files/figure-html/unnamed-chunk-5-1.png){width=864}\n:::\n:::\n\n\nNotice the dramatic difference in variability. With n = 20, sample correlations range widely around the true value—sometimes even appearing negative when the true correlation is 0.5! With n = 100, the distribution is much tighter, and our sample r is a more reliable estimate of $\\rho$.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Quantify the variability\ncat(\"True population correlation: ρ =\", rho_true, \"\\n\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTrue population correlation: ρ = 0.5 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"With n =\", n_small, \":\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nWith n = 20 :\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"  Mean of sample r:\", round(mean(r_small), 3), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  Mean of sample r: 0.483 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"  SD of sample r:\", round(sd(r_small), 3), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  SD of sample r: 0.181 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"  95% of samples give r between\", round(quantile(r_small, 0.025), 3),\n    \"and\", round(quantile(r_small, 0.975), 3), \"\\n\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  95% of samples give r between 0.056 and 0.781 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"With n =\", n_large, \":\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nWith n = 100 :\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"  Mean of sample r:\", round(mean(r_large), 3), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  Mean of sample r: 0.497 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"  SD of sample r:\", round(sd(r_large), 3), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  SD of sample r: 0.075 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"  95% of samples give r between\", round(quantile(r_large, 0.025), 3),\n    \"and\", round(quantile(r_large, 0.975), 3), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  95% of samples give r between 0.347 and 0.634 \n```\n\n\n:::\n:::\n\n\n::: {.callout-tip}\n## Practical Implications\n\n1. **Small samples yield unreliable correlations**: With n < 30, sample r can differ substantially from $\\rho$\n2. **Confidence intervals are essential**: Report CIs to communicate uncertainty, not just the point estimate\n3. **Replication matters**: A single study with r = 0.4 (n = 25) is consistent with true correlations anywhere from near-zero to quite strong\n4. **Publication bias distorts the literature**: Studies with \"significant\" correlations are more likely to be published, inflating effect sizes in the literature\n:::\n\n### Fisher's Z-Transformation\n\nThe sampling distribution of r is not symmetric, especially when $\\rho$ is far from zero. Fisher's z-transformation stabilizes the variance and makes the distribution approximately normal:\n\n$$z = \\frac{1}{2} \\ln\\left(\\frac{1 + r}{1 - r}\\right) = \\text{arctanh}(r)$$\n\nThe standard error of z is approximately $\\frac{1}{\\sqrt{n-3}}$, which depends only on sample size—not on the true correlation. This transformation is used to construct confidence intervals for correlation and to compare correlations across groups.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Fisher's z-transformation for confidence interval\nr_observed <- 0.6\nn <- 50\n\n# Transform to z\nz <- atanh(r_observed)  # Same as 0.5 * log((1 + r) / (1 - r))\nse_z <- 1 / sqrt(n - 3)\n\n# 95% CI in z scale\nz_lower <- z - 1.96 * se_z\nz_upper <- z + 1.96 * se_z\n\n# Transform back to r scale\nr_lower <- tanh(z_lower)\nr_upper <- tanh(z_upper)\n\ncat(\"Observed r:\", r_observed, \"with n =\", n, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nObserved r: 0.6 with n = 50 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"95% CI for ρ: [\", round(r_lower, 3), \",\", round(r_upper, 3), \"]\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n95% CI for ρ: [ 0.386 , 0.753 ]\n```\n\n\n:::\n:::\n\n\n## Parametric Assumptions\n\nPearson's correlation assumes that both variables are normally distributed (or at least that the relationship is linear and homoscedastic). When these assumptions are violated, nonparametric alternatives may be more appropriate.\n\n## Nonparametric Correlation\n\n**Spearman's rank correlation** replaces values with their ranks before calculating correlation. It measures monotonic (consistently increasing or decreasing) rather than strictly linear relationships and is robust to outliers.\n\n**Kendall's tau** is another rank-based measure that counts concordant and discordant pairs. It is particularly appropriate for small samples or data with many ties.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Compare methods on non-normal data\nset.seed(42)\nx <- rexp(30, rate = 0.1)\ny <- x + rexp(30, rate = 0.2)\n\ncat(\"Pearson:\", round(cor(x, y, method = \"pearson\"), 3), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nPearson: 0.764 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Spearman:\", round(cor(x, y, method = \"spearman\"), 3), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSpearman: 0.808 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Kendall:\", round(cor(x, y, method = \"kendall\"), 3), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nKendall: 0.623 \n```\n\n\n:::\n:::\n\n\n## Correlation Is Not Causation\n\nA correlation between X and Y might arise because X causes Y, because Y causes X, because a third variable Z causes both, or simply by chance. Correlation alone cannot distinguish these possibilities.\n\nTo establish causation, you need experimental manipulation (changing X and observing Y), temporal precedence (X occurs before Y), and ruling out confounding variables. Observational correlations are valuable for generating hypotheses but insufficient for establishing causal relationships.\n\n## Interpreting Correlation Magnitude\n\nWhile there are no universal standards, these guidelines provide rough interpretation:\n\n| |r| | Interpretation |\n|:--|:--|\n| 0.00 - 0.19 | Negligible |\n| 0.20 - 0.39 | Weak |\n| 0.40 - 0.59 | Moderate |\n| 0.60 - 0.79 | Strong |\n| 0.80 - 1.00 | Very strong |\n\nContext matters greatly. In physics, correlations below 0.99 might be disappointing. In psychology or ecology, correlations of 0.3 can be considered meaningful.\n\n### Coefficient of Determination\n\nThe square of the correlation coefficient, $r^2$, is called the **coefficient of determination**. It represents the proportion of variance in one variable that is explained by its linear relationship with the other.\n\nIf $r = 0.7$, then $r^2 = 0.49$, meaning about 49% of the variance in Y is explained by its relationship with X. This leaves 51% unexplained—due to other factors, measurement error, or the relationship not being perfectly linear.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Visualize explained vs unexplained variance\nset.seed(123)\nx <- 1:50\ny <- 2 * x + rnorm(50, sd = 15)\n\nr <- cor(x, y)\nr_squared <- r^2\n\ncat(\"Correlation (r):\", round(r, 3), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nCorrelation (r): 0.901 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"R-squared (r²):\", round(r_squared, 3), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nR-squared (r²): 0.812 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Variance explained:\", round(r_squared * 100, 1), \"%\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nVariance explained: 81.2 %\n```\n\n\n:::\n:::\n\n\n## Common Pitfalls with Correlation\n\n::: {.callout-warning}\n## Watch Out For These Mistakes\n\n1. **Restricted range**: If you only sample part of the range of X or Y, correlation will appear weaker than it truly is\n2. **Outliers**: A single extreme point can dramatically inflate or deflate the correlation\n3. **Non-linearity**: Correlation only measures linear relationships; a perfect curved relationship can have r ≈ 0\n4. **Aggregation effects**: Correlations computed on grouped data (e.g., country averages) are often much stronger than correlations on individual data (ecological fallacy)\n5. **Confounding**: A third variable may create a spurious correlation between X and Y\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Demonstrating the effect of outliers\nset.seed(42)\nx_base <- rnorm(30)\ny_base <- rnorm(30)\n\npar(mfrow = c(1, 2))\n\n# Without outlier\nplot(x_base, y_base, pch = 19, col = \"blue\",\n     main = paste(\"Without outlier: r =\", round(cor(x_base, y_base), 3)),\n     xlab = \"X\", ylab = \"Y\", xlim = c(-3, 5), ylim = c(-3, 5))\n\n# With outlier\nx_out <- c(x_base, 4)\ny_out <- c(y_base, 4)\nplot(x_out, y_out, pch = 19, col = c(rep(\"blue\", 30), \"red\"),\n     main = paste(\"With outlier: r =\", round(cor(x_out, y_out), 3)),\n     xlab = \"X\", ylab = \"Y\", xlim = c(-3, 5), ylim = c(-3, 5))\n```\n\n::: {.cell-output-display}\n![](15-correlation_files/figure-html/unnamed-chunk-10-1.png){width=768}\n:::\n:::\n\n\nA single outlier has shifted the correlation from near zero to moderately positive. Always visualize your data and consider robust alternatives like Spearman's correlation when outliers are present.\n\n## Summary\n\nCorrelation quantifies the strength and direction of linear relationships between variables:\n\n- Covariance measures how variables move together, but depends on units\n- Pearson's r standardizes covariance to range from -1 to +1\n- Spearman and Kendall provide robust rank-based alternatives\n- Correlation does not imply causation\n- Always visualize data and check for non-linearity and outliers\n\n## Practice Exercises\n\nFor hands-on practice with correlation and linear models, see @sec-ex-linear in the Practice Exercises appendix. The exercises include:\n\n- Calculating and interpreting correlation coefficients\n- Creating scatterplots with trend lines\n- Understanding the effect of outliers on correlation\n- Simple linear regression analysis\n\n## Additional Resources\n\n- @logan2010biostatistical - Detailed coverage of correlation with biological examples\n- @james2023islr - Excellent discussion of correlation in the context of statistical learning\n",
    "supporting": [
      "15-correlation_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}