{
  "hash": "0257e0de41fb67aa88d6c221c28f4d95",
  "result": {
    "engine": "knitr",
    "markdown": "# High Performance Computing {#sec-hpc}\n\n\n::: {.cell}\n\n:::\n\n\n## Why High Performance Computing?\n\nAs datasets grow and analyses become more complex, your laptop may not be enough. Genomic datasets can be terabytes in size. Simulations might require millions of iterations. Machine learning models may need to be trained on billions of data points. High Performance Computing (HPC) provides the resources to tackle problems that exceed what personal computers can handle.\n\nHPC systems come in different forms. Computing clusters—collections of interconnected computers working together—are common at universities and research institutions. Cloud computing services from Amazon (AWS), Google, and Microsoft (Azure) provide on-demand access to computing resources. GPUs (Graphics Processing Units) accelerate certain types of parallel computations.\n\n## Computing Clusters\n\nA typical university computing cluster consists of a head node (login node) where you submit jobs, and many compute nodes where jobs actually run. The head node manages the queue of waiting jobs and allocates resources.\n\nAt the University of Oregon, the Talapas cluster provides researchers with access to thousands of CPU cores and specialized hardware including GPUs. Access requires an account, which graduate students can request through their research groups.\n\n## Connecting to Remote Systems\n\nYou access remote systems through SSH (Secure Shell):\n\n```bash\nssh username@talapas-login.uoregon.edu\n```\n\nAfter authenticating, you are in a terminal on the remote system, working in a Unix environment just as you would locally. File transfer between your computer and the cluster uses `scp` or `rsync`:\n\n```bash\n# Copy file to cluster\nscp data.csv username@talapas-login.uoregon.edu:~/project/\n\n# Copy file from cluster\nscp username@talapas-login.uoregon.edu:~/project/results.csv ./\n```\n\n## Job Schedulers\n\nYou do not run computationally intensive jobs directly on the login node. Instead, you submit them to a job scheduler (like SLURM on Talapas) that queues jobs and runs them when resources become available.\n\nA basic SLURM submission script:\n\n```bash\n#!/bin/bash\n#SBATCH --job-name=my_analysis\n#SBATCH --account=your_account\n#SBATCH --partition=short\n#SBATCH --time=2:00:00\n#SBATCH --nodes=1\n#SBATCH --ntasks=1\n#SBATCH --cpus-per-task=4\n#SBATCH --mem=16G\n\n# Load required software\nmodule load R/4.2.1\n\n# Run your script\nRscript my_analysis.R\n```\n\nSubmit with `sbatch script.sh`. Check job status with `squeue -u username`. Cancel jobs with `scancel job_id`.\n\n## Resource Requests\n\nJobs must request resources: time, memory, and CPUs. Request enough to complete your job but not so much that it waits unnecessarily in the queue. Start with conservative estimates and adjust based on actual usage.\n\nCommon SLURM directives:\n- `--time`: Maximum runtime (job is killed if exceeded)\n- `--mem`: Memory per node\n- `--cpus-per-task`: Number of CPU cores\n- `--array`: For running many similar jobs\n\n## Environment Modules\n\nHPC systems use environment modules to manage software. Instead of installing software yourself, you load pre-installed modules:\n\n```bash\nmodule avail              # List available software\nmodule load R/4.2.1       # Load R\nmodule load python/3.10   # Load Python\nmodule list               # Show loaded modules\nmodule purge              # Unload all modules\n```\n\n## Running R on a Cluster\n\nR scripts run non-interactively on clusters. Instead of using RStudio, you write your analysis as a script and run it with `Rscript`:\n\n```r\n# my_analysis.R\nlibrary(tidyverse)\n\n# Read data\ndata <- read.csv(\"large_dataset.csv\")\n\n# Perform analysis\nresults <- data |>\n  group_by(category) |>\n  summarize(mean_value = mean(value))\n\n# Save results\nwrite.csv(results, \"output.csv\")\n```\n\n## Parallelization in R\n\nR can use multiple CPU cores to speed up computations. The `parallel` package provides tools for parallel processing:\n\n```r\nlibrary(parallel)\n\n# Detect number of cores\nn_cores <- detectCores()\n\n# Create a cluster\ncl <- makeCluster(n_cores - 1)\n\n# Parallel apply\nresults <- parLapply(cl, data_list, analysis_function)\n\n# Stop the cluster\nstopCluster(cl)\n```\n\nThe `future` and `furrr` packages provide more user-friendly parallelization.\n\n## Cloud Computing\n\nCloud platforms (AWS, Google Cloud, Azure) offer computing resources on demand. You pay for what you use rather than having fixed resources.\n\nAdvantages:\n- Scale up quickly when needed\n- No hardware maintenance\n- Access to specialized hardware (GPUs, large memory instances)\n\nDisadvantages:\n- Costs can accumulate quickly\n- Requires learning platform-specific tools\n- Data transfer can be slow and expensive\n\n## Best Practices\n\n**Start small**: Test your code on a small subset before running on full data.\n\n**Use version control**: Keep your scripts in Git for reproducibility.\n\n**Document everything**: Future you (and others) need to understand what you did.\n\n**Save intermediate results**: If a job fails, you do not want to start from scratch.\n\n**Monitor resource usage**: Check how much time and memory your jobs actually use.\n\n**Clean up**: Delete unnecessary files; storage is shared.\n\n## Getting Help\n\nMost HPC systems have documentation and support staff. At UO, Research Advanced Computing Services (RACS) provides Talapas documentation and consultations. Reading the documentation before asking questions will make your interactions more productive.\n\nLearning to use HPC effectively takes time, but the ability to run large-scale analyses is essential for modern bioengineering research.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}