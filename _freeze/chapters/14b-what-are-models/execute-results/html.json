{
  "hash": "dc05246f0c98f0b3dd4689f3f1f8236a",
  "result": {
    "engine": "knitr",
    "markdown": "# What are Models? {#sec-what-are-models}\n\n\n::: {.cell}\n\n:::\n\n\n## The Essence of Modeling\n\nA **model** is a simplified representation of reality that helps us understand, explain, or predict phenomena. In statistics and data science, models are mathematical relationships between variables that capture patterns in data while ignoring irrelevant details.\n\n> \"All models are wrong, but some are useful.\" — George Box\n\nThis famous quote captures the fundamental truth of modeling: no model perfectly represents reality, but a good model can still provide valuable insights and predictions.\n\n## Two Cultures of Statistical Modeling\n\nIn his influential 2001 paper, Leo Breiman identified **two cultures** in statistical modeling:\n\n**The Data Modeling Culture** (traditional statistics):\n\n- Assumes data are generated by a specific stochastic model\n- Focus on estimating parameters and testing hypotheses\n- Emphasis on interpretability and understanding mechanisms\n- Examples: linear regression, ANOVA, generalized linear models\n\n**The Algorithmic Modeling Culture** (machine learning):\n\n- Treats the data-generating mechanism as unknown\n- Focus on predictive accuracy\n- Emphasis on performance over interpretability\n- Examples: random forests, neural networks, boosting\n\nBoth approaches have value. Traditional models excel at inference and explanation; algorithmic approaches often produce better predictions. The choice depends on whether your goal is understanding or prediction.\n\n## The General Linear Model Framework\n\nMost statistical models you encounter are special cases of the **General Linear Model (GLM)**:\n\n$$Y = X\\beta + \\epsilon$$\n\nwhere:\n\n- $Y$ is the response variable (what we want to predict/explain)\n- $X$ is the design matrix of predictor variables\n- $\\beta$ are coefficients we estimate\n- $\\epsilon$ is random error\n\nThis framework unifies many methods:\n\n| Method | Response Type | Predictors |\n|:-------|:-------------|:-----------|\n| One-sample t-test | Continuous | None (intercept only) |\n| Two-sample t-test | Continuous | One categorical (2 levels) |\n| ANOVA | Continuous | One or more categorical |\n| Simple regression | Continuous | One continuous |\n| Multiple regression | Continuous | Multiple (any type) |\n| ANCOVA | Continuous | Mixed categorical and continuous |\n\nThe beauty of this unified framework is that once you understand regression, you understand the entire family of linear models.\n\n## Components of a Statistical Model\n\nEvery statistical model specifies:\n\n1. **Response variable**: What we want to predict or explain\n2. **Predictor variables**: Information we use to make predictions\n3. **Functional form**: How predictors relate to the response (linear, polynomial, etc.)\n4. **Error structure**: Assumptions about variability (normally distributed, etc.)\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Visualizing a simple linear model\nset.seed(42)\nx <- 1:50\ny <- 2 + 0.5*x + rnorm(50, sd = 3)\n\nplot(x, y, pch = 19, col = \"steelblue\",\n     main = \"Components of a Linear Model\",\n     xlab = \"Predictor (X)\", ylab = \"Response (Y)\")\n\n# Fitted line (the model)\nmodel <- lm(y ~ x)\nabline(model, col = \"red\", lwd = 2)\n\n# Show residuals for a few points\nsegments(x[c(10,25,40)], y[c(10,25,40)],\n         x[c(10,25,40)], fitted(model)[c(10,25,40)],\n         col = \"darkgreen\", lwd = 2)\ntext(x[25] + 3, (y[25] + fitted(model)[25])/2,\n     \"ε (residual)\", col = \"darkgreen\")\n\nlegend(\"topleft\",\n       c(\"Data points\", \"Model (E[Y|X])\", \"Residuals (ε)\"),\n       pch = c(19, NA, NA), lty = c(NA, 1, 1),\n       col = c(\"steelblue\", \"red\", \"darkgreen\"), lwd = 2)\n```\n\n::: {.cell-output-display}\n![](14b-what-are-models_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\n## Model Fitting: Finding the Best Parameters\n\n**Model fitting** is the process of finding parameter values that make the model best explain the observed data.\n\n### Least Squares\n\nFor linear models, **least squares** minimizes the sum of squared residuals:\n\n$$\\text{minimize} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2$$\n\nThis produces the best linear unbiased estimators (BLUE) under certain conditions.\n\n### Maximum Likelihood\n\n**Maximum likelihood estimation (MLE)** finds parameters that maximize the probability of observing the data:\n\n$$\\hat{\\theta} = \\arg\\max_{\\theta} L(\\theta | \\text{data}) = \\arg\\max_{\\theta} \\prod_{i=1}^{n} f(y_i | \\theta)$$\n\nFor normally distributed errors, least squares and MLE give identical results. MLE extends to non-normal distributions and complex models.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Visualize likelihood for estimating a mean\nset.seed(123)\ndata <- rnorm(20, mean = 5, sd = 2)\n\n# Calculate log-likelihood for different values of mu\nmu_values <- seq(3, 7, length.out = 100)\nlog_lik <- sapply(mu_values, function(mu) {\n  sum(dnorm(data, mean = mu, sd = 2, log = TRUE))\n})\n\npar(mfrow = c(1, 2))\n\n# Data histogram\nhist(data, breaks = 10, main = \"Sample Data\", xlab = \"Value\",\n     col = \"lightblue\", border = \"white\")\nabline(v = mean(data), col = \"red\", lwd = 2)\n\n# Log-likelihood curve\nplot(mu_values, log_lik, type = \"l\", lwd = 2, col = \"blue\",\n     xlab = expression(mu), ylab = \"Log-likelihood\",\n     main = \"Maximum Likelihood Estimation\")\nabline(v = mu_values[which.max(log_lik)], col = \"red\", lwd = 2, lty = 2)\ntext(mu_values[which.max(log_lik)], min(log_lik) + 2,\n     paste(\"MLE =\", round(mu_values[which.max(log_lik)], 2)))\n```\n\n::: {.cell-output-display}\n![](14b-what-are-models_files/figure-html/unnamed-chunk-3-1.png){width=768}\n:::\n:::\n\n\n## Overfitting: When Models Learn Too Much\n\n**Overfitting** occurs when a model captures noise rather than signal—it fits the training data extremely well but fails to generalize to new data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Demonstrate overfitting with polynomial regression\nset.seed(42)\nn <- 20\nx <- seq(0, 1, length.out = n)\ny_true <- sin(2*pi*x)\ny <- y_true + rnorm(n, sd = 0.3)\n\npar(mfrow = c(1, 2))\n\n# Underfitting (too simple)\nplot(x, y, pch = 19, main = \"Underfitting (degree 1)\")\nabline(lm(y ~ x), col = \"red\", lwd = 2)\n\n# Overfitting (too complex)\nplot(x, y, pch = 19, main = \"Overfitting (degree 15)\")\nx_new <- seq(0, 1, length.out = 100)\nlines(x_new, predict(lm(y ~ poly(x, 15)),\n                     newdata = data.frame(x = x_new)),\n      col = \"red\", lwd = 2)\n```\n\n::: {.cell-output-display}\n![](14b-what-are-models_files/figure-html/unnamed-chunk-4-1.png){width=768}\n:::\n:::\n\n\nSigns of overfitting:\n\n- Model fits training data nearly perfectly\n- Predictions on new data are poor\n- Coefficients are extremely large or unstable\n- Small changes in data produce very different models\n\n### The Bias-Variance Tradeoff\n\nPrediction error has two sources:\n\n**Bias**: Error from oversimplifying—missing important patterns\n\n**Variance**: Error from oversensitivity—fitting noise\n\n$$\\text{Total Error} = \\text{Bias}^2 + \\text{Variance} + \\text{Irreducible Error}$$\n\nSimple models have high bias but low variance. Complex models have low bias but high variance. The goal is to find the sweet spot.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Conceptual bias-variance plot\ncomplexity <- 1:20\nbias_sq <- 10 / complexity\nvariance <- 0.5 * complexity\ntotal <- bias_sq + variance + 2  # irreducible error = 2\n\nplot(complexity, total, type = \"l\", lwd = 2,\n     ylim = c(0, max(total) + 1),\n     xlab = \"Model Complexity\", ylab = \"Error\",\n     main = \"Bias-Variance Tradeoff\")\nlines(complexity, bias_sq, col = \"blue\", lwd = 2, lty = 2)\nlines(complexity, variance, col = \"red\", lwd = 2, lty = 2)\nabline(h = 2, col = \"gray\", lty = 3)\n\n# Optimal complexity\noptimal <- which.min(total)\npoints(optimal, total[optimal], pch = 19, cex = 1.5, col = \"darkgreen\")\n\nlegend(\"topright\",\n       c(\"Total Error\", \"Bias²\", \"Variance\", \"Irreducible\"),\n       col = c(\"black\", \"blue\", \"red\", \"gray\"),\n       lty = c(1, 2, 2, 3), lwd = 2)\n```\n\n::: {.cell-output-display}\n![](14b-what-are-models_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\n## Feature Engineering and Transformations\n\nThe raw predictor variables may not capture the true relationship. **Feature engineering** creates new variables that better represent the underlying patterns.\n\nCommon transformations:\n\n- **Polynomial terms**: $x^2$, $x^3$ for curved relationships\n- **Log transform**: $\\log(x)$ for multiplicative relationships\n- **Interactions**: $x_1 \\times x_2$ when effects depend on each other\n- **Categorical encoding**: Converting categories to numbers\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Example: log transformation\nset.seed(42)\nx <- runif(50, 1, 100)\ny <- 2 * log(x) + rnorm(50, sd = 0.5)\n\npar(mfrow = c(1, 2))\n\n# Raw scale - looks nonlinear\nplot(x, y, pch = 19, main = \"Original Scale\",\n     xlab = \"X\", ylab = \"Y\")\nabline(lm(y ~ x), col = \"red\", lwd = 2)\n\n# Log scale - linear\nplot(log(x), y, pch = 19, main = \"After Log Transform\",\n     xlab = \"log(X)\", ylab = \"Y\")\nabline(lm(y ~ log(x)), col = \"red\", lwd = 2)\n```\n\n::: {.cell-output-display}\n![](14b-what-are-models_files/figure-html/unnamed-chunk-6-1.png){width=768}\n:::\n:::\n\n\n## Model Selection\n\nWhen multiple models are possible, how do we choose? Several criteria exist:\n\n**Adjusted R²**: Penalizes for additional parameters\n$$R^2_{adj} = 1 - \\frac{(1-R^2)(n-1)}{n-p-1}$$\n\n**AIC (Akaike Information Criterion)**: Balances fit and complexity\n$$AIC = -2\\ln(L) + 2k$$\n\n**BIC (Bayesian Information Criterion)**: Heavier penalty for complexity\n$$BIC = -2\\ln(L) + k\\ln(n)$$\n\nLower AIC/BIC values indicate better models (balancing fit and parsimony).\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Model selection example\ndata(mtcars)\n\n# Compare models of increasing complexity\nm1 <- lm(mpg ~ wt, data = mtcars)\nm2 <- lm(mpg ~ wt + hp, data = mtcars)\nm3 <- lm(mpg ~ wt + hp + disp, data = mtcars)\nm4 <- lm(mpg ~ wt + hp + disp + drat + qsec, data = mtcars)\n\n# Compare using AIC\nmodels <- list(m1, m2, m3, m4)\ncomparison <- data.frame(\n  Model = c(\"mpg ~ wt\", \"mpg ~ wt + hp\",\n            \"mpg ~ wt + hp + disp\",\n            \"mpg ~ wt + hp + disp + drat + qsec\"),\n  R_squared = sapply(models, function(m) summary(m)$r.squared),\n  Adj_R_squared = sapply(models, function(m) summary(m)$adj.r.squared),\n  AIC = sapply(models, AIC),\n  BIC = sapply(models, BIC)\n)\n\nknitr::kable(comparison, digits = 2)\n```\n\n::: {.cell-output-display}\n\n\n|Model                              | R_squared| Adj_R_squared|    AIC|    BIC|\n|:----------------------------------|---------:|-------------:|------:|------:|\n|mpg ~ wt                           |      0.75|          0.74| 166.03| 170.43|\n|mpg ~ wt + hp                      |      0.83|          0.81| 156.65| 162.52|\n|mpg ~ wt + hp + disp               |      0.83|          0.81| 158.64| 165.97|\n|mpg ~ wt + hp + disp + drat + qsec |      0.85|          0.82| 158.28| 168.54|\n\n\n:::\n:::\n\n\n## Cross-Validation for Model Assessment\n\nThe gold standard for evaluating predictive performance is **cross-validation**: testing the model on data it hasn't seen.\n\n**K-fold cross-validation**:\n1. Split data into K parts\n2. For each part: train on the other K-1 parts, test on the held-out part\n3. Average performance across all K tests\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(boot)\n\n# Compare polynomial degrees using cross-validation\nset.seed(42)\nn <- 100\nx <- seq(0, 4*pi, length.out = n)\ny <- sin(x) + rnorm(n, sd = 0.5)\ndata_cv <- data.frame(x, y)\n\n# Calculate CV error for different polynomial degrees\ndegrees <- 1:15\ncv_errors <- sapply(degrees, function(d) {\n  model <- glm(y ~ poly(x, d), data = data_cv)\n  cv.glm(data_cv, model, K = 10)$delta[1]\n})\n\nplot(degrees, cv_errors, type = \"b\", pch = 19,\n     xlab = \"Polynomial Degree\", ylab = \"CV Error\",\n     main = \"Cross-Validation for Model Selection\")\nabline(v = which.min(cv_errors), col = \"red\", lty = 2)\n```\n\n::: {.cell-output-display}\n![](14b-what-are-models_files/figure-html/unnamed-chunk-8-1.png){width=576}\n:::\n:::\n\n\n## Prediction vs. Explanation\n\nDifferent goals require different approaches:\n\n**For Explanation**:\n\n- Simpler models are often preferable\n- Focus on coefficient interpretation\n- Statistical significance matters\n- Understand which variables drive the relationship\n\n**For Prediction**:\n\n- Model complexity can be higher if it helps\n- Focus on out-of-sample performance\n- Accuracy metrics matter most\n- Understanding why is secondary\n\nIn biology and bioengineering, we often want both—models that predict well AND provide mechanistic insight. This tension shapes many modeling decisions.\n\n## Practical Modeling Workflow\n\n1. **Define the question**: What are you trying to learn or predict?\n\n2. **Explore the data**: Visualize relationships, check distributions, identify issues\n\n3. **Choose candidate models**: Based on data type, assumptions, and goals\n\n4. **Fit and evaluate**: Use appropriate metrics and validation\n\n5. **Check assumptions**: Residual analysis, diagnostic plots\n\n6. **Iterate**: Refine based on diagnostics\n\n7. **Report honestly**: Including limitations and uncertainty\n\n## Summary\n\n- Models are simplified representations of reality that help us understand and predict\n- The general linear model framework unifies many common statistical methods\n- Model fitting finds parameters that best explain the data (least squares, MLE)\n- Overfitting occurs when models learn noise instead of signal\n- The bias-variance tradeoff governs model complexity choices\n- Feature engineering can improve model performance\n- Cross-validation provides honest estimates of predictive performance\n- Different goals (prediction vs. explanation) may favor different approaches\n\n## Additional Resources\n\n- @james2023islr - Comprehensive introduction to statistical learning concepts\n- @crawley2007r - Practical guide to statistical modeling in R\n- Breiman, L. (2001). Statistical Modeling: The Two Cultures. *Statistical Science*, 16(3), 199-231.\n",
    "supporting": [
      "14b-what-are-models_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}