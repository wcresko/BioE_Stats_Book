{
  "hash": "9516e3fc21dd23c05a829b8038fdf23c",
  "result": {
    "engine": "knitr",
    "markdown": "# Nonparametric Tests {#sec-nonparametric}\n\n\n::: {.cell}\n\n:::\n\n\n## When Assumptions Fail\n\nParametric tests like the t-test make assumptions about the underlying data distributionâ€”typically that data are normally distributed with equal variances across groups. When these assumptions are violated, the tests may give misleading results. Nonparametric tests provide alternatives that make fewer assumptions about the data.\n\nNonparametric methods are sometimes called distribution-free methods because they do not assume a specific probability distribution. Instead, they typically work with ranks or signs of data rather than the raw values. This makes them robust to outliers and applicable to ordinal data where parametric methods would be inappropriate.\n\n## The Mann-Whitney U Test\n\nThe Mann-Whitney U test (also called the Wilcoxon rank-sum test) is the nonparametric equivalent of the two-sample t-test. It tests whether two independent groups tend to have different values, based on comparing the ranks of observations rather than the observations themselves.\n\nThe null hypothesis is that the distributions of the two groups are identical. The alternative is that one group tends to have larger values than the other.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Generate data with non-normal distributions\nset.seed(518)\ngroup1 <- sample(rnorm(n = 10000, mean = 2, sd = 0.5), size = 100)\ngroup2 <- sample(rnorm(n = 10000, mean = 5, sd = 1.5), size = 100)\n\n# Mann-Whitney U test\nwilcox.test(group1, group2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tWilcoxon rank sum test with continuity correction\n\ndata:  group1 and group2\nW = 440, p-value < 2.2e-16\nalternative hypothesis: true location shift is not equal to 0\n```\n\n\n:::\n:::\n\n\nThe test works by combining all observations, ranking them, and comparing the sum of ranks in each group. If one group tends to have higher values, its rank sum will be larger than expected by chance.\n\n## Wilcoxon Signed-Rank Test\n\nFor paired data, the Wilcoxon signed-rank test is the nonparametric alternative to the paired t-test. It tests whether the median difference between pairs is zero.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Paired data example\nset.seed(123)\nbefore <- rnorm(20, mean = 100, sd = 15)\nafter <- before + rexp(20, rate = 0.2)  # Skewed improvement\n\nwilcox.test(after, before, paired = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tWilcoxon signed rank exact test\n\ndata:  after and before\nV = 210, p-value = 1.907e-06\nalternative hypothesis: true location shift is not equal to 0\n```\n\n\n:::\n:::\n\n\nThe test calculates the differences between pairs, ranks their absolute values, and considers the signs of the differences. Under the null hypothesis, positive and negative differences should be equally likely and of similar magnitude.\n\n## Kruskal-Wallis Test\n\nThe Kruskal-Wallis test extends the Mann-Whitney U test to more than two groups, serving as a nonparametric alternative to one-way ANOVA. It tests whether at least one group tends to have different values from the others.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Example with three groups\nset.seed(42)\ndata <- data.frame(\n  value = c(rexp(30, 0.1), rexp(30, 0.15), rexp(30, 0.2)),\n  group = factor(rep(c(\"A\", \"B\", \"C\"), each = 30))\n)\n\nkruskal.test(value ~ group, data = data)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tKruskal-Wallis rank sum test\n\ndata:  value by group\nKruskal-Wallis chi-squared = 9.3507, df = 2, p-value = 0.009322\n```\n\n\n:::\n:::\n\n\nLike ANOVA, a significant Kruskal-Wallis test tells you that groups differ but not which specific groups differ from which others. Post-hoc pairwise comparisons can follow up on a significant result.\n\n## Advantages and Limitations\n\n**Advantages of nonparametric tests:**\n\nNonparametric tests do not require normally distributed data. They are robust to outliers since they work with ranks rather than raw values. They can be applied to ordinal data where the assumption of interval-level measurement would be violated. They often have good power relative to parametric tests even when parametric assumptions are met.\n\n**Limitations:**\n\nWhen parametric assumptions are met, nonparametric tests are slightly less powerful than their parametric counterparts. They test hypotheses about distributions or medians rather than means, which may not always align with research questions. They can be more difficult to extend to complex designs with multiple factors or covariates.\n\n## Choosing Between Parametric and Nonparametric\n\nThe choice depends on your data and research question. If your data are reasonably normal (or your sample is large enough for the Central Limit Theorem to apply) and you care about means, parametric tests are appropriate and efficient. If your data are severely non-normal, contain outliers, or are ordinal in nature, nonparametric tests provide a safer alternative.\n\nWith large samples, the Central Limit Theorem ensures that parametric tests are robust to non-normality, so the choice matters less. With small samples, checking assumptions becomes more important.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}