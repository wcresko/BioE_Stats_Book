{
  "hash": "c649cdcb140016cec02728f944fd3737",
  "result": {
    "engine": "knitr",
    "markdown": "# Nonparametric Tests {#sec-nonparametric}\n\n\n::: {.cell}\n\n:::\n\n\n## When Assumptions Fail\n\nParametric tests like the t-test make assumptions about the underlying data distribution—typically that data are normally distributed with equal variances across groups. When these assumptions are violated, the tests may give misleading results. Nonparametric tests provide alternatives that make fewer assumptions about the data.\n\nNonparametric methods are sometimes called distribution-free methods because they do not assume a specific probability distribution. Instead, they typically work with ranks or signs of data rather than the raw values. This makes them robust to outliers and applicable to ordinal data where parametric methods would be inappropriate.\n\n## The Mann-Whitney U Test\n\nThe Mann-Whitney U test (also called the Wilcoxon rank-sum test) is the nonparametric equivalent of the two-sample t-test. It tests whether two independent groups tend to have different values, based on comparing the ranks of observations rather than the observations themselves.\n\nThe null hypothesis is that the distributions of the two groups are identical. The alternative is that one group tends to have larger values than the other.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Generate data with non-normal distributions\nset.seed(518)\ngroup1 <- sample(rnorm(n = 10000, mean = 2, sd = 0.5), size = 100)\ngroup2 <- sample(rnorm(n = 10000, mean = 5, sd = 1.5), size = 100)\n\n# Mann-Whitney U test\nwilcox.test(group1, group2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tWilcoxon rank sum test with continuity correction\n\ndata:  group1 and group2\nW = 440, p-value < 2.2e-16\nalternative hypothesis: true location shift is not equal to 0\n```\n\n\n:::\n:::\n\n\nThe test works by combining all observations, ranking them, and comparing the sum of ranks in each group. If one group tends to have higher values, its rank sum will be larger than expected by chance.\n\n## Wilcoxon Signed-Rank Test\n\nFor paired data, the Wilcoxon signed-rank test is the nonparametric alternative to the paired t-test. It tests whether the median difference between pairs is zero.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Paired data example\nset.seed(123)\nbefore <- rnorm(20, mean = 100, sd = 15)\nafter <- before + rexp(20, rate = 0.2)  # Skewed improvement\n\nwilcox.test(after, before, paired = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tWilcoxon signed rank exact test\n\ndata:  after and before\nV = 210, p-value = 1.907e-06\nalternative hypothesis: true location shift is not equal to 0\n```\n\n\n:::\n:::\n\n\nThe test calculates the differences between pairs, ranks their absolute values, and considers the signs of the differences. Under the null hypothesis, positive and negative differences should be equally likely and of similar magnitude.\n\n## Kruskal-Wallis Test\n\nThe Kruskal-Wallis test extends the Mann-Whitney U test to more than two groups, serving as a nonparametric alternative to one-way ANOVA. It tests whether at least one group tends to have different values from the others.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Example with three groups\nset.seed(42)\ndata <- data.frame(\n  value = c(rexp(30, 0.1), rexp(30, 0.15), rexp(30, 0.2)),\n  group = factor(rep(c(\"A\", \"B\", \"C\"), each = 30))\n)\n\nkruskal.test(value ~ group, data = data)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tKruskal-Wallis rank sum test\n\ndata:  value by group\nKruskal-Wallis chi-squared = 9.3507, df = 2, p-value = 0.009322\n```\n\n\n:::\n:::\n\n\nLike ANOVA, a significant Kruskal-Wallis test tells you that groups differ but not which specific groups differ from which others. Post-hoc pairwise comparisons can follow up on a significant result.\n\n## Advantages and Limitations\n\n**Advantages of nonparametric tests:**\n\nNonparametric tests do not require normally distributed data. They are robust to outliers since they work with ranks rather than raw values. They can be applied to ordinal data where the assumption of interval-level measurement would be violated. They often have good power relative to parametric tests even when parametric assumptions are met.\n\n**Limitations:**\n\nWhen parametric assumptions are met, nonparametric tests are slightly less powerful than their parametric counterparts. They test hypotheses about distributions or medians rather than means, which may not always align with research questions. They can be more difficult to extend to complex designs with multiple factors or covariates.\n\n## Choosing Between Parametric and Nonparametric\n\nThe choice depends on your data and research question. If your data are reasonably normal (or your sample is large enough for the Central Limit Theorem to apply) and you care about means, parametric tests are appropriate and efficient. If your data are severely non-normal, contain outliers, or are ordinal in nature, nonparametric tests provide a safer alternative.\n\nWith large samples, the Central Limit Theorem ensures that parametric tests are robust to non-normality, so the choice matters less. With small samples, checking assumptions becomes more important.\n\n## Frequency Analysis: Chi-Square Tests\n\nWhen data consist of counts in categories rather than continuous measurements, we need tests designed for categorical data. The chi-square ($\\chi^2$) test compares observed frequencies to expected frequencies under a null hypothesis.\n\n### Goodness-of-Fit Test\n\nThe chi-square goodness-of-fit test asks whether observed frequencies match expected proportions. For example, do offspring genotypes follow expected Mendelian ratios?\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Test whether observed counts match expected 3:1 ratio\nobserved <- c(75, 25)  # Dominant, Recessive phenotypes\nexpected_ratio <- c(3, 1)\nexpected <- sum(observed) * expected_ratio / sum(expected_ratio)\n\n# Chi-square test\nchisq.test(observed, p = expected_ratio / sum(expected_ratio))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tChi-squared test for given probabilities\n\ndata:  observed\nX-squared = 0, df = 1, p-value = 1\n```\n\n\n:::\n:::\n\n\nThe test statistic is:\n\n$$\\chi^2 = \\sum \\frac{(O_i - E_i)^2}{E_i}$$\n\nwhere $O_i$ are observed counts and $E_i$ are expected counts. Under the null hypothesis (observed = expected), this follows a chi-square distribution with $k-1$ degrees of freedom, where $k$ is the number of categories.\n\n### Tests of Independence: Contingency Tables\n\nWhen we have counts cross-classified by two categorical variables, a contingency table displays the frequencies. The chi-square test of independence asks whether the two variables are associated.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Example: Is treatment outcome associated with gender?\ntreatment_data <- matrix(c(\n  45, 35,   # Males: Success, Failure\n  55, 15    # Females: Success, Failure\n), nrow = 2, byrow = TRUE)\nrownames(treatment_data) <- c(\"Male\", \"Female\")\ncolnames(treatment_data) <- c(\"Success\", \"Failure\")\n\ntreatment_data\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n       Success Failure\nMale        45      35\nFemale      55      15\n```\n\n\n:::\n\n```{.r .cell-code}\n# Chi-square test of independence\nchisq.test(treatment_data)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tPearson's Chi-squared test with Yates' continuity correction\n\ndata:  treatment_data\nX-squared = 7.3962, df = 1, p-value = 0.006536\n```\n\n\n:::\n:::\n\n\nExpected counts under independence are calculated as:\n\n$$E_{ij} = \\frac{(\\text{Row Total}_i) \\times (\\text{Column Total}_j)}{\\text{Grand Total}}$$\n\n::: {.callout-warning}\n## Assumptions of Chi-Square Tests\n\n- Observations must be independent\n- Expected counts should be at least 5 in each cell (some sources say 80% of cells should have expected counts ≥ 5)\n- For 2×2 tables with small expected counts, use Fisher's exact test instead\n:::\n\n### Fisher's Exact Test\n\nWhen sample sizes are small, Fisher's exact test provides exact p-values rather than relying on the chi-square approximation:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Small sample example\nsmall_table <- matrix(c(3, 1, 1, 3), nrow = 2)\nfisher.test(small_table)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tFisher's Exact Test for Count Data\n\ndata:  small_table\np-value = 0.4857\nalternative hypothesis: true odds ratio is not equal to 1\n95 percent confidence interval:\n   0.2117329 621.9337505\nsample estimates:\nodds ratio \n  6.408309 \n```\n\n\n:::\n:::\n\n\n### G-Test (Likelihood Ratio Test)\n\nThe G-test is an alternative to the chi-square test based on the likelihood ratio. It has better theoretical properties and is preferred by some statisticians:\n\n$$G = 2 \\sum O_i \\ln\\left(\\frac{O_i}{E_i}\\right)$$\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# G-test for the treatment data\n# Using observed and expected from chi-square\ntest_result <- chisq.test(treatment_data)\nobserved_counts <- as.vector(treatment_data)\nexpected_counts <- as.vector(test_result$expected)\n\nG <- 2 * sum(observed_counts * log(observed_counts / expected_counts))\np_value <- 1 - pchisq(G, df = 1)\n\ncat(\"G statistic:\", round(G, 3), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nG statistic: 8.563 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"p-value:\", round(p_value, 4), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\np-value: 0.0034 \n```\n\n\n:::\n:::\n\n\n### Odds Ratios\n\nFor 2×2 tables, the **odds ratio** quantifies the strength of association between two binary variables:\n\n$$OR = \\frac{a/b}{c/d} = \\frac{ad}{bc}$$\n\nwhere the table is:\n\n|  | Outcome+ | Outcome- |\n|:--|:--|:--|\n| Exposure+ | a | b |\n| Exposure- | c | d |\n\nAn odds ratio of 1 indicates no association. OR > 1 indicates positive association; OR < 1 indicates negative association.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Calculate odds ratio for treatment data\na <- treatment_data[1, 1]  # Male, Success\nb <- treatment_data[1, 2]  # Male, Failure\nc <- treatment_data[2, 1]  # Female, Success\nd <- treatment_data[2, 2]  # Female, Failure\n\nodds_ratio <- (a * d) / (b * c)\ncat(\"Odds ratio:\", round(odds_ratio, 3), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nOdds ratio: 0.351 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Using fisher.test to get OR with confidence interval\nfisher.test(treatment_data)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tFisher's Exact Test for Count Data\n\ndata:  treatment_data\np-value = 0.005273\nalternative hypothesis: true odds ratio is not equal to 1\n95 percent confidence interval:\n 0.1579843 0.7609357\nsample estimates:\nodds ratio \n 0.3531441 \n```\n\n\n:::\n:::\n\n\nAn odds ratio of 0.35 indicates that males have lower odds of success compared to females in this example.\n\n### McNemar's Test for Paired Data\n\nWhen categorical data are paired (e.g., before/after measurements on the same subjects), McNemar's test is appropriate:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Before/after treatment: did opinion change?\nbefore_after <- matrix(c(\n  40, 10,  # Agree before: Agree after, Disagree after\n  25, 25   # Disagree before: Agree after, Disagree after\n), nrow = 2, byrow = TRUE)\n\nmcnemar.test(before_after)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tMcNemar's Chi-squared test with continuity correction\n\ndata:  before_after\nMcNemar's chi-squared = 5.6, df = 1, p-value = 0.01796\n```\n\n\n:::\n:::\n\n\nThe test focuses on the discordant pairs—cases where the response changed—and asks whether changes in one direction are more common than changes in the other direction.\n\n## Summary\n\nNonparametric and frequency-based tests provide alternatives when parametric assumptions fail or data are categorical:\n\n- Mann-Whitney U and Kruskal-Wallis for comparing groups with non-normal data\n- Wilcoxon signed-rank for paired non-normal data\n- Chi-square tests for categorical data (goodness-of-fit and independence)\n- Fisher's exact test for small samples\n- Odds ratios to quantify association strength\n- McNemar's test for paired categorical data\n\n## Additional Resources\n\n- @logan2010biostatistical - Comprehensive coverage of nonparametric and categorical data analysis\n- @crawley2007r - Detailed treatment of chi-square and contingency table methods in R\n",
    "supporting": [
      "13-nonparametric-tests_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}