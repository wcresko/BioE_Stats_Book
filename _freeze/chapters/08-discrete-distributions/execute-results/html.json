{
  "hash": "e30667f40e0f94d795b08f8fbb06a234",
  "result": {
    "engine": "knitr",
    "markdown": "# Discrete Probability Distributions {#sec-discrete-distributions}\n\n\n::: {.cell}\n\n:::\n\n\n## What Are Discrete Distributions?\n\nDiscrete probability distributions describe random variables that take on distinct, countable values. The number of heads in ten coin flips, the count of bacterial colonies on a plate, and the number of defective items in a batch are all discrete random variables. Understanding these distributions allows you to model count data, calculate probabilities of specific outcomes, and perform statistical tests.\n\n## The Binomial Distribution\n\nThe binomial distribution arises when you perform a fixed number of independent trials, each with the same probability of success. It answers questions like: If I flip a coin 20 times, what is the probability of getting exactly 12 heads?\n\nThe probability of observing exactly $k$ successes in $n$ trials, when each trial has success probability $p$, is:\n\n$$P(X = k) = \\binom{n}{k} p^k (1-p)^{n-k}$$\n\nThe binomial coefficient $\\binom{n}{k}$ counts the number of ways to arrange $k$ successes among $n$ trials.\n\nThe mean of a binomial distribution is $\\mu = np$ and the variance is $\\sigma^2 = np(1-p)$.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Simulate 1000 experiments of 20 coin flips each\nset.seed(42)\nheads <- rbinom(n = 1000, size = 20, prob = 0.5)\n\nhist(heads, breaks = 0:20, col = \"steelblue\", \n     main = \"Distribution of Heads in 20 Coin Flips\",\n     xlab = \"Number of Heads\")\n```\n\n::: {.cell-output-display}\n![](08-discrete-distributions_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\nWith a fair coin ($p = 0.5$) and 20 flips, we expect about 10 heads on average. The distribution is symmetric and centered at 10.\n\nIn R, functions for the binomial distribution include:\n\n- `dbinom(k, n, p)` - probability of exactly k successes\n- `pbinom(k, n, p)` - probability of k or fewer successes (cumulative)\n- `qbinom(q, n, p)` - quantile function (inverse of cumulative)\n- `rbinom(n, size, p)` - generate random samples\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Probability of exactly 10 heads in 20 flips\ndbinom(10, size = 20, prob = 0.5)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.1761971\n```\n\n\n:::\n\n```{.r .cell-code}\n# Probability of 10 or fewer heads\npbinom(10, size = 20, prob = 0.5)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.5880985\n```\n\n\n:::\n:::\n\n\n## The Poisson Distribution\n\nThe Poisson distribution models the number of events occurring in a fixed interval of time or space, when events occur independently at a constant average rate. It is appropriate for count data like the number of mutations in a DNA sequence, phone calls received per hour, or organisms per quadrat in an ecological survey.\n\nThe probability of observing exactly $r$ events when the average rate is $\\lambda$ is:\n\n$$P(Y = r) = \\frac{e^{-\\lambda} \\lambda^r}{r!}$$\n\nA remarkable property of the Poisson distribution is that the mean and variance are both equal to $\\lambda$. This provides a simple check: if your count data has variance much larger than its mean, a simple Poisson model may not be appropriate (a situation called overdispersion, common in biological data).\n\n![](../images/week_2.004.jpeg){fig-align=\"center\"}\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Show Poisson distributions with different lambda values\npar(mfrow = c(2, 2))\nfor (lambda in c(1, 3, 5, 10)) {\n  x <- 0:20\n  plot(x, dpois(x, lambda), type = \"h\", lwd = 3, col = \"steelblue\",\n       main = paste(\"Poisson, λ =\", lambda),\n       xlab = \"Count\", ylab = \"Probability\")\n}\n```\n\n::: {.cell-output-display}\n![](08-discrete-distributions_files/figure-html/unnamed-chunk-4-1.png){width=768}\n:::\n:::\n\n\n![](../images/week_2.005.jpeg){fig-align=\"center\"}\n\nAs $\\lambda$ increases, the Poisson distribution becomes more symmetric and approaches a normal distribution.\n\n### A Historical Example: Horse Kick Deaths in the Prussian Army\n\nOne of the earliest applications of the Poisson distribution was in 1898, when it was used to model the number of soldier deaths from horse kicks in 14 different corps of the Prussian army. As shown in @fig-horse-kicks, the Poisson distribution does a remarkable job modeling these unfortunate events.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Data from Ladislaus Bortkiewicz (1898)\nobserved <- c(109, 65, 22, 3, 1)  # Deaths: 0, 1, 2, 3, 4\nexpected <- dpois(0:4, lambda = 0.7) * 200  # 200 corps-years, estimated lambda\n\ndeaths <- 0:4\nbarplot(rbind(observed, expected), beside = TRUE,\n        col = c(\"steelblue\", \"coral\"),\n        names.arg = deaths,\n        ylim = c(0, 120),\n        ylab = \"Frequency\",\n        xlab = \"Deaths per Year\",\n        main = \"Horse Kick Deaths: Observed vs. Poisson Predicted\")\nlegend(\"topright\", fill = c(\"steelblue\", \"coral\"),\n       legend = c(\"Observed\", \"Predicted from Poisson\"), bty = \"n\")\n```\n\n::: {.cell-output-display}\n![Distribution of horse kick deaths per corps per year in the Prussian army (1875-1894). The Poisson distribution closely matches the observed data.](08-discrete-distributions_files/figure-html/fig-horse-kicks-1.png){#fig-horse-kicks fig-align='center' width=576}\n:::\n:::\n\n\nThe Poisson distribution is particularly effective at modeling the distribution of rare, independent events like this.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Probability of exactly 2 events when lambda = 1\ndpois(x = 2, lambda = 1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.1839397\n```\n\n\n:::\n\n```{.r .cell-code}\n# Plot Poisson probabilities\nplot(dpois(x = 0:10, lambda = 3), type = \"h\", lwd = 3,\n     xlab = \"Count\", ylab = \"Probability\",\n     main = \"Poisson Distribution (λ = 3)\")\n```\n\n::: {.cell-output-display}\n![](08-discrete-distributions_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\n## The Geometric Distribution\n\nThe geometric distribution describes the number of trials needed to achieve the first success. If each trial has success probability $p$, the probability that the first success occurs on trial $k$ is:\n\n$$P(X = k) = (1-p)^{k-1} p$$\n\nThe mean is $1/p$ and the variance is $(1-p)/p^2$.\n\nFor example, if the probability of a cell successfully transfecting is 0.1, the geometric distribution tells us how many cells we need to attempt before getting our first successful transfection.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Probability of first success on each trial\np <- 0.1\ntrials <- 1:30\nprobs <- dgeom(trials - 1, prob = p)  # dgeom counts failures before first success\n\nplot(trials, probs, type = \"h\", lwd = 2, col = \"steelblue\",\n     xlab = \"Trial Number of First Success\",\n     ylab = \"Probability\",\n     main = \"Geometric Distribution (p = 0.1)\")\n```\n\n::: {.cell-output-display}\n![](08-discrete-distributions_files/figure-html/unnamed-chunk-6-1.png){width=576}\n:::\n:::\n\n\n![](../images/prob.017.jpeg){fig-align=\"center\"}\n\n## The Negative Binomial Distribution\n\nThe negative binomial distribution generalizes the geometric distribution. It describes the number of trials needed to achieve $r$ successes. If each trial has success probability $p$, the probability that the $r$th success occurs on trial $k$ is:\n\n$$P(X = k) = \\binom{k-1}{r-1} p^r (1-p)^{k-r}$$\n\nThe mean is $r/p$ and the variance is $r(1-p)/p^2$.\n\nConsider a predator that must capture 10 prey to reach reproductive maturity. If the daily probability of catching prey is 0.1, the negative binomial distribution describes when the predator will be ready to reproduce.\n\n![](../images/prob.018.jpeg){fig-align=\"center\"}\n\nThe negative binomial is also commonly used to model overdispersed count data—counts with variance greater than their mean—which the simple Poisson cannot accommodate.\n\n## Common Pattern in R\n\nR uses a consistent naming convention for distribution functions:\n\n| Prefix | Purpose | Example |\n|:------:|:--------|:--------|\n| `d` | Probability mass/density function | `dbinom()`, `dpois()` |\n| `p` | Cumulative distribution function | `pbinom()`, `ppois()` |\n| `q` | Quantile function | `qbinom()`, `qpois()` |\n| `r` | Random number generation | `rbinom()`, `rpois()` |\n\nThis pattern applies to all distributions in R:\n\n| Distribution | Functions |\n|:-------------|:----------|\n| Binomial | `dbinom`, `pbinom`, `qbinom`, `rbinom` |\n| Poisson | `dpois`, `ppois`, `qpois`, `rpois` |\n| Geometric | `dgeom`, `pgeom`, `qgeom`, `rgeom` |\n| Negative Binomial | `dnbinom`, `pnbinom`, `qnbinom`, `rnbinom` |\n\n## Choosing the Right Distribution\n\nSelecting the appropriate distribution depends on the nature of your data and the process generating it.\n\nUse the **binomial** when you have a fixed number of independent trials with constant success probability and you are counting successes. Examples include the number of patients responding to treatment out of a fixed sample, the number of correct answers on a test, or the number of defective items in a batch.\n\nUse the **Poisson** when you are counting events in a fixed interval of time or space, events occur independently, and the average rate is constant. Examples include mutations per gene, radioactive decays per minute, or organisms per quadrat. Remember that for Poisson data, mean should approximately equal variance.\n\nUse the **geometric** when you are counting trials until the first success. Examples include the number of attempts until a successful measurement or the number of patients screened until finding one eligible for a trial.\n\nUse the **negative binomial** when counting trials until a specified number of successes, or when modeling overdispersed count data (variance exceeds mean).\n\n## Practice with Simulations\n\nUnderstanding distributions deepens through simulation. Generate data from each distribution, visualize it, and calculate summary statistics. Compare the theoretical mean and variance to what you observe in your simulated samples.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Compare theoretical and empirical properties\nset.seed(123)\n\n# Poisson with lambda = 5\npois_sample <- rpois(10000, lambda = 5)\n\ncat(\"Poisson (λ = 5):\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nPoisson (λ = 5):\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Theoretical mean:\", 5, \"  Observed:\", mean(pois_sample), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTheoretical mean: 5   Observed: 4.9746 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Theoretical var:\", 5, \"  Observed:\", var(pois_sample), \"\\n\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTheoretical var: 5   Observed: 4.896444 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Binomial with n = 20, p = 0.3\nbinom_sample <- rbinom(10000, size = 20, prob = 0.3)\n\ncat(\"Binomial (n = 20, p = 0.3):\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nBinomial (n = 20, p = 0.3):\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Theoretical mean:\", 20 * 0.3, \"  Observed:\", mean(binom_sample), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTheoretical mean: 6   Observed: 5.9732 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Theoretical var:\", 20 * 0.3 * 0.7, \"  Observed:\", var(binom_sample), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTheoretical var: 4.2   Observed: 4.149097 \n```\n\n\n:::\n:::\n\n\nThis kind of simulation-based exploration builds intuition that complements formal mathematical understanding.\n",
    "supporting": [
      "08-discrete-distributions_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}