{
  "hash": "da432d88fa454f32bd9297e831d013af",
  "result": {
    "engine": "knitr",
    "markdown": "# Statistical Power {#sec-power}\n\n\n::: {.cell}\n\n:::\n\n\n## What is Statistical Power?\n\nPower is the probability of correctly rejecting a false null hypothesis—the probability of detecting an effect when one truly exists. If the true effect size is non-zero, power tells us how likely our study is to find it.\n\nPower = 1 - $\\beta$, where $\\beta$ is the probability of a Type II error (failing to reject a false null hypothesis). We typically aim for power of at least 80%, meaning we accept a 20% chance of missing a true effect.\n\n![](../images/images_6b.002.jpeg){fig-align=\"center\"}\n\n## Why Power Matters\n\nA study with low power has poor chances of detecting true effects. Even if an effect exists, the study may fail to find statistical significance. Worse, significant results from underpowered studies tend to overestimate effect sizes—a phenomenon called the \"winner's curse.\"\n\nUnderstanding power helps us interpret results appropriately. If we fail to reject the null hypothesis, was it because no effect exists, or because our study lacked the power to detect it?\n\n## Determinants of Power\n\nPower depends on four factors that are mathematically related:\n\n$$\\text{Power} \\propto \\frac{(\\text{Effect Size}) \\times (\\alpha) \\times (\\sqrt{n})}{\\sigma}$$\n\n**Effect Size**: Larger effects are easier to detect. Effect size can be measured in original units or standardized (like Cohen's d).\n\n**Sample Size (n)**: Larger samples provide more information and higher power.\n\n**Significance Level ($\\alpha$)**: Using a more lenient alpha (e.g., 0.10 instead of 0.05) increases power but also increases Type I error risk.\n\n**Variability ($\\sigma$)**: Less variable data makes effects easier to detect.\n\n![](../images/images_6b.003.jpeg){fig-align=\"center\"}\n\n## Cohen's d: Standardized Effect Size\n\nCohen's d expresses the difference between means in standard deviation units:\n\n$$d = \\frac{\\mu_1 - \\mu_2}{s_{pooled}}$$\n\nConventional benchmarks (Cohen, 1988):\n- d = 0.2: small effect\n- d = 0.5: medium effect\n- d = 0.8: large effect\n\nThese benchmarks are only guidelines—what counts as \"small\" depends on the research context.\n\n## A Priori Power Analysis\n\nBefore collecting data, power analysis helps determine the sample size needed to detect effects of interest. This requires specifying:\n\n1. The expected effect size\n2. The desired power (typically 0.80)\n3. The significance level (typically 0.05)\n4. The statistical test to be used\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# How many subjects needed to detect d = 0.5 with 80% power?\npwr.t.test(d = 0.5, sig.level = 0.05, power = 0.80, type = \"two.sample\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n     Two-sample t test power calculation \n\n              n = 63.76561\n              d = 0.5\n      sig.level = 0.05\n          power = 0.8\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n```\n\n\n:::\n:::\n\n\nAbout 64 subjects per group are needed to detect a medium effect with 80% power using a two-sample t-test.\n\n## Power Curves\n\nPower curves show how power changes with sample size or effect size:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Power curve for different effect sizes\nsample_sizes <- seq(10, 200, by = 5)\neffect_sizes <- c(0.2, 0.5, 0.8)\n\npower_data <- expand.grid(n = sample_sizes, d = effect_sizes)\npower_data$power <- mapply(function(n, d) {\n  pwr.t.test(n = n, d = d, sig.level = 0.05, type = \"two.sample\")$power\n}, power_data$n, power_data$d)\n\nggplot(power_data, aes(x = n, y = power, color = factor(d))) +\n  geom_line(size = 1.2) +\n  geom_hline(yintercept = 0.8, linetype = \"dashed\") +\n  labs(title = \"Power Curves for Two-Sample t-Test\",\n       x = \"Sample Size per Group\",\n       y = \"Power\",\n       color = \"Effect Size (d)\") +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](18-statistical-power_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\n## Power for ANOVA\n\nFor ANOVA, effect size is measured by Cohen's f:\n\n$$f = \\frac{\\sigma_{between}}{\\sigma_{within}}$$\n\nBenchmarks: f = 0.10 (small), f = 0.25 (medium), f = 0.40 (large).\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Sample size for ANOVA with 3 groups\npwr.anova.test(k = 3, f = 0.25, sig.level = 0.05, power = 0.80)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n     Balanced one-way analysis of variance power calculation \n\n              k = 3\n              n = 52.3966\n              f = 0.25\n      sig.level = 0.05\n          power = 0.8\n\nNOTE: n is number in each group\n```\n\n\n:::\n:::\n\n\n## Simulation-Based Power Analysis\n\nFor complex designs, simulation provides a flexible approach:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Simulation-based power for comparing two Poisson distributions\nset.seed(42)\n\npower_sim <- function(n, lambda1, lambda2, n_sims = 1000) {\n  significant <- replicate(n_sims, {\n    x1 <- rpois(n, lambda1)\n    x2 <- rpois(n, lambda2)\n    t.test(x1, x2)$p.value < 0.05\n  })\n  mean(significant)\n}\n\n# Power for different sample sizes\nsample_sizes <- seq(10, 100, by = 10)\npowers <- sapply(sample_sizes, power_sim, lambda1 = 10, lambda2 = 12)\n\nplot(sample_sizes, powers, type = \"b\", pch = 19,\n     xlab = \"Sample Size per Group\", ylab = \"Power\",\n     main = \"Simulated Power (λ1=10 vs λ2=12)\")\nabline(h = 0.8, lty = 2, col = \"red\")\n```\n\n::: {.cell-output-display}\n![](18-statistical-power_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\n## Post Hoc Power Analysis\n\nCalculating power after a study is completed is controversial. Post hoc power calculated from observed effect sizes is mathematically determined by the p-value and adds no new information. It cannot tell you whether a non-significant result reflects a true null or insufficient power.\n\nIf you want to understand what your study could detect, specify effect sizes based on scientific considerations, not observed results.\n\n## Practical Recommendations\n\nAlways conduct power analysis before data collection. Use realistic effect size estimates based on pilot data or previous literature. Consider what effect size would be practically meaningful, not just what you think exists.\n\nBe conservative—effects are often smaller than expected. Plan for some attrition or missing data. When in doubt, collect more data if feasible.\n",
    "supporting": [
      "18-statistical-power_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}