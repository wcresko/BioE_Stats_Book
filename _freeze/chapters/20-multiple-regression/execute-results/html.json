{
  "hash": "02192e84f8c70c6af8ddffde816338ba",
  "result": {
    "engine": "knitr",
    "markdown": "# Multiple Regression {#sec-multiple-regression}\n\n\n::: {.cell}\n\n:::\n\n\n## Beyond One Predictor\n\nSimple linear regression uses a single predictor. But the response variable often depends on multiple factors. A patient's blood pressure might depend on age, weight, sodium intake, and medication. Gene expression might depend on temperature, time, and treatment condition.\n\nMultiple regression extends linear regression to multiple predictors:\n\n$$y_i = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\ldots + \\beta_p x_p + \\epsilon_i$$\n\n![](../images/images_5a.006.jpeg){fig-align=\"center\"}\n\n## Goals of Multiple Regression\n\nMultiple regression serves two main purposes. First, it often improves prediction by incorporating multiple sources of information. Second, it allows us to investigate the effect of each predictor while controlling for the others—the effect of X1 \"holding X2 constant.\"\n\nThis second goal is powerful but requires caution. In observational data, controlling for variables statistically is not the same as controlling them experimentally. Confounding variables you do not measure cannot be controlled.\n\n## Additive vs. Multiplicative Models\n\nAn **additive model** assumes predictors contribute independently:\n\n$$y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\epsilon$$\n\nA **multiplicative model** includes interactions—the effect of one predictor depends on the value of another:\n\n$$y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_1 x_2 + \\epsilon$$\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Example with two predictors\nset.seed(42)\nn <- 100\nx1 <- rnorm(n)\nx2 <- rnorm(n)\ny <- 2 + 3*x1 + 2*x2 + 1.5*x1*x2 + rnorm(n)\n\n# Additive model\nadd_model <- lm(y ~ x1 + x2)\n\n# Model with interaction\nint_model <- lm(y ~ x1 * x2)\n\nsummary(int_model)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = y ~ x1 * x2)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.55125 -0.69885 -0.03771  0.56441  2.42157 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  2.00219    0.10108   19.81   <2e-16 ***\nx1           2.84494    0.09734   29.23   <2e-16 ***\nx2           2.04126    0.11512   17.73   <2e-16 ***\nx1:x2        1.35163    0.09228   14.65   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.005 on 96 degrees of freedom\nMultiple R-squared:  0.9289,\tAdjusted R-squared:  0.9267 \nF-statistic:   418 on 3 and 96 DF,  p-value: < 2.2e-16\n```\n\n\n:::\n:::\n\n\n## Interpretation of Coefficients\n\nIn multiple regression, each coefficient represents the expected change in Y for a one-unit change in that predictor, **holding all other predictors constant**.\n\nThis \"holding constant\" interpretation makes the coefficients different from what you would get from separate simple regressions. The coefficient for X1 in multiple regression represents the unique contribution of X1 after accounting for X2.\n\n## Multicollinearity\n\nWhen predictors are correlated with each other, interpreting individual coefficients becomes problematic. This **multicollinearity** inflates standard errors and can make coefficients unstable.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Check for multicollinearity visually\nlibrary(car)\npairs(~ x1 + x2, main = \"Scatterplot Matrix\")\n```\n\n::: {.cell-output-display}\n![](20-multiple-regression_files/figure-html/unnamed-chunk-3-1.png){width=576}\n:::\n:::\n\n\nThe **Variance Inflation Factor (VIF)** quantifies multicollinearity. VIF > 10 suggests serious problems; VIF > 5 warrants attention.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nvif(int_model)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      x1       x2    x1:x2 \n1.006276 1.061022 1.066455 \n```\n\n\n:::\n:::\n\n\n## Model Selection\n\nWith many potential predictors, how do we choose which to include? Adding variables always improves fit to the training data but may hurt prediction on new data through overfitting.\n\nSeveral criteria balance fit and complexity:\n\n**Adjusted R²** penalizes for the number of predictors.\n\n**AIC (Akaike Information Criterion)** estimates prediction error, penalizing complexity. Lower is better.\n\n**BIC (Bayesian Information Criterion)** similar to AIC but penalizes complexity more heavily.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Compare models\nAIC(add_model, int_model)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n          df      AIC\nadd_model  4 406.1808\nint_model  5 290.7926\n```\n\n\n:::\n\n```{.r .cell-code}\nBIC(add_model, int_model)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n          df      BIC\nadd_model  4 416.6015\nint_model  5 303.8185\n```\n\n\n:::\n:::\n\n\n## Model Selection Strategies\n\n**Forward selection** starts with no predictors and adds them one at a time based on statistical criteria.\n\n**Backward elimination** starts with all predictors and removes them one at a time.\n\n**All subsets** examines all possible combinations and selects the best.\n\nNo strategy is universally best. Automated selection can lead to overfitting and unstable models. Theory-driven model building—starting with predictors you have scientific reasons to include—is often preferable.\n\n## Polynomial Regression\n\nPolynomial terms can capture non-linear relationships while still using the linear regression framework:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Non-linear relationship\nx <- seq(0, 10, length.out = 100)\ny <- 2 + 0.5*x - 0.1*x^2 + rnorm(100, sd = 0.5)\n\n# Fit polynomial models\nmodel1 <- lm(y ~ poly(x, 1))  # Linear\nmodel2 <- lm(y ~ poly(x, 2))  # Quadratic\nmodel3 <- lm(y ~ poly(x, 5))  # Degree 5\n\n# Compare\nAIC(model1, model2, model3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n       df      AIC\nmodel1  3 250.3022\nmodel2  4 121.8041\nmodel3  7 126.6399\n```\n\n\n:::\n:::\n\n\nHigher-degree polynomials fit better but risk overfitting. The principle of parsimony suggests using the simplest adequate model.\n\n![](../images/images_5b.010.jpeg){fig-align=\"center\"}\n\n## Assumptions\n\nMultiple regression shares assumptions with simple regression: linearity (in each predictor), independence, normality of residuals, and constant variance. Additionally, predictors should not be perfectly correlated (no perfect multicollinearity).\n\nCheck assumptions with residual plots. Partial regression plots can help diagnose problems with individual predictors.\n\n## Practical Guidelines\n\nStart with a theoretically motivated model rather than throwing in all available predictors. Check for multicollinearity before interpreting coefficients. Use cross-validation to assess prediction performance. Report standardized coefficients when comparing the relative importance of predictors on different scales.\n\nBe humble about causation. Multiple regression describes associations; experimental manipulation is needed to establish causation.\n",
    "supporting": [
      "20-multiple-regression_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}