{
  "hash": "3fd55ec277f814d3f6ebe630f05665b5",
  "result": {
    "engine": "knitr",
    "markdown": "# Foundations of Probability {#sec-probability}\n\n\n::: {.cell}\n\n:::\n\n\n## Why Probability Matters\n\nIn games of chance, probability has a very intuitive definition. We know what it means that the chance of a pair of dice coming up seven is 1 in 6. However, probability is used much more broadly today, with the word commonly appearing in everyday language. Google's auto-complete of \"What are the chances of\" gives us: \"having twins\", \"rain today\", \"getting struck by lightning\", and \"getting cancer\". One goal of this chapter is to help us understand how probability is useful to understand and describe real-world events when performing data analysis.\n\nBecause knowing how to compute probabilities gives you an edge in games of chance, throughout history many smart individuals—including famous mathematicians such as Cardano, Fermat, and Pascal—spent time and energy thinking through the math of these games. As a result, Probability Theory was born. Probability continues to be highly useful in modern games of chance. For example, in poker, we can compute the probability of winning a hand based on the cards on the table. Casinos rely on probability theory to develop games that almost certainly guarantee a profit.\n\nProbability theory is useful in many other contexts and, in particular, in areas that depend on data affected by chance in some way. All of the other chapters in this part build upon probability theory. Knowledge of probability is therefore indispensable for data science.\n\n## Two Interpretations of Probability\n\nThere are two main ways to think about what probability means.\n\nThe **frequentist interpretation** views probability as mathematically convenient approximations to long-run relative frequencies. If we say the probability of heads when flipping a fair coin is 0.5, we mean that if we flipped the coin many, many times, about half the flips would come up heads. This interpretation grounds probability in observable, repeatable phenomena.\n\nThe **subjective (Bayesian) interpretation** views probability as a measure of belief or uncertainty. A probability statement expresses the opinion of some individual regarding how certain an event is to occur, given their current information. This interpretation allows us to assign probabilities to one-time events and to update beliefs as we gather evidence.\n\nBoth interpretations have their uses, and modern statistics draws on both perspectives. For now, the frequentist interpretation provides good intuition for the concepts we will develop.\n\n## Random Variables and Sample Spaces\n\nA **random variable** is a quantity that can take on different values with different probabilities. The outcome of a coin flip, the number of bacterial colonies on a plate, and the expression level of a gene are all random variables.\n\nThe **sample space** of a random variable is the set of all possible values it can take. For a coin flip, the sample space is {Heads, Tails}. For a die roll, it is {1, 2, 3, 4, 5, 6}. For the concentration of a protein, it might be any non-negative real number.\n\nA **probability distribution** describes how likely each value in the sample space is:\n\n- For discrete random variables (those that take distinct values), we use a **probability mass function** that gives the probability of each possible value\n- For continuous random variables (those that can take any value in a range), we use a **probability density function** from which probabilities are calculated by integration\n\nOne fundamental rule: the probabilities across the entire sample space must sum (or integrate) to 1. Something from the sample space must happen.\n\n## Discrete Probability\n\nWe start by covering some basic principles related to categorical data. This subset of probability is referred to as *discrete probability*. It will help us understand the probability theory we will later introduce for numeric and continuous data, which is much more common in data science applications.\n\n### Relative Frequency\n\nA precise definition of probability can be given by noting all possible outcomes and counting how many satisfy the condition for our event. For example, if we have 2 red beads and 3 blue beads inside an urn and we pick one at random, what is the probability of picking a red one?\n\nOur intuition tells us that the answer is 2/5 or 40%. There are five possible outcomes, of which two satisfy the condition necessary for the event \"pick a red bead\". Since each of the five outcomes has the same chance of occurring, we conclude that the probability is 0.4 for red and 0.6 for blue.\n\nA more tangible way to think about the probability of an event is as the proportion of times the event occurs when we repeat the experiment an infinite number of times, independently, and under the same conditions.\n\n### Notation\n\nWe use the notation $\\mbox{Pr}(A)$ to denote the probability of event $A$ happening. We use the very general term *event* to refer to things that can happen when something occurs by chance. In our previous example, the event was \"picking a red bead\". In a political poll in which we call 100 likely voters at random, an example of an event is \"calling 48 Democrats and 52 Republicans\".\n\nIn data science applications, we will often deal with continuous variables. These events will often be things like \"is this person taller than 6 feet\". In this case, we write events in a more mathematical form: $X \\geq 6$.\n\n### Probability Distributions for Categorical Data\n\nIf we know the relative frequency of the different categories, defining a distribution for categorical outcomes is straightforward. We simply assign a probability to each category. In cases that can be thought of as beads in an urn, for each bead type, their proportion defines the distribution.\n\nIf we are randomly calling likely voters from a population that is 44% Democrat, 44% Republican, 10% undecided, and 2% Green Party, these proportions define the probability for each group:\n\n| Group | Probability |\n|:------|:------------|\n| Republican | 0.44 |\n| Democrat | 0.44 |\n| Undecided | 0.10 |\n| Green | 0.02 |\n\n## Distribution Moments and Parameters\n\nProbability distributions can be characterized by their **moments**—metrics that describe the shape of the distribution. The first four moments correspond to important properties:\n\n1. **Mean** ($\\mu$) - the center or expected value\n2. **Variance** ($\\sigma^2$) - the spread or dispersion\n3. **Skewness** - the asymmetry of the distribution\n4. **Kurtosis** - the \"tailedness\" or peakedness\n\nFor a discrete random variable X, the **expected value** (mean) is:\n\n$$E[X] = \\sum_{\\text{all } x} x \\cdot P(X = x) = \\mu$$\n\nThe **variance** measures dispersion around the mean:\n\n$$\\text{Var}(X) = E[(X - \\mu)^2] = \\sigma^2$$\n\nThese parameters are crucial because they describe real properties of the systems we study. In biology, for example, the mean height of a population tells us about the typical value, while the variance tells us about the diversity of heights among individuals.\n\n## Monte Carlo Simulations\n\nComputers provide a way to actually perform random experiments. Random number generators permit us to mimic the process of picking at random. An example is the `sample` function in R.\n\nFirst, we use the function `rep` to generate the urn:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbeads <- rep(c(\"red\", \"blue\"), times = c(2, 3))\nbeads\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"red\"  \"red\"  \"blue\" \"blue\" \"blue\"\n```\n\n\n:::\n:::\n\n\nThen use `sample` to pick a bead at random:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsample(beads, 1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"red\"\n```\n\n\n:::\n:::\n\n\nThis line of code produces one random outcome. We want to repeat this experiment a large enough number of times to make the results practically equivalent to repeating forever. **This is an example of a Monte Carlo simulation**.\n\nTo perform our first Monte Carlo simulation, we use the `replicate` function, which permits us to repeat the same task any number of times. Here, we repeat the random event B = 10,000 times:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1986)  # For reproducibility\nB <- 10000\nevents <- replicate(B, sample(beads, 1))\n```\n:::\n\n\nWe can now see if our definition actually agrees with this Monte Carlo simulation approximation:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntab <- table(events)\nprop.table(tab)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nevents\n  blue    red \n0.6014 0.3986 \n```\n\n\n:::\n:::\n\n\nThe numbers above are the estimated probabilities provided by this Monte Carlo simulation. Statistical theory tells us that as $B$ gets larger, the estimates get closer to 3/5 = 0.6 and 2/5 = 0.4.\n\n### With and Without Replacement\n\nThe function `sample` has an argument that permits us to pick more than one element from the urn. By default, this selection occurs *without replacement*: after a bead is selected, it is not put back in the bag.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsample(beads, 5)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"blue\" \"red\"  \"blue\" \"red\"  \"blue\"\n```\n\n\n:::\n:::\n\n\nThis results in rearrangements that always have three blue and two red beads because we can't select more beads than exist.\n\nHowever, we can sample *with replacement*: return the bead back to the urn after selecting it:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nevents <- sample(beads, B, replace = TRUE)\nprop.table(table(events))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nevents\n blue   red \n0.601 0.399 \n```\n\n\n:::\n:::\n\n\nNot surprisingly, we get results very similar to those previously obtained with `replicate`.\n\n## The Bernoulli Distribution\n\nThe simplest probability distribution describes a single event with two possible outcomes—success or failure, yes or no, heads or tails. This is the **Bernoulli distribution**.\n\nConsider flipping a fair coin once:\n\n$$P(X = \\text{Head}) = \\frac{1}{2} = 0.5 = p$$\n\nAnd the probability of tails is:\n\n$$P(X = \\text{Tail}) = \\frac{1}{2} = 0.5 = 1 - p = q$$\n\nIf the coin is not fair, $p$ might differ from 0.5, but the probabilities still sum to 1:\n\n$$p + (1-p) = 1$$\n\nThis same framework applies to any binary outcome: whether a patient responds to treatment, whether an allele is inherited from a parent, or whether a product passes quality control.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Flip a coin 1000 times\nset.seed(42)\nflips <- rbinom(1000, 1, 0.5)\n\nbarplot(table(flips) / 1000,\n        names.arg = c(\"Tails\", \"Heads\"),\n        ylab = \"Probability\",\n        ylim = c(0, 0.75),\n        col = \"steelblue\",\n        main = \"Estimated Bernoulli Distribution\")\n```\n\n::: {.cell-output-display}\n![](07-probability-foundations_files/figure-html/unnamed-chunk-8-1.png){width=480}\n:::\n:::\n\n\n## Probability Rules\n\nTwo fundamental rules govern how probabilities combine. Most probability distributions can be built up from these simple rules.\n\n### The AND Rule (Multiplication)\n\nThe probability that two independent events both occur is the product of their individual probabilities. If you flip a coin twice:\n\n$$P(\\text{First = Head AND Second = Head}) = p \\times p = p^2$$\n\nMore generally, for independent events A and B:\n\n$$P(A \\text{ and } B) = P(A) \\times P(B)$$\n\nFor a fair coin with $p = 0.5$:\n\n- $P(\\text{HH}) = 0.5 \\times 0.5 = 0.25$\n- $P(\\text{HT}) = 0.5 \\times 0.5 = 0.25$\n- $P(\\text{TH}) = 0.5 \\times 0.5 = 0.25$\n- $P(\\text{TT}) = 0.5 \\times 0.5 = 0.25$\n\n### The OR Rule (Addition)\n\nThe probability that at least one of two mutually exclusive events occurs is the sum of their probabilities:\n\n$$P(A \\text{ or } B) = P(A) + P(B) - P(A \\text{ and } B)$$\n\nFor mutually exclusive events (those that cannot both occur), the intersection is empty:\n\n$$P(A \\text{ or } B) = P(A) + P(B)$$\n\nThe probability of getting exactly one head in two flips (either HT or TH):\n\n$$P(\\text{one head}) = P(\\text{HT}) + P(\\text{TH}) = 0.25 + 0.25 = 0.5$$\n\n### Multiplication Rule Under Independence\n\nWhen events are independent, the multiplication rule simplifies:\n\n$$P(A \\text{ and } B \\text{ and } C) = P(A) \\times P(B) \\times P(C)$$\n\nBut we must be careful—assuming independence can result in very different and incorrect probability calculations when we don't actually have independence.\n\n::: {.callout-warning}\n## Independence Matters\n\nImagine a court case where the suspect was described as having a mustache and a beard. The defendant has both, and the prosecution brings in an \"expert\" who testifies that 1/10 men have beards and 1/5 have mustaches, concluding that only $1/10 \\times 1/5 = 0.02$ have both.\n\nBut to multiply like this we need to assume independence! If the conditional probability of a man having a mustache given that he has a beard is 0.95, then the correct probability is much higher: $1/10 \\times 0.95 = 0.095$.\n:::\n\n## Independence\n\nWe say two events are **independent** if the outcome of one does not affect the other. The classic example is coin tosses. Every time we toss a fair coin, the probability of seeing heads is 1/2 regardless of what previous tosses have revealed. The same is true when we pick beads from an urn with replacement.\n\nMany examples of events that are not independent come from card games. When we deal the first card, the probability of getting a King is 1/13 since there are thirteen possibilities. Now if we deal a King for the first card and don't replace it into the deck, the probability of a second card being a King is only 3/51. These events are **not independent**: the first outcome affected the next one.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Demonstrate non-independence with sequential draws\nset.seed(1)\nx <- sample(beads, 5)\nx[1:4]  # First four draws\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"red\"  \"blue\" \"blue\" \"blue\"\n```\n\n\n:::\n\n```{.r .cell-code}\nx[5]    # If first four are blue, the fifth must be...\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"red\"\n```\n\n\n:::\n:::\n\n\nIf you have to guess the color of the first bead, you would predict blue since blue has a 60% chance. But if you know the first four were blue, the probability of the fifth being red is now 100%, not 40%. The events are not independent, so the probabilities change.\n\n## Conditional Probabilities\n\nWhen events are not independent, *conditional probabilities* are useful. The conditional probability $P(B|A)$ is the probability of B given that A has occurred:\n\n$$P(\\text{Card 2 is a King} \\mid \\text{Card 1 is a King}) = \\frac{3}{51}$$\n\nWe use the $\\mid$ as shorthand for \"given that\" or \"conditional on\".\n\nWhen two events A and B are independent:\n\n$$P(A \\mid B) = P(A)$$\n\nThis is the mathematical definition of independence: the fact that B happened does not affect the probability of A happening.\n\nThe general multiplication rule relates joint and conditional probability:\n\n$$P(A \\text{ and } B) = P(A) \\times P(B \\mid A)$$\n\nThis can be extended to more events:\n\n$$P(A \\text{ and } B \\text{ and } C) = P(A) \\times P(B \\mid A) \\times P(C \\mid A \\text{ and } B)$$\n\n## Bayes' Theorem\n\nRearranging the multiplication rule yields Bayes' theorem, a cornerstone of probabilistic reasoning:\n\n$$P(A|B) = \\frac{P(B|A) \\times P(A)}{P(B)}$$\n\nBayes' theorem tells us how to update our beliefs about A after observing B. In Bayesian statistics, this is written as:\n\n$$P(\\theta|d) = \\frac{P(d|\\theta) \\times P(\\theta)}{P(d)}$$\n\nwhere:\n\n- $P(\\theta|d)$ = posterior probability distribution\n- $P(d|\\theta)$ = likelihood function for $\\theta$\n- $P(\\theta)$ = prior probability distribution\n- $P(d)$ = marginal likelihood (normalizing constant)\n\n## Likelihood vs. Probability\n\nA subtle but important distinction exists between probability and likelihood.\n\n**Probability** is the chance of observing particular data given a model or parameter value. If we know a coin has $p = 0.5$, what is the probability of observing 7 heads in 10 flips?\n\n**Likelihood** is how well a parameter value explains observed data. Given that we observed 7 heads in 10 flips, how likely is it that the true probability is $p = 0.5$ versus $p = 0.7$?\n\nMathematically, the likelihood function uses the same formula as probability, but we think of it differently:\n\n$$L(\\text{parameter} | \\text{data}) = P(\\text{data} | \\text{parameter})$$\n\nMaximum likelihood estimation finds the parameter value that makes the observed data most probable—the value that maximizes the likelihood function.\n\n## Combinations and Permutations\n\nFor more complicated probability calculations, we need to count possibilities systematically. The `gtools` package provides useful functions.\n\n### Permutations (Order Matters)\n\nA **permutation** is an arrangement where order matters. For any list of size `n`, the `permutations` function computes all different arrangements when selecting `r` items:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# All ways to arrange 2 items from {1, 2, 3}\npermutations(3, 2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2]\n[1,]    1    2\n[2,]    1    3\n[3,]    2    1\n[4,]    2    3\n[5,]    3    1\n[6,]    3    2\n```\n\n\n:::\n:::\n\n\nNotice that order matters: 3,1 is different than 1,3. Also, (1,1), (2,2), and (3,3) do not appear because once we pick a number, it can't appear again.\n\n### Combinations (Order Doesn't Matter)\n\nA **combination** is a selection where order doesn't matter:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncombinations(3, 2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2]\n[1,]    1    2\n[2,]    1    3\n[3,]    2    3\n```\n\n\n:::\n:::\n\n\nThe outcome (2,1) doesn't appear because (1,2) already represents the same combination.\n\n### Example: Blackjack\n\nLet's compute the probability of getting a \"Natural 21\" in Blackjack—an Ace and a face card in the first two cards:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Build a deck\nsuits <- c(\"Diamonds\", \"Clubs\", \"Hearts\", \"Spades\")\nnumbers <- c(\"Ace\", \"Deuce\", \"Three\", \"Four\", \"Five\", \"Six\", \"Seven\",\n             \"Eight\", \"Nine\", \"Ten\", \"Jack\", \"Queen\", \"King\")\ndeck <- expand.grid(number = numbers, suit = suits)\ndeck <- paste(deck$number, deck$suit)\n\n# Define aces and face cards\naces <- paste(\"Ace\", suits)\nfacecard <- c(\"King\", \"Queen\", \"Jack\", \"Ten\")\nfacecard <- expand.grid(number = facecard, suit = suits)\nfacecard <- paste(facecard$number, facecard$suit)\n\n# All possible two-card hands (order doesn't matter)\nhands <- combinations(52, 2, v = deck)\n\n# Probability of Natural 21\nmean((hands[,1] %in% aces & hands[,2] %in% facecard) |\n     (hands[,2] %in% aces & hands[,1] %in% facecard))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.04826546\n```\n\n\n:::\n:::\n\n\n## Classic Examples\n\n### The Monty Hall Problem\n\nIn the game show \"Let's Make a Deal,\" contestants pick one of three doors. Behind one door is a car; behind the others are goats. After you pick a door, Monty Hall opens one of the remaining doors to reveal a goat. Then he asks: \"Do you want to switch doors?\"\n\nIntuition suggests it shouldn't matter—you're choosing between two doors, so shouldn't the probability be 50-50? Let's use Monte Carlo simulation:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nB <- 10000\nmonty_hall <- function(strategy) {\n  doors <- as.character(1:3)\n  prize <- sample(c(\"car\", \"goat\", \"goat\"))\n  prize_door <- doors[prize == \"car\"]\n  my_pick <- sample(doors, 1)\n  show <- sample(doors[!doors %in% c(my_pick, prize_door)], 1)\n\n  if (strategy == \"stick\") {\n    choice <- my_pick\n  } else {\n    choice <- doors[!doors %in% c(my_pick, show)]\n  }\n  choice == prize_door\n}\n\nstick_wins <- replicate(B, monty_hall(\"stick\"))\nswitch_wins <- replicate(B, monty_hall(\"switch\"))\n\ncat(\"Probability of winning when sticking:\", mean(stick_wins), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nProbability of winning when sticking: 0.3365 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Probability of winning when switching:\", mean(switch_wins), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nProbability of winning when switching: 0.6643 \n```\n\n\n:::\n:::\n\n\nSwitching doubles your chances! The key insight: when you first pick, you have a 1/3 chance of being right. Monty's reveal doesn't change that. Since the probability the car is behind one of the other doors was 2/3, and Monty showed you which one doesn't have it, switching gives you that 2/3 probability.\n\n### The Birthday Problem\n\nIn a room with 50 people, what's the probability that at least two share a birthday?\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Monte Carlo simulation\nB <- 10000\nsame_birthday <- function(n) {\n  bdays <- sample(1:365, n, replace = TRUE)\n  any(duplicated(bdays))\n}\n\nresults <- replicate(B, same_birthday(50))\ncat(\"Probability with 50 people:\", mean(results), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nProbability with 50 people: 0.9701 \n```\n\n\n:::\n\n```{.r .cell-code}\n# How does this change with group size?\ncompute_prob <- function(n, B = 10000) {\n  results <- replicate(B, same_birthday(n))\n  mean(results)\n}\n\nn <- seq(1, 60)\nprob <- sapply(n, compute_prob)\nqplot(n, prob, geom = \"line\") +\n  geom_hline(yintercept = 0.5, linetype = \"dashed\", color = \"red\") +\n  labs(x = \"Group Size\", y = \"Probability of Shared Birthday\",\n       title = \"The Birthday Problem\")\n```\n\n::: {.cell-output-display}\n![](07-probability-foundations_files/figure-html/unnamed-chunk-14-1.png){width=576}\n:::\n:::\n\n\nWith just 23 people, there's already a 50% chance of a shared birthday! People tend to underestimate these probabilities because they think about the probability that someone shares *their* birthday, not the probability that *any two people* share a birthday.\n\nWe can also compute this exactly using the multiplication rule:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Probability that all n people have UNIQUE birthdays\nexact_prob <- function(n) {\n  prob_unique <- seq(365, 365 - n + 1) / 365\n  1 - prod(prob_unique)\n}\n\neprob <- sapply(n, exact_prob)\ncat(\"Exact probability with 50 people:\", exact_prob(50), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nExact probability with 50 people: 0.9703736 \n```\n\n\n:::\n:::\n\n\n## Covariance and Correlation\n\nWhen two variables are not independent, they share information—knowing one tells you something about the other. This shared information is quantified by **covariance**, a measure of how two variables vary together.\n\nIf high values of X tend to occur with high values of Y (and low with low), the covariance is positive. If high values of X tend to occur with low values of Y, the covariance is negative. If there is no relationship, the covariance is near zero.\n\n**Correlation** is covariance standardized to fall between -1 and 1, making it easier to interpret. A correlation of 1 means perfect positive linear relationship; -1 means perfect negative linear relationship; 0 means no linear relationship.\n\nThese concepts will become central when we discuss regression and other methods for relating variables to each other.\n\n## How Large is \"Large Enough\" for Monte Carlo?\n\nThe theory described here requires repeating experiments over and over forever. In practice, we can't do this. In the examples above, we used $B = 10,000$ Monte Carlo experiments and it turned out to provide accurate estimates.\n\nOne practical approach is to check for the stability of the estimate:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nB_values <- 10^seq(1, 5, len = 100)\ncompute_prob_B <- function(B, n = 25) {\n  same_day <- replicate(B, same_birthday(n))\n  mean(same_day)\n}\n\nprob <- sapply(B_values, compute_prob_B)\nqplot(log10(B_values), prob, geom = \"line\") +\n  geom_hline(yintercept = exact_prob(25), color = \"red\", linetype = \"dashed\") +\n  labs(x = \"log10(Number of Simulations)\", y = \"Estimated Probability\",\n       title = \"Monte Carlo Convergence\")\n```\n\n::: {.cell-output-display}\n![](07-probability-foundations_files/figure-html/unnamed-chunk-16-1.png){width=576}\n:::\n:::\n\n\nThe values start to stabilize (vary less than 0.01) around 1000 simulations. The exact probability is shown in red.\n\n## Summary\n\nThis chapter introduced the language of probability:\n\n- **Random variables** can take different values with different probabilities\n- The **sample space** contains all possible outcomes\n- **Probability distributions** describe how likely each outcome is\n- The **AND rule** (multiply) and **OR rule** (add) combine probabilities\n- **Independence** means one event doesn't affect another's probability\n- **Conditional probability** describes probability given other information\n- **Bayes' theorem** updates beliefs based on new evidence\n- **Monte Carlo simulations** estimate probabilities through repeated random sampling\n- Classic problems like Monty Hall and birthdays reveal counterintuitive probability results\n\nUnderstanding these foundations is essential for all statistical inference that follows.\n\n## Practice Exercises\n\n### Exercise P.1: Simulating Coin Flips\n\nUse R to simulate coin flips:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(123)\n```\n:::\n\n\n1. Simulate 100 fair coin flips using `rbinom()` or `sample()`\n2. Calculate the proportion of heads\n3. Repeat with 1000 and 10000 flips—how does the proportion change?\n4. Create a histogram of results from many simulations\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Simulate coin flips\nn_flips <- 1000\nflips <- rbinom(n_flips, size = 1, prob = 0.5)\nmean(flips)  # Proportion of heads (1s)\n\n# Or using sample\nflips <- sample(c(\"H\", \"T\"), n_flips, replace = TRUE)\nmean(flips == \"H\")\n```\n:::\n\n\n### Exercise P.2: Binomial Distribution\n\nExplore the binomial distribution:\n\n1. Use `rbinom()` to simulate 1000 experiments, each with 20 coin flips\n2. Create a histogram of the number of heads per experiment\n3. What is the most common outcome? Does this match your expectation?\n4. Change the probability to simulate an unfair coin\n5. How does the distribution change with 200 or 2000 flips per experiment?\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# 1000 experiments, 20 flips each, fair coin\nset.seed(42)\nresults <- rbinom(1000, size = 20, prob = 0.5)\nhist(results, breaks = 20, col = \"steelblue\",\n     main = \"Number of Heads in 20 Flips\",\n     xlab = \"Number of Heads\")\n```\n\n::: {.cell-output-display}\n![](07-probability-foundations_files/figure-html/unnamed-chunk-19-1.png){width=576}\n:::\n:::\n\n\n### Exercise P.3: The Birthday Problem\n\nUse Monte Carlo simulation to explore the birthday problem:\n\n1. Write a function that simulates whether any two people in a group share a birthday\n2. Estimate the probability for groups of size 10, 23, and 50\n3. Plot the probability as a function of group size\n4. At what group size does the probability exceed 50%?\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Birthday simulation function\nsame_birthday <- function(n, B = 10000) {\n  matches <- replicate(B, {\n    birthdays <- sample(1:365, n, replace = TRUE)\n    any(duplicated(birthdays))\n  })\n  mean(matches)\n}\n\n# Test for different group sizes\nsizes <- 2:50\nprobs <- sapply(sizes, same_birthday)\nplot(sizes, probs, type = \"l\",\n     xlab = \"Group Size\", ylab = \"Probability of Shared Birthday\")\nabline(h = 0.5, col = \"red\", lty = 2)\n```\n:::\n\n\n### Exercise P.4: Conditional Probability\n\nExplore conditional probability with card simulations:\n\n1. Create a virtual deck of 52 cards\n2. Calculate the probability of drawing a King\n3. Given that the first card drawn is a King, what is the probability the second card is also a King?\n4. Use simulation to verify your calculation\n\n### Exercise P.5: The Monty Hall Problem\n\nSimulate the Monty Hall problem:\n\n1. Write a function that simulates one round of the game\n2. Compare the win rate when you stick versus when you switch\n3. Run 10,000 simulations for each strategy\n4. Does switching really double your chances?\n\n## Additional Resources\n\n- @irizarry2019introduction - A gitbook by a statistician with excellent introductions to key topics in statistical inference\n- @logan2010biostatistical - A comprehensive introduction to R for statistical analysis\n- For a detailed reference of common probability distributions, see @sec-probability-distributions\n",
    "supporting": [
      "07-probability-foundations_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}