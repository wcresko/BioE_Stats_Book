[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Statistics for the Biosciences and Bioengineering",
    "section": "",
    "text": "This books is meant to accompany the graduate statistics courses I teach at the University of Oregon, including Statistics for Bioengineering. This book provides the foundational skills needed for a successful scientific career in bioengineering and biosciences, combining rigorous statistical theory with practical computational implementation in R. I’ve written this book to",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Why This Book?</span>"
    ]
  },
  {
    "objectID": "index.html#why-this-book",
    "href": "index.html#why-this-book",
    "title": "Statistics for the Biosciences and Bioengineering",
    "section": "0.1 Why This Book?",
    "text": "0.1 Why This Book?\nModern biosciences and bioengineering research generates vast amounts of data—from RNA sequencing experiments to biomaterial characterization studies, from neural recordings to clinical trial outcomes. Making sense of this data requires more than just running statistical tests; it demands a deep understanding of the principles underlying those tests and the computational skills to implement them properly.\nThis course takes a practical, hands-on approach to learning statistics. Rather than deriving every formula from first principles, we will focus on understanding when and why to apply particular methods, how to implement them in R, and how to interpret and communicate results. Throughout the book, you’ll work with real biological data and develop the skills to analyze your own research questions.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Why This Book?</span>"
    ]
  },
  {
    "objectID": "index.html#what-you-will-learn",
    "href": "index.html#what-you-will-learn",
    "title": "Statistics for the Biosciences and Bioengineering",
    "section": "0.2 What You Will Learn",
    "text": "0.2 What You Will Learn\nThe material spans several interconnected domains:\nComputational Foundations. You will develop proficiency in basic Unix and R programming, including the tidyverse ecosystem for data manipulation and visualization. These tools form the backbone of modern reproducible research practices.\nStatistical Theory. The book covers probability distributions, parameter estimation, hypothesis testing, and the logic of statistical inference. Understanding these concepts allows you to choose appropriate methods and interpret results correctly.\nPractical Analysis Methods. From t-tests to linear regression to analysis of variance, you’ll learn to implement a wide range of statistical techniques. Each method is presented with clear guidance on assumptions, implementation in R, and interpretation of output.\nReproducible Research. Using Markdown, Git, and GitHub, you’ll learn to document your analyses in ways that others (including your future self) can understand and reproduce.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Why This Book?</span>"
    ]
  },
  {
    "objectID": "index.html#additional-resources",
    "href": "index.html#additional-resources",
    "title": "Statistics for the Biosciences and Bioengineering",
    "section": "0.3 Additional Resources",
    "text": "0.3 Additional Resources\n\nR for Data Science (RDS). 2025. Wickham, Çetinkaya-Rundel, and Grolemund. O’Reilly Press.\nModern Statistics with R (MSR). 2025. Måns Thulin, CRC Press.\nAn Introduction to Statistical Learning (ISLR). 2023. James, Witten, Hastie, Tibshirani. Springer.\nModern Statistics for Modern Biology. 2019. Holmes and Huber. Cambridge University Press.\nggPlot2: Elegant Graphics for Data Analysis, 3rd Edition. Wickham, Navarro, Pedersen. Springer.\nThe Visual Display of Quantitative Information. Tufte, E.R. Graphics Press.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Why This Book?</span>"
    ]
  },
  {
    "objectID": "index.html#software-requirements",
    "href": "index.html#software-requirements",
    "title": "Statistics for the Biosciences and Bioengineering",
    "section": "0.4 Software Requirements",
    "text": "0.4 Software Requirements\n\nLatest version of R (install here)\nLatest version of RStudio (install here)\nLaTeX for document preparation",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Why This Book?</span>"
    ]
  },
  {
    "objectID": "index.html#how-to-use-this-book",
    "href": "index.html#how-to-use-this-book",
    "title": "Statistics for the Biosciences and Bioengineering",
    "section": "0.5 How to Use This Book",
    "text": "0.5 How to Use This Book\nEach chapter builds on previous material, so working through the book sequentially is recommended for beginners. However, the modular organization also allows readers with some background to jump to specific topics of interest.\nCode examples are provided throughout the text, and you are strongly encouraged to type them yourself rather than copying and pasting. The act of typing reinforces learning and helps you notice details that might otherwise slip by. When you encounter errors—and you will—treat them as learning opportunities.\nThe exercises at the end of each chapter progress from straightforward applications of the material to more challenging problems requiring synthesis across topics. Attempting these exercises, even when difficult, is essential for developing genuine competence.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Why This Book?</span>"
    ]
  },
  {
    "objectID": "index.html#acknowledgments",
    "href": "index.html#acknowledgments",
    "title": "Statistics for the Biosciences and Bioengineering",
    "section": "0.6 Acknowledgments",
    "text": "0.6 Acknowledgments\nThis book grew out of many years of teaching biostatistics at the University of Oregon. I am grateful to the many students whose questions and struggles have shaped how I present this material, and to colleagues who have shared their insights on effective teaching of statistics. In particular Clay Small, Andrew Muehlheisen, Hannah Tavalire, Peter Ralph, Sabrina Moustofi and Hope Healey helped create previous versions of this material.\n\nBill Cresko Institute of Ecology and Evolution Knight Campus for Accelerating Scientific Impact\nUniversity of Oregon",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Why This Book?</span>"
    ]
  },
  {
    "objectID": "chapters/01-introduction.html",
    "href": "chapters/01-introduction.html",
    "title": "2  Introduction",
    "section": "",
    "text": "2.1 The Role of Statistics in Bioengineering\nBioengineering sits at the intersection of biology, engineering, and medicine, a field where understanding complex systems requires both precise measurement and rigorous analysis. Whether you are developing new biomaterials, engineering tissues, designing medical devices, or analyzing genomic data, you will encounter situations where you need to draw conclusions from imperfect data. Statistics provides the framework for doing this responsibly.\nAt its core, statistics addresses a fundamental problem: we almost never know the world perfectly, yet we still need to make decisions and draw conclusions. When you measure the mechanical properties of a hydrogel, characterize the response of neurons to a stimulus, or quantify gene expression in different treatment groups, you obtain samples from larger populations. Statistics gives us the tools to estimate underlying parameters from these samples and to quantify our uncertainty about those estimates.",
    "crumbs": [
      "Core Data Science Tools",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapters/01-introduction.html#the-role-of-statistics-in-bioengineering",
    "href": "chapters/01-introduction.html#the-role-of-statistics-in-bioengineering",
    "title": "2  Introduction",
    "section": "",
    "text": "Figure 2.2: Statistical literacy is essential for modern bioengineering careers",
    "crumbs": [
      "Core Data Science Tools",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapters/01-introduction.html#why-statistics-matters-for-your-career",
    "href": "chapters/01-introduction.html#why-statistics-matters-for-your-career",
    "title": "2  Introduction",
    "section": "2.2 Why Statistics Matters for Your Career",
    "text": "2.2 Why Statistics Matters for Your Career\nThe importance of statistical literacy for bioengineers cannot be overstated. Experimental design, data analysis, and the interpretation of results form the backbone of scientific research. Understanding statistics allows you to design experiments that can actually answer your questions, analyze data appropriately, and communicate your findings clearly and honestly.\nBeyond research, statistical thinking is increasingly important in industry applications. Quality control in biomanufacturing relies on statistical process control. Clinical trials require sophisticated statistical designs. Machine learning algorithms that power diagnostic tools and drug discovery pipelines are fundamentally statistical methods. Familiarity with these concepts will serve you throughout your career.\n\n\n\n\n\n\nFigure 2.3: Computational approaches enable powerful and reproducible data analysis",
    "crumbs": [
      "Core Data Science Tools",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapters/01-introduction.html#coding-and-scripting-for-data-analysis",
    "href": "chapters/01-introduction.html#coding-and-scripting-for-data-analysis",
    "title": "2  Introduction",
    "section": "2.3 Coding and Scripting for Data Analysis",
    "text": "2.3 Coding and Scripting for Data Analysis\nThis course emphasizes computational approaches to statistics. While it is possible to perform many statistical calculations by hand or using spreadsheet software, modern data analysis almost always involves programming. The ability to write code opens up enormous possibilities.\nProgramming is incredibly fast and powerful, particularly for repeated actions. A single command can accomplish what would require thousands of mouse clicks. You gain the ability to analyze large datasets that spreadsheet software cannot handle efficiently. You have access to thousands of free programs created by and for scientists. Your analyses become reproducible—you can document exactly what you did and share that documentation with others.\n\n\n\n\n\n\nFigure 2.4: Coding and scripting form the foundation of modern data science\n\n\n\nWe distinguish between coding and scripting, though the line between them has blurred considerably. Coding generally refers to programming in compiled languages like C++ or Fortran, where source code is translated into machine code before execution. Scripting typically involves interpreted languages like Python, R, or Julia, where commands are executed on the fly without a separate compilation step. Compiled code tends to run faster but is less flexible during development; scripting languages offer more interactivity at some cost in execution speed. Modern analytical pipelines typically combine both approaches, using scripting languages for data manipulation and visualization while calling compiled code for computationally intensive operations.",
    "crumbs": [
      "Core Data Science Tools",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapters/01-introduction.html#what-you-will-learn",
    "href": "chapters/01-introduction.html#what-you-will-learn",
    "title": "2  Introduction",
    "section": "2.4 What You Will Learn",
    "text": "2.4 What You Will Learn\nThis course provides broad coverage of the core components of modern statistics while giving you the computational tools necessary to carry out your work. By the end, you will be able to read and write code in Unix and R, implement reproducible research practices through Markdown, GitHub, and cloud computing platforms, perform exploratory data analysis and visualization, understand probability in the context of distributions and sampling, and conduct a wide range of statistical analyses from t-tests to linear models to machine learning methods.\nThe course is organized around progressive skill building. We start with the computational foundations—Unix, R, and tools for reproducible research. We then develop the probability theory needed to understand statistical inference. With these foundations in place, we cover classical hypothesis testing and parametric methods before moving to more advanced topics like linear models and statistical learning.",
    "crumbs": [
      "Core Data Science Tools",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapters/01-introduction.html#course-philosophy",
    "href": "chapters/01-introduction.html#course-philosophy",
    "title": "2  Introduction",
    "section": "2.5 Course Philosophy",
    "text": "2.5 Course Philosophy\nThis is a practical course, and we will learn by doing. Class time will be devoted primarily to hands-on coding practice rather than traditional lecturing. You will work through exercises, debug code, and analyze real data. This active approach to learning is more challenging than passive note-taking, but it produces much deeper understanding.\nExpect to struggle at times. Programming is frustrating, especially when you are learning. Error messages will seem cryptic. Code that should work will not work. Problems that seem simple will prove difficult. This is normal, and working through these challenges is how you develop genuine competence. The goal is not to avoid mistakes but to develop the skills to diagnose and fix them.\nThroughout the course, we emphasize reproducibility and transparency. Your analyses should be documented in ways that allow others to understand and verify what you did. This is not just good practice for collaboration; it also helps you when you return to your own work months or years later.",
    "crumbs": [
      "Core Data Science Tools",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapters/01-introduction.html#statistical-thinking",
    "href": "chapters/01-introduction.html#statistical-thinking",
    "title": "2  Introduction",
    "section": "2.6 Statistical Thinking",
    "text": "2.6 Statistical Thinking\nStatistics ultimately aims to turn data into conclusions about the world. We want to make point estimates and construct confidence intervals that quantify our uncertainty. We design experiments that can distinguish between competing hypotheses. We test those hypotheses using data. When dealing with high-dimensional data, we need methods to reduce complexity while preserving important information.\nAll of this requires a firm understanding of probability, sampling, and distributions. Probability provides the mathematical framework for reasoning about uncertainty. Understanding how samples relate to populations allows us to make inferences about things we cannot directly observe. Knowledge of common probability distributions tells us what to expect under various conditions and helps us identify when data deviate from expectations.\nWe will explore two major approaches to statistical inference. Frequentist statistics, the classical approach taught in most introductory courses, interprets probabilities as long-run frequencies and uses null hypothesis testing as its primary framework. Hierarchical probabilistic modeling, including maximum likelihood estimation and Bayesian methods, provides complementary tools that are increasingly important in modern statistical practice. Both perspectives have their uses, and understanding both will make you a more versatile analyst.",
    "crumbs": [
      "Core Data Science Tools",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapters/01-introduction.html#getting-started",
    "href": "chapters/01-introduction.html#getting-started",
    "title": "2  Introduction",
    "section": "2.7 Getting Started",
    "text": "2.7 Getting Started\nThe remainder of this chapter covers the practical matters of getting your computational environment set up. In subsequent chapters, we will dive into the material itself, beginning with Unix and the command line before moving to R and RStudio. With these tools in place, we will begin our exploration of probability, inference, and statistical modeling.\nThe journey ahead requires effort, but the skills you develop will serve you throughout your career. Let’s begin.",
    "crumbs": [
      "Core Data Science Tools",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapters/02-installing-tools.html",
    "href": "chapters/02-installing-tools.html",
    "title": "3  Installing Core Tools",
    "section": "",
    "text": "3.1 Overview of Required Software\nBefore you can begin your journey in data science and statistical analysis, you need to set up your computing environment with the essential tools. This chapter guides you through installing R, RStudio, LaTeX, and command-line tools on Mac, Windows, and Linux systems.\nThe core tools you will need throughout this course include:",
    "crumbs": [
      "Core Data Science Tools",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Installing Core Tools</span>"
    ]
  },
  {
    "objectID": "chapters/02-installing-tools.html#overview-of-required-software",
    "href": "chapters/02-installing-tools.html#overview-of-required-software",
    "title": "3  Installing Core Tools",
    "section": "",
    "text": "Tool\nPurpose\n\n\n\n\nR\nStatistical computing and graphics\n\n\nRStudio\nIntegrated development environment (IDE) for R\n\n\nLaTeX\nDocument preparation system for typesetting\n\n\nQuarto\nScientific publishing system (included with RStudio)\n\n\nCommand-line tools\nUnix shell access for file management and scripting",
    "crumbs": [
      "Core Data Science Tools",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Installing Core Tools</span>"
    ]
  },
  {
    "objectID": "chapters/02-installing-tools.html#installing-r",
    "href": "chapters/02-installing-tools.html#installing-r",
    "title": "3  Installing Core Tools",
    "section": "3.2 Installing R",
    "text": "3.2 Installing R\nR is the foundation of our statistical computing environment. It must be installed before RStudio.\n\nMac\n\nVisit the Comprehensive R Archive Network (CRAN): https://cran.r-project.org/\nClick “Download R for macOS”\nDownload the latest .pkg file appropriate for your Mac:\n\nFor Apple Silicon Macs (M1/M2/M3): Download the arm64 version\nFor Intel Macs: Download the x86_64 version\n\nOpen the downloaded file and follow the installation prompts\nR will be installed in your Applications folder\n\n\n\n\n\n\n\nChecking Your Mac’s Processor\n\n\n\nClick the Apple menu () → “About This Mac”. If you see “Apple M1” or similar, you have an Apple Silicon Mac. If you see “Intel”, you have an Intel Mac.\n\n\n\n\nWindows\n\nVisit CRAN: https://cran.r-project.org/\nClick “Download R for Windows”\nClick “base”\nClick the download link for the latest version\nRun the downloaded .exe installer\nAccept the default options during installation\nR will be added to your Start menu\n\n\n\nLinux (Ubuntu/Debian)\nOpen a terminal and run:\n# Update package index\nsudo apt update\n\n# Install R\nsudo apt install r-base r-base-dev\n\n# Verify installation\nR --version\nFor other Linux distributions, consult the CRAN documentation for specific instructions.",
    "crumbs": [
      "Core Data Science Tools",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Installing Core Tools</span>"
    ]
  },
  {
    "objectID": "chapters/02-installing-tools.html#installing-rstudio",
    "href": "chapters/02-installing-tools.html#installing-rstudio",
    "title": "3  Installing Core Tools",
    "section": "3.3 Installing RStudio",
    "text": "3.3 Installing RStudio\nRStudio provides an integrated development environment that makes working with R much more convenient. It includes a code editor, console, file browser, and many other tools.\n\nAll Platforms\n\nVisit the RStudio download page: https://posit.co/download/rstudio-desktop/\nThe website should automatically detect your operating system\nClick the download button for your platform\nRun the installer:\n\nMac: Open the .dmg file and drag RStudio to your Applications folder\nWindows: Run the .exe installer and follow the prompts\nLinux: Install the .deb or .rpm package using your package manager\n\n\n\n\nVerifying the Installation\n\nOpen RStudio (not R directly)\nIn the Console pane, type R.version and press Enter\nYou should see version information displayed\n\n# Check R version\nR.version.string\n\n# Check that common packages can load\nlibrary(stats)",
    "crumbs": [
      "Core Data Science Tools",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Installing Core Tools</span>"
    ]
  },
  {
    "objectID": "chapters/02-installing-tools.html#installing-latex",
    "href": "chapters/02-installing-tools.html#installing-latex",
    "title": "3  Installing Core Tools",
    "section": "3.4 Installing LaTeX",
    "text": "3.4 Installing LaTeX\nLaTeX is used for creating beautifully typeset documents, particularly those containing mathematical equations. You have two main options: a full LaTeX distribution or the lightweight TinyTeX.\n\nOption 1: TinyTeX (Recommended)\nTinyTeX is a minimal LaTeX distribution that works well with R and Quarto. It automatically installs missing packages as needed.\nIn RStudio, open the Console and run:\n# Install tinytex package\ninstall.packages(\"tinytex\")\n\n# Install TinyTeX distribution\ntinytex::install_tinytex()\nThis process may take several minutes. Once complete, restart RStudio.\n\n\nOption 2: Full LaTeX Distribution\nIf you prefer a complete LaTeX installation:\nMac:\n\nDownload MacTeX from https://www.tug.org/mactex/\nRun the installer (this is a large download, approximately 4 GB)\nRestart your computer after installation\n\nWindows:\n\nDownload MiKTeX from https://miktex.org/download\nRun the installer\nDuring installation, choose “Yes” for automatic package installation\nRestart your computer\n\nLinux:\n# Ubuntu/Debian\nsudo apt install texlive-full\n\n# Or for a smaller installation\nsudo apt install texlive texlive-latex-extra texlive-fonts-recommended\n\n\nVerifying LaTeX Installation\nIn RStudio, run:\n# Check if LaTeX is available\ntinytex::is_tinytex()\n\n# Or check the LaTeX path\nSys.which(\"pdflatex\")",
    "crumbs": [
      "Core Data Science Tools",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Installing Core Tools</span>"
    ]
  },
  {
    "objectID": "chapters/02-installing-tools.html#installing-command-line-tools",
    "href": "chapters/02-installing-tools.html#installing-command-line-tools",
    "title": "3  Installing Core Tools",
    "section": "3.5 Installing Command-Line Tools",
    "text": "3.5 Installing Command-Line Tools\n\nMac\nmacOS has a Unix foundation, so command-line tools are partially available by default. However, you should install the full Xcode Command Line Tools:\n\nOpen Terminal (Applications → Utilities → Terminal)\nType the following command and press Enter:\n\nxcode-select --install\n\nA dialog will appear asking to install the tools—click “Install”\nWait for the installation to complete (may take 5-10 minutes)\n\nAlternatively, if you plan to develop software, you can install the full Xcode from the Mac App Store.\n\n\nWindows\nWindows does not natively include Unix command-line tools, but you have several options:\nOption 1: Windows Subsystem for Linux (WSL) - Recommended\nWSL allows you to run a full Linux environment within Windows:\n\nOpen PowerShell as Administrator (right-click Start → Windows Terminal (Admin))\nRun:\n\nwsl --install\n\nRestart your computer when prompted\nAfter restart, Ubuntu will install automatically\nCreate a username and password when prompted\nYou now have access to a full Linux terminal\n\nOption 2: Git Bash\nA lighter alternative that provides basic Unix commands:\n\nDownload Git for Windows: https://git-scm.com/download/win\nRun the installer\nSelect “Use Git from the Windows Command Prompt” during installation\nGit Bash will be available from your Start menu\n\n\n\nLinux\nLinux distributions come with a full suite of command-line tools by default. Open your terminal application to access them.",
    "crumbs": [
      "Core Data Science Tools",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Installing Core Tools</span>"
    ]
  },
  {
    "objectID": "chapters/02-installing-tools.html#installing-additional-r-packages",
    "href": "chapters/02-installing-tools.html#installing-additional-r-packages",
    "title": "3  Installing Core Tools",
    "section": "3.6 Installing Additional R Packages",
    "text": "3.6 Installing Additional R Packages\nOnce R and RStudio are installed, you should install the tidyverse collection of packages:\n# Install tidyverse (may take several minutes)\ninstall.packages(\"tidyverse\")\n\n# Install additional useful packages\ninstall.packages(c(\"knitr\", \"rmarkdown\", \"quarto\", \"devtools\"))\n\n# For this course, also install\ninstall.packages(c(\"pwr\", \"car\", \"lme4\", \"gapminder\"))\n\n\n\n\n\n\nPackage Installation\n\n\n\nPackage installation only needs to be done once. After installation, you load packages in each R session using library().",
    "crumbs": [
      "Core Data Science Tools",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Installing Core Tools</span>"
    ]
  },
  {
    "objectID": "chapters/02-installing-tools.html#installing-quarto",
    "href": "chapters/02-installing-tools.html#installing-quarto",
    "title": "3  Installing Core Tools",
    "section": "3.7 Installing Quarto",
    "text": "3.7 Installing Quarto\nQuarto is a scientific and technical publishing system that works with R, Python, and other languages. Recent versions of RStudio include Quarto, but you can also install it separately.\n\nAll Platforms\n\nVisit https://quarto.org/docs/get-started/\nDownload the installer for your operating system\nRun the installer\n\n\n\nVerifying Quarto Installation\nIn RStudio’s Terminal pane:\nquarto --version\nOr create a new Quarto document in RStudio (File → New File → Quarto Document) and verify it renders correctly.",
    "crumbs": [
      "Core Data Science Tools",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Installing Core Tools</span>"
    ]
  },
  {
    "objectID": "chapters/02-installing-tools.html#troubleshooting-common-issues",
    "href": "chapters/02-installing-tools.html#troubleshooting-common-issues",
    "title": "3  Installing Core Tools",
    "section": "3.8 Troubleshooting Common Issues",
    "text": "3.8 Troubleshooting Common Issues\n\nR and RStudio\n\n\n\n\n\n\n\nProblem\nSolution\n\n\n\n\nRStudio cannot find R\nReinstall R, then reinstall RStudio\n\n\nPackage installation fails\nCheck your internet connection; try install.packages(\"name\", dependencies = TRUE)\n\n\n“Package not available” error\nUpdate R to the latest version\n\n\n\n\n\nLaTeX\n\n\n\n\n\n\n\nProblem\nSolution\n\n\n\n\nPDF compilation fails\nRun tinytex::tlmgr_update() to update packages\n\n\nMissing .sty file\nRun tinytex::tlmgr_install(\"package-name\")\n\n\nTinyTeX installation hangs\nCheck firewall settings; try installing behind a different network\n\n\n\n\n\nCommand Line\n\n\n\n\n\n\n\nProblem\nSolution\n\n\n\n\n“Command not found” errors\nVerify the tool is installed; check your PATH environment variable\n\n\nWSL installation fails\nEnable virtualization in BIOS; run Windows Update\n\n\nPermission denied\nUse sudo on Mac/Linux; run as Administrator on Windows",
    "crumbs": [
      "Core Data Science Tools",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Installing Core Tools</span>"
    ]
  },
  {
    "objectID": "chapters/02-installing-tools.html#keeping-software-updated",
    "href": "chapters/02-installing-tools.html#keeping-software-updated",
    "title": "3  Installing Core Tools",
    "section": "3.9 Keeping Software Updated",
    "text": "3.9 Keeping Software Updated\nRegularly update your tools to get bug fixes and new features:\nR and RStudio: - Check for RStudio updates: Help → Check for Updates - Update R by downloading the latest version from CRAN\nR Packages:\n# Update all installed packages\nupdate.packages(ask = FALSE)\nTinyTeX:\ntinytex::tlmgr_update()\nCommand-line tools:\n# Mac (with Homebrew)\nbrew update && brew upgrade\n\n# Ubuntu/Debian Linux\nsudo apt update && sudo apt upgrade\n\n# Windows (WSL)\nsudo apt update && sudo apt upgrade",
    "crumbs": [
      "Core Data Science Tools",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Installing Core Tools</span>"
    ]
  },
  {
    "objectID": "chapters/02-installing-tools.html#summary",
    "href": "chapters/02-installing-tools.html#summary",
    "title": "3  Installing Core Tools",
    "section": "3.10 Summary",
    "text": "3.10 Summary\nYou now have a complete scientific computing environment:\n\nR provides the statistical computing foundation\nRStudio offers a powerful integrated development environment\nLaTeX/TinyTeX enables professional document typesetting\nCommand-line tools provide essential utilities for file management and scripting\nQuarto supports reproducible scientific publishing\n\nWith these tools installed, you are ready to begin learning data science and statistical analysis. In the following chapters, we will explore each of these tools in depth.",
    "crumbs": [
      "Core Data Science Tools",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Installing Core Tools</span>"
    ]
  },
  {
    "objectID": "chapters/02-installing-tools.html#practice-exercises",
    "href": "chapters/02-installing-tools.html#practice-exercises",
    "title": "3  Installing Core Tools",
    "section": "3.11 Practice Exercises",
    "text": "3.11 Practice Exercises\n\nExercise I.1: Verify Your Installation\n\nOpen RStudio and run sessionInfo() to see your R configuration\nInstall the tidyverse package if you have not already\nCreate a new Quarto document and render it to PDF\nOpen a terminal and run pwd (Mac/Linux) or cd (Windows) to verify command-line access\n\n\n\nExercise I.2: Explore RStudio\n\nFamiliarize yourself with the four main panes in RStudio\nTry the keyboard shortcut Ctrl+Enter (Windows/Linux) or Cmd+Enter (Mac) to run code\nCreate and save an R script (.R file)\nAccess the built-in help by typing ?mean in the console",
    "crumbs": [
      "Core Data Science Tools",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Installing Core Tools</span>"
    ]
  },
  {
    "objectID": "chapters/02-unix-command-line.html",
    "href": "chapters/02-unix-command-line.html",
    "title": "4  Unix and the Command Line",
    "section": "",
    "text": "4.1 What is Unix?\nUnix is a family of operating systems that originated at Bell Labs in 1969 and was released publicly in 1973. Its design philosophy emphasizes modularity—small programs that do one thing well and can be combined to accomplish complex tasks. This approach has proven remarkably durable, and Unix-based systems remain dominant in scientific computing, web servers, and high-performance computing environments.\nLinux is an open-source implementation of Unix that runs on everything from embedded devices to the world’s fastest supercomputers. MacOS is built on a Unix foundation, which means Mac users have native access to Unix commands. Windows historically used a different approach, but recent versions include the Windows Subsystem for Linux (WSL), allowing Windows users to run Linux environments alongside their Windows applications.\nUnderstanding Unix is essential for modern data science. You will need it to access remote computing resources like supercomputer clusters, to run bioinformatics software that is only available through the command line, and to automate repetitive tasks. The skills you develop here will transfer across platforms and remain relevant throughout your career.",
    "crumbs": [
      "Core Data Science Tools",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Unix and the Command Line</span>"
    ]
  },
  {
    "objectID": "chapters/02-unix-command-line.html#the-shell-and-terminal",
    "href": "chapters/02-unix-command-line.html#the-shell-and-terminal",
    "title": "4  Unix and the Command Line",
    "section": "4.2 The Shell and Terminal",
    "text": "4.2 The Shell and Terminal\nThe shell is a program that interprets your commands and communicates with the operating system. When you type a command, the shell parses it, figures out what you want to do, and tells the operating system to do it. The results are then displayed back to you.\nBash (Bourne Again SHell) is the most common shell on Linux systems and was the default on MacOS until recently (MacOS now defaults to zsh, which is very similar). The shell runs inside a terminal application, which provides the window where you type commands and see output.\n\n\n\n\n\n\nFigure 4.2: The shell interprets commands and communicates with the operating system\n\n\n\nOn Mac, you can access the terminal by opening the Terminal app or a third-party alternative like iTerm2. On Linux, look for a Terminal application in your system menus. Windows users should install the Windows Subsystem for Linux following Microsoft’s documentation, then access it through the Ubuntu app or similar.\nRStudio also includes a terminal pane, which can be convenient when you want shell access without leaving your R development environment.",
    "crumbs": [
      "Core Data Science Tools",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Unix and the Command Line</span>"
    ]
  },
  {
    "objectID": "chapters/02-unix-command-line.html#anatomy-of-a-shell-command",
    "href": "chapters/02-unix-command-line.html#anatomy-of-a-shell-command",
    "title": "4  Unix and the Command Line",
    "section": "4.3 Anatomy of a Shell Command",
    "text": "4.3 Anatomy of a Shell Command\nShell commands follow a consistent structure. You type a command name, possibly followed by options that modify its behavior, and arguments that specify what the command should operate on. The shell waits at a prompt—typically $ for regular users or # for administrators—indicating it is ready to accept input.\n\n\n\n\n\n\nFigure 4.3: Anatomy of a shell command showing command, options, and arguments\n\n\n\nConsider the command ls -l Documents. Here, ls is the command (list directory contents), -l is an option (use long format), and Documents is the argument (the directory to list). Options usually begin with a dash and can often be combined: ls -la combines the -l (long format) and -a (show hidden files) options.",
    "crumbs": [
      "Core Data Science Tools",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Unix and the Command Line</span>"
    ]
  },
  {
    "objectID": "chapters/02-unix-command-line.html#file-system-organization",
    "href": "chapters/02-unix-command-line.html#file-system-organization",
    "title": "4  Unix and the Command Line",
    "section": "4.4 File System Organization",
    "text": "4.4 File System Organization\nUnix organizes files in a hierarchical structure of directories (folders) and files. The root directory, represented by a single forward slash /, sits at the top of this hierarchy and contains all other directories.\n\n\n\n\n\n\nFigure 4.4: Unix organizes files in a hierarchical directory structure\n\n\n\nYour home directory is your personal workspace, typically located at /Users/yourusername on Mac or /home/yourusername on Linux. The tilde character ~ serves as a shorthand for your home directory, so ~/Documents refers to the Documents folder in your home directory.\nEvery file and directory has a path—a specification of its location in the file system. Absolute paths start from the root directory and give the complete location, like /Users/wcresko/Documents/data.csv. Relative paths specify location relative to your current directory, so if you are in your home directory, Documents/data.csv refers to the same file.",
    "crumbs": [
      "Core Data Science Tools",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Unix and the Command Line</span>"
    ]
  },
  {
    "objectID": "chapters/02-unix-command-line.html#navigation-commands",
    "href": "chapters/02-unix-command-line.html#navigation-commands",
    "title": "4  Unix and the Command Line",
    "section": "4.5 Navigation Commands",
    "text": "4.5 Navigation Commands\nThe most fundamental navigation command is pwd (print working directory), which tells you where you currently are in the file system. This is often the first thing you type when opening a terminal to orient yourself.\n\n\n\n\n\n\nFigure 4.5: The pwd command shows your current location in the file system\n\n\n\npwd\nThe ls command lists the contents of a directory. Without arguments, it lists the current directory. With a path argument, it lists that location.\nls                  # list current directory\nls Documents        # list the Documents folder\nls -l               # long format with details\nls -a               # include hidden files (starting with .)\nls -la              # combine long format and hidden files\nls -lS              # long format, sorted by size\n\n\n\n\n\n\nFigure 4.6: The ls command lists directory contents with various options\n\n\n\nThe cd command (change directory) moves you to a different location.\ncd Documents        # move into Documents\ncd ..               # move up one level (parent directory)\ncd ~                # move to home directory\ncd /                # move to root directory\ncd -                # move to previous location\n\n\n\n\n\n\nFigure 4.7: The cd command changes your working directory",
    "crumbs": [
      "Core Data Science Tools",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Unix and the Command Line</span>"
    ]
  },
  {
    "objectID": "chapters/02-unix-command-line.html#working-with-files-and-directories",
    "href": "chapters/02-unix-command-line.html#working-with-files-and-directories",
    "title": "4  Unix and the Command Line",
    "section": "4.6 Working with Files and Directories",
    "text": "4.6 Working with Files and Directories\nCreating new directories uses the mkdir command.\nmkdir project_data\nmkdir -p analysis/results/figures  # create nested directories\nThe -p flag tells mkdir to create parent directories as needed, which is useful for creating nested folder structures in one command.\nTo remove an empty directory, use rmdir.\nrmdir empty_folder                 # remove an empty directory\nNote that rmdir only works on empty directories. For directories with contents, you need rm -r (discussed below).\nCreating a new, empty file uses the touch command.\ntouch newfile.txt                  # create an empty file\ntouch notes.md data.csv            # create multiple files\nThe touch command is also useful for updating the modification timestamp of existing files without changing their contents.\nMoving and renaming files uses the mv command.\nmv old_name.txt new_name.txt       # rename a file\nmv file.txt Documents/             # move file to Documents\nmv file.txt Documents/newname.txt  # move and rename\nCopying files uses cp.\ncp original.txt copy.txt           # copy a file\ncp -r folder/ backup/              # copy a directory recursively\nRemoving files uses rm. Be careful with this command—there is no trash can or undo in the shell.\nrm unwanted_file.txt               # remove a file\nrm -r unwanted_folder/             # remove a directory and contents\nrm -i file.txt                     # ask for confirmation before removing\n\n\n\n\n\n\nInterrupting Commands\n\n\n\nIf you need to stop a running command—perhaps you started a process that is taking too long or realize you made a mistake—press Ctrl-C. This sends an interrupt signal that terminates most running processes and returns you to the command prompt.",
    "crumbs": [
      "Core Data Science Tools",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Unix and the Command Line</span>"
    ]
  },
  {
    "objectID": "chapters/02-unix-command-line.html#viewing-file-contents",
    "href": "chapters/02-unix-command-line.html#viewing-file-contents",
    "title": "4  Unix and the Command Line",
    "section": "4.7 Viewing File Contents",
    "text": "4.7 Viewing File Contents\nSeveral commands let you examine file contents without opening them in an editor.\nThe cat command displays the entire contents of a file.\ncat data.txt\nFor longer files, head and tail show the beginning and end.\nhead data.csv          # first 10 lines\nhead -n 20 data.csv    # first 20 lines\ntail data.csv          # last 10 lines\ntail -f logfile.txt    # follow a file as it grows\nThe less command opens an interactive viewer that lets you scroll through large files.\nless large_data.txt\nInside less, use arrow keys to scroll, / to search, and q to quit.\nThe wc command counts lines, words, and characters.\nwc data.txt            # lines, words, characters\nwc -l data.txt         # just lines",
    "crumbs": [
      "Core Data Science Tools",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Unix and the Command Line</span>"
    ]
  },
  {
    "objectID": "chapters/02-unix-command-line.html#getting-help",
    "href": "chapters/02-unix-command-line.html#getting-help",
    "title": "4  Unix and the Command Line",
    "section": "4.8 Getting Help",
    "text": "4.8 Getting Help\nUnix provides documentation through manual pages, accessible with the man command.\nman ls                 # manual page for ls command\nManual pages can be dense, but they are comprehensive. Use the spacebar to page through, / to search, and q to exit. Many commands also accept a --help flag that provides a shorter summary.\nls --help\nOf course, the internet provides extensive resources. When you encounter an unfamiliar command or error message, searching online often leads to helpful explanations and examples.",
    "crumbs": [
      "Core Data Science Tools",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Unix and the Command Line</span>"
    ]
  },
  {
    "objectID": "chapters/02-unix-command-line.html#pipes-and-redirection",
    "href": "chapters/02-unix-command-line.html#pipes-and-redirection",
    "title": "4  Unix and the Command Line",
    "section": "4.9 Pipes and Redirection",
    "text": "4.9 Pipes and Redirection\nOne of Unix’s most powerful features is the ability to combine simple commands into complex pipelines. The pipe operator | sends the output of one command to another command as input.\nls -l | head -n 5           # list files, show only first 5\ncat data.txt | wc -l        # count lines in file\nRedirection operators send output to files instead of the screen.\nls -l &gt; file_list.txt       # write output to file (overwrite)\nls -l &gt;&gt; file_list.txt      # append output to file\nThese features enable powerful text processing. Combined with tools like grep (search for patterns), sort, and cut (extract columns), you can accomplish sophisticated data manipulation with compact commands.\ngrep \"gene\" data.txt                    # find lines containing \"gene\"\ngrep -c \"gene\" data.txt                 # count matching lines\nsort data.txt                           # sort lines alphabetically\nsort -n numbers.txt                     # sort numerically\ncut -f1,3 data.tsv                      # extract columns 1 and 3 from tab-separated file",
    "crumbs": [
      "Core Data Science Tools",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Unix and the Command Line</span>"
    ]
  },
  {
    "objectID": "chapters/02-unix-command-line.html#advanced-text-processing",
    "href": "chapters/02-unix-command-line.html#advanced-text-processing",
    "title": "4  Unix and the Command Line",
    "section": "4.10 Advanced Text Processing",
    "text": "4.10 Advanced Text Processing\nThe basic commands above are just the beginning. Unix provides powerful tools for searching, manipulating, and transforming text files—skills that are invaluable when working with biological data.\n\nPattern Matching with grep\nThe grep command becomes even more powerful when you use special characters to define patterns. These patterns, called regular expressions, allow you to search for complex text structures.\nCommon special characters in grep patterns:\n\n^ matches the beginning of a line\n$ matches the end of a line\n. matches any single character (except newline)\n* matches zero or more of the preceding character\n\\s matches any whitespace character\n\ngrep \"^embryo\" data.tsv          # lines starting with \"embryo\"\ngrep \"gene$\" data.tsv            # lines ending with \"gene\"\ngrep \"sample.*control\" data.tsv  # lines with \"sample\" followed by anything then \"control\"\ngrep \"^embryo_10\\s\" data.tsv     # lines starting with \"embryo_10\" followed by whitespace\nUseful grep flags include:\n\n-c counts matching lines instead of displaying them\n-v returns lines that do NOT match the pattern (inverse match)\n-n includes line numbers in the output\n-i performs case-insensitive matching\n\ngrep -v \"^#\" data.tsv            # exclude comment lines starting with #\ngrep -n \"error\" logfile.txt      # show line numbers for matches\ngrep -c \"ATCG\" sequences.fasta   # count lines containing this sequence\n\n\nSearch and Replace with sed\nThe sed (stream editor) command is commonly used for search-and-replace operations. The basic syntax uses slashes to separate the pattern, replacement, and flags:\nsed 's/old/new/' file.txt        # replace first occurrence on each line\nsed 's/old/new/g' file.txt       # replace all occurrences (global)\nsed 's/\\t/,/g' file.tsv          # convert tabs to commas\nsed 's/^/prefix_/' file.txt      # add prefix to beginning of each line\nBy default, sed prints the modified text to the terminal. To modify a file in place, use the -i flag (use with caution):\nsed -i 's/old/new/g' file.txt    # modify file in place\nA safer approach is to redirect output to a new file:\nsed 's/old/new/g' input.txt &gt; output.txt\n\n\nColumn Operations with cut and join\nThe cut command extracts specific columns from delimited files. By default, it assumes tab-delimited data.\ncut -f1,2 data.tsv               # extract columns 1 and 2 (tab-delimited)\ncut -f1,3 -d\",\" data.csv         # extract columns 1 and 3 (comma-delimited)\ncut -f2-5 data.tsv               # extract columns 2 through 5\nThe join command combines two files based on a common field, similar to a database join. Both files should be sorted on the join field.\njoin file1.txt file2.txt         # join on first field\njoin -1 2 -2 1 file1.txt file2.txt  # join file1's column 2 with file2's column 1\n\n\nSorting with Advanced Options\nThe sort command has many options for controlling how data is sorted.\nsort -n data.txt                 # sort numerically\nsort -r data.txt                 # sort in reverse order\nsort -k2,2 data.tsv              # sort by second column\nsort -k2,2 -n data.tsv           # sort by second column numerically\nsort -k2,2 -nr data.tsv          # sort by second column, numerically, in reverse\nsort -u data.txt                 # sort and remove duplicate lines\nsort -t\",\" -k3,3 data.csv        # sort comma-separated file by third column\n\n\nFlexible Text Processing with awk\nThe awk command is an extremely powerful tool for processing structured text. It treats each line as a record and each column as a field, making it ideal for tabular data. Fields are referenced using $1, $2, etc., where $0 represents the entire line.\nawk '{print $1}' data.tsv                    # print first column\nawk '{print $1, $3}' data.tsv                # print columns 1 and 3\nawk -F\",\" '{print $1, $2}' data.csv          # specify comma as delimiter\nawk '{print NR, $0}' data.txt                # print line numbers with each line\nOne of awk’s strengths is its ability to filter rows based on conditions:\nawk '$3 &gt; 100 {print $1, $3}' data.tsv       # print columns 1 and 3 where column 3 &gt; 100\nawk '$2 == \"control\" {print $0}' data.tsv    # print lines where column 2 is \"control\"\nawk 'NR &gt; 1 {print $0}' data.tsv             # skip header (print from line 2 onward)\nawk '$4 &gt;= 0.05 {print $1}' results.tsv      # extract IDs where p-value &gt;= 0.05\nYou can also perform calculations:\nawk '{sum += $2} END {print sum}' data.tsv           # sum of column 2\nawk '{sum += $2} END {print sum/NR}' data.tsv        # average of column 2\nawk '{print $1, $2 * 1000}' data.tsv                 # multiply column 2 by 1000\n\n\nCombining Commands in Pipelines\nThe real power of Unix text processing comes from combining these tools. Here are some examples relevant to biological data analysis:\n# Count unique gene names in column 1 (skipping header)\ntail -n +2 data.tsv | cut -f1 | sort | uniq | wc -l\n\n# Extract rows with significant p-values and sort by effect size\nawk '$5 &lt; 0.05' results.tsv | sort -k3,3 -nr | head -20\n\n# Convert a file from comma to tab-delimited and extract specific columns\nsed 's/,/\\t/g' data.csv | cut -f1,3,5 &gt; subset.tsv\n\n# Find all unique values in column 2 and count occurrences\ncut -f2 data.tsv | sort | uniq -c | sort -nr\n\n# Process a FASTA file to count sequences per chromosome\ngrep \"^&gt;\" sequences.fasta | cut -d\":\" -f1 | sort | uniq -c\n\n\n\n\n\n\nLearning More\n\n\n\nThese tools have many more capabilities than we can cover here. The man pages provide comprehensive documentation, and online resources like the GNU Awk User’s Guide offer in-depth tutorials. With practice, you will develop intuition for which tool to use for different tasks.",
    "crumbs": [
      "Core Data Science Tools",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Unix and the Command Line</span>"
    ]
  },
  {
    "objectID": "chapters/02-unix-command-line.html#wildcards-and-pattern-matching",
    "href": "chapters/02-unix-command-line.html#wildcards-and-pattern-matching",
    "title": "4  Unix and the Command Line",
    "section": "4.11 Wildcards and Pattern Matching",
    "text": "4.11 Wildcards and Pattern Matching\nOne of Unix’s most powerful features is wildcards—special characters that match multiple files at once. Instead of typing each filename individually, you can specify patterns that match many files simultaneously.\nThe asterisk * matches any number of any characters (including zero characters):\nls *.csv              # all CSV files\nls data*              # all files starting with \"data\"\nls *.txt *.md         # all text and markdown files\nrm temp*              # remove all files starting with \"temp\"\nThe question mark ? matches exactly one character:\nls sample?.txt        # sample1.txt, sample2.txt, etc. (but not sample10.txt)\nls data_??.csv        # data_01.csv, data_AB.csv, etc.\nSquare brackets [] match any single character from a set:\nls sample[123].txt    # sample1.txt, sample2.txt, or sample3.txt\nls data_[0-9].csv     # data_0.csv through data_9.csv\nls file[A-Z].txt      # fileA.txt through fileZ.txt\n\n\n\n\n\n\nWildcard Safety\n\n\n\nCombining rm with wildcards can be dangerous. The command rm * deletes everything in the current directory without confirmation. Always use ls first to see what a wildcard pattern matches before using it with rm. Consider using rm -i for interactive confirmation when removing files with wildcards.\n\n\nWildcards are expanded by the shell before the command runs, so they work with any command:\n# Count lines in all CSV files\nwc -l *.csv\n\n# Copy all R scripts to a backup folder\ncp *.R backup/\n\n# Search for \"gene\" in all text files\ngrep \"gene\" *.txt",
    "crumbs": [
      "Core Data Science Tools",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Unix and the Command Line</span>"
    ]
  },
  {
    "objectID": "chapters/02-unix-command-line.html#environment-variables",
    "href": "chapters/02-unix-command-line.html#environment-variables",
    "title": "4  Unix and the Command Line",
    "section": "4.12 Environment Variables",
    "text": "4.12 Environment Variables\nUnix maintains settings called environment variables that affect how the shell and programs behave. These variables store information about your user session, system configuration, and program preferences. Environment variables are distinguished by a $ prefix when you reference them.\nSeveral important environment variables are set automatically:\n# Your home directory\necho $HOME\n/home/wcresko\n\n# Your username\necho $USER\nwcresko\n\n# Your current shell program\necho $SHELL\n/bin/bash\n\n# Where Unix looks for executable programs\necho $PATH\n/usr/local/bin:/usr/bin:/bin:/home/wcresko/bin\n\n# Your current working directory\necho $PWD\n/home/wcresko/projects\nThe PATH variable is particularly important—it contains a colon-separated list of directories where Unix searches for programs. When you type a command like ls or python, Unix looks through each directory in your PATH until it finds a matching executable. You can find where a program is located using which:\nwhich python\n/usr/bin/python\n\nwhich R\n/usr/local/bin/R\nYou can set your own environment variables and use them in commands:\n# Set a variable\nPROJECT_DIR=~/projects/analysis\n\n# Use it (note the $)\ncd $PROJECT_DIR\nls $PROJECT_DIR/data\nTo make environment variables available to subprocesses (programs you launch), use export:\nexport PROJECT_DIR=~/projects/analysis\nTo see all environment variables currently set, use env or printenv:\nenv | head -20       # Show first 20 environment variables\n\nShell Configuration Files\nYour shell can be customized through configuration files that run when you open a terminal. For Bash, the main configuration files are ~/.bashrc (for interactive shells) and ~/.bash_profile (for login shells). For zsh (the default on modern macOS), use ~/.zshrc.\nCommon customizations include:\n# Add a directory to your PATH\nexport PATH=\"$HOME/bin:$PATH\"\n\n# Create an alias (shortcut) for a common command\nalias ll='ls -la'\nalias rm='rm -i'    # Always ask before deleting\n\n# Set default options for programs\nexport R_LIBS_USER=\"$HOME/R/library\"\nAfter editing configuration files, apply the changes by either starting a new terminal or running:\nsource ~/.bashrc",
    "crumbs": [
      "Core Data Science Tools",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Unix and the Command Line</span>"
    ]
  },
  {
    "objectID": "chapters/02-unix-command-line.html#file-permissions",
    "href": "chapters/02-unix-command-line.html#file-permissions",
    "title": "4  Unix and the Command Line",
    "section": "4.13 File Permissions",
    "text": "4.13 File Permissions\nUnix is a multi-user system, and every file has permissions controlling who can read, write, or execute it. Understanding permissions is essential for security and for running scripts on shared computing clusters.\nWhen you run ls -l, you see permission strings at the beginning of each line:\nls -l\n-rwxr-xr-x 1 wcresko staff 2048 Jan 15 10:30 analysis.sh\ndrwxr-xr-x 3 wcresko staff   96 Jan 15 09:00 data\nThe permission string -rwxr-xr-x encodes three sets of permissions:\n\n\n\nPosition\nMeaning\n\n\n\n\n1st character\nType: - (file), d (directory), l (link)\n\n\nCharacters 2-4\nOwner permissions\n\n\nCharacters 5-7\nGroup permissions\n\n\nCharacters 8-10\nOthers (everyone else) permissions\n\n\n\nWithin each set, the three characters represent:\n\nr (read): View file contents or list directory contents\nw (write): Modify file or add/remove files in directory\nx (execute): Run file as program or enter directory\n- (dash): Permission denied for that operation\n\nSo -rwxr-xr-x means: this is a regular file; the owner can read, write, and execute; the group can read and execute; others can read and execute.\n\nChanging Permissions with chmod\nThe chmod command changes file permissions. You can use symbolic notation or numeric (octal) notation.\nSymbolic notation uses letters and operators:\n# Add execute permission for the owner\nchmod u+x script.sh\n\n# Remove write permission for group and others\nchmod go-w data.csv\n\n# Set exact permissions\nchmod u=rwx,g=rx,o=rx script.sh\nThe letters are: u (user/owner), g (group), o (others), a (all). The operators are: + (add), - (remove), = (set exactly).\nOctal notation uses numbers where each permission has a value:\n\n\n\nPermission\nValue\n\n\n\n\nread (r)\n4\n\n\nwrite (w)\n2\n\n\nexecute (x)\n1\n\n\n\nAdd the values for each set. For example, rwx = 4+2+1 = 7, and r-x = 4+0+1 = 5.\nchmod 755 script.sh    # rwxr-xr-x (executable script)\nchmod 644 data.csv     # rw-r--r-- (readable data file)\nchmod 700 private/     # rwx------ (private directory)\nCommon permission patterns:\n\n755: Executable scripts (owner can modify, everyone can run)\n644: Data files (owner can modify, everyone can read)\n700: Private directories (only owner has access)\n600: Private files (only owner can read/write)\n\n\n\nMaking Scripts Executable\nWhen you write a shell script, you need to make it executable before you can run it directly:\n# Create a simple script\necho '#!/bin/bash' &gt; myscript.sh\necho 'echo \"Hello, World!\"' &gt;&gt; myscript.sh\n\n# Try to run it (will fail)\n./myscript.sh\n# bash: ./myscript.sh: Permission denied\n\n# Make it executable\nchmod +x myscript.sh\n\n# Now it works\n./myscript.sh\n# Hello, World!\nThe #!/bin/bash line at the top of the script (called a “shebang”) tells Unix which program should interpret the script.",
    "crumbs": [
      "Core Data Science Tools",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Unix and the Command Line</span>"
    ]
  },
  {
    "objectID": "chapters/02-unix-command-line.html#connecting-to-remote-systems",
    "href": "chapters/02-unix-command-line.html#connecting-to-remote-systems",
    "title": "4  Unix and the Command Line",
    "section": "4.14 Connecting to Remote Systems",
    "text": "4.14 Connecting to Remote Systems\nThe ssh command (secure shell) lets you connect to remote computers.\nssh username@server.university.edu\nYou will use this to connect to computing clusters like Talapas for computationally intensive work. Once connected, you work in a shell environment on the remote system just as you would locally.\nThe scp command copies files between your computer and remote systems.\nscp local_file.txt username@server.edu:~/destination/\nscp username@server.edu:~/remote_file.txt ./local_copy.txt",
    "crumbs": [
      "Core Data Science Tools",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Unix and the Command Line</span>"
    ]
  },
  {
    "objectID": "chapters/02-unix-command-line.html#practice-exercises",
    "href": "chapters/02-unix-command-line.html#practice-exercises",
    "title": "4  Unix and the Command Line",
    "section": "4.15 Practice Exercises",
    "text": "4.15 Practice Exercises\nThe best way to learn command-line skills is through practice. Save a digital record of your work so that you can study it later if you need to.\n\nExercise U.1: Basic Navigation and File Operations\nOpen up a terminal and execute the following using Unix commands:\n\nPrint your working directory using pwd\nNavigate to a directory somewhere below your home directory where you want to practice writing files\nMake 5 directories called dir_1, dir_2, dir_3, dir_4, and dir_5 using mkdir\nWithin each of those directories, create files called file_1.txt, file_2.txt, and file_3.txt using touch\nOpen file_1.txt in dir_1 using a plain text editor (such as nano or vim), type a few words, and save it\nPrint file_1.txt in dir_1 to the terminal using cat\nDelete all files in dir_3 using rm\nList all of the contents of your current directory line-by-line using ls -l\nDelete dir_3 using rmdir\n\n\n\nExercise U.2: Working with Data Files\nFor this exercise, create a sample tab-separated file or download a GFF file from a genomics database.\n\nNavigate to dir_1\nCopy a data file (using its absolute path) to your current directory\nDelete the copy that is in your current directory, then copy it again using a relative path this time\nUse at least 3 different Unix commands to examine all or parts of your data file (try cat, head, tail, less, and wc)\nWhat is the file size? Use ls -lh to find out\nHow many lines does the file have? Use wc -l\nHow many lines begin with a specific pattern (like a chromosome name)? Use grep -c \"^pattern\"\nHow many unique entries are there in a specific column? Use cut and sort -u | wc -l\nSort the file based on reverse numeric order in a specific field using sort -k -nr\nCapture specific fields and write to a new file using cut and redirection\nReplace all instances of one string with another using sed 's/old/new/g'\n\n\n\nExercise U.3: Building Pipelines\nPractice combining commands with pipes to answer questions about your data:\n\nCount the number of unique values in the third column of a tab-separated file:\ncut -f3 data.tsv | sort | uniq | wc -l\nFind all lines containing a pattern, extract specific columns, and sort the results:\ngrep \"pattern\" data.tsv | cut -f1,2,5 | sort -k3,3 -n\nCreate a pipeline that filters rows based on a condition, extracts columns, and saves to a new file\nUse awk to filter rows where a numeric column exceeds a threshold:\nawk '$5 &gt; 1000 {print $1, $2, $5}' data.tsv\n\n\n\nExercise U.4: File Permissions and Scripts\n\nCreate a simple shell script that prints “Hello, World!” and the current date\nTry to run the script—what error do you get?\nMake the script executable using chmod +x\nRun the script and verify it works\nExamine the permissions of various files in your system using ls -l\nPractice changing permissions using both symbolic notation (chmod u+x) and octal notation (chmod 755)",
    "crumbs": [
      "Core Data Science Tools",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Unix and the Command Line</span>"
    ]
  },
  {
    "objectID": "chapters/02-unix-command-line.html#additional-resources",
    "href": "chapters/02-unix-command-line.html#additional-resources",
    "title": "4  Unix and the Command Line",
    "section": "4.16 Additional Resources",
    "text": "4.16 Additional Resources\n\nUnix/Linux Command Reference - A comprehensive cheat sheet of common commands\nUnix and Perl Primer for Biologists - An outstanding tutorial by Keith Bradnam and Ian Korf, specifically designed for life scientists\nIntroduction to Shell for Data Science - DataCamp’s interactive tutorial\nThe GNU Awk User’s Guide - Comprehensive documentation for mastering awk\nSoftware Carpentry Shell Lessons - Excellent tutorials designed for researchers",
    "crumbs": [
      "Core Data Science Tools",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Unix and the Command Line</span>"
    ]
  },
  {
    "objectID": "chapters/03-r-rstudio.html",
    "href": "chapters/03-r-rstudio.html",
    "title": "5  R and RStudio",
    "section": "",
    "text": "5.1 What is R?\nR is a computer programming language and environment especially useful for graphic visualization and statistical analysis of data. It is an offshoot of a language developed in 1976 at Bell Laboratories called S. R is an interpreted language, meaning that every time code is run it must be translated to machine language by the R interpreter, as opposed to being compiled prior to running. R is the premier computational platform for statistical analysis thanks to its GNU open-source status and countless packages contributed by diverse members of the scientific community.",
    "crumbs": [
      "Core Data Science Tools",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>R and RStudio</span>"
    ]
  },
  {
    "objectID": "chapters/03-r-rstudio.html#why-r",
    "href": "chapters/03-r-rstudio.html#why-r",
    "title": "5  R and RStudio",
    "section": "5.2 Why R?",
    "text": "5.2 Why R?\nR is a programming language designed specifically for statistical computing and graphics. Created in the early 1990s as an open-source implementation of the S language, R has become the lingua franca of statistical analysis in academia and is widely used in industry as well.\nSeveral features make R particularly well-suited for data analysis. It provides an extensive collection of statistical and graphical techniques built into the language. It is powerful, flexible, and completely free. It runs on Windows, Mac, and Linux, so your code will work across platforms. New capabilities are constantly being added through packages contributed by the community, with thousands of packages available for specialized analyses.\nR excels at reproducibility. You can keep your scripts to document exactly what analyses you performed. Unlike point-and-click software where actions leave no trace, R code provides a complete record of your analytical workflow. This record can be shared with collaborators, included in publications, and revisited years later when you need to remember how you produced a particular result.\nYou can write your own functions in R, extending the language to meet your specific needs. Extensive online help and active user communities mean that answers to most questions are a web search away. The RStudio integrated development environment makes working with R much more pleasant, especially for newcomers. And with tools like R Markdown and Quarto, you can embed your analyses in polished documents, presentations, websites, and books—this book itself was created with these tools.",
    "crumbs": [
      "Core Data Science Tools",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>R and RStudio</span>"
    ]
  },
  {
    "objectID": "chapters/03-r-rstudio.html#installing-r-and-rstudio",
    "href": "chapters/03-r-rstudio.html#installing-r-and-rstudio",
    "title": "5  R and RStudio",
    "section": "5.3 Installing R and RStudio",
    "text": "5.3 Installing R and RStudio\nR must be installed before RStudio. Download R from https://www.r-project.org, selecting the version appropriate for your operating system. Follow the installation instructions for your platform.\nRStudio is an integrated development environment (IDE) that makes working with R much easier. Download the free RStudio Desktop from https://www.rstudio.com. RStudio provides a console for running R commands, an editor for writing scripts, tools for viewing plots and data, and integration with version control systems.\nAfter installing both programs, launch RStudio. You will see a window divided into panes, each serving a different purpose. The console pane is where R commands are executed. The source pane is where you edit scripts and documents. The environment pane shows what objects currently exist in your R session. The files/plots/packages/help pane provides access to various utilities.",
    "crumbs": [
      "Core Data Science Tools",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>R and RStudio</span>"
    ]
  },
  {
    "objectID": "chapters/03-r-rstudio.html#r-basics",
    "href": "chapters/03-r-rstudio.html#r-basics",
    "title": "5  R and RStudio",
    "section": "5.4 R Basics",
    "text": "5.4 R Basics\nR evaluates expressions and returns results. You can use it as a calculator by typing arithmetic expressions at the console.\n\n\nCode\n4 * 4\n\n\n[1] 16\n\n\nCode\n(4 + 3 * 2^2)\n\n\n[1] 16\n\n\nNotice that R follows standard mathematical order of operations: exponentiation before multiplication and division, which come before addition and subtraction. Parentheses can override this ordering.",
    "crumbs": [
      "Core Data Science Tools",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>R and RStudio</span>"
    ]
  },
  {
    "objectID": "chapters/03-r-rstudio.html#variables-and-assignment",
    "href": "chapters/03-r-rstudio.html#variables-and-assignment",
    "title": "5  R and RStudio",
    "section": "5.5 Variables and Assignment",
    "text": "5.5 Variables and Assignment\nMore useful than evaluating isolated expressions is storing values in variables for later use. Variables are assigned using the &lt;- operator (a less-than sign followed by a hyphen).\n\n\nCode\nx &lt;- 2\nx * 3\n\n\n[1] 6\n\n\nCode\ny &lt;- x * 3\ny - 2\n\n\n[1] 4\n\n\nVariable names must begin with a letter but can contain letters, numbers, periods, and underscores after the first character. R is case-sensitive, so myVariable, MyVariable, and myvariable are three different names. Choose descriptive names that make your code readable. It is good practice to avoid periods in variable names, as they have other functionality in related programming languages like Python.\n\n\n\n\n\n\nInvalid Variable Names\n\n\n\nVariable names cannot begin with numbers or contain operators. The following will produce errors:\n\n\nCode\n3y &lt;- 3    # cannot start with a number\n3*y &lt;- 3   # cannot include operators\n\n\n\n\n\nReserved Words\nR has reserved words that cannot be used as variable names because they have special meaning in the language:\n\n\n\nReserved Words\nPurpose\n\n\n\n\nif, else\nConditional statements\n\n\nfor, while, repeat\nLoops\n\n\nfunction\nFunction definition\n\n\nin, next, break\nLoop control\n\n\nTRUE, FALSE\nLogical constants\n\n\nNULL, NA, NaN, Inf\nSpecial values\n\n\n\nR also has semi-reserved names—built-in functions and constants that you can technically overwrite but should avoid:\n\n\nCode\n# These work but are dangerous:\nT &lt;- 5       # Overwrites TRUE abbreviation\nc &lt;- \"text\"  # Shadows the c() function\nmean &lt;- 42   # Shadows mean()\n\n# If you accidentally overwrite something, remove it:\nrm(c)        # Restores access to c()\n\n\n\n\n\n\n\n\nAvoid Common Name Collisions\n\n\n\nNever name variables T, F (abbreviations for TRUE/FALSE), c, t, mean, sum, data, or df. These are commonly used R functions, and shadowing them leads to confusing errors.\n\n\nNote that when you assign a value to a variable, R does not print anything. To see a variable’s value, type its name alone or use the print() function.\n\n\nCode\nz &lt;- 100\nz\n\n\n[1] 100\n\n\nCode\nprint(z)\n\n\n[1] 100",
    "crumbs": [
      "Core Data Science Tools",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>R and RStudio</span>"
    ]
  },
  {
    "objectID": "chapters/03-r-rstudio.html#understanding-r-objects",
    "href": "chapters/03-r-rstudio.html#understanding-r-objects",
    "title": "5  R and RStudio",
    "section": "5.6 Understanding R Objects",
    "text": "5.6 Understanding R Objects\nA fundamental principle of R is that everything is an object. Numbers, text, datasets, functions—all are stored as objects with specific properties. Understanding this helps you debug problems and write better code.\nEvery object has a class (which determines how functions treat it) and a type (its underlying storage mode). Use class() and typeof() to examine objects:\n\n\nCode\n# Numbers are objects\nx &lt;- 42\nclass(x)\n\n\n[1] \"numeric\"\n\n\nCode\ntypeof(x)\n\n\n[1] \"double\"\n\n\nCode\n# Text strings are objects\nname &lt;- \"Gene Expression\"\nclass(name)\n\n\n[1] \"character\"\n\n\nCode\n# Even functions are objects!\nclass(mean)\n\n\n[1] \"function\"\n\n\nThe str() function (structure) provides a compact display of any object’s structure—it is one of the most useful diagnostic tools in R:\n\n\nCode\n# Examine a vector\nstr(c(1, 2, 3, 4, 5))\n\n\n num [1:5] 1 2 3 4 5\n\n\nCode\n# Examine a data frame\nstr(head(iris))\n\n\n'data.frame':   6 obs. of  5 variables:\n $ Sepal.Length: num  5.1 4.9 4.7 4.6 5 5.4\n $ Sepal.Width : num  3.5 3 3.2 3.1 3.6 3.9\n $ Petal.Length: num  1.4 1.4 1.3 1.5 1.4 1.7\n $ Petal.Width : num  0.2 0.2 0.2 0.2 0.2 0.4\n $ Species     : Factor w/ 3 levels \"setosa\",\"versicolor\",..: 1 1 1 1 1 1\n\n\nWhen functions produce errors or unexpected results, checking the class of your objects is often the first step toward understanding what went wrong.",
    "crumbs": [
      "Core Data Science Tools",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>R and RStudio</span>"
    ]
  },
  {
    "objectID": "chapters/03-r-rstudio.html#functions",
    "href": "chapters/03-r-rstudio.html#functions",
    "title": "5  R and RStudio",
    "section": "5.7 Functions",
    "text": "5.7 Functions\nFunctions are the workhorses of R. A function takes inputs (called arguments), performs some operation, and returns an output. R has many built-in functions, and packages provide thousands more.\n\n\nCode\nlog(10)\n\n\n[1] 2.302585\n\n\nCode\nsqrt(16)\n\n\n[1] 4\n\n\nCode\nexp(1)\n\n\n[1] 2.718282\n\n\nFunctions are called by typing their name followed by parentheses containing their arguments. Many functions accept multiple arguments, separated by commas. Arguments can be specified by position or by name.\n\n\nCode\nround(3.14159, digits = 2)\n\n\n[1] 3.14\n\n\nCode\nround(3.14159, 2)  # same result, argument specified by position\n\n\n[1] 3.14\n\n\nTo learn about a function, use the help system. Type ?functionname or help(functionname) to open the documentation.\n\n\nCode\n?round\nhelp(sqrt)",
    "crumbs": [
      "Core Data Science Tools",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>R and RStudio</span>"
    ]
  },
  {
    "objectID": "chapters/03-r-rstudio.html#vectors",
    "href": "chapters/03-r-rstudio.html#vectors",
    "title": "5  R and RStudio",
    "section": "5.8 Vectors",
    "text": "5.8 Vectors\nThe fundamental data structure in R is the vector, an ordered collection of values of the same type. You create vectors using the c() function (for concatenate or combine).\n\n\nCode\nnumbers &lt;- c(1, 2, 3, 4, 5)\nnumbers\n\n\n[1] 1 2 3 4 5\n\n\nCode\nnames &lt;- c(\"Alice\", \"Bob\", \"Carol\")\nnames\n\n\n[1] \"Alice\" \"Bob\"   \"Carol\"\n\n\nMany operations in R are vectorized, meaning they operate on entire vectors at once rather than requiring you to loop through elements.\n\n\nCode\nnumbers * 2\n\n\n[1]  2  4  6  8 10\n\n\nCode\nnumbers + 10\n\n\n[1] 11 12 13 14 15\n\n\nCode\nnumbers^2\n\n\n[1]  1  4  9 16 25\n\n\nYou can access individual elements using square brackets with an index (R uses 1-based indexing, so the first element is at position 1).\n\n\nCode\nnumbers[1]\n\n\n[1] 1\n\n\nCode\nnumbers[3]\n\n\n[1] 3\n\n\nCode\nnumbers[c(1, 3, 5)]\n\n\n[1] 1 3 5",
    "crumbs": [
      "Core Data Science Tools",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>R and RStudio</span>"
    ]
  },
  {
    "objectID": "chapters/03-r-rstudio.html#creating-sequences",
    "href": "chapters/03-r-rstudio.html#creating-sequences",
    "title": "5  R and RStudio",
    "section": "5.9 Creating Sequences",
    "text": "5.9 Creating Sequences\nR provides convenient functions for creating regular sequences.\n\n\nCode\n1:10\n\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\n\nCode\nseq(0, 10, by = 2)\n\n\n[1]  0  2  4  6  8 10\n\n\nCode\nseq(0, 1, length.out = 5)\n\n\n[1] 0.00 0.25 0.50 0.75 1.00\n\n\nCode\nrep(1, times = 5)\n\n\n[1] 1 1 1 1 1\n\n\nCode\nrep(c(1, 2), times = 3)\n\n\n[1] 1 2 1 2 1 2",
    "crumbs": [
      "Core Data Science Tools",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>R and RStudio</span>"
    ]
  },
  {
    "objectID": "chapters/03-r-rstudio.html#generating-random-numbers",
    "href": "chapters/03-r-rstudio.html#generating-random-numbers",
    "title": "5  R and RStudio",
    "section": "5.10 Generating Random Numbers",
    "text": "5.10 Generating Random Numbers\nR can generate random numbers from various probability distributions, which is invaluable for simulation and understanding statistical concepts.\n\n\nCode\n# Draw 1000 values from a normal distribution with mean 0 and SD 10\nx &lt;- rnorm(1000, mean = 0, sd = 10)\nhist(x)\n\n\n\n\n\n\n\n\nFigure 5.2: Histogram of 1000 random draws from a normal distribution with mean 0 and standard deviation 10\n\n\n\n\n\n\n\nCode\n# Draw from a binomial distribution: 1000 experiments, 20 trials each, p=0.5\nheads &lt;- rbinom(n = 1000, size = 20, prob = 0.5)\nhist(heads)\n\n\n\n\n\n\n\n\nFigure 5.3: Histogram of binomial distribution results from 1000 experiments of 20 coin flips each\n\n\n\n\n\nThe set.seed() function allows you to make random simulations reproducible by initializing the random number generator to a known state.\n\n\nCode\nset.seed(42)\nrnorm(5)\n\n\n[1]  1.3709584 -0.5646982  0.3631284  0.6328626  0.4042683\n\n\nCode\nset.seed(42)  # same seed produces same \"random\" numbers\nrnorm(5)\n\n\n[1]  1.3709584 -0.5646982  0.3631284  0.6328626  0.4042683",
    "crumbs": [
      "Core Data Science Tools",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>R and RStudio</span>"
    ]
  },
  {
    "objectID": "chapters/03-r-rstudio.html#data-frames",
    "href": "chapters/03-r-rstudio.html#data-frames",
    "title": "5  R and RStudio",
    "section": "5.11 Data Frames",
    "text": "5.11 Data Frames\nData frames are R’s structure for tabular data—rows of observations and columns of variables. Each column can contain a different type of data (numeric, character, logical), but all values within a column must be the same type.\n\n\nCode\n# Create a data frame from vectors\nhydrogel_concentration &lt;- factor(c(\"low\", \"high\", \"high\", \"high\", \n                                    \"medium\", \"medium\", \"medium\", \"low\"))\ncompression &lt;- c(3.4, 3.4, 8.4, 3, 5.6, 8.1, 8.3, 4.5)\nconductivity &lt;- c(0, 9.2, 3.8, 5, 5.6, 4.1, 7.1, 5.3)\n\nmydata &lt;- data.frame(hydrogel_concentration, compression, conductivity)\nmydata\n\n\n  hydrogel_concentration compression conductivity\n1                    low         3.4          0.0\n2                   high         3.4          9.2\n3                   high         8.4          3.8\n4                   high         3.0          5.0\n5                 medium         5.6          5.6\n6                 medium         8.1          4.1\n7                 medium         8.3          7.1\n8                    low         4.5          5.3\n\n\nAccess columns using the $ operator or square brackets.\n\n\nCode\nmydata$compression\n\n\n[1] 3.4 3.4 8.4 3.0 5.6 8.1 8.3 4.5\n\n\nCode\nmydata[, 2]  # second column\n\n\n[1] 3.4 3.4 8.4 3.0 5.6 8.1 8.3 4.5\n\n\nCode\nmydata[1, ]  # first row\n\n\n  hydrogel_concentration compression conductivity\n1                    low         3.4            0\n\n\nCode\nmydata[1, 2] # first row, second column\n\n\n[1] 3.4",
    "crumbs": [
      "Core Data Science Tools",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>R and RStudio</span>"
    ]
  },
  {
    "objectID": "chapters/03-r-rstudio.html#reading-and-writing-data",
    "href": "chapters/03-r-rstudio.html#reading-and-writing-data",
    "title": "5  R and RStudio",
    "section": "5.12 Reading and Writing Data",
    "text": "5.12 Reading and Writing Data\nReal analyses typically begin by reading data from external files. R provides functions for various file formats.\n\n\nCode\n# Read comma-separated values\ndata &lt;- read.csv(\"mydata.csv\")\n\n# Read tab-separated values\ndata &lt;- read.table(\"mydata.txt\", header = TRUE, sep = \"\\t\")\n\n# Read Excel files (requires readxl package)\nlibrary(readxl)\ndata &lt;- read_excel(\"mydata.xlsx\")\n\n\nSimilarly, you can write data to files.\n\n\nCode\nwrite.csv(mydata, \"output.csv\", row.names = FALSE)\nwrite.table(mydata, \"output.txt\", sep = \"\\t\", row.names = FALSE)",
    "crumbs": [
      "Core Data Science Tools",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>R and RStudio</span>"
    ]
  },
  {
    "objectID": "chapters/03-r-rstudio.html#basic-plotting",
    "href": "chapters/03-r-rstudio.html#basic-plotting",
    "title": "5  R and RStudio",
    "section": "5.13 Basic Plotting",
    "text": "5.13 Basic Plotting\nR has extensive graphics capabilities. The base plot() function creates scatterplots and other basic visualizations.\n\n\nCode\nx &lt;- 1:10\ny &lt;- x^2\nplot(x, y,\n     xlab = \"X values\",\n     ylab = \"Y squared\",\n     main = \"A Simple Plot\",\n     col = \"blue\",\n     pch = 19)\n\n\n\n\n\n\n\n\nFigure 5.4: A simple scatterplot showing the relationship between x and x squared\n\n\n\n\n\nHistograms visualize the distribution of a single variable.\n\n\nCode\ndata &lt;- rnorm(1000)\nhist(data, breaks = 30, col = \"lightblue\", main = \"Normal Distribution\")\n\n\n\n\n\n\n\n\nFigure 5.5: Histogram of 1000 random samples from a standard normal distribution\n\n\n\n\n\nBoxplots compare distributions across groups.\n\n\nCode\nboxplot(compression ~ hydrogel_concentration, data = mydata,\n        xlab = \"Concentration\", ylab = \"Compression\")\n\n\n\n\n\n\n\n\nFigure 5.6: Boxplot comparing compression values across hydrogel concentration levels\n\n\n\n\n\nWe will explore the more sophisticated ggplot2 package for graphics in a later chapter.",
    "crumbs": [
      "Core Data Science Tools",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>R and RStudio</span>"
    ]
  },
  {
    "objectID": "chapters/03-r-rstudio.html#scripts-and-reproducibility",
    "href": "chapters/03-r-rstudio.html#scripts-and-reproducibility",
    "title": "5  R and RStudio",
    "section": "5.14 Scripts and Reproducibility",
    "text": "5.14 Scripts and Reproducibility\nWhile you can type commands directly at the console, for anything beyond simple explorations you should write scripts—text files containing R commands that can be saved, edited, and rerun.\nIn RStudio, create a new script with File &gt; New File &gt; R Script. Type your commands in the script editor, and run them by placing your cursor on a line and pressing Ctrl+Enter (Cmd+Enter on Mac) or by selecting code and clicking Run.\nScripts should be self-contained, including all the commands needed to reproduce your analysis from start to finish. Begin scripts by loading required packages, then reading data, then performing analyses. Add comments (lines beginning with #) to explain what your code does and why.\n\n\nCode\n# Analysis of hydrogel mechanical properties\n# Author: Your Name\n# Date: 2025-04-01\n\n# Load required packages\nlibrary(tidyverse)\n\n# Read data\ndata &lt;- read.csv(\"hydrogel_data.csv\")\n\n# Calculate summary statistics\nsummary(data)\n\n# Create visualization\nggplot(data, aes(x = concentration, y = compression)) +\n  geom_boxplot()",
    "crumbs": [
      "Core Data Science Tools",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>R and RStudio</span>"
    ]
  },
  {
    "objectID": "chapters/03-r-rstudio.html#getting-help",
    "href": "chapters/03-r-rstudio.html#getting-help",
    "title": "5  R and RStudio",
    "section": "5.15 Getting Help",
    "text": "5.15 Getting Help\nWhen you encounter problems, R provides several resources. The ? operator opens documentation for functions. The help.search() function searches the help system for topics. The example() function runs examples from a function’s documentation.\n\n\nCode\n?mean\nhelp.search(\"regression\")\nexample(plot)\n\n\nBeyond R’s built-in help, the internet offers vast resources. Stack Overflow has answers to almost any R question you can imagine. Package vignettes provide tutorials for specific packages. The RStudio community forums are welcoming to beginners.\nWhen asking for help online, provide a minimal reproducible example—the smallest piece of code that demonstrates your problem, including sample data. This makes it much easier for others to understand and solve your issue.",
    "crumbs": [
      "Core Data Science Tools",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>R and RStudio</span>"
    ]
  },
  {
    "objectID": "chapters/03-r-rstudio.html#data-types-in-r",
    "href": "chapters/03-r-rstudio.html#data-types-in-r",
    "title": "5  R and RStudio",
    "section": "5.16 Data Types in R",
    "text": "5.16 Data Types in R\nR has several fundamental data types that you will work with frequently.\n\nCharacter Strings\nAssignments and operations can be performed on characters as well as numbers. Characters need to be set off by quotation marks to differentiate them from numeric objects or variable names.\n\n\nCode\nx &lt;- \"I Love\"\nprint(x)\n\n\n[1] \"I Love\"\n\n\nCode\ny &lt;- \"Biostatistics\"\nprint(y)\n\n\n[1] \"Biostatistics\"\n\n\nCode\n# Combine strings using c()\nz &lt;- c(x, y)\nprint(z)\n\n\n[1] \"I Love\"        \"Biostatistics\"\n\n\nThe variable z is now a vector of character objects. Note that we are overwriting our previous numeric assignments—a good general rule is to use descriptive, unique names for each variable.\n\n\nFactors\nSometimes we would like to treat character objects as if they were categorical units for subsequent calculations. These are called factors, and we can convert a character vector to factor class.\n\n\nCode\nz_factor &lt;- as.factor(z)\nprint(z_factor)\n\n\n[1] I Love        Biostatistics\nLevels: Biostatistics I Love\n\n\nCode\nclass(z_factor)\n\n\n[1] \"factor\"\n\n\nNote that factor levels are reported alphabetically. The class() function tells us what type of object we are working with—it is one of the most important diagnostic tools in R. Often you can debug your code simply by checking and changing the class of an object.\nFactors are especially important for statistical analyses where we might want to calculate the mean or variance for different experimental treatments. In that case, the treatments would be coded as different levels of a factor.\n\n\nMissing Values (NA)\nR uses special values to represent missing or undefined data. The most common is NA, which stands for “Not Available.”\n\n\nCode\nclass(NA)\n\n\n[1] \"logical\"\n\n\nNA is a logical data type and is distinct from the character string “NA”, the numeric 0, or an empty string. It is also a reserved word and cannot be used as a variable name.\nAny instance of a blank entry in your data file will be read into R as NA. Many functions in R will not work by default if passed any NA values:\n\n\nCode\nnum &lt;- c(0, 1, 2, NA, 4)\nmean(num)\n\n\n[1] NA\n\n\nCode\n# Use na.rm = TRUE to ignore missing values\nmean(num, na.rm = TRUE)\n\n\n[1] 1.75\n\n\nCode\n# Check for missing values\nis.na(num)\n\n\n[1] FALSE FALSE FALSE  TRUE FALSE\n\n\n\n\nFloating-Point Precision\nA common source of confusion involves floating-point arithmetic. Computers represent decimal numbers with limited precision, which can lead to unexpected results:\n\n\nCode\n# This seems wrong, but is due to how computers store decimals\n0.1 + 0.2 == 0.3\n\n\n[1] FALSE\n\n\nCode\n# The actual values differ slightly\nprint(0.1 + 0.2, digits = 20)\n\n\n[1] 0.30000000000000004441\n\n\nCode\nprint(0.3, digits = 20)\n\n\n[1] 0.2999999999999999889\n\n\nNever use == to compare floating-point numbers directly. Instead, use all.equal() which checks if values are “nearly equal” within a small tolerance:\n\n\nCode\n# Safe comparison for floating-point numbers\nall.equal(0.1 + 0.2, 0.3)\n\n\n[1] TRUE\n\n\nCode\n# Use isTRUE() if you need a logical result\nisTRUE(all.equal(0.1 + 0.2, 0.3))\n\n\n[1] TRUE\n\n\nThe tidyverse provides dplyr::near() as a convenient alternative, especially when filtering data frames:\n\n\nCode\n# Works well in filter operations\nlibrary(dplyr)\ndata |&gt; filter(near(value, target_value))\n\n\n\n\n\n\n\n\nFloating-Point Comparisons\n\n\n\nAlways use all.equal() or near() instead of == when comparing decimal calculations. This is a common source of bugs in data analysis code.",
    "crumbs": [
      "Core Data Science Tools",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>R and RStudio</span>"
    ]
  },
  {
    "objectID": "chapters/03-r-rstudio.html#more-on-vectors",
    "href": "chapters/03-r-rstudio.html#more-on-vectors",
    "title": "5  R and RStudio",
    "section": "5.17 More on Vectors",
    "text": "5.17 More on Vectors\n\nIndexing Vectors\nIsolating specific elements from vectors is called indexing. R uses 1-based indexing with square brackets [].\n\n\nCode\nx &lt;- c(10, 20, 30, 40, 50, 100, 200)\n\n# First element\nx[1]\n\n\n[1] 10\n\n\nCode\n# Third element\nx[3]\n\n\n[1] 30\n\n\nCode\n# Series of consecutive elements\nx[1:4]\n\n\n[1] 10 20 30 40\n\n\nCode\n# Last four elements\nx[4:7]\n\n\n[1]  40  50 100 200\n\n\nCode\n# Non-consecutive elements using c()\nx[c(1:3, 5)]\n\n\n[1] 10 20 30 50\n\n\nCode\n# All elements EXCEPT the first two\nx[-c(1:2)]\n\n\n[1]  30  40  50 100 200\n\n\n\n\nUseful Functions for Vectors\nFunctions that provide information about vectors:\n\nhead(): returns the first elements of an object\ntail(): returns the last elements of an object\nlength(): returns the number of elements in a vector\nclass(): returns the class of elements in a vector\n\nFunctions that modify or generate vectors:\n\nsort(): returns a sorted vector\nseq(): creates a sequence of values\nrep(): repeats values\n\n\n\nCode\nrep(1, 5)\n\n\n[1] 1 1 1 1 1\n\n\nCode\nrep(\"treatment\", 5)\n\n\n[1] \"treatment\" \"treatment\" \"treatment\" \"treatment\" \"treatment\"\n\n\nFunctions for random sampling:\n\nsample(): randomly selects elements from a vector\nrnorm(): draws values from a normal distribution\nrbinom(): draws values from a binomial distribution\nset.seed(): sets the random number generator seed for reproducibility\n\nFunctions to change data types:\n\nas.numeric(): converts to numeric class\nas.factor(): converts to factor class\nas.character(): converts to character class",
    "crumbs": [
      "Core Data Science Tools",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>R and RStudio</span>"
    ]
  },
  {
    "objectID": "chapters/03-r-rstudio.html#lists",
    "href": "chapters/03-r-rstudio.html#lists",
    "title": "5  R and RStudio",
    "section": "5.18 Lists",
    "text": "5.18 Lists\nLists in R are aggregates of different objects that can be mixed types and different lengths.\n\n\nCode\nvec1 &lt;- c(10, 20, 30, 40, 50, 100, 200)\nvec2 &lt;- c(\"happy\", \"sad\", \"grumpy\")\nvec3 &lt;- factor(c(\"high\", \"low\"))\n\nmylist &lt;- list(vec1, vec2, vec3)\nprint(mylist)\n\n\n[[1]]\n[1]  10  20  30  40  50 100 200\n\n[[2]]\n[1] \"happy\"  \"sad\"    \"grumpy\"\n\n[[3]]\n[1] high low \nLevels: high low\n\n\nCode\nclass(mylist)\n\n\n[1] \"list\"\n\n\nCode\nstr(mylist)\n\n\nList of 3\n $ : num [1:7] 10 20 30 40 50 100 200\n $ : chr [1:3] \"happy\" \"sad\" \"grumpy\"\n $ : Factor w/ 2 levels \"high\",\"low\": 1 2\n\n\nElements of lists are indexed with double square brackets [[]]. To access the second element of mylist:\n\n\nCode\nmylist[[2]]\n\n\n[1] \"happy\"  \"sad\"    \"grumpy\"\n\n\nCode\n# The second item of the second element\nmylist[[2]][2]\n\n\n[1] \"sad\"\n\n\nThe str() function (for “structure”) is extremely useful for understanding complex R objects.",
    "crumbs": [
      "Core Data Science Tools",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>R and RStudio</span>"
    ]
  },
  {
    "objectID": "chapters/03-r-rstudio.html#matrices",
    "href": "chapters/03-r-rstudio.html#matrices",
    "title": "5  R and RStudio",
    "section": "5.19 Matrices",
    "text": "5.19 Matrices\nMatrices in R are two-dimensional arrays where all elements must be the same type. They are indexed by [row, column].\n\n\nCode\n# Create a 3x3 matrix\nmatrix(1:9, nrow = 3, ncol = 3)\n\n\n     [,1] [,2] [,3]\n[1,]    1    4    7\n[2,]    2    5    8\n[3,]    3    6    9\n\n\nUseful matrix functions include:\n\ndim(): returns the dimensions (rows and columns)\nt(): transposes a matrix (swaps rows and columns)\ncbind(): combines columns\nrbind(): combines rows",
    "crumbs": [
      "Core Data Science Tools",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>R and RStudio</span>"
    ]
  },
  {
    "objectID": "chapters/03-r-rstudio.html#installing-and-using-packages",
    "href": "chapters/03-r-rstudio.html#installing-and-using-packages",
    "title": "5  R and RStudio",
    "section": "5.20 Installing and Using Packages",
    "text": "5.20 Installing and Using Packages\nBase R includes many useful functions, but the real power comes from packages—collections of functions contributed by the community. Packages are distributed via the Comprehensive R Archive Network (CRAN).\n\n\nCode\n# Install a package (only need to do once)\ninstall.packages(\"name_of_package\")\n\n# Check if package is installed\ninstalled.packages(\"name_of_package\")\n\n# Load package for use (needed each session)\nlibrary(name_of_package)\n\n\nNote that install.packages() requires the package name in quotation marks, while library() does not.\n\nNamespace Conflicts\nWhen you load multiple packages, function names can collide. If two packages define a function with the same name, the most recently loaded package “wins,” and its version masks the earlier one. R warns you when this happens:\n\n\nCode\nlibrary(dplyr)\n# Attaching package: 'dplyr'\n# The following objects are masked from 'package:stats':\n#     filter, lag\n\n\nThis message indicates that dplyr’s filter() and lag() functions are now masking the base R functions with those names. If you need the masked version, use the package prefix:\n\n\nCode\n# Use dplyr's filter (now the default after loading dplyr)\ndata |&gt; filter(x &gt; 5)\n\n# Explicitly use base R's filter\nstats::filter(x, method = \"convolution\")\n\n# You can use the prefix even without loading a package\nstringr::str_detect(text, \"pattern\")\n\n\nCommon conflicts occur between:\n\ndplyr::filter() and stats::filter()\ndplyr::lag() and stats::lag()\ndplyr::select() and MASS::select()\n\n\n\n\n\n\n\nAvoiding Conflicts\n\n\n\nThe :: notation explicitly specifies which package’s function to use. When writing scripts, it is good practice to use package::function() for functions that commonly conflict, making your code’s behavior explicit and predictable.",
    "crumbs": [
      "Core Data Science Tools",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>R and RStudio</span>"
    ]
  },
  {
    "objectID": "chapters/03-r-rstudio.html#the-split-apply-combine-approach",
    "href": "chapters/03-r-rstudio.html#the-split-apply-combine-approach",
    "title": "5  R and RStudio",
    "section": "5.21 The Split-Apply-Combine Approach",
    "text": "5.21 The Split-Apply-Combine Approach\nA common pattern in data analysis is to split data by groups, apply a function to each group, and combine the results. R provides several functions for this workflow.\n\nThe replicate() Function\nRepeats an expression multiple times and collects the results:\n\n\nCode\n# Shuffle integers 1-10 five times\nreplicate(5, sample(1:10, size = 10, replace = FALSE))\n\n\n      [,1] [,2] [,3] [,4] [,5]\n [1,]    3    9    9    3    5\n [2,]    1    2   10    8    4\n [3,]    8    3    3    6    9\n [4,]    9    6    4    9    1\n [5,]   10    5    2    4   10\n [6,]    7    4    1    7    7\n [7,]    4    1    5    5    6\n [8,]    5   10    8   10    2\n [9,]    6    8    6    2    8\n[10,]    2    7    7    1    3\n\n\n\n\nThe apply() Family\nThe apply() function applies a function to rows or columns of a matrix or data frame:\n\n\nCode\n# Create sample matrix\nm &lt;- matrix(1:12, nrow = 3, ncol = 4)\nm\n\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    4    7   10\n[2,]    2    5    8   11\n[3,]    3    6    9   12\n\n\nCode\n# Sum across rows (MARGIN = 1)\napply(m, 1, sum)\n\n\n[1] 22 26 30\n\n\nCode\n# Sum across columns (MARGIN = 2)\napply(m, 2, sum)\n\n\n[1]  6 15 24 33\n\n\n\n\nThe tapply() Function\nApplies a function to subsets of a vector, grouped by a factor:\n\n\nCode\n# Find maximum petal length for each species\ntapply(iris$Petal.Length, iris$Species, max)\n\n\n    setosa versicolor  virginica \n       1.9        5.1        6.9 \n\n\n\n\nThe aggregate() Function\nSummarizes multiple variables by groups:\n\n\nCode\n# Mean of each variable by species\naggregate(iris[, 1:4], by = list(Species = iris$Species), FUN = mean)\n\n\n     Species Sepal.Length Sepal.Width Petal.Length Petal.Width\n1     setosa        5.006       3.428        1.462       0.246\n2 versicolor        5.936       2.770        4.260       1.326\n3  virginica        6.588       2.974        5.552       2.026",
    "crumbs": [
      "Core Data Science Tools",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>R and RStudio</span>"
    ]
  },
  {
    "objectID": "chapters/03-r-rstudio.html#conditional-statements-with-ifelse",
    "href": "chapters/03-r-rstudio.html#conditional-statements-with-ifelse",
    "title": "5  R and RStudio",
    "section": "5.22 Conditional Statements with ifelse()",
    "text": "5.22 Conditional Statements with ifelse()\nThe ifelse() function provides vectorized conditional logic. The first argument is a logical test, the second is the value if TRUE, and the third is the value if FALSE.\n\n\nCode\n# Create a character vector\ntreatment &lt;- c(rep(\"treatment\", 5), rep(\"control\", 3),\n               rep(\"treatment\", 4), rep(\"control\", 6))\n\n# Assign colors based on treatment\ncolors &lt;- ifelse(treatment == \"treatment\", \"red\", \"blue\")\nprint(colors)\n\n\n [1] \"red\"  \"red\"  \"red\"  \"red\"  \"red\"  \"blue\" \"blue\" \"blue\" \"red\"  \"red\" \n[11] \"red\"  \"red\"  \"blue\" \"blue\" \"blue\" \"blue\" \"blue\" \"blue\"",
    "crumbs": [
      "Core Data Science Tools",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>R and RStudio</span>"
    ]
  },
  {
    "objectID": "chapters/03-r-rstudio.html#for-loops",
    "href": "chapters/03-r-rstudio.html#for-loops",
    "title": "5  R and RStudio",
    "section": "5.23 For Loops",
    "text": "5.23 For Loops\nFor loops iterate through a sequence, executing code for each value. However, R is vectorized, so many operations that would require loops in other languages can be done more efficiently without them.\nWhen loops are necessary, pre-allocate output objects for better performance:\n\n\nCode\n# Pre-allocate a numeric vector\nresults &lt;- numeric(5)\n\nfor (i in 1:5) {\n  results[i] &lt;- i^2\n}\nresults\n\n\n[1]  1  4  9 16 25\n\n\n\n\n\n\n\n\nAvoiding Loops\n\n\n\nBefore writing a loop, consider whether the task can be accomplished with vectorized operations or the apply family of functions. These approaches are often faster and more readable.",
    "crumbs": [
      "Core Data Science Tools",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>R and RStudio</span>"
    ]
  },
  {
    "objectID": "chapters/03-r-rstudio.html#more-on-plotting",
    "href": "chapters/03-r-rstudio.html#more-on-plotting",
    "title": "5  R and RStudio",
    "section": "5.24 More on Plotting",
    "text": "5.24 More on Plotting\n\nCustomizing Plots with par()\nMany plotting parameters are controlled by the par() function. Understanding par() dramatically increases your plotting capabilities.\n\n\nCode\n# Create multiple panels\npar(mfrow = c(1, 2))  # 1 row, 2 columns\n\nseq_1 &lt;- seq(0, 10, by = 0.1)\nseq_2 &lt;- seq(10, 0, by = -0.1)\n\nplot(seq_1, xlab = \"Index\", ylab = \"Value\", type = \"p\", col = \"red\",\n     main = \"Increasing Sequence\")\nplot(seq_2, xlab = \"Index\", ylab = \"Value\", type = \"l\", col = \"blue\",\n     main = \"Decreasing Sequence\")\n\n\n\n\n\n\n\n\nFigure 5.7: Multiple plot panels showing increasing (points) and decreasing (lines) sequences\n\n\n\n\n\n\n\nVectorized Graphical Parameters\nGraphical parameters like col, pch (point character), and cex (character expansion) are vectorized:\n\n\nCode\nseq_1 &lt;- seq(0, 10, by = 0.1)\nseq_2 &lt;- seq(10, 0, by = -0.1)\n\n# First 10 points blue, rest red\ncolors &lt;- c(rep(\"blue\", 10), rep(\"red\", 91))\n\nplot(seq_1, seq_2, xlab = \"Sequence 1\", ylab = \"Sequence 2\",\n     col = colors, pch = 19,\n     main = \"Two-Color Scatterplot\")\n\n\n\n\n\n\n\n\nFigure 5.8: Scatterplot demonstrating vectorized graphical parameters with two colors\n\n\n\n\n\n\n\nUseful Plotting Arguments\nKey arguments for plot() and related functions:\n\nmain: plot title\nxlab, ylab: axis labels\nxlim, ylim: axis limits\ncol: color\npch: point character (0-25)\ncex: character/point size multiplier\nlwd: line width\ntype: “p” for points, “l” for lines, “b” for both",
    "crumbs": [
      "Core Data Science Tools",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>R and RStudio</span>"
    ]
  },
  {
    "objectID": "chapters/03-r-rstudio.html#introduction-to-r-markdown",
    "href": "chapters/03-r-rstudio.html#introduction-to-r-markdown",
    "title": "5  R and RStudio",
    "section": "5.25 Introduction to R Markdown",
    "text": "5.25 Introduction to R Markdown\nR Markdown combines R code with formatted text to create reproducible documents. Files have the .Rmd extension and can be rendered (“knitted”) to HTML, PDF, or Word.\n\nGetting Started\nInstall the rmarkdown package, then in RStudio: File → New File → R Markdown.\n\n\nBasic Formatting\n## Section Header\n### Subsection Header\n\nText can be *italicized* or **bolded** or ***both***.\n\nLinks: [Link Text](https://example.com)\n\n\nCode Chunks\nR code is placed in code chunks delimited by three backticks:\n```{r}\nseq(1, 10, 1)\n```\nChunk options control whether code is evaluated (eval), displayed (echo), and more:\n```{r, eval = TRUE, echo = TRUE}\nseq(1, 10, 1)\n```\n\n\nKnitting\nClick the “Knit” button in RStudio to render your document. Start with HTML output, which has the fewest dependencies.\n\n\n\n\n\n\nLearning More\n\n\n\nFor comprehensive R Markdown documentation, see the R Markdown introduction and R Markdown cheat sheet.",
    "crumbs": [
      "Core Data Science Tools",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>R and RStudio</span>"
    ]
  },
  {
    "objectID": "chapters/03-r-rstudio.html#practice-exercises",
    "href": "chapters/03-r-rstudio.html#practice-exercises",
    "title": "5  R and RStudio",
    "section": "5.26 Practice Exercises",
    "text": "5.26 Practice Exercises\n\nExercise R.1: Exploring RStudio\nTake a few minutes to familiarize yourself with the RStudio environment:\n\nLocate the four main panes:\n\nThe code editor (top left)\nThe workspace and history (top right)\nThe plots and files window (bottom right)\nThe R console (bottom left)\n\nIn the plots and files window, click on the Packages and Help tabs to see what they offer\nSee what types of new files can be made in RStudio by clicking File → New File\nOpen a new R script and a new R Markdown file to see the difference\n\n\n\nExercise R.2: Basic Mathematics in R\nInsert a code chunk and complete the following tasks:\n\nAdd and subtract numbers\nMultiply and divide numbers\nRaise a number to a power using the ^ symbol\nCreate a more complex equation involving all of these operations to convince yourself that R follows the normal priority of mathematical evaluation (PEMDAS)\n\n\n\nCode\n# Example:\n(4 + 3 * 2^2) / 5 - 1\n\n\n\n\nExercise R.3: Assigning Variables and Functions\n\nAssign three variables using basic mathematical operations\nTake the log of your three variables using log()\nUse the print() function to display your most complex variable\nUse the c() (concatenate) function combined with paste() to create and print a sentence\n\n\n\nCode\n# Example:\nx &lt;- 10\ny &lt;- x * 2\nz &lt;- sqrt(x + y)\nprint(paste(\"The value of z is\", z))\n\n\n\n\nExercise R.4: Vectors and Factors\n\nCreate a numeric vector using the c() function with at least 5 elements\nCreate a character vector and convert it to a factor using as.factor()\n\n\n\nCode\n# Example:\nvec1 &lt;- c(\"control\", \"treatment\", \"control\", \"treatment\", \"control\")\nfac1 &lt;- as.factor(vec1)\nprint(fac1)\n\n\n[1] control   treatment control   treatment control  \nLevels: control treatment\n\n\nCode\nlevels(fac1)\n\n\n[1] \"control\"   \"treatment\"\n\n\n\nUse str() and class() to evaluate your variables\nWhat is the difference between a character vector and a factor?\n\n\n\nExercise R.5: Basic Statistics\n\nCreate a numeric vector with at least 10 elements\nCalculate the mean(), sd(), sum(), length(), and var() of your vector\nUse the log() and sqrt() functions on your vector\nWhat happens when you try to apply mean() to a factor? Try it and explain the result\n\n\n\nCode\n# Example:\nmy_vector &lt;- c(12, 15, 18, 22, 25, 28, 31, 35, 38, 42)\nmean(my_vector)\nsd(my_vector)\n\n\n\n\nExercise R.6: Creating Sequences and Random Sampling\nSet the random seed for reproducibility, then:\n\n\nCode\nset.seed(42)\n\n\n\nCreate a vector with 100 elements using seq() and calculate the mean and standard deviation\nCreate a variable and sample() it with equal probability—experiment with the size and replace arguments\nCreate a normally distributed variable of 10000 elements using rnorm(), then sample that distribution with and without replacement\nUse hist() to plot your normally distributed variable\n\n\n\nExercise R.7: Basic Visualization\nCreate visualizations with proper axis labels and colors:\n\nCreate a sequence variable using seq() and make two different plots by changing the type argument (\"p\" for points, \"l\" for lines, \"b\" for both)\nCreate a normally distributed variable using rnorm() and make histograms with different breaks values—what does breaks control?\nUse par(mfrow = c(2, 2)) to create a 2×2 grid of plots\n\n\n\nCode\npar(mfrow = c(2, 2))\nx &lt;- seq(1, 100, by = 1)\nplot(x, type = \"p\", main = \"Points\", col = \"blue\")\nplot(x, type = \"l\", main = \"Lines\", col = \"red\")\ny &lt;- rnorm(1000)\nhist(y, breaks = 10, main = \"10 Breaks\", col = \"lightblue\")\nhist(y, breaks = 50, main = \"50 Breaks\", col = \"lightgreen\")\n\n\n\n\nExercise R.8: Creating Data Frames\n\nCreate a data frame with at least three columns: one character/factor, one numeric, and one logical\nAssign row names to your data frame using rownames()\nExamine your data frame structure using str()\nCalculate the mean of each numeric variable\nUse head() and tail() to view portions of your data frame\n\n\n\nCode\n# Example:\ntreatment &lt;- c(\"control\", \"low\", \"medium\", \"high\", \"control\", \"low\")\nresponse &lt;- c(12.3, 15.6, 18.9, 24.2, 11.8, 16.1)\nsignificant &lt;- c(FALSE, TRUE, TRUE, TRUE, FALSE, TRUE)\nmy_data &lt;- data.frame(treatment, response, significant)\nstr(my_data)\n\n\n\n\nExercise R.9: Data Import and Indexing\n\nCreate a simple CSV file or use a built-in dataset like iris\nUse read.csv() to read in your file (or access iris directly)\nUse str() and head() to examine the data structure\nUse $ and [ ] operators to select different parts of the data frame\nCreate a plot of two numeric variables\nUse tapply() to calculate summary statistics grouped by a categorical variable\nExport your data frame using write.csv()\n\n\n\nCode\n# Example with iris:\ndata(iris)\nstr(iris)\nhead(iris)\niris$Sepal.Length[1:5]  # First 5 sepal lengths\niris[1:3, ]  # First 3 rows\nplot(iris$Sepal.Length, iris$Petal.Length, col = iris$Species)\ntapply(iris$Sepal.Length, iris$Species, mean)\n\n\n\n\nExercise R.10: Understanding Object Types\nExplore how R handles different data types:\n\nCreate variables of different classes: numeric, character, logical, and factor\nWhat happens when you try to perform arithmetic on character data?\nExperiment with type coercion using as.numeric(), as.character(), and as.factor()\nWhat happens when you add a character element to a numeric vector?",
    "crumbs": [
      "Core Data Science Tools",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>R and RStudio</span>"
    ]
  },
  {
    "objectID": "chapters/03-r-rstudio.html#additional-resources",
    "href": "chapters/03-r-rstudio.html#additional-resources",
    "title": "5  R and RStudio",
    "section": "5.27 Additional Resources",
    "text": "5.27 Additional Resources\n\nLogan (2010) - A comprehensive introduction to R for statistical analysis\nA Primer for Computational Biology - Free online textbook by S.T. O’Neil\nR Colors Reference - Visual guide to R colors\nIntroduction to Colors in R - Tutorial on using colors effectively\n\n\n\n\n\n\n\nLogan, Murray. 2010. Biostatistical Design and Analysis Using r. Wiley-Blackwell.",
    "crumbs": [
      "Core Data Science Tools",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>R and RStudio</span>"
    ]
  },
  {
    "objectID": "chapters/04-markdown-latex.html",
    "href": "chapters/04-markdown-latex.html",
    "title": "6  Markdown and LaTeX",
    "section": "",
    "text": "6.1 The Power of Plain Text\nScientific communication requires more than just words—we need formatted text, mathematical equations, code, figures, and tables. Traditionally, researchers used word processors for this purpose, but word processors have significant limitations for technical writing. They obscure the structure of documents behind visual formatting. They make collaboration difficult because different versions become hard to reconcile. They separate code from the documents that describe it, making it easy for analyses and their descriptions to get out of sync.\nMarkup languages offer a different approach. You write in plain text, adding simple annotations that specify how the document should be formatted. A processor then converts this annotated text into beautifully formatted output. Because the source is plain text, it can be version-controlled, compared across versions, and edited with any text editor.",
    "crumbs": [
      "Core Data Science Tools",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Markdown and LaTeX</span>"
    ]
  },
  {
    "objectID": "chapters/04-markdown-latex.html#what-is-markdown",
    "href": "chapters/04-markdown-latex.html#what-is-markdown",
    "title": "6  Markdown and LaTeX",
    "section": "6.2 What is Markdown?",
    "text": "6.2 What is Markdown?\nMarkdown is a lightweight markup language designed to be easy to read and write. Created by John Gruber in 2004, it uses intuitive syntax that looks reasonable even before processing. A document written in Markdown can be rendered into HTML, PDF, Word documents, presentations, websites, and more.\nThe basic philosophy is that common formatting should be quick to type and not interrupt the flow of writing. Bold text is wrapped in double asterisks. Italics use single asterisks. Headers are indicated by hash marks at the start of a line.\n\nText Formatting\nTo make text italic, wrap it in single asterisks or underscores:\n*italic text* or _italic text_\nFor bold text, use double asterisks or underscores:\n**bold text** or __bold text__\nYou can combine them for bold italic:\n***bold and italic***\n\n\nHeaders\nHeaders structure your document into sections. Use hash marks at the start of a line, with more hashes indicating lower-level headers:\n# First-Level Header\n## Second-Level Header\n### Third-Level Header\n\n\nLists\nCreate bullet lists by starting lines with dashes, asterisks, or plus signs:\n- First item\n- Second item\n    - Sub-item (indent with spaces or tabs)\n    - Another sub-item\n- Third item\nFor numbered lists, use numbers followed by periods:\n1. First step\n2. Second step\n3. Third step\n\n\nBlock Quotes\nBlock quotes are useful for highlighting important passages or attributing quotes:\n&gt; \"You know the greatest danger facing us is ourselves, \n&gt; an irrational fear of the unknown. But there's no such \n&gt; thing as the unknown — only things temporarily hidden, \n&gt; temporarily not understood.\"\n&gt;\n&gt; --- Captain James T. Kirk\nThis renders as:\n\n“You know the greatest danger facing us is ourselves, an irrational fear of the unknown. But there’s no such thing as the unknown — only things temporarily hidden, temporarily not understood.”\n— Captain James T. Kirk\n\n\n\nLinks and Images\nLinks use square brackets for the text and parentheses for the URL:\n[Link text](https://www.example.com)\nImages use the same syntax with an exclamation mark prefix:\n![Image caption](path/to/image.png)\n\n\nCode\nInline code is wrapped in single backticks: `code`. Code blocks use triple backticks, optionally specifying the language for syntax highlighting:\n```r\nx &lt;- rnorm(100)\nmean(x)\n```",
    "crumbs": [
      "Core Data Science Tools",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Markdown and LaTeX</span>"
    ]
  },
  {
    "objectID": "chapters/04-markdown-latex.html#what-is-latex",
    "href": "chapters/04-markdown-latex.html#what-is-latex",
    "title": "6  Markdown and LaTeX",
    "section": "6.3 What is LaTeX?",
    "text": "6.3 What is LaTeX?\nLaTeX (pronounced “LAH-tek” or “LAY-tek”) is a document preparation system for high-quality typesetting, particularly of technical and scientific documents. Originally created by Leslie Lamport in the 1980s, LaTeX builds on the TeX typesetting system developed by Donald Knuth.\nLaTeX excels at mathematical notation. Complex equations that would be tedious or impossible to create in a word processor can be expressed elegantly in LaTeX. The system handles numbering, cross-references, bibliographies, and other scholarly apparatus automatically.\nMost importantly for our purposes, LaTeX can be embedded directly in Markdown documents. This gives us the best of both worlds: the simplicity of Markdown for prose and the power of LaTeX for mathematics.",
    "crumbs": [
      "Core Data Science Tools",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Markdown and LaTeX</span>"
    ]
  },
  {
    "objectID": "chapters/04-markdown-latex.html#mathematical-notation-with-latex",
    "href": "chapters/04-markdown-latex.html#mathematical-notation-with-latex",
    "title": "6  Markdown and LaTeX",
    "section": "6.4 Mathematical Notation with LaTeX",
    "text": "6.4 Mathematical Notation with LaTeX\n\nInline and Display Math\nMathematical expressions can appear inline with text or as centered display equations. Inline math is wrapped in single dollar signs: $x^2$ produces \\(x^2\\). Display equations use double dollar signs:\n$$E = mc^2$$\nProduces:\n\\[E = mc^2\\]\n\n\nGreek Letters and Symbols\nGreek letters are typed as their names preceded by a backslash:\n$$\\alpha, \\beta, \\gamma, \\delta, \\epsilon$$\n$$\\mu, \\sigma, \\pi, \\theta, \\lambda$$\n\\[\\alpha, \\beta, \\gamma, \\delta, \\epsilon\\] \\[\\mu, \\sigma, \\pi, \\theta, \\lambda\\]\nCommon mathematical symbols:\n$$\\neq, \\approx, \\leq, \\geq, \\pm, \\times, \\div$$\n\\[\\neq, \\approx, \\leq, \\geq, \\pm, \\times, \\div\\]\n\n\nSuperscripts and Subscripts\nSuperscripts use the caret ^ and subscripts use the underscore _:\n$$x^2, x_i, x_i^2, x_{ij}$$\n\\[x^2, x_i, x_i^2, x_{ij}\\]\nFor multi-character superscripts or subscripts, use curly braces:\n$$x^{n+1}, x_{i,j}$$\n\\[x^{n+1}, x_{i,j}\\]\n\n\nFractions\nFractions use the \\frac{numerator}{denominator} command:\n$$\\frac{a}{b}, \\frac{x^2 + 1}{x - 1}$$\n\\[\\frac{a}{b}, \\frac{x^2 + 1}{x - 1}\\]\n\n\nSquare Roots\nSquare roots and nth roots:\n$$\\sqrt{x}, \\sqrt[3]{x}, \\sqrt[n]{x}$$\n\\[\\sqrt{x}, \\sqrt[3]{x}, \\sqrt[n]{x}\\]\n\n\nSummation and Products\nSums and products with limits:\n$$\\sum_{i=1}^{n} x_i, \\prod_{i=1}^{n} x_i$$\n\\[\\sum_{i=1}^{n} x_i, \\prod_{i=1}^{n} x_i\\]\n\n\nIntegrals\nIntegrals with limits:\n$$\\int_{a}^{b} f(x) \\, dx$$\n$$\\iint f(x,y) \\, dx \\, dy$$\n\\[\\int_{a}^{b} f(x) \\, dx\\] \\[\\iint f(x,y) \\, dx \\, dy\\]\n\n\nStatistical Formulas\nHere are some common statistical formulas in LaTeX:\nThe sample mean:\n$$\\bar{x} = \\frac{1}{n}\\sum_{i=1}^{n} x_i$$\n\\[\\bar{x} = \\frac{1}{n}\\sum_{i=1}^{n} x_i\\]\nThe sample variance:\n$$s^2 = \\frac{1}{n-1}\\sum_{i=1}^{n} (x_i - \\bar{x})^2$$\n\\[s^2 = \\frac{1}{n-1}\\sum_{i=1}^{n} (x_i - \\bar{x})^2\\]\nThe binomial probability formula:\n$$P(X=k) = \\binom{n}{k} p^{k} (1-p)^{n-k}$$\n\\[P(X=k) = \\binom{n}{k} p^{k} (1-p)^{n-k}\\]\nThe Poisson probability formula:\n$$P(Y=r) = \\frac{e^{-\\mu}\\mu^r}{r!}$$\n\\[P(Y=r) = \\frac{e^{-\\mu}\\mu^r}{r!}\\]\nThe normal distribution density:\n$$f(x) = \\frac{1}{\\sigma\\sqrt{2\\pi}} e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}$$\n\\[f(x) = \\frac{1}{\\sigma\\sqrt{2\\pi}} e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}\\]\n\n\nMatrices\nMatrices are created with the matrix environment:\n$$\\begin{matrix}\na & b \\\\\nc & d\n\\end{matrix}$$\n\\[\\begin{matrix}\na & b \\\\\nc & d\n\\end{matrix}\\]\nFor brackets around the matrix, use pmatrix (parentheses) or bmatrix (square brackets):\n$$\\begin{pmatrix}\na & b \\\\\nc & d\n\\end{pmatrix}$$\n\\[\\begin{pmatrix}\na & b \\\\\nc & d\n\\end{pmatrix}\\]",
    "crumbs": [
      "Core Data Science Tools",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Markdown and LaTeX</span>"
    ]
  },
  {
    "objectID": "chapters/04-markdown-latex.html#quarto-and-r-markdown",
    "href": "chapters/04-markdown-latex.html#quarto-and-r-markdown",
    "title": "6  Markdown and LaTeX",
    "section": "6.5 Quarto and R Markdown",
    "text": "6.5 Quarto and R Markdown\nQuarto and R Markdown extend Markdown by allowing you to embed executable code chunks. When the document is rendered, the code runs and its output—whether text, tables, or figures—is automatically included in the final document.\n\nDocument Structure\nA Quarto document has three main parts:\n\nYAML Header: Document metadata and settings (enclosed by ---)\nMarkdown Content: Text and formatting\nCode Chunks: Executable code blocks\n\n\n\nYAML Header\nThe YAML header appears at the top of the document and configures how it will be rendered:\n---\ntitle: \"My Analysis Report\"\nauthor: \"Your Name\"\ndate: today\nformat: html\n---\nCommon YAML options include:\n\n\n\nOption\nDescription\n\n\n\n\ntitle\nDocument title\n\n\nauthor\nAuthor name(s)\n\n\ndate\nPublication date (today for current date)\n\n\nformat\nOutput format: html, pdf, docx\n\n\ntoc\nInclude table of contents (true/false)\n\n\ncode-fold\nCollapse code by default\n\n\nexecute\nGlobal code execution options\n\n\n\nA more complete header might look like:\n---\ntitle: \"Genomic Analysis Report\"\nauthor: \"Jane Scientist\"\ndate: today\nformat:\n  html:\n    theme: cosmo\n    toc: true\n    code-fold: true\nexecute:\n  echo: true\n  warning: false\n---\n\n\nCode Chunks\nA code chunk in Quarto looks like this:\n```{r}\nx &lt;- rnorm(100)\nmean(x)\n```\nWhen rendered, this shows both the code and its output:\n\n\nCode\nx &lt;- rnorm(100)\nmean(x)\n\n\n[1] 0.03075548\n\n\n\n\nChunk Options\nChunk options control how code is displayed and executed. In Quarto, options are specified with #| comments at the start of the chunk:\n```{r}\n#| label: my-analysis\n#| echo: false\n#| fig-width: 6\n#| fig-height: 4\nhist(rnorm(1000))\n```\nEssential chunk options:\n\n\n\nOption\nDescription\nDefault\n\n\n\n\necho\nShow code in output\ntrue\n\n\neval\nExecute the code\ntrue\n\n\ninclude\nInclude chunk in output\ntrue\n\n\nwarning\nShow warnings\ntrue\n\n\nmessage\nShow messages\ntrue\n\n\nfig-cap\nFigure caption\nnone\n\n\nfig-width\nFigure width (inches)\n7\n\n\nfig-height\nFigure height (inches)\n5\n\n\n\n\n\nCallout Blocks\nQuarto provides special callout blocks for highlighting information:\n::: {.callout-note}\nThis is a note with additional information.\n:::\n\n::: {.callout-warning}\nThis warns readers about potential issues.\n:::\n\n::: {.callout-tip}\nThis provides helpful tips.\n:::\nThese render as visually distinct boxes that draw attention to important content.\n\n\n\n\n\n\nNote\n\n\n\nThis is an example of a callout note—useful for supplementary information.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nThis is a warning callout—use for potential pitfalls or important cautions.\n\n\n\n\nCross-References\nQuarto supports cross-references to figures, tables, and sections. Label items and reference them with @:\n```{r}\n#| label: fig-scatter\n#| fig-cap: \"Relationship between x and y\"\nplot(x, y)\n```\n\nSee @fig-scatter for the scatter plot.\nFor sections, add a label after the header:\n## Methods {#sec-methods}\n\nAs described in @sec-methods, we used...\n\n\nRendering Documents\nTo render a Quarto document:\n\nIn RStudio: Click the “Render” button or press Ctrl+Shift+K (Windows/Linux) or Cmd+Shift+K (Mac)\nCommand line: Run quarto render document.qmd\n\nYou can render to multiple formats:\nquarto render document.qmd --to html\nquarto render document.qmd --to pdf\nquarto render document.qmd --to docx",
    "crumbs": [
      "Core Data Science Tools",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Markdown and LaTeX</span>"
    ]
  },
  {
    "objectID": "chapters/04-markdown-latex.html#why-this-matters",
    "href": "chapters/04-markdown-latex.html#why-this-matters",
    "title": "6  Markdown and LaTeX",
    "section": "6.6 Why This Matters",
    "text": "6.6 Why This Matters\nThe combination of Markdown, LaTeX, and executable code enables truly reproducible research. Your analysis code lives in the same document as your prose. When data change, you re-render the document and everything updates automatically. Collaborators can see exactly what you did. Future you can understand past you’s work.\nThese tools have become standard in data science. Learning them now will pay dividends throughout your career, whether you are writing homework assignments, thesis chapters, journal articles, or technical reports.",
    "crumbs": [
      "Core Data Science Tools",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Markdown and LaTeX</span>"
    ]
  },
  {
    "objectID": "chapters/04-markdown-latex.html#practice-exercises",
    "href": "chapters/04-markdown-latex.html#practice-exercises",
    "title": "6  Markdown and LaTeX",
    "section": "6.7 Practice Exercises",
    "text": "6.7 Practice Exercises\n\nExercise M.1: R Markdown Basics\nCreate a new R Markdown document and practice:\n\nCreating headers at different levels using #, ##, and ###\nMaking text italic and bold\nCreating ordered and unordered lists\nInserting a hyperlink\nCreating a code chunk that generates output\nKnitting the document to HTML\n\n\n\nExercise M.2: Code Chunk Options\nExperiment with code chunk options:\n\nCreate a code chunk with echo=TRUE and eval=TRUE (default behavior)\nCreate the same code chunk with echo=FALSE—what happens?\nTry eval=FALSE—what happens?\nUse fig.width and fig.height to control plot dimensions\n\n\n\nExercise M.3: LaTeX Mathematical Notation\nPractice writing equations in LaTeX:\n\nWrite the equation for the mean: \\(\\bar{x} = \\frac{1}{n}\\sum_{i=1}^{n} x_i\\)\nWrite the equation for variance\nWrite the normal distribution probability density function\nUse both inline math ($...$) and display math ($$...$$)\n\n\n\nExercise M.4: Tables in Markdown\nCreate tables using:\n\nBasic Markdown table syntax\nThe kable() function from the knitr package\n\n\n\nCode\nlibrary(knitr)\nkable(head(iris, 5))",
    "crumbs": [
      "Core Data Science Tools",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Markdown and LaTeX</span>"
    ]
  },
  {
    "objectID": "chapters/07-tidy-data.html",
    "href": "chapters/07-tidy-data.html",
    "title": "7  Tidy Data",
    "section": "",
    "text": "7.1 What is Tidy Data?\nData comes in many shapes, and not all shapes are equally convenient for analysis. The concept of “tidy data” provides a standard way to organize data that works well with R and makes many analyses straightforward. In tidy data, each variable forms a column, each observation forms a row, and each type of observational unit forms a table.\nThis structure might seem obvious, but real-world data rarely arrives in tidy form. Spreadsheets often encode information in column names, spread a single variable across multiple columns, or combine multiple variables in a single column. Data wrangling is the process of transforming messy data into tidy data.\nThe three interrelated rules that make a dataset tidy are:\nThese rules are interrelated because it is impossible to satisfy only two of the three. This interrelationship leads to an even simpler set of practical instructions:",
    "crumbs": [
      "Data Exploration",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Tidy Data</span>"
    ]
  },
  {
    "objectID": "chapters/07-tidy-data.html#what-is-tidy-data",
    "href": "chapters/07-tidy-data.html#what-is-tidy-data",
    "title": "7  Tidy Data",
    "section": "",
    "text": "Figure 7.2: In tidy data, each variable forms a column and each observation forms a row\n\n\n\n\n\n\nEach variable forms a column\nEach observation forms a row\nEach type of observational unit forms a table\n\n\n\nPut each dataset in a tibble (or data frame)\nPut each variable in a column",
    "crumbs": [
      "Data Exploration",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Tidy Data</span>"
    ]
  },
  {
    "objectID": "chapters/07-tidy-data.html#types-of-data",
    "href": "chapters/07-tidy-data.html#types-of-data",
    "title": "7  Tidy Data",
    "section": "7.2 Types of Data",
    "text": "7.2 Types of Data\nUnderstanding the types of data you are working with guides how you analyze them and how they should be represented in R.\n\nCategorical Data\nCategorical data classify observations into groups. There are two main types:\nNominal categorical data have no inherent order. Examples include: - Species names (e.g., Homo sapiens, Mus musculus) - Treatment groups (control, treatment A, treatment B) - Colors (red, blue, green) - Gene names (BRCA1, TP53, EGFR)\nOrdinal categorical data have a meaningful order, but the distances between categories may not be equal. Examples include: - Ratings (low, medium, high) - Educational levels (high school, bachelor’s, master’s, doctorate) - Disease severity (mild, moderate, severe) - T-shirt sizes (small, medium, large, extra large)\nIn R, categorical data are typically represented as factors, which store both the values and the set of possible levels. Factors are essential for statistical modeling and ensure that categories are handled consistently.\n\n\nQuantitative Data\nQuantitative data are numerical measurements. Understanding the scale of measurement is important for choosing appropriate statistical tests.\nDiscrete quantitative data can only take specific values, usually whole numbers: - Counts (number of offspring, number of cells) - Integers (age in years) - Scores on a fixed scale\nContinuous quantitative data can theoretically take any value within a range: - Mass, length, volume - Temperature - Concentration - Gene expression levels\nQuantitative data can also be classified by their scale properties:\nInterval data have meaningful differences between values but no true zero point: - Temperature in Celsius or Fahrenheit (0° does not mean “no temperature”) - Calendar dates (year 0 is arbitrary) - pH scale\nRatio data have a true zero and meaningful ratios: - Mass, length, volume (zero means “none”) - Counts (zero means “zero items”) - Concentration (zero means “nothing present”) - Time duration\nThe distinction matters because ratio data support statements like “twice as much” while interval data do not. Water at 20°C is not “twice as hot” as water at 10°C, but 20 grams is twice as heavy as 10 grams.\n\n\nData Types in R\nDifferent types of data are represented by different data types in R. Understanding these types helps you work with data more effectively.\n\n\n\n\n\n\n\n\n\nData Type\nDescription\nExamples\nR Type\n\n\n\n\nCharacter\nText strings\n“control”, “treatment”, gene names\ncharacter\n\n\nFactor\nCategorical with levels, unordered\nSpecies, treatment groups, colors\nfactor\n\n\nOrdered Factor\nCategorical with meaningful order\n“low” &lt; “medium” &lt; “high”\nordered\n\n\nInteger\nWhole numbers\nCount data, discrete measurements\ninteger\n\n\nNumeric\nReal numbers (decimal)\nMeasurements, ratios, continuous data\nnumeric or double\n\n\nLogical\nTrue/false values\nTest results, conditions\nlogical\n\n\nDate\nCalendar dates\n“2024-01-15”, sample collection dates\nDate\n\n\nDate-Time\nTimestamps with time\n“2024-01-15 14:30:00”\nPOSIXct\n\n\n\nYou can check the type of any variable using the class() or typeof() functions:\n\n\nCode\n# Examples of different data types\nx &lt;- \"hello\"          # character\ny &lt;- factor(c(\"A\", \"B\", \"A\"))  # factor\nz &lt;- 42               # numeric\nw &lt;- 42L              # integer (the L suffix)\nv &lt;- TRUE             # logical\nd &lt;- as.Date(\"2024-01-15\")  # Date\n\n# Check types\nclass(x)\n\n\n[1] \"character\"\n\n\nCode\nclass(y)\n\n\n[1] \"factor\"\n\n\nCode\nclass(z)\n\n\n[1] \"numeric\"\n\n\nCode\ntypeof(z)\n\n\n[1] \"double\"\n\n\nCode\nclass(w)\n\n\n[1] \"integer\"\n\n\nCode\ntypeof(w)\n\n\n[1] \"integer\"\n\n\nCode\nclass(v)\n\n\n[1] \"logical\"\n\n\nCode\nclass(d)\n\n\n[1] \"Date\"\n\n\nUnderstanding these types is crucial because:\n\nStatistical functions expect specific data types\nCategorical variables should be factors for modeling\nPlotting functions treat factors differently from character strings\nMathematical operations only work on numeric types",
    "crumbs": [
      "Data Exploration",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Tidy Data</span>"
    ]
  },
  {
    "objectID": "chapters/07-tidy-data.html#data-file-best-practices",
    "href": "chapters/07-tidy-data.html#data-file-best-practices",
    "title": "7  Tidy Data",
    "section": "7.3 Data File Best Practices",
    "text": "7.3 Data File Best Practices\nWhen working with data files, following consistent practices will save you time and prevent errors. These guidelines apply whether you are using Unix tools, R, or any other analysis software.\n\nDo\n\nStore data in plain text formats such as tab-separated (.tsv) or comma-separated (.csv) files. These nonproprietary formats can be read by any software and will remain accessible for years to come.\nKeep an unedited copy of original data files. Even when your analysis requires modifications, preserve the raw data separately.\nUse descriptive, consistent names for files and variables. A name like experiment1_control_measurements.tsv is far more useful than data2.txt.\nMaintain metadata that documents what each variable means, how data were collected, and any processing steps applied.\nAdd new observations as rows and new variables as columns to maintain a consistent rectangular structure.\nInclude a header row with variable names that are concise but descriptive.\nUse consistent date formats, preferably ISO 8601 format (YYYY-MM-DD).\nRepresent missing values consistently, typically as empty cells or NA.\n\n\n\nDon’t\n\nDon’t mix data types within a column. If a column contains numbers, every entry should be a number (or explicitly missing).\nDon’t use special characters in file or directory names. Stick to letters, numbers, underscores, and hyphens. Avoid spaces, which can cause problems with command-line tools.\nDon’t use delimiter characters in data values. If your file is comma-delimited, don’t use commas within data entries. For example, use 2024-03-08 rather than March 8, 2024 for dates.\nDon’t copy data from formatted documents like Microsoft Word directly into data files. Hidden formatting characters can corrupt your data.\nDon’t edit data files in spreadsheet programs that might silently convert values (for example, Excel’s tendency to convert gene names like SEPT1 to dates).\nDon’t use color or formatting to encode information. All information should be in the cell values themselves, not in formatting that will be lost.\nDon’t leave empty rows or columns as visual separators. These can cause problems when reading data into analysis software.\n\n\n\n\n\n\n\nPreserving Raw Data\n\n\n\nPerhaps the most important principle is to never modify your original raw data files. Keep them in a separate directory with restricted write permissions if possible. All data cleaning and transformation should be done programmatically (in scripts that can be re-run), with outputs saved to new files.",
    "crumbs": [
      "Data Exploration",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Tidy Data</span>"
    ]
  },
  {
    "objectID": "chapters/07-tidy-data.html#the-tidyverse",
    "href": "chapters/07-tidy-data.html#the-tidyverse",
    "title": "7  Tidy Data",
    "section": "7.4 The Tidyverse",
    "text": "7.4 The Tidyverse\nThe tidyverse is a collection of R packages designed for data science. These packages share a common philosophy and are designed to work together seamlessly. The core tidyverse packages include:\n\nggplot2: Data visualization using the grammar of graphics\ndplyr: Data manipulation and transformation\ntidyr: Reshaping and tidying data\nreadr: Reading rectangular data (CSV, TSV, etc.)\npurrr: Functional programming tools\ntibble: Enhanced data frames\nstringr: String manipulation\nforcats: Working with factors (categorical data)\n\n\n\n\n\n\n\nFigure 7.3: The tidyverse is a collection of R packages designed for data science\n\n\n\nLoading the tidyverse loads all core packages at once:\n\n\nCode\nlibrary(tidyverse)\n\n\nThe message shows which packages are attached and notes any functions that conflict with base R or other packages. For example, dplyr::filter() masks stats::filter(), meaning if you type filter(), you will get the dplyr version. To use the stats version, you would need to specify stats::filter() explicitly.",
    "crumbs": [
      "Data Exploration",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Tidy Data</span>"
    ]
  },
  {
    "objectID": "chapters/07-tidy-data.html#tibbles",
    "href": "chapters/07-tidy-data.html#tibbles",
    "title": "7  Tidy Data",
    "section": "7.5 Tibbles",
    "text": "7.5 Tibbles\nTibbles are the tidyverse’s enhanced data frames. They are designed to be more user-friendly and consistent than traditional data frames, while maintaining backward compatibility for most operations.\n\n\n\n\n\n\nFigure 7.4: Tibbles are the tidyverse’s enhanced data frames with improved printing\n\n\n\n\nCreating Tibbles\nYou can create tibbles using the tibble() function:\n\n\nCode\n# Create a tibble\nmy_tibble &lt;- tibble(\n  name = c(\"Alice\", \"Bob\", \"Carol\", \"David\"),\n  age = c(25, 32, 28, 45),\n  height_cm = c(165, 178, 172, 180),\n  treatment = factor(c(\"control\", \"drug_A\", \"drug_A\", \"control\"))\n)\nmy_tibble\n\n\n# A tibble: 4 × 4\n  name    age height_cm treatment\n  &lt;chr&gt; &lt;dbl&gt;     &lt;dbl&gt; &lt;fct&gt;    \n1 Alice    25       165 control  \n2 Bob      32       178 drug_A   \n3 Carol    28       172 drug_A   \n4 David    45       180 control  \n\n\nYou can also convert existing data frames to tibbles:\n\n\nCode\n# Convert a data frame to a tibble\niris_tibble &lt;- as_tibble(iris)\niris_tibble\n\n\n# A tibble: 150 × 5\n   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n          &lt;dbl&gt;       &lt;dbl&gt;        &lt;dbl&gt;       &lt;dbl&gt; &lt;fct&gt;  \n 1          5.1         3.5          1.4         0.2 setosa \n 2          4.9         3            1.4         0.2 setosa \n 3          4.7         3.2          1.3         0.2 setosa \n 4          4.6         3.1          1.5         0.2 setosa \n 5          5           3.6          1.4         0.2 setosa \n 6          5.4         3.9          1.7         0.4 setosa \n 7          4.6         3.4          1.4         0.3 setosa \n 8          5           3.4          1.5         0.2 setosa \n 9          4.4         2.9          1.4         0.2 setosa \n10          4.9         3.1          1.5         0.1 setosa \n# ℹ 140 more rows\n\n\n\n\nAdvantages of Tibbles\nBetter printing: Tibbles print more informatively than data frames. They show: - Only the first 10 rows by default - Only columns that fit on screen - The dimensions (rows × columns) - The data type of each column (shown below the column name)\nThe column type abbreviations include: - &lt;chr&gt;: character strings - &lt;dbl&gt;: double (numeric with decimals) - &lt;int&gt;: integer - &lt;lgl&gt;: logical (TRUE/FALSE) - &lt;fct&gt;: factor - &lt;date&gt;: dates\nStricter subsetting: Tibbles never do partial matching of names and always generate a warning if you try to access a column that doesn’t exist.\nNo row names: Tibbles don’t use row names (they are never useful). If you have meaningful row names, convert them to a proper column.\nConsistent subsetting: [ always returns a tibble, and $ doesn’t do partial matching.\n\n\nWorking with Tibbles\nMost functions that work with data frames also work with tibbles:\n\n\nCode\n# Dimensions\ndim(my_tibble)\n\n\n[1] 4 4\n\n\nCode\nnrow(my_tibble)\n\n\n[1] 4\n\n\nCode\nncol(my_tibble)\n\n\n[1] 4\n\n\nCode\n# Column names\nnames(my_tibble)\n\n\n[1] \"name\"      \"age\"       \"height_cm\" \"treatment\"\n\n\nCode\ncolnames(my_tibble)\n\n\n[1] \"name\"      \"age\"       \"height_cm\" \"treatment\"\n\n\nCode\n# Summary statistics\nsummary(my_tibble)\n\n\n     name                age          height_cm       treatment\n Length:4           Min.   :25.00   Min.   :165.0   control:2  \n Class :character   1st Qu.:27.25   1st Qu.:170.2   drug_A :2  \n Mode  :character   Median :30.00   Median :175.0              \n                    Mean   :32.50   Mean   :173.8              \n                    3rd Qu.:35.25   3rd Qu.:178.5              \n                    Max.   :45.00   Max.   :180.0              \n\n\nYou can extract columns as vectors using $ or [[:\n\n\nCode\n# Extract the 'name' column as a vector\nmy_tibble$name\n\n\n[1] \"Alice\" \"Bob\"   \"Carol\" \"David\"\n\n\nCode\n# Alternative syntax\nmy_tibble[[\"name\"]]\n\n\n[1] \"Alice\" \"Bob\"   \"Carol\" \"David\"\n\n\nThe glimpse() function provides a transposed summary, useful for datasets with many columns:\n\n\nCode\nglimpse(iris_tibble)\n\n\nRows: 150\nColumns: 5\n$ Sepal.Length &lt;dbl&gt; 5.1, 4.9, 4.7, 4.6, 5.0, 5.4, 4.6, 5.0, 4.4, 4.9, 5.4, 4.…\n$ Sepal.Width  &lt;dbl&gt; 3.5, 3.0, 3.2, 3.1, 3.6, 3.9, 3.4, 3.4, 2.9, 3.1, 3.7, 3.…\n$ Petal.Length &lt;dbl&gt; 1.4, 1.4, 1.3, 1.5, 1.4, 1.7, 1.4, 1.5, 1.4, 1.5, 1.5, 1.…\n$ Petal.Width  &lt;dbl&gt; 0.2, 0.2, 0.2, 0.2, 0.2, 0.4, 0.3, 0.2, 0.2, 0.1, 0.2, 0.…\n$ Species      &lt;fct&gt; setosa, setosa, setosa, setosa, setosa, setosa, setosa, s…",
    "crumbs": [
      "Data Exploration",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Tidy Data</span>"
    ]
  },
  {
    "objectID": "chapters/07-tidy-data.html#reshaping-data-with-tidyr",
    "href": "chapters/07-tidy-data.html#reshaping-data-with-tidyr",
    "title": "7  Tidy Data",
    "section": "7.6 Reshaping Data with tidyr",
    "text": "7.6 Reshaping Data with tidyr\nSometimes data is not in the right shape for your analysis. The tidyr package provides functions to reshape data while maintaining tidy principles.\n\nWide vs. Long Format\nData can be organized in wide or long format:\nWide format has one row per subject/unit, with multiple columns for different measurements or time points:\n\n\nCode\n# Example: wide format\nwide_data &lt;- tibble(\n  sample = c(\"A\", \"B\", \"C\"),\n  time_0 = c(10, 15, 12),\n  time_1 = c(14, 18, 15),\n  time_2 = c(18, 22, 19)\n)\nwide_data\n\n\n# A tibble: 3 × 4\n  sample time_0 time_1 time_2\n  &lt;chr&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 A          10     14     18\n2 B          15     18     22\n3 C          12     15     19\n\n\nLong format has one row per observation, with variables in separate columns:\n\n\nCode\n# Example: long format\nlong_data &lt;- tibble(\n  sample = c(\"A\", \"A\", \"A\", \"B\", \"B\", \"B\", \"C\", \"C\", \"C\"),\n  timepoint = c(0, 1, 2, 0, 1, 2, 0, 1, 2),\n  measurement = c(10, 14, 18, 15, 18, 22, 12, 15, 19)\n)\nlong_data\n\n\n# A tibble: 9 × 3\n  sample timepoint measurement\n  &lt;chr&gt;      &lt;dbl&gt;       &lt;dbl&gt;\n1 A              0          10\n2 A              1          14\n3 A              2          18\n4 B              0          15\n5 B              1          18\n6 B              2          22\n7 C              0          12\n8 C              1          15\n9 C              2          19\n\n\nTidy data is typically in long format, as this structure makes it easy to: - Plot data with ggplot2 (mapping variables to aesthetics) - Group and summarize by categorical variables - Model relationships between variables\n\n\npivot_longer(): Wide to Long\npivot_longer() takes wide data and makes it long by gathering columns into rows:\n\n\nCode\n# Convert wide to long format\nwide_data |&gt;\n  pivot_longer(\n    cols = starts_with(\"time_\"),    # which columns to pivot\n    names_to = \"timepoint\",          # name for the new category column\n    values_to = \"measurement\"        # name for the new values column\n  )\n\n\n# A tibble: 9 × 3\n  sample timepoint measurement\n  &lt;chr&gt;  &lt;chr&gt;           &lt;dbl&gt;\n1 A      time_0             10\n2 A      time_1             14\n3 A      time_2             18\n4 B      time_0             15\n5 B      time_1             18\n6 B      time_2             22\n7 C      time_0             12\n8 C      time_1             15\n9 C      time_2             19\n\n\nYou can also clean up the column names during the pivot:\n\n\nCode\n# Remove the \"time_\" prefix and convert to numeric\nwide_data |&gt;\n  pivot_longer(\n    cols = starts_with(\"time_\"),\n    names_to = \"timepoint\",\n    names_prefix = \"time_\",          # remove this prefix\n    names_transform = as.integer,    # convert to integer\n    values_to = \"measurement\"\n  )\n\n\n# A tibble: 9 × 3\n  sample timepoint measurement\n  &lt;chr&gt;      &lt;int&gt;       &lt;dbl&gt;\n1 A              0          10\n2 A              1          14\n3 A              2          18\n4 B              0          15\n5 B              1          18\n6 B              2          22\n7 C              0          12\n8 C              1          15\n9 C              2          19\n\n\n\n\npivot_wider(): Long to Wide\npivot_wider() does the reverse, spreading rows into columns:\n\n\nCode\n# Convert long to wide format\nlong_data |&gt;\n  pivot_wider(\n    names_from = timepoint,          # which column contains new column names\n    values_from = measurement,       # which column contains the values\n    names_prefix = \"time_\"           # add prefix to new column names\n  )\n\n\n# A tibble: 3 × 4\n  sample time_0 time_1 time_2\n  &lt;chr&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 A          10     14     18\n2 B          15     18     22\n3 C          12     15     19\n\n\n\n\nseparate(): Split One Column into Multiple\nseparate() splits a single column into multiple columns:\n\n\nCode\n# Data with combined values\ncombined_data &lt;- tibble(\n  sample_info = c(\"control_rep1\", \"control_rep2\", \"treatment_rep1\", \"treatment_rep2\"),\n  value = c(10, 12, 25, 23)\n)\n\n# Separate into treatment and replicate\ncombined_data |&gt;\n  separate(\n    col = sample_info,\n    into = c(\"treatment\", \"replicate\"),\n    sep = \"_\"\n  )\n\n\n# A tibble: 4 × 3\n  treatment replicate value\n  &lt;chr&gt;     &lt;chr&gt;     &lt;dbl&gt;\n1 control   rep1         10\n2 control   rep2         12\n3 treatment rep1         25\n4 treatment rep2         23\n\n\nYou can also separate based on character position:\n\n\nCode\n# Separate by position\ndate_data &lt;- tibble(date = c(\"20240115\", \"20240116\"))\ndate_data |&gt;\n  separate(\n    col = date,\n    into = c(\"year\", \"month\", \"day\"),\n    sep = c(4, 6)  # split after positions 4 and 6\n  )\n\n\n# A tibble: 2 × 3\n  year  month day  \n  &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;\n1 2024  01    15   \n2 2024  01    16   \n\n\n\n\nunite(): Combine Multiple Columns\nunite() combines multiple columns into one:\n\n\nCode\n# Data with separate date components\ndate_parts &lt;- tibble(\n  year = c(2024, 2024, 2024),\n  month = c(1, 2, 3),\n  day = c(15, 20, 8)\n)\n\n# Combine into a single date column\ndate_parts |&gt;\n  unite(\n    col = \"date\",\n    year, month, day,\n    sep = \"-\"\n  )\n\n\n# A tibble: 3 × 1\n  date     \n  &lt;chr&gt;    \n1 2024-1-15\n2 2024-2-20\n3 2024-3-8 \n\n\nYou can remove the original columns or keep them:\n\n\nCode\n# Keep original columns\ndate_parts |&gt;\n  unite(\n    col = \"date\",\n    year, month, day,\n    sep = \"-\",\n    remove = FALSE  # keep the original columns\n  )\n\n\n# A tibble: 3 × 4\n  date       year month   day\n  &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 2024-1-15  2024     1    15\n2 2024-2-20  2024     2    20\n3 2024-3-8   2024     3     8\n\n\n\n\nCommon Reshaping Patterns\nMultiple measurements per subject over time (wide to long):\n\n\nCode\n# Example: patient measurements at multiple time points\npatient_data &lt;- tibble(\n  patient_id = c(\"P001\", \"P002\", \"P003\"),\n  baseline = c(120, 135, 128),\n  week_4 = c(115, 130, 125),\n  week_8 = c(110, 125, 122)\n)\n\n# Convert to long format for analysis\npatient_data |&gt;\n  pivot_longer(\n    cols = -patient_id,  # all columns except patient_id\n    names_to = \"timepoint\",\n    values_to = \"blood_pressure\"\n  )\n\n\n# A tibble: 9 × 3\n  patient_id timepoint blood_pressure\n  &lt;chr&gt;      &lt;chr&gt;              &lt;dbl&gt;\n1 P001       baseline             120\n2 P001       week_4               115\n3 P001       week_8               110\n4 P002       baseline             135\n5 P002       week_4               130\n6 P002       week_8               125\n7 P003       baseline             128\n8 P003       week_4               125\n9 P003       week_8               122\n\n\nMultiple measurements of different types (wide to long):\n\n\nCode\n# Example: different measurements per sample\nmeasurements &lt;- tibble(\n  sample = c(\"S1\", \"S2\", \"S3\"),\n  mass_g = c(2.5, 3.1, 2.8),\n  length_mm = c(45, 52, 48),\n  width_mm = c(12, 15, 13)\n)\n\n# Convert to long format\nmeasurements |&gt;\n  pivot_longer(\n    cols = -sample,\n    names_to = c(\"measurement\", \"unit\"),\n    names_sep = \"_\",\n    values_to = \"value\"\n  )\n\n\n# A tibble: 9 × 4\n  sample measurement unit  value\n  &lt;chr&gt;  &lt;chr&gt;       &lt;chr&gt; &lt;dbl&gt;\n1 S1     mass        g       2.5\n2 S1     length      mm     45  \n3 S1     width       mm     12  \n4 S2     mass        g       3.1\n5 S2     length      mm     52  \n6 S2     width       mm     15  \n7 S3     mass        g       2.8\n8 S3     length      mm     48  \n9 S3     width       mm     13",
    "crumbs": [
      "Data Exploration",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Tidy Data</span>"
    ]
  },
  {
    "objectID": "chapters/07-tidy-data.html#practice-exercises",
    "href": "chapters/07-tidy-data.html#practice-exercises",
    "title": "7  Tidy Data",
    "section": "7.7 Practice Exercises",
    "text": "7.7 Practice Exercises\n\nExercise 7.1: Understanding Data Types\nConsider the following variables from a biological experiment. For each, identify: - Whether it is categorical or quantitative - If categorical: nominal or ordinal - If quantitative: discrete or continuous, interval or ratio - What R data type would be most appropriate\n\nSpecies name (Drosophila melanogaster, Caenorhabditis elegans)\nNumber of offspring produced\nTemperature in Celsius\nTreatment group (control, low dose, high dose)\nGene expression level (measured in RPKM)\nSurvival status (alive/dead)\nTumor stage (I, II, III, IV)\nBody mass in grams\nSample collection date\npH level\n\n\n\nExercise 7.2: Creating Tibbles\nCreate a tibble with data from a hypothetical experiment:\n\n\nCode\n# Create a tibble with:\n# - 6 samples (S1 through S6)\n# - 2 treatment groups (control and experimental), 3 samples each\n# - A numeric measurement for each sample\n# - A logical variable indicating if the measurement exceeded a threshold\n\n\nAfter creating the tibble: 1. Print it and examine the column types 2. Use glimpse() to get an overview 3. Extract the measurement column as a vector 4. Calculate the mean measurement for each treatment group\n\n\nExercise 7.3: Reshaping Data\nCreate a wide-format dataset with measurements at three time points:\n\n\nCode\nwide_experiment &lt;- tibble(\n  gene = c(\"BRCA1\", \"TP53\", \"EGFR\"),\n  hour_0 = c(5.2, 8.1, 3.4),\n  hour_2 = c(6.1, 8.5, 4.2),\n  hour_4 = c(7.3, 9.2, 5.1)\n)\n\n\n\nConvert this to long format using pivot_longer()\nCreate a new column indicating the fold-change from hour 0\nConvert back to wide format, with columns for each time point’s fold-change\n\n\n\nExercise 7.4: Combining and Separating Columns\nYou have data where sample IDs encode multiple pieces of information:\n\n\nCode\nmessy_samples &lt;- tibble(\n  sample_id = c(\"Exp1_ControlA_Rep1\", \"Exp1_ControlA_Rep2\",\n                \"Exp1_TreatmentB_Rep1\", \"Exp1_TreatmentB_Rep2\"),\n  measurement = c(45, 48, 62, 58)\n)\n\n\n\nUse separate() to split sample_id into: experiment, treatment, and replicate\nAfter analysis, use unite() to create a new ID combining treatment and replicate\n\n\n\nExercise 7.5: Data File Organization\nReview a data file you have used (or create a sample one). Evaluate it against the best practices:\n\nIs it in plain text format?\nDoes each column contain only one data type?\nAre variable names descriptive and consistent?\nAre missing values represented consistently?\nAre there any special characters that could cause problems?\nIs the data in tidy format (each variable a column, each observation a row)?\n\nIf any issues exist, create a cleaned version following best practices.\n\n\nExercise 7.6: Working with Real Data\nLoad the built-in airquality dataset:\n\n\nCode\ndata(airquality)\nair &lt;- as_tibble(airquality)\n\n\n\nExamine the structure using glimpse()\nWhat data types are the columns?\nConvert Month to a factor with meaningful labels (e.g., “May”, “June”)\nThe data is already in tidy format. Explain why.\nCreate a wider version where each month becomes a column showing mean Ozone levels\n\n\n\nExercise 7.7: Identifying Non-Tidy Data\nConsider this dataset structure:\n\n\nCode\nnon_tidy &lt;- tibble(\n  gene = c(\"BRCA1\", \"TP53\"),\n  control_1 = c(5.2, 8.1),\n  control_2 = c(5.1, 8.3),\n  treatment_1 = c(7.3, 9.2),\n  treatment_2 = c(7.5, 9.1)\n)\n\n\n\nWhy is this not tidy data?\nWhat are the actual variables in this dataset?\nReshape it into tidy format\nWhat are the advantages of the tidy version for analysis and visualization?",
    "crumbs": [
      "Data Exploration",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Tidy Data</span>"
    ]
  },
  {
    "objectID": "chapters/08-data-wrangling.html",
    "href": "chapters/08-data-wrangling.html",
    "title": "8  Data Wrangling with dplyr",
    "section": "",
    "text": "8.1 The Pipe Operator\nData wrangling is the process of transforming, cleaning, and organizing data to make it suitable for analysis. The dplyr package, part of the tidyverse, provides a powerful and intuitive grammar for data manipulation. This chapter covers the essential tools and techniques for working with data in R.\nOne of the most important concepts in modern R programming is the pipe operator. The pipe |&gt; (or the tidyverse’s %&gt;%) passes the result of one operation as the first argument of the next, allowing you to chain operations together in a readable sequence.\nCode\n# Without pipe: nested and hard to read\nsummarize(group_by(filter(flights, !is.na(arr_delay)), dest),\n          mean_delay = mean(arr_delay))\n\n# With pipe: clear sequence of operations\nflights |&gt;\n  filter(!is.na(arr_delay)) |&gt;\n  group_by(dest) |&gt;\n  summarize(mean_delay = mean(arr_delay))\nRead the pipe as “then”—take flights, then filter, then group, then summarize. This left-to-right, top-to-bottom flow mirrors how we think about data transformations.",
    "crumbs": [
      "Data Exploration",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Data Wrangling with dplyr</span>"
    ]
  },
  {
    "objectID": "chapters/08-data-wrangling.html#the-pipe-operator",
    "href": "chapters/08-data-wrangling.html#the-pipe-operator",
    "title": "8  Data Wrangling with dplyr",
    "section": "",
    "text": "Figure 8.1: The pipe operator passes results from one operation to the next, enabling clear data transformation workflows",
    "crumbs": [
      "Data Exploration",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Data Wrangling with dplyr</span>"
    ]
  },
  {
    "objectID": "chapters/08-data-wrangling.html#key-dplyr-verbs",
    "href": "chapters/08-data-wrangling.html#key-dplyr-verbs",
    "title": "8  Data Wrangling with dplyr",
    "section": "8.2 Key dplyr Verbs",
    "text": "8.2 Key dplyr Verbs\nThe dplyr package provides a grammar for data manipulation centered around five key verbs. These verbs handle most common data manipulation tasks and can be combined to solve complex problems.\n\nfilter(): Subset Rows\nfilter() selects rows that meet specified conditions. This is essential for focusing your analysis on specific subsets of data.\n\n\nCode\n# Load example data\nlibrary(nycflights13)\ndata(flights)\n\n# Flights in November or December\nflights |&gt; filter(month == 11 | month == 12)\n\n# Flights with arrival delay greater than 2 hours\nflights |&gt; filter(arr_delay &gt; 120)\n\n# Multiple conditions with AND\nflights |&gt; filter(month == 1, day == 1)\n\n# Using %in% for set membership\nflights |&gt; filter(dest %in% c(\"SFO\", \"OAK\", \"SJC\"))\n\n\nConditions use comparison operators: - == (equals), != (not equals) - &lt;, &gt;, &lt;=, &gt;= (comparison) - & (and), | (or) - ! (not) - %in% (membership in a set)\n\n\nselect(): Choose Columns\nselect() picks columns by name, which is useful when working with wide datasets containing many variables.\n\n\nCode\n# Select specific columns\nflights |&gt; select(year, month, day)\n\n# Select a range of columns\nflights |&gt; select(year:day)\n\n# Drop columns with minus sign\nflights |&gt; select(-year, -month)\n\n# Select by pattern\nflights |&gt; select(starts_with(\"dep\"))\nflights |&gt; select(ends_with(\"time\"))\nflights |&gt; select(contains(\"delay\"))\n\n# Reorder columns\nflights |&gt; select(carrier, flight, everything())\n\n\nHelper functions for select(): - starts_with(), ends_with(), contains() - match patterns - matches() - regular expressions - num_range() - numeric ranges like “x1”, “x2”, “x3” - everything() - all remaining columns\n\n\narrange(): Sort Rows\narrange() reorders rows by column values. By default, it sorts in ascending order.\n\n\nCode\n# Sort by year, then month, then day\nflights |&gt; arrange(year, month, day)\n\n# Sort in descending order\nflights |&gt; arrange(desc(dep_delay))\n\n# Missing values always sorted to the end\nflights |&gt; arrange(dep_time)\n\n\n\n\nmutate(): Create New Columns\nmutate() adds new columns that are functions of existing columns. This is one of the most frequently used dplyr functions.\n\n\nCode\n# Create single new column\nflights |&gt; mutate(gain = arr_delay - dep_delay)\n\n# Create multiple columns, referencing newly created ones\nflights |&gt;\n  mutate(\n    gain = arr_delay - dep_delay,\n    hours = air_time / 60,\n    gain_per_hour = gain / hours\n  )\n\n# Keep only new variables with transmute()\nflights |&gt;\n  transmute(\n    gain = arr_delay - dep_delay,\n    hours = air_time / 60,\n    gain_per_hour = gain / hours\n  )\n\n\nCommon functions used with mutate(): - Arithmetic: +, -, *, /, ^ - Modular arithmetic: %/% (integer division), %% (remainder) - Logs: log(), log2(), log10() - Offsets: lead(), lag() - Cumulative: cumsum(), cumprod(), cummin(), cummax() - Ranking: min_rank(), row_number(), dense_rank()\n\n\nsummarize(): Aggregate Data\nsummarize() (or summarise()) collapses multiple rows into summary values. It’s particularly powerful when combined with group_by().\n\n\nCode\n# Single summary statistic\nflights |&gt;\n  summarize(mean_delay = mean(dep_delay, na.rm = TRUE))\n\n# Multiple summary statistics\nflights |&gt;\n  summarize(\n    mean_delay = mean(dep_delay, na.rm = TRUE),\n    median_delay = median(dep_delay, na.rm = TRUE),\n    sd_delay = sd(dep_delay, na.rm = TRUE),\n    n = n()\n  )\n\n\nCommon summary functions: - Central tendency: mean(), median() - Spread: sd(), var(), IQR(), mad() - Range: min(), max(), quantile() - Position: first(), last(), nth() - Count: n(), n_distinct()",
    "crumbs": [
      "Data Exploration",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Data Wrangling with dplyr</span>"
    ]
  },
  {
    "objectID": "chapters/08-data-wrangling.html#grouping-with-group_by",
    "href": "chapters/08-data-wrangling.html#grouping-with-group_by",
    "title": "8  Data Wrangling with dplyr",
    "section": "8.3 Grouping with group_by()",
    "text": "8.3 Grouping with group_by()\nThe real power of summarize() emerges when combined with group_by(), which splits data into groups for separate analysis. This is fundamental for comparative statistics.\n\n\nCode\n# Group by destination, then summarize\nflights |&gt;\n  group_by(dest) |&gt;\n  summarize(\n    count = n(),\n    mean_delay = mean(arr_delay, na.rm = TRUE),\n    mean_distance = mean(distance)\n  )\n\n# Group by multiple variables\nflights |&gt;\n  group_by(year, month, day) |&gt;\n  summarize(\n    flights = n(),\n    mean_delay = mean(dep_delay, na.rm = TRUE)\n  )\n\n# Grouping affects mutate() too\nflights |&gt;\n  group_by(dest) |&gt;\n  mutate(\n    delay_rank = min_rank(desc(arr_delay)),\n    delay_vs_mean = arr_delay - mean(arr_delay, na.rm = TRUE)\n  )\n\n\nImportant notes about grouping: - summarize() removes one level of grouping - Use ungroup() to remove all grouping - Grouped data frames print with group information at the top\n\n\nCode\n# Remove grouping\nflights |&gt;\n  group_by(dest) |&gt;\n  summarize(mean_delay = mean(arr_delay, na.rm = TRUE)) |&gt;\n  ungroup()",
    "crumbs": [
      "Data Exploration",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Data Wrangling with dplyr</span>"
    ]
  },
  {
    "objectID": "chapters/08-data-wrangling.html#handling-missing-values",
    "href": "chapters/08-data-wrangling.html#handling-missing-values",
    "title": "8  Data Wrangling with dplyr",
    "section": "8.4 Handling Missing Values",
    "text": "8.4 Handling Missing Values\nMissing values are ubiquitous in real data. In R, missing values are represented as NA (Not Available). Most operations involving NA return NA, which requires careful handling.\n\n\nCode\n# Example of NA behavior\nx &lt;- c(1, 2, NA, 4)\nmean(x)              # Returns NA\nmean(x, na.rm = TRUE)  # Returns 2.333333\n\n\n\nDetecting Missing Values\nUse is.na() to identify missing values:\n\n\nCode\n# Check for NAs\nis.na(x)\n\n# Count missing values\nsum(is.na(x))\n\n# Count missing values per column\nflights |&gt;\n  summarize(across(everything(), ~sum(is.na(.))))\n\n\n\n\nFiltering Missing Values\n\n\nCode\n# Remove rows with NA in specific column\nflights |&gt; filter(!is.na(dep_delay))\n\n# Keep only complete cases (no NAs in any column)\nflights |&gt; filter(complete.cases(flights))\n\n# Using tidyr's drop_na()\nflights |&gt; drop_na(dep_delay)  # Drop rows with NA in dep_delay\nflights |&gt; drop_na()            # Drop rows with NA in any column\n\n\n\n\nReplacing Missing Values\n\n\nCode\n# Replace NA with specific value\nflights |&gt;\n  mutate(dep_delay = replace_na(dep_delay, 0))\n\n# Replace NA with mean\nflights |&gt;\n  mutate(dep_delay = if_else(is.na(dep_delay),\n                              mean(dep_delay, na.rm = TRUE),\n                              dep_delay))\n\n# Using tidyr's replace_na() for multiple columns\nflights |&gt;\n  replace_na(list(dep_delay = 0, arr_delay = 0))",
    "crumbs": [
      "Data Exploration",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Data Wrangling with dplyr</span>"
    ]
  },
  {
    "objectID": "chapters/08-data-wrangling.html#joining-data",
    "href": "chapters/08-data-wrangling.html#joining-data",
    "title": "8  Data Wrangling with dplyr",
    "section": "8.5 Joining Data",
    "text": "8.5 Joining Data\nReal data often comes in multiple tables that need to be combined. Join operations merge tables based on matching values in key columns. Understanding joins is essential for working with relational data.\n\nBasic Join Example\n\n\nCode\n# Example tables\nsamples &lt;- tibble(\n  sample_id = c(\"S1\", \"S2\", \"S3\"),\n  treatment = c(\"control\", \"low\", \"high\"),\n  concentration = c(0.0, 0.1, 1.0)\n)\n\nmeasurements &lt;- tibble(\n  sample_id = c(\"S1\", \"S1\", \"S2\", \"S2\", \"S3\", \"S3\"),\n  replicate = c(1, 2, 1, 2, 1, 2),\n  value = c(2.3, 2.1, 5.4, 5.6, 10.2, 10.8)\n)\n\n# Join tables\nmeasurements |&gt;\n  left_join(samples, by = \"sample_id\")\n\n\n\n\nTypes of Joins\nDifferent joins handle non-matching rows differently:\n\n\n\n\n\n\n\n\nJoin Type\nFunction\nResult\n\n\n\n\nLeft Join\nleft_join()\nKeep all rows from left table, add matching data from right table. Non-matching right rows are dropped, creating NAs in right columns.\n\n\nRight Join\nright_join()\nKeep all rows from right table, add matching data from left table. Non-matching left rows are dropped, creating NAs in left columns.\n\n\nInner Join\ninner_join()\nKeep only rows with matches in both tables. Most restrictive join, only complete matches retained.\n\n\nFull Join\nfull_join()\nKeep all rows from both tables. Creates NAs where matches don’t exist. Most inclusive join.\n\n\nSemi Join\nsemi_join()\nKeep rows from left table that have matches in right table, but don’t add columns from right. Filtering join.\n\n\nAnti Join\nanti_join()\nKeep rows from left table with NO matches in right table. Useful for finding mismatches. Filtering join.\n\n\n\n\n\nPractical Join Examples\n\n\nCode\n# Inner join: only complete matches\ninner_join(measurements, samples, by = \"sample_id\")\n\n# Left join: keep all measurements\nleft_join(measurements, samples, by = \"sample_id\")\n\n# Full join: keep everything\nfull_join(measurements, samples, by = \"sample_id\")\n\n# Anti join: find measurements without sample info (data quality check)\nmeasurements |&gt;\n  anti_join(samples, by = \"sample_id\")\n\n# Semi join: filter samples that have measurements\nsamples |&gt;\n  semi_join(measurements, by = \"sample_id\")\n\n\n\n\nJoining on Multiple Keys\n\n\nCode\n# Join on multiple columns\ntable1 |&gt;\n  left_join(table2, by = c(\"sample_id\", \"time_point\"))\n\n# Join when column names differ\ntable1 |&gt;\n  left_join(table2, by = c(\"id\" = \"sample_id\"))",
    "crumbs": [
      "Data Exploration",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Data Wrangling with dplyr</span>"
    ]
  },
  {
    "objectID": "chapters/08-data-wrangling.html#additional-dplyr-functions",
    "href": "chapters/08-data-wrangling.html#additional-dplyr-functions",
    "title": "8  Data Wrangling with dplyr",
    "section": "8.6 Additional dplyr Functions",
    "text": "8.6 Additional dplyr Functions\nBeyond the five core verbs, dplyr provides many specialized functions for common data manipulation tasks.\n\ncase_when(): Vectorized Conditional Logic\nThe case_when() function provides a clean way to handle multiple if-else conditions:\n\n\nCode\n# Create sample data\nexpression_data &lt;- tibble(\n  gene = c(\"BRCA1\", \"TP53\", \"EGFR\", \"KRAS\", \"MYC\"),\n  fold_change = c(0.5, 1.2, 3.5, -2.1, 0.9)\n)\n\n# Classify regulation status\nexpression_data |&gt;\n  mutate(\n    regulation = case_when(\n      fold_change &gt; 2 ~ \"strongly upregulated\",\n      fold_change &gt; 1 ~ \"upregulated\",\n      fold_change &lt; -2 ~ \"strongly downregulated\",\n      fold_change &lt; -1 ~ \"downregulated\",\n      TRUE ~ \"unchanged\"  # default case\n    )\n  )\n\n\nThe syntax is: condition ~ value. Conditions are evaluated in order, and the first TRUE condition determines the result. Use TRUE ~ value as a catch-all default.\n\n\ncount() and n_distinct()\ncount() is a convenient shortcut for grouping and counting:\n\n\nCode\n# Count observations per group\nflights |&gt; count(carrier)\n\n# Equivalent to:\nflights |&gt;\n  group_by(carrier) |&gt;\n  summarize(n = n())\n\n# Count with sorting\nflights |&gt; count(carrier, sort = TRUE)\n\n# Count combinations\nflights |&gt; count(carrier, origin)\n\n# Add weights\nflights |&gt; count(carrier, wt = distance)  # sum of distance, not count\n\n\nUse n_distinct() to count unique values:\n\n\nCode\n# Count unique destinations per carrier\nflights |&gt;\n  group_by(carrier) |&gt;\n  summarize(\n    n_flights = n(),\n    n_destinations = n_distinct(dest)\n  )\n\n\n\n\nslice() Variants: Select Rows by Position\nWhile filter() selects rows by condition, slice() and its variants select by position or rank:\n\n\nCode\n# First n rows\nflights |&gt; slice_head(n = 5)\n\n# Last n rows\nflights |&gt; slice_tail(n = 5)\n\n# Random sample\nflights |&gt; slice_sample(n = 10)\nflights |&gt; slice_sample(prop = 0.01)  # 1% of rows\n\n# Top n by value\nflights |&gt; slice_max(dep_delay, n = 5)\nflights |&gt; slice_min(dep_delay, n = 5)\n\n# Works with groups\nflights |&gt;\n  group_by(carrier) |&gt;\n  slice_max(distance, n = 1)  # Longest flight per carrier\n\n\n\n\npull(): Extract a Column as a Vector\nselect() returns a data frame with one column. pull() extracts a column as a vector:\n\n\nCode\n# Returns a tibble with one column\nflights |&gt; select(dep_delay)\n\n# Returns a vector\nflights |&gt; pull(dep_delay)\n\n# Useful for piping into functions that expect vectors\nflights |&gt;\n  filter(month == 1) |&gt;\n  pull(dep_delay) |&gt;\n  mean(na.rm = TRUE)\n\n\n\n\ndistinct(): Remove Duplicate Rows\nRemove duplicate rows based on specified columns:\n\n\nCode\n# Unique values in one column\nflights |&gt; distinct(carrier)\n\n# Unique combinations of multiple columns\nflights |&gt; distinct(carrier, origin)\n\n# Keep all columns (removes only exact duplicate rows)\nflights |&gt; distinct()\n\n# Keep other columns along with distinct values\nflights |&gt; distinct(carrier, .keep_all = TRUE)",
    "crumbs": [
      "Data Exploration",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Data Wrangling with dplyr</span>"
    ]
  },
  {
    "objectID": "chapters/08-data-wrangling.html#working-with-factors",
    "href": "chapters/08-data-wrangling.html#working-with-factors",
    "title": "8  Data Wrangling with dplyr",
    "section": "8.7 Working with Factors",
    "text": "8.7 Working with Factors\nFactors are R’s way of representing categorical data with a fixed set of possible values (called levels). The forcats package (part of tidyverse) provides tools for working with factors, which is essential for controlling category order in analyses and visualizations.\n\nWhy Factor Order Matters\nBy default, R orders factor levels alphabetically, which often produces suboptimal visualizations:\n\n\nCode\n# Sample data\ngene_data &lt;- tibble(\n  gene = c(\"BRCA1\", \"TP53\", \"EGFR\", \"KRAS\", \"MYC\"),\n  expression = c(5.2, 8.1, 3.4, 6.7, 9.2)\n)\n\n# Default: alphabetical order\nggplot(gene_data, aes(x = gene, y = expression)) +\n  geom_col(fill = \"steelblue\") +\n  labs(\n    title = \"Gene Expression (Alphabetical Order)\",\n    x = \"Gene\",\n    y = \"Expression Level\"\n  ) +\n  theme_minimal()\n\n\n\n\nReordering Factors\nUse fct_reorder() to order factors by another variable:\n\n\nCode\n# Reorder by expression value\nlibrary(forcats)\n\ngene_data |&gt;\n  mutate(gene = fct_reorder(gene, expression)) |&gt;\n  ggplot(aes(x = gene, y = expression)) +\n  geom_col(fill = \"steelblue\") +\n  labs(\n    title = \"Gene Expression (Ordered by Value)\",\n    x = \"Gene\",\n    y = \"Expression Level\"\n  ) +\n  theme_minimal()\n\n\n\n\nAdditional forcats Functions\n\n\nCode\n# Reorder by frequency\ngene_counts |&gt;\n  mutate(gene = fct_infreq(gene))\n\n# Reverse factor order\ngene_data |&gt;\n  mutate(gene = fct_rev(gene))\n\n# Reorder manually\ngene_data |&gt;\n  mutate(gene = fct_relevel(gene, \"TP53\", \"MYC\", \"BRCA1\"))\n\n\n\n\nRecoding Factor Levels\nUse fct_recode() to change level names:\n\n\nCode\n# Original factor\nstatus &lt;- factor(c(\"WT\", \"WT\", \"KO\", \"HET\", \"KO\"))\n\n# Recode to more descriptive names\nstatus_full &lt;- fct_recode(status,\n  \"Wild Type\" = \"WT\",\n  \"Knockout\" = \"KO\",\n  \"Heterozygous\" = \"HET\"\n)\n\n\n\n\nCollapsing Factor Levels\nCombine rare categories using fct_lump_n() or fct_lump_prop():\n\n\nCode\n# Keep only the top 3 most common levels\ngene_variants |&gt;\n  mutate(variant = fct_lump_n(variant, n = 3))\n\n# Combine levels that appear in less than 5% of data\ngene_variants |&gt;\n  mutate(variant = fct_lump_prop(variant, prop = 0.05))\n\n# Combine manually\ngene_variants |&gt;\n  mutate(variant = fct_collapse(variant,\n    common = c(\"SNP1\", \"SNP2\", \"SNP3\"),\n    rare = c(\"SNP4\", \"SNP5\")\n  ))",
    "crumbs": [
      "Data Exploration",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Data Wrangling with dplyr</span>"
    ]
  },
  {
    "objectID": "chapters/08-data-wrangling.html#practice-exercises",
    "href": "chapters/08-data-wrangling.html#practice-exercises",
    "title": "8  Data Wrangling with dplyr",
    "section": "8.8 Practice Exercises",
    "text": "8.8 Practice Exercises\n\nExercise 1: Basic Data Manipulation\nUsing the nycflights13::flights dataset:\n\nFind all flights that departed in summer (June, July, August)\nSelect columns related to departure and arrival times\nSort flights by total delay (departure + arrival delay)\nCreate a new variable for flight speed in mph\n\n\n\nCode\nlibrary(nycflights13)\n\n# Your solution here\nflights |&gt;\n  filter(month %in% c(6, 7, 8)) |&gt;\n  select(month, day, dep_time, arr_time, dep_delay, arr_delay) |&gt;\n  mutate(\n    total_delay = dep_delay + arr_delay,\n    speed_mph = distance / (air_time / 60)\n  ) |&gt;\n  arrange(desc(total_delay))\n\n\n\n\nExercise 2: Grouping and Summarizing\n\nCalculate the mean, median, and standard deviation of departure delays by carrier\nFind which carrier has the most flights to each destination\nDetermine the busiest hour of the day (by scheduled departure)\n\n\n\nCode\n# Part 1: Delay statistics by carrier\nflights |&gt;\n  group_by(carrier) |&gt;\n  summarize(\n    mean_delay = mean(dep_delay, na.rm = TRUE),\n    median_delay = median(dep_delay, na.rm = TRUE),\n    sd_delay = sd(dep_delay, na.rm = TRUE),\n    n_flights = n()\n  ) |&gt;\n  arrange(desc(mean_delay))\n\n# Part 2: Top carrier per destination\nflights |&gt;\n  count(dest, carrier) |&gt;\n  group_by(dest) |&gt;\n  slice_max(n, n = 1)\n\n# Part 3: Busiest departure hour\nflights |&gt;\n  mutate(hour = dep_time %/% 100) |&gt;\n  count(hour, sort = TRUE)\n\n\n\n\nExercise 3: Data Joining\nCreate two datasets and practice joining them:\n\n\nCode\n# Create sample data\ncell_lines &lt;- tibble(\n  cell_line = c(\"HEK293\", \"HeLa\", \"CHO\", \"Jurkat\"),\n  cell_type = c(\"epithelial\", \"epithelial\", \"ovary\", \"lymphocyte\"),\n  species = c(\"human\", \"human\", \"hamster\", \"human\")\n)\n\nexperiments &lt;- tibble(\n  cell_line = c(\"HEK293\", \"HEK293\", \"HeLa\", \"Jurkat\", \"Jurkat\", \"PC12\"),\n  treatment = c(\"control\", \"drug_A\", \"drug_A\", \"control\", \"drug_B\", \"control\"),\n  viability = c(98, 65, 45, 99, 82, 95)\n)\n\n# Questions:\n# 1. Add cell type and species information to experiments\n# 2. Find cell lines in the database with no experiments\n# 3. Find experiments for cell lines not in the database\n# 4. Create a complete dataset with all cell lines and experiments\n\n# Solutions:\n# 1. Left join\nexperiments |&gt;\n  left_join(cell_lines, by = \"cell_line\")\n\n# 2. Anti join\ncell_lines |&gt;\n  anti_join(experiments, by = \"cell_line\")\n\n# 3. Anti join (reverse)\nexperiments |&gt;\n  anti_join(cell_lines, by = \"cell_line\")\n\n# 4. Full join\nexperiments |&gt;\n  full_join(cell_lines, by = \"cell_line\")\n\n\n\n\nExercise 4: Complex Wrangling Pipeline\nAnalyze flight delays by time of day and season:\n\n\nCode\nflights |&gt;\n  # Create time and season categories\n  mutate(\n    time_of_day = case_when(\n      dep_time &lt; 600 ~ \"night\",\n      dep_time &lt; 1200 ~ \"morning\",\n      dep_time &lt; 1800 ~ \"afternoon\",\n      TRUE ~ \"evening\"\n    ),\n    season = case_when(\n      month %in% c(12, 1, 2) ~ \"winter\",\n      month %in% c(3, 4, 5) ~ \"spring\",\n      month %in% c(6, 7, 8) ~ \"summer\",\n      month %in% c(9, 10, 11) ~ \"fall\"\n    )\n  ) |&gt;\n  # Remove NAs\n  filter(!is.na(dep_delay), !is.na(time_of_day)) |&gt;\n  # Group and summarize\n  group_by(season, time_of_day) |&gt;\n  summarize(\n    mean_delay = mean(dep_delay),\n    median_delay = median(dep_delay),\n    n_flights = n(),\n    .groups = \"drop\"\n  ) |&gt;\n  # Arrange for readability\n  arrange(season, time_of_day)\n\n\n\n\nExercise 5: Working with Factors\nCreate a publication-ready plot with properly ordered factors:\n\n\nCode\n# Sample biological data\nenzyme_data &lt;- tibble(\n  enzyme = rep(c(\"Lipase\", \"Amylase\", \"Protease\", \"Cellulase\"), each = 5),\n  temperature = rep(c(20, 30, 40, 50, 60), 4),\n  activity = c(\n    # Lipase\n    12, 24, 48, 36, 18,\n    # Amylase\n    18, 32, 55, 42, 20,\n    # Protease\n    8, 16, 35, 28, 12,\n    # Cellulase\n    15, 28, 62, 48, 22\n  )\n)\n\n# Calculate optimal temperature for each enzyme\noptimal_temps &lt;- enzyme_data |&gt;\n  group_by(enzyme) |&gt;\n  summarize(optimal_temp = temperature[which.max(activity)])\n\n# Reorder enzymes by optimal temperature\nenzyme_data |&gt;\n  left_join(optimal_temps, by = \"enzyme\") |&gt;\n  mutate(enzyme = fct_reorder(enzyme, optimal_temp)) |&gt;\n  ggplot(aes(x = temperature, y = activity, color = enzyme)) +\n  geom_line(linewidth = 1) +\n  geom_point(size = 2) +\n  labs(\n    title = \"Enzyme Activity vs Temperature\",\n    subtitle = \"Ordered by optimal temperature\",\n    x = \"Temperature (°C)\",\n    y = \"Enzyme Activity (U/mL)\",\n    color = \"Enzyme\"\n  ) +\n  theme_minimal()\n\n\n\n\nExercise 6: Missing Data Handling\nPractice different strategies for handling missing values:\n\n\nCode\n# Create data with missing values\nmessy_data &lt;- tibble(\n  sample_id = 1:10,\n  measurement_1 = c(5.2, NA, 6.8, 7.1, NA, 8.9, 9.2, NA, 7.5, 6.9),\n  measurement_2 = c(12.1, 13.5, NA, 14.2, 15.8, NA, 16.9, 17.2, NA, 14.8),\n  measurement_3 = c(NA, 22.5, 23.1, NA, 24.8, 25.3, NA, 26.7, 27.2, 28.1)\n)\n\n# Questions:\n# 1. How many missing values are in each column?\n# 2. Remove rows with any missing values\n# 3. Remove rows with missing values in measurement_1 only\n# 4. Replace missing values with column means\n# 5. Flag which measurements are missing\n\n# Solutions:\n# 1. Count NAs per column\nmessy_data |&gt;\n  summarize(across(everything(), ~sum(is.na(.))))\n\n# 2. Complete cases only\nmessy_data |&gt;\n  drop_na()\n\n# 3. Drop NAs in specific column\nmessy_data |&gt;\n  drop_na(measurement_1)\n\n# 4. Replace with means\nmessy_data |&gt;\n  mutate(across(starts_with(\"measurement\"),\n                ~replace_na(., mean(., na.rm = TRUE))))\n\n# 5. Create missing indicators\nmessy_data |&gt;\n  mutate(across(starts_with(\"measurement\"),\n                ~is.na(.),\n                .names = \"{.col}_missing\"))\n\n\nThese exercises cover the core data wrangling operations you’ll use in most analyses. Practice combining these techniques to solve real-world data problems efficiently.",
    "crumbs": [
      "Data Exploration",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Data Wrangling with dplyr</span>"
    ]
  },
  {
    "objectID": "chapters/09-exploratory-data-analysis.html",
    "href": "chapters/09-exploratory-data-analysis.html",
    "title": "9  Exploratory Data Analysis",
    "section": "",
    "text": "9.1 Introduction to EDA\nExploratory Data Analysis (EDA) is the critical first step in any data analysis project. Before running statistical tests, fitting models, or drawing conclusions, you need to understand your data intimately. EDA is the practice of using visualization and summary statistics to explore datasets, uncover patterns, detect anomalies, test assumptions, and formulate hypotheses.\nThe term “Exploratory Data Analysis” was popularized by John Tukey in his groundbreaking 1977 book (Tukey 1977). Tukey emphasized that EDA is fundamentally different from confirmatory data analysis. While confirmatory analysis tests specific hypotheses with formal statistical procedures, exploratory analysis is open-ended, creative, and iterative. It is detective work—you are looking for clues about what your data can tell you.\nEDA serves several critical purposes:\nEDA is not a one-time activity. You will return to exploratory analysis repeatedly throughout a project as you clean data, fit models, and interpret results. Each stage reveals new questions that send you back to explore further.",
    "crumbs": [
      "Data Exploration",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/09-exploratory-data-analysis.html#introduction-to-eda",
    "href": "chapters/09-exploratory-data-analysis.html#introduction-to-eda",
    "title": "9  Exploratory Data Analysis",
    "section": "",
    "text": "Tukey’s Philosophy\n\n\n\n“Exploratory data analysis is detective work—numerical detective work—or counting detective work—or graphical detective work… The most important maxim for data analysis is: ‘Far better an approximate answer to the right question, which is often vague, than an exact answer to the wrong question, which can always be made precise.’” — John Tukey\n\n\n\n\nUnderstanding data structure: What variables do you have? What types are they? How are they organized?\nDetecting errors and anomalies: Are there implausible values, outliers, or data entry mistakes?\nChecking assumptions: Do the data meet the assumptions required for planned statistical tests?\nIdentifying patterns and relationships: What trends, correlations, or groupings exist in the data?\nGenerating hypotheses: What questions might these data answer? What further analyses are warranted?",
    "crumbs": [
      "Data Exploration",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/09-exploratory-data-analysis.html#the-eda-mindset",
    "href": "chapters/09-exploratory-data-analysis.html#the-eda-mindset",
    "title": "9  Exploratory Data Analysis",
    "section": "9.2 The EDA Mindset",
    "text": "9.2 The EDA Mindset\nEffective exploratory analysis requires a particular mindset. Be curious but skeptical. Look for patterns but question them. Be systematic but flexible. Most importantly, resist the urge to jump immediately to statistical testing.\nStart broad, then narrow. Begin with the big picture—what is the overall structure of your data? How many observations and variables? What are the basic distributions? Then zoom in on interesting features, unusual patterns, or potential problems.\nUse multiple approaches. Never rely on a single summary statistic or plot type. Look at your data from many angles. A histogram might suggest normality that a Q-Q plot reveals as questionable. A correlation coefficient might suggest no relationship that a scatterplot shows is actually nonlinear.\nDocument your exploration. Keep notes about what you find, questions that arise, and decisions you make. This documentation serves both as a record for yourself and as the foundation for your eventual analysis narrative.\nTrust your visual system. Humans are extraordinarily good at pattern detection. A well-designed plot can reveal relationships that summary statistics obscure. Anscombe’s Quartet demonstrated this dramatically—four datasets with identical means, variances, and correlations but completely different underlying patterns visible only through visualization.",
    "crumbs": [
      "Data Exploration",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/09-exploratory-data-analysis.html#understanding-your-data",
    "href": "chapters/09-exploratory-data-analysis.html#understanding-your-data",
    "title": "9  Exploratory Data Analysis",
    "section": "9.3 Understanding Your Data",
    "text": "9.3 Understanding Your Data\nThe first step in any EDA is to understand the basic structure of your dataset. R provides several functions for getting an overview of your data.\n\nBasic Data Inspection\nThe str() function shows the structure of an object:\n\n\nCode\n# Load example dataset\ndata(iris)\n\n# View structure\nstr(iris)\n\n\n'data.frame':   150 obs. of  5 variables:\n $ Sepal.Length: num  5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ...\n $ Sepal.Width : num  3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ...\n $ Petal.Length: num  1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ...\n $ Petal.Width : num  0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ...\n $ Species     : Factor w/ 3 levels \"setosa\",\"versicolor\",..: 1 1 1 1 1 1 1 1 1 1 ...\n\n\nThis reveals that iris has 150 observations of 5 variables: four numeric measurements and one factor. The glimpse() function from dplyr provides similar information in a more compact format:\n\n\nCode\nglimpse(iris)\n\n\nRows: 150\nColumns: 5\n$ Sepal.Length &lt;dbl&gt; 5.1, 4.9, 4.7, 4.6, 5.0, 5.4, 4.6, 5.0, 4.4, 4.9, 5.4, 4.…\n$ Sepal.Width  &lt;dbl&gt; 3.5, 3.0, 3.2, 3.1, 3.6, 3.9, 3.4, 3.4, 2.9, 3.1, 3.7, 3.…\n$ Petal.Length &lt;dbl&gt; 1.4, 1.4, 1.3, 1.5, 1.4, 1.7, 1.4, 1.5, 1.4, 1.5, 1.5, 1.…\n$ Petal.Width  &lt;dbl&gt; 0.2, 0.2, 0.2, 0.2, 0.2, 0.4, 0.3, 0.2, 0.2, 0.1, 0.2, 0.…\n$ Species      &lt;fct&gt; setosa, setosa, setosa, setosa, setosa, setosa, setosa, s…\n\n\nThe summary() function provides basic statistics for each variable:\n\n\nCode\nsummary(iris)\n\n\n  Sepal.Length    Sepal.Width     Petal.Length    Petal.Width   \n Min.   :4.300   Min.   :2.000   Min.   :1.000   Min.   :0.100  \n 1st Qu.:5.100   1st Qu.:2.800   1st Qu.:1.600   1st Qu.:0.300  \n Median :5.800   Median :3.000   Median :4.350   Median :1.300  \n Mean   :5.843   Mean   :3.057   Mean   :3.758   Mean   :1.199  \n 3rd Qu.:6.400   3rd Qu.:3.300   3rd Qu.:5.100   3rd Qu.:1.800  \n Max.   :7.900   Max.   :4.400   Max.   :6.900   Max.   :2.500  \n       Species  \n setosa    :50  \n versicolor:50  \n virginica :50  \n                \n                \n                \n\n\nFor numeric variables, summary() returns the minimum, first quartile, median, mean, third quartile, and maximum. For factors, it shows counts for each level.\n\n\nChecking Dimensions and Types\nAlways verify the dimensions of your data:\n\n\nCode\n# Number of rows and columns\ndim(iris)\n\n\n[1] 150   5\n\n\nCode\n# Number of rows\nnrow(iris)\n\n\n[1] 150\n\n\nCode\n# Number of columns\nncol(iris)\n\n\n[1] 5\n\n\nCode\n# Column names\nnames(iris)\n\n\n[1] \"Sepal.Length\" \"Sepal.Width\"  \"Petal.Length\" \"Petal.Width\"  \"Species\"     \n\n\nCode\n# Column types\nsapply(iris, class)\n\n\nSepal.Length  Sepal.Width Petal.Length  Petal.Width      Species \n   \"numeric\"    \"numeric\"    \"numeric\"    \"numeric\"     \"factor\" \n\n\n\n\nIdentifying Missing Values\nMissing data is common and can profoundly affect your analysis. Check for missing values systematically:\n\n\nCode\n# Check for any missing values\nany(is.na(iris))\n\n\n[1] FALSE\n\n\nCode\n# Count missing values per column\ncolSums(is.na(iris))\n\n\nSepal.Length  Sepal.Width Petal.Length  Petal.Width      Species \n           0            0            0            0            0 \n\n\nCode\n# Visualize missing data pattern (using a dataset with missing values)\n# Create example data with missing values\niris_missing &lt;- iris\niris_missing[sample(1:150, 10), \"Sepal.Length\"] &lt;- NA\niris_missing[sample(1:150, 5), \"Petal.Width\"] &lt;- NA\n\n# Summary of missing values\ncolSums(is.na(iris_missing))\n\n\nSepal.Length  Sepal.Width Petal.Length  Petal.Width      Species \n          10            0            0            5            0 \n\n\n\n\n\n\n\n\nMissing Data Patterns\n\n\n\nWhen you find missing values, ask: Are they missing completely at random (MCAR), missing at random (MAR), or missing not at random (MNAR)? The pattern of missingness affects how you should handle it. Never blindly delete rows with missing data without understanding why values are missing.",
    "crumbs": [
      "Data Exploration",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/09-exploratory-data-analysis.html#univariate-analysis",
    "href": "chapters/09-exploratory-data-analysis.html#univariate-analysis",
    "title": "9  Exploratory Data Analysis",
    "section": "9.4 Univariate Analysis",
    "text": "9.4 Univariate Analysis\nAfter understanding the basic structure, examine each variable individually. Univariate analysis reveals the distribution, central tendency, spread, and anomalies in single variables.\n\nDistributions of Continuous Variables\nFor continuous variables, visualize the distribution using histograms, density plots, and boxplots.\nHistograms show the frequency of values in bins:\n\n\nCode\nggplot(iris, aes(x = Sepal.Length)) +\n  geom_histogram(bins = 20, fill = \"steelblue\", color = \"white\") +\n  labs(x = \"Sepal Length (cm)\",\n       y = \"Count\",\n       title = \"Distribution of Sepal Length\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nFigure 9.1: Histogram of sepal length showing the distribution of values across bins\n\n\n\n\n\nDensity plots provide a smooth estimate of the distribution:\n\n\nCode\nggplot(iris, aes(x = Sepal.Length)) +\n  geom_density(fill = \"coral\", alpha = 0.5) +\n  labs(x = \"Sepal Length (cm)\",\n       y = \"Density\",\n       title = \"Density Plot of Sepal Length\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nFigure 9.2: Density plot of sepal length showing a smoothed distribution estimate\n\n\n\n\n\nBoxplots summarize the distribution using quartiles and identify potential outliers:\n\n\nCode\nggplot(iris, aes(y = Sepal.Length)) +\n  geom_boxplot(fill = \"lightgreen\") +\n  labs(y = \"Sepal Length (cm)\",\n       title = \"Boxplot of Sepal Length\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nFigure 9.3: Boxplot of sepal length showing median, quartiles, and potential outliers\n\n\n\n\n\n\n\nComparing Distributions Across Variables\nYou can compare distributions of multiple variables using faceted plots:\n\n\nCode\niris |&gt;\n  pivot_longer(cols = -Species,\n               names_to = \"measurement\",\n               values_to = \"value\") |&gt;\n  ggplot(aes(x = value, fill = measurement)) +\n  geom_histogram(bins = 20, color = \"white\", show.legend = FALSE) +\n  facet_wrap(~measurement, scales = \"free\") +\n  labs(x = \"Measurement (cm)\",\n       y = \"Count\",\n       title = \"Distributions of Iris Measurements\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nFigure 9.4: Histograms of all four iris measurements showing different distributional shapes\n\n\n\n\n\n\n\nDistributions of Categorical Variables\nFor categorical variables, use bar charts and frequency tables.\n\n\nCode\nggplot(iris, aes(x = Species, fill = Species)) +\n  geom_bar(show.legend = FALSE) +\n  labs(x = \"Species\",\n       y = \"Count\",\n       title = \"Distribution of Iris Species\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nFigure 9.5: Bar chart showing the frequency of each iris species in the dataset\n\n\n\n\n\nFrequency tables provide the same information numerically:\n\n\nCode\n# Frequency table\ntable(iris$Species)\n\n\n\n    setosa versicolor  virginica \n        50         50         50 \n\n\nCode\n# Proportions\nprop.table(table(iris$Species))\n\n\n\n    setosa versicolor  virginica \n 0.3333333  0.3333333  0.3333333 \n\n\n\n\nSummary Statistics\nCalculate summary statistics to quantify central tendency and spread:\n\n\nCode\n# Mean and median\nmean(iris$Sepal.Length)\n\n\n[1] 5.843333\n\n\nCode\nmedian(iris$Sepal.Length)\n\n\n[1] 5.8\n\n\nCode\n# Standard deviation and variance\nsd(iris$Sepal.Length)\n\n\n[1] 0.8280661\n\n\nCode\nvar(iris$Sepal.Length)\n\n\n[1] 0.6856935\n\n\nCode\n# Interquartile range\nIQR(iris$Sepal.Length)\n\n\n[1] 1.3\n\n\nCode\n# Range\nrange(iris$Sepal.Length)\n\n\n[1] 4.3 7.9\n\n\nCode\n# Quantiles\nquantile(iris$Sepal.Length, probs = c(0.25, 0.50, 0.75))\n\n\n25% 50% 75% \n5.1 5.8 6.4 \n\n\nCode\n# Summary statistics for all numeric variables\niris |&gt;\n  select(where(is.numeric)) |&gt;\n  summary()\n\n\n  Sepal.Length    Sepal.Width     Petal.Length    Petal.Width   \n Min.   :4.300   Min.   :2.000   Min.   :1.000   Min.   :0.100  \n 1st Qu.:5.100   1st Qu.:2.800   1st Qu.:1.600   1st Qu.:0.300  \n Median :5.800   Median :3.000   Median :4.350   Median :1.300  \n Mean   :5.843   Mean   :3.057   Mean   :3.758   Mean   :1.199  \n 3rd Qu.:6.400   3rd Qu.:3.300   3rd Qu.:5.100   3rd Qu.:1.800  \n Max.   :7.900   Max.   :4.400   Max.   :6.900   Max.   :2.500  \n\n\nYou can also calculate summary statistics by group:\n\n\nCode\niris |&gt;\n  group_by(Species) |&gt;\n  summarize(\n    n = n(),\n    mean_sepal = mean(Sepal.Length),\n    sd_sepal = sd(Sepal.Length),\n    median_sepal = median(Sepal.Length),\n    iqr_sepal = IQR(Sepal.Length)\n  )\n\n\n# A tibble: 3 × 6\n  Species        n mean_sepal sd_sepal median_sepal iqr_sepal\n  &lt;fct&gt;      &lt;int&gt;      &lt;dbl&gt;    &lt;dbl&gt;        &lt;dbl&gt;     &lt;dbl&gt;\n1 setosa        50       5.01    0.352          5       0.400\n2 versicolor    50       5.94    0.516          5.9     0.7  \n3 virginica     50       6.59    0.636          6.5     0.675",
    "crumbs": [
      "Data Exploration",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/09-exploratory-data-analysis.html#bivariate-analysis",
    "href": "chapters/09-exploratory-data-analysis.html#bivariate-analysis",
    "title": "9  Exploratory Data Analysis",
    "section": "9.5 Bivariate Analysis",
    "text": "9.5 Bivariate Analysis\nAfter understanding individual variables, explore relationships between pairs of variables. The appropriate visualization depends on the types of variables being compared.\n\nContinuous vs Continuous\nFor two continuous variables, scatterplots are the primary tool:\n\n\nCode\nggplot(iris, aes(x = Sepal.Length, y = Sepal.Width)) +\n  geom_point(alpha = 0.6, size = 2) +\n  geom_smooth(method = \"lm\", se = TRUE, color = \"red\") +\n  labs(x = \"Sepal Length (cm)\",\n       y = \"Sepal Width (cm)\",\n       title = \"Relationship Between Sepal Dimensions\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nFigure 9.6: Scatterplot showing the positive relationship between sepal length and width\n\n\n\n\n\nAdd color to reveal group patterns:\n\n\nCode\nggplot(iris, aes(x = Sepal.Length, y = Sepal.Width, color = Species)) +\n  geom_point(alpha = 0.7, size = 2.5) +\n  labs(x = \"Sepal Length (cm)\",\n       y = \"Sepal Width (cm)\",\n       title = \"Sepal Dimensions by Species\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nFigure 9.7: Scatterplot colored by species revealing distinct clusters in the data\n\n\n\n\n\nCalculate correlation coefficients to quantify linear relationships:\n\n\nCode\n# Pearson correlation\ncor(iris$Sepal.Length, iris$Sepal.Width)\n\n\n[1] -0.1175698\n\n\nCode\n# Correlation matrix for all numeric variables\niris |&gt;\n  select(where(is.numeric)) |&gt;\n  cor()\n\n\n             Sepal.Length Sepal.Width Petal.Length Petal.Width\nSepal.Length    1.0000000  -0.1175698    0.8717538   0.8179411\nSepal.Width    -0.1175698   1.0000000   -0.4284401  -0.3661259\nPetal.Length    0.8717538  -0.4284401    1.0000000   0.9628654\nPetal.Width     0.8179411  -0.3661259    0.9628654   1.0000000\n\n\n\n\n\n\n\n\nCorrelation vs Causation\n\n\n\nCorrelation measures the strength of linear association between variables, but it does not imply causation. Two variables may be correlated because one causes the other, because both are caused by a third variable, or purely by chance. Always examine scatterplots—correlation coefficients can be misleading for nonlinear relationships.\n\n\nCreate a correlation plot matrix to view all pairwise relationships:\n\n\nCode\n# Using GGally package for pairs plot\nif (!require(GGally)) install.packages(\"GGally\")\nlibrary(GGally)\n\nggpairs(iris,\n        columns = 1:4,\n        aes(color = Species, alpha = 0.5),\n        upper = list(continuous = \"cor\"),\n        lower = list(continuous = \"points\")) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nFigure 9.8: Pairs plot showing all pairwise relationships among iris measurements\n\n\n\n\n\n\n\nContinuous vs Categorical\nFor a continuous variable across categorical groups, use boxplots or violin plots:\n\n\nCode\nggplot(iris, aes(x = Species, y = Sepal.Length, fill = Species)) +\n  geom_boxplot(alpha = 0.7, show.legend = FALSE) +\n  labs(x = \"Species\",\n       y = \"Sepal Length (cm)\",\n       title = \"Sepal Length by Species\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nFigure 9.9: Boxplots comparing sepal length distributions across the three iris species\n\n\n\n\n\nViolin plots combine density plots and boxplots to show the full distribution:\n\n\nCode\nggplot(iris, aes(x = Species, y = Sepal.Length, fill = Species)) +\n  geom_violin(alpha = 0.7, show.legend = FALSE) +\n  geom_boxplot(width = 0.2, alpha = 0.5, show.legend = FALSE) +\n  labs(x = \"Species\",\n       y = \"Sepal Length (cm)\",\n       title = \"Sepal Length Distribution by Species\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nFigure 9.10: Violin plots showing the full distribution of sepal length for each species\n\n\n\n\n\nYou can also use overlapping density plots:\n\n\nCode\nggplot(iris, aes(x = Sepal.Length, fill = Species)) +\n  geom_density(alpha = 0.5) +\n  labs(x = \"Sepal Length (cm)\",\n       y = \"Density\",\n       title = \"Sepal Length Distribution by Species\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nFigure 9.11: Overlapping density plots comparing sepal length distributions across species\n\n\n\n\n\n\n\nCategorical vs Categorical\nFor two categorical variables, use contingency tables and mosaic plots.\n\n\nCode\n# Create example categorical data using mpg dataset\ndata(mpg)\n\n# Contingency table\ntable(mpg$class, mpg$drv)\n\n\n            \n              4  f  r\n  2seater     0  0  5\n  compact    12 35  0\n  midsize     3 38  0\n  minivan     0 11  0\n  pickup     33  0  0\n  subcompact  4 22  9\n  suv        51  0 11\n\n\nCode\n# Proportions\nprop.table(table(mpg$class, mpg$drv))\n\n\n            \n                      4          f          r\n  2seater    0.00000000 0.00000000 0.02136752\n  compact    0.05128205 0.14957265 0.00000000\n  midsize    0.01282051 0.16239316 0.00000000\n  minivan    0.00000000 0.04700855 0.00000000\n  pickup     0.14102564 0.00000000 0.00000000\n  subcompact 0.01709402 0.09401709 0.03846154\n  suv        0.21794872 0.00000000 0.04700855\n\n\nVisualize with a stacked or grouped bar chart:\n\n\nCode\nggplot(mpg, aes(x = class, fill = drv)) +\n  geom_bar(position = \"dodge\") +\n  labs(x = \"Vehicle Class\",\n       y = \"Count\",\n       fill = \"Drive Type\",\n       title = \"Vehicle Class by Drive Type\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\nFigure 9.12: Grouped bar chart showing the relationship between vehicle class and drive type\n\n\n\n\n\nMosaic plots show the relative proportions:\n\n\nCode\nif (!require(ggmosaic)) install.packages(\"ggmosaic\")\n\n\nError in `contrib.url()`:\n! trying to use CRAN without setting a mirror\n\n\nCode\nlibrary(ggmosaic)\n\n\nError in `library()`:\n! there is no package called 'ggmosaic'\n\n\nCode\nggplot(mpg) +\n  geom_mosaic(aes(x = product(drv, class), fill = drv)) +\n  labs(x = \"Vehicle Class\",\n       y = \"Drive Type\",\n       title = \"Mosaic Plot: Class vs Drive Type\") +\n  theme_minimal()\n\n\nError in `geom_mosaic()`:\n! could not find function \"geom_mosaic\"",
    "crumbs": [
      "Data Exploration",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/09-exploratory-data-analysis.html#detecting-outliers-and-anomalies",
    "href": "chapters/09-exploratory-data-analysis.html#detecting-outliers-and-anomalies",
    "title": "9  Exploratory Data Analysis",
    "section": "9.6 Detecting Outliers and Anomalies",
    "text": "9.6 Detecting Outliers and Anomalies\nOutliers are observations that differ markedly from other observations in the dataset. They may represent errors, rare but valid extreme values, or the most interesting part of your data. Never automatically delete outliers—investigate them.\n\nVisual Detection\nBoxplots automatically flag potential outliers (values beyond 1.5 × IQR from the quartiles):\n\n\nCode\nggplot(iris, aes(y = Sepal.Width)) +\n  geom_boxplot(fill = \"lightblue\") +\n  labs(y = \"Sepal Width (cm)\",\n       title = \"Potential Outliers in Sepal Width\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nFigure 9.13: Boxplot identifying potential outliers in sepal width measurements\n\n\n\n\n\nScatterplots reveal multivariate outliers:\n\n\nCode\n# Calculate z-scores to identify outliers\niris_z &lt;- iris |&gt;\n  mutate(\n    z_sepal_length = scale(Sepal.Length),\n    z_sepal_width = scale(Sepal.Width),\n    is_outlier = abs(z_sepal_length) &gt; 2.5 | abs(z_sepal_width) &gt; 2.5\n  )\n\nggplot(iris_z, aes(x = Sepal.Length, y = Sepal.Width, color = is_outlier)) +\n  geom_point(size = 2.5, alpha = 0.7) +\n  scale_color_manual(values = c(\"FALSE\" = \"steelblue\", \"TRUE\" = \"red\"),\n                     labels = c(\"Normal\", \"Potential Outlier\")) +\n  labs(x = \"Sepal Length (cm)\",\n       y = \"Sepal Width (cm)\",\n       color = \"Status\",\n       title = \"Outlier Detection Using Z-scores\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nFigure 9.14: Scatterplot with points colored to highlight potential outliers\n\n\n\n\n\n\n\nStatistical Detection\nSeveral statistical methods identify outliers:\n\n\nCode\n# Z-score method (values &gt; 3 SD from mean)\nz_scores &lt;- scale(iris$Sepal.Length)\noutliers_z &lt;- which(abs(z_scores) &gt; 3)\ncat(\"Outliers using z-score method:\", outliers_z, \"\\n\")\n\n\nOutliers using z-score method:  \n\n\nCode\n# IQR method (values beyond 1.5 × IQR from quartiles)\nQ1 &lt;- quantile(iris$Sepal.Length, 0.25)\nQ3 &lt;- quantile(iris$Sepal.Length, 0.75)\nIQR_val &lt;- Q3 - Q1\nlower_bound &lt;- Q1 - 1.5 * IQR_val\nupper_bound &lt;- Q3 + 1.5 * IQR_val\n\noutliers_iqr &lt;- which(iris$Sepal.Length &lt; lower_bound |\n                       iris$Sepal.Length &gt; upper_bound)\ncat(\"Outliers using IQR method:\", outliers_iqr, \"\\n\")\n\n\nOutliers using IQR method:  \n\n\n\n\n\n\n\n\nInvestigating Outliers\n\n\n\nWhen you identify outliers, ask:\n\nIs this a data entry error or measurement error?\nIs this a valid but extreme observation?\nDoes this observation belong to a different population?\nHow does removing or keeping this value affect your conclusions?\n\nDocument your decisions about outlier handling.\n\n\n\n\nRobust Alternatives to Mean and Standard Deviation\nWhen outliers are present, the mean and standard deviation can be heavily influenced by extreme values. Robust statistics provide alternatives that are less sensitive to outliers.\nMedian vs Mean: The median is the value that splits the data in half—50% above, 50% below. Unlike the mean, the median is unaffected by extreme values.\nMAD vs Standard Deviation: The Median Absolute Deviation (MAD) is a robust measure of spread. It is calculated as the median of the absolute deviations from the median:\n\\[\\text{MAD} = \\text{median}(|X_i - \\text{median}(X)|)\\]\nTo make MAD comparable to the standard deviation for normally distributed data, it is often scaled by a factor of 1.4826:\n\\[\\text{MAD}_{\\text{scaled}} = 1.4826 \\times \\text{MAD}\\]\n\n\nCode\n# Compare robust and non-robust statistics\n# Create data with an outlier\nnormal_data &lt;- c(rnorm(99, mean = 50, sd = 5), 200)  # One extreme outlier\n\n# Non-robust statistics\ncat(\"Mean:\", round(mean(normal_data), 2), \"\\n\")\n\n\nMean: 51.08 \n\n\nCode\ncat(\"SD:\", round(sd(normal_data), 2), \"\\n\")\n\n\nSD: 15.75 \n\n\nCode\n# Robust statistics\ncat(\"Median:\", round(median(normal_data), 2), \"\\n\")\n\n\nMedian: 49.69 \n\n\nCode\ncat(\"MAD (scaled):\", round(mad(normal_data), 2), \"\\n\")\n\n\nMAD (scaled): 5.09 \n\n\nCode\n# IQR is also robust\ncat(\"IQR:\", round(IQR(normal_data), 2), \"\\n\")\n\n\nIQR: 6.88 \n\n\nThe MAD and IQR give sensible estimates of spread that are not inflated by the single outlier.\n\n\n\n\n\n\nTukey’s Fences\n\n\n\nThe boxplot outlier rule (1.5 × IQR from the quartiles) is known as Tukey’s fences. Values outside the inner fences (1.5 × IQR) are considered potential outliers, while values outside the outer fences (3 × IQR) are considered extreme outliers. This rule is robust because it is based on quartiles rather than the mean and standard deviation.\n\n\nCode\n# Calculate Tukey's fences\nQ1 &lt;- quantile(normal_data, 0.25)\nQ3 &lt;- quantile(normal_data, 0.75)\nIQR_val &lt;- Q3 - Q1\n\ninner_fence_lower &lt;- Q1 - 1.5 * IQR_val\ninner_fence_upper &lt;- Q3 + 1.5 * IQR_val\nouter_fence_lower &lt;- Q1 - 3 * IQR_val\nouter_fence_upper &lt;- Q3 + 3 * IQR_val\n\ncat(\"Inner fences:\", round(inner_fence_lower, 2), \"to\", round(inner_fence_upper, 2), \"\\n\")\n\n\nInner fences: 35.92 to 63.44 \n\n\nCode\ncat(\"Outer fences:\", round(outer_fence_lower, 2), \"to\", round(outer_fence_upper, 2), \"\\n\")\n\n\nOuter fences: 25.6 to 73.76 \n\n\nCode\ncat(\"Outlier value:\", max(normal_data), \"- far beyond outer fence\\n\")\n\n\nOutlier value: 200 - far beyond outer fence",
    "crumbs": [
      "Data Exploration",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/09-exploratory-data-analysis.html#data-quality-checks",
    "href": "chapters/09-exploratory-data-analysis.html#data-quality-checks",
    "title": "9  Exploratory Data Analysis",
    "section": "9.7 Data Quality Checks",
    "text": "9.7 Data Quality Checks\nBeyond outliers, perform systematic quality checks to identify problems in your data.\n\nChecking for Duplicates\nDuplicate records can skew your analysis:\n\n\nCode\n# Check for completely duplicated rows\nany(duplicated(iris))\n\n\n[1] TRUE\n\n\nCode\n# Find duplicated rows\niris[duplicated(iris), ]\n\n\n    Sepal.Length Sepal.Width Petal.Length Petal.Width   Species\n143          5.8         2.7          5.1         1.9 virginica\n\n\nCode\n# Remove duplicates (if appropriate)\niris_unique &lt;- distinct(iris)\n\n\n\n\nValidating Ranges\nCheck that values fall within plausible ranges:\n\n\nCode\n# Check for negative values (impossible for lengths)\niris |&gt;\n  select(where(is.numeric)) |&gt;\n  summarize(across(everything(),\n                   list(min = min, max = max)))\n\n\n  Sepal.Length_min Sepal.Length_max Sepal.Width_min Sepal.Width_max\n1              4.3              7.9               2             4.4\n  Petal.Length_min Petal.Length_max Petal.Width_min Petal.Width_max\n1                1              6.9             0.1             2.5\n\n\nCode\n# Flag implausible values\niris |&gt;\n  filter(Sepal.Length &lt; 0 | Sepal.Length &gt; 100) |&gt;\n  nrow()\n\n\n[1] 0\n\n\n\n\nChecking Consistency\nLook for logical inconsistencies:\n\n\nCode\n# Example: petal length should not exceed sepal length\niris |&gt;\n  filter(Petal.Length &gt; Sepal.Length) |&gt;\n  select(Species, Sepal.Length, Petal.Length)\n\n\n[1] Species      Sepal.Length Petal.Length\n&lt;0 rows&gt; (or 0-length row.names)\n\n\nCode\n# Check for impossible combinations\n# (This dataset has none, but the pattern is important)",
    "crumbs": [
      "Data Exploration",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/09-exploratory-data-analysis.html#eda-workflow-a-systematic-approach",
    "href": "chapters/09-exploratory-data-analysis.html#eda-workflow-a-systematic-approach",
    "title": "9  Exploratory Data Analysis",
    "section": "9.8 EDA Workflow: A Systematic Approach",
    "text": "9.8 EDA Workflow: A Systematic Approach\nEffective EDA follows a systematic workflow. While the specific steps vary by project, a general framework includes:\n1. Load and inspect the data - Read the data into R - Check dimensions, structure, and variable types - Examine the first and last few rows\n2. Check data quality - Identify missing values - Look for duplicates - Validate ranges and consistency\n3. Univariate analysis - Examine each variable individually - Create appropriate visualizations - Calculate summary statistics\n4. Bivariate and multivariate analysis - Explore relationships between variables - Create scatterplots, correlation matrices, and grouped comparisons - Look for patterns, clusters, and anomalies\n5. Identify and investigate anomalies - Detect outliers using visual and statistical methods - Investigate unusual patterns - Document decisions about data handling\n6. Formulate questions and hypotheses - Based on patterns observed, what questions arise? - What hypotheses warrant testing? - What further analyses are needed?\n7. Document findings - Summarize key characteristics of the data - Note data quality issues and how you addressed them - Record interesting patterns and potential analyses\nLet’s apply this workflow to a real dataset:\n\n\nCode\n# Load data\ndata(mpg)\n\n# 1. Inspect\nglimpse(mpg)\n\n\nRows: 234\nColumns: 11\n$ manufacturer &lt;chr&gt; \"audi\", \"audi\", \"audi\", \"audi\", \"audi\", \"audi\", \"audi\", \"…\n$ model        &lt;chr&gt; \"a4\", \"a4\", \"a4\", \"a4\", \"a4\", \"a4\", \"a4\", \"a4 quattro\", \"…\n$ displ        &lt;dbl&gt; 1.8, 1.8, 2.0, 2.0, 2.8, 2.8, 3.1, 1.8, 1.8, 2.0, 2.0, 2.…\n$ year         &lt;int&gt; 1999, 1999, 2008, 2008, 1999, 1999, 2008, 1999, 1999, 200…\n$ cyl          &lt;int&gt; 4, 4, 4, 4, 6, 6, 6, 4, 4, 4, 4, 6, 6, 6, 6, 6, 6, 8, 8, …\n$ trans        &lt;chr&gt; \"auto(l5)\", \"manual(m5)\", \"manual(m6)\", \"auto(av)\", \"auto…\n$ drv          &lt;chr&gt; \"f\", \"f\", \"f\", \"f\", \"f\", \"f\", \"f\", \"4\", \"4\", \"4\", \"4\", \"4…\n$ cty          &lt;int&gt; 18, 21, 20, 21, 16, 18, 18, 18, 16, 20, 19, 15, 17, 17, 1…\n$ hwy          &lt;int&gt; 29, 29, 31, 30, 26, 26, 27, 26, 25, 28, 27, 25, 25, 25, 2…\n$ fl           &lt;chr&gt; \"p\", \"p\", \"p\", \"p\", \"p\", \"p\", \"p\", \"p\", \"p\", \"p\", \"p\", \"p…\n$ class        &lt;chr&gt; \"compact\", \"compact\", \"compact\", \"compact\", \"compact\", \"c…\n\n\nCode\n# 2. Check quality\ncolSums(is.na(mpg))\n\n\nmanufacturer        model        displ         year          cyl        trans \n           0            0            0            0            0            0 \n         drv          cty          hwy           fl        class \n           0            0            0            0            0 \n\n\nCode\n# 3. Univariate analysis\nsummary(mpg |&gt; select(where(is.numeric)))\n\n\n     displ            year           cyl             cty             hwy       \n Min.   :1.600   Min.   :1999   Min.   :4.000   Min.   : 9.00   Min.   :12.00  \n 1st Qu.:2.400   1st Qu.:1999   1st Qu.:4.000   1st Qu.:14.00   1st Qu.:18.00  \n Median :3.300   Median :2004   Median :6.000   Median :17.00   Median :24.00  \n Mean   :3.472   Mean   :2004   Mean   :5.889   Mean   :16.86   Mean   :23.44  \n 3rd Qu.:4.600   3rd Qu.:2008   3rd Qu.:8.000   3rd Qu.:19.00   3rd Qu.:27.00  \n Max.   :7.000   Max.   :2008   Max.   :8.000   Max.   :35.00   Max.   :44.00  \n\n\nCode\n# 4. Bivariate analysis\n\n\n\n\nCode\np1 &lt;- ggplot(mpg, aes(x = displ, y = hwy)) +\n  geom_point(alpha = 0.5) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  labs(x = \"Engine Displacement (L)\",\n       y = \"Highway MPG\",\n       title = \"Fuel Efficiency vs Engine Size\") +\n  theme_minimal()\n\np2 &lt;- ggplot(mpg, aes(x = class, y = hwy, fill = class)) +\n  geom_boxplot(show.legend = FALSE) +\n  labs(x = \"Vehicle Class\",\n       y = \"Highway MPG\",\n       title = \"Fuel Efficiency by Class\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\np1 + p2\n\n\n\n\n\n\n\n\nFigure 9.15: Exploratory analysis of fuel efficiency showing relationships with engine displacement and vehicle class\n\n\n\n\n\n\n\nCode\n# 5. Key findings\nmpg |&gt;\n  group_by(class) |&gt;\n  summarize(\n    n = n(),\n    mean_hwy = mean(hwy),\n    mean_cty = mean(cty),\n    mean_displ = mean(displ)\n  ) |&gt;\n  arrange(desc(mean_hwy))\n\n\n# A tibble: 7 × 5\n  class          n mean_hwy mean_cty mean_displ\n  &lt;chr&gt;      &lt;int&gt;    &lt;dbl&gt;    &lt;dbl&gt;      &lt;dbl&gt;\n1 compact       47     28.3     20.1       2.33\n2 subcompact    35     28.1     20.4       2.66\n3 midsize       41     27.3     18.8       2.92\n4 2seater        5     24.8     15.4       6.16\n5 minivan       11     22.4     15.8       3.39\n6 suv           62     18.1     13.5       4.46\n7 pickup        33     16.9     13         4.42",
    "crumbs": [
      "Data Exploration",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/09-exploratory-data-analysis.html#practice-exercises",
    "href": "chapters/09-exploratory-data-analysis.html#practice-exercises",
    "title": "9  Exploratory Data Analysis",
    "section": "9.9 Practice Exercises",
    "text": "9.9 Practice Exercises\n\nExercise E.1: Basic Data Inspection\nLoad the mtcars dataset and perform a basic inspection:\n\nView the structure using str() and glimpse()\nCalculate basic dimensions (rows, columns)\nIdentify variable types\nCheck for missing values\n\n\n\nCode\n# Your code here\ndata(mtcars)\n\n\n\n\nExercise E.2: Univariate Analysis\nUsing the mtcars dataset:\n\nCreate a histogram of mpg with an appropriate number of bins\nCreate a density plot of hp (horsepower)\nCreate a boxplot of wt (weight)\nCalculate summary statistics (mean, median, SD, IQR) for mpg\n\n\n\nCode\n# Your code here\n\n\n\n\nExercise E.3: Grouped Comparisons\nCompare mpg across different numbers of cylinders (cyl):\n\nConvert cyl to a factor\nCreate side-by-side boxplots\nCreate overlapping density plots\nCalculate summary statistics by group\n\n\n\nCode\n# Your code here\n\n\n\n\nExercise E.4: Bivariate Relationships\nExplore the relationship between wt (weight) and mpg:\n\nCreate a scatterplot\nAdd a regression line\nCalculate the correlation coefficient\nColor points by cyl to see if the relationship varies by cylinder count\n\n\n\nCode\n# Your code here\n\n\n\n\nExercise E.5: Outlier Detection\nIdentify potential outliers in the mtcars dataset:\n\nCreate a boxplot of hp to visually identify outliers\nUse the IQR method to identify outliers in hp\nUse z-scores to identify multivariate outliers in mpg and wt\nCreate a scatterplot highlighting identified outliers\n\n\n\nCode\n# Your code here\n\n\n\n\nExercise E.6: Comprehensive EDA\nPerform a complete EDA on the diamonds dataset (from ggplot2):\n\nLoad the data and inspect its structure\nCheck for missing values and duplicates\nExamine the distribution of price\nCompare price across different cut categories\nExplore the relationship between carat and price\nIdentify any unusual patterns or outliers\nWrite a brief summary of your findings\n\n\n\nCode\n# Your code here\ndata(diamonds)\n\n\n\n\n\n\n\n\nEDA Never Ends\n\n\n\nRemember that EDA is iterative. Each discovery leads to new questions. Each visualization suggests another perspective. The goal is not to exhaustively analyze every possible relationship but to develop a deep understanding of your data that guides subsequent analysis and interpretation.",
    "crumbs": [
      "Data Exploration",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/09-exploratory-data-analysis.html#summary",
    "href": "chapters/09-exploratory-data-analysis.html#summary",
    "title": "9  Exploratory Data Analysis",
    "section": "9.10 Summary",
    "text": "9.10 Summary\nExploratory Data Analysis is the foundation of sound statistical practice. Before testing hypotheses or fitting models, you must understand your data through systematic exploration. This chapter covered:\n\nThe EDA mindset: Curiosity, skepticism, and systematic exploration\nData structure: Using str(), glimpse(), and summary() to understand your data\nUnivariate analysis: Examining distributions through histograms, density plots, boxplots, and summary statistics\nBivariate analysis: Exploring relationships using scatterplots, grouped comparisons, and correlation\nOutlier detection: Visual and statistical methods for identifying anomalies\nData quality: Checking for missing values, duplicates, and inconsistencies\nEDA workflow: A systematic approach to exploratory analysis\n\nThe techniques in this chapter will serve you in every data analysis project. Make EDA a habit—your statistical conclusions will be more reliable, your interpretations more nuanced, and your communication more convincing when they are grounded in thorough exploratory analysis.\n\n\n\n\n\n\nTukey, John W. 1977. Exploratory Data Analysis. Reading, MA: Addison-Wesley.",
    "crumbs": [
      "Data Exploration",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/06-data-visualization.html",
    "href": "chapters/06-data-visualization.html",
    "title": "10  Data Visualization",
    "section": "",
    "text": "10.1 Why Visualize Data?\nBefore diving into the mechanics of creating plots, consider why visualization matters. The human visual system excels at detecting patterns, spotting outliers, and perceiving relationships—abilities that summary statistics cannot replace.\nConsider Anscombe’s Quartet—four datasets with nearly identical summary statistics (same mean, variance, and correlation) but completely different patterns:\nCode\n# Reshape Anscombe's built-in dataset\nanscombe_long &lt;- anscombe |&gt;\n  pivot_longer(everything(),\n               names_to = c(\".value\", \"set\"),\n               names_pattern = \"(.)(.)\")\n\nggplot(anscombe_long, aes(x = x, y = y)) +\n  geom_point(size = 2, color = \"steelblue\") +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"coral\") +\n  facet_wrap(~set, ncol = 2) +\n  labs(title = \"Same Mean, Variance, and Correlation—Different Stories\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nFigure 10.1: Anscombe’s Quartet: Four datasets with identical summary statistics but very different patterns\nAll four datasets have nearly identical statistical summaries, yet they represent fundamentally different phenomena: a linear relationship, a curved relationship, an outlier-driven relationship, and a vertical cluster with one outlier. Summary statistics alone would suggest these datasets are equivalent—only visualization reveals the truth.",
    "crumbs": [
      "Data Exploration",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "chapters/06-data-visualization.html#why-visualize-data",
    "href": "chapters/06-data-visualization.html#why-visualize-data",
    "title": "10  Data Visualization",
    "section": "",
    "text": "Always Visualize Your Data\n\n\n\nNever trust summary statistics alone. Before running statistical tests, visualize your data to check assumptions, identify outliers, and understand the underlying patterns.",
    "crumbs": [
      "Data Exploration",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "chapters/06-data-visualization.html#choosing-the-right-chart-type",
    "href": "chapters/06-data-visualization.html#choosing-the-right-chart-type",
    "title": "10  Data Visualization",
    "section": "10.2 Choosing the Right Chart Type",
    "text": "10.2 Choosing the Right Chart Type\nDifferent questions call for different visualizations. Matching your question to the right chart type is the first step toward effective communication:\n\n\n\n\n\n\n\n\nQuestion\nChart Type\nWhy\n\n\n\n\nHow are values distributed?\nHistogram, density plot\nShows shape, center, spread\n\n\nHow do groups compare?\nBox plot, bar chart\nSide-by-side comparison\n\n\nHow do two variables relate?\nScatter plot\nShows correlation, patterns\n\n\nHow does a value change over time?\nLine plot\nConnects sequential observations\n\n\nWhat is the composition?\nStacked bar chart\nShows parts of a whole\n\n\n\n\n\n\n\n\n\nStart with the Question\n\n\n\nBefore creating any visualization, ask yourself: “What question am I trying to answer?” The chart type should emerge from the question, not the other way around.",
    "crumbs": [
      "Data Exploration",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "chapters/06-data-visualization.html#the-grammar-of-graphics",
    "href": "chapters/06-data-visualization.html#the-grammar-of-graphics",
    "title": "10  Data Visualization",
    "section": "10.3 The Grammar of Graphics",
    "text": "10.3 The Grammar of Graphics\nData visualization is both an art and a science. A well-designed graphic can reveal patterns, communicate findings, and guide analysis in ways that tables of numbers cannot. The ggplot2 package implements a coherent system for creating graphics based on Leland Wilkinson’s “Grammar of Graphics”—a framework that describes the fundamental components from which all statistical graphics can be built.\nJust as grammar provides rules for constructing sentences from words, the grammar of graphics provides rules for constructing visualizations from components. Every graphic is composed of data, aesthetic mappings that connect variables to visual properties, and geometric objects that represent data points. Additional components like scales, statistical transformations, coordinate systems, and facets allow for sophisticated customizations.\n\n\n\n\n\n\nFigure 10.2: The grammar of graphics breaks visualizations into fundamental components",
    "crumbs": [
      "Data Exploration",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "chapters/06-data-visualization.html#building-plots-with-ggplot2",
    "href": "chapters/06-data-visualization.html#building-plots-with-ggplot2",
    "title": "10  Data Visualization",
    "section": "10.4 Building Plots with ggplot2",
    "text": "10.4 Building Plots with ggplot2\nThe basic structure of a ggplot2 call begins with the ggplot() function, which creates a coordinate system. You add layers to this foundation using the + operator.\n\n\nCode\nggplot(data = mpg, mapping = aes(x = displ, y = hwy)) +\n  geom_point()\n\n\n\n\n\n\n\n\nFigure 10.3: A basic scatterplot of highway fuel efficiency versus engine displacement\n\n\n\n\n\nThis creates a scatterplot of highway fuel efficiency against engine displacement using the built-in mpg dataset. The aes() function establishes the aesthetic mapping—which variables map to which visual properties. Here, displ maps to the x-axis and hwy to the y-axis. The geom_point() function adds a layer of points.",
    "crumbs": [
      "Data Exploration",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "chapters/06-data-visualization.html#aesthetic-mappings",
    "href": "chapters/06-data-visualization.html#aesthetic-mappings",
    "title": "10  Data Visualization",
    "section": "10.5 Aesthetic Mappings",
    "text": "10.5 Aesthetic Mappings\nAesthetics are visual properties of the plot. Beyond position (x and y), common aesthetics include color, size, shape, and transparency (alpha). You can map variables to these aesthetics to encode additional information.\n\n\nCode\nggplot(mpg, aes(x = displ, y = hwy, color = class)) +\n  geom_point(size = 3, alpha = 0.7)\n\n\n\n\n\n\n\n\nFigure 10.4: Scatterplot with color mapped to vehicle class\n\n\n\n\n\nNow the color of each point indicates the vehicle class. The legend is created automatically. Note that aesthetics defined inside aes() are mapped to variables, while those defined outside (like size = 3) apply uniformly to all points.",
    "crumbs": [
      "Data Exploration",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "chapters/06-data-visualization.html#geometric-objects",
    "href": "chapters/06-data-visualization.html#geometric-objects",
    "title": "10  Data Visualization",
    "section": "10.6 Geometric Objects",
    "text": "10.6 Geometric Objects\nGeometric objects, or geoms, determine what type of plot you create. Different geoms represent data in different ways.\n\nScatterplots with geom_point()\nPoints are good for showing the relationship between two continuous variables:\n\n\nCode\nggplot(mpg, aes(x = displ, y = hwy)) +\n  geom_point()\n\n\n\n\n\n\n\n\nFigure 10.5: Simple scatterplot using geom_point()\n\n\n\n\n\n\n\nLine Plots with geom_line() and geom_smooth()\nLines connect points in order, useful for time series or showing trends:\n\n\nCode\nggplot(mpg, aes(x = displ, y = hwy)) +\n  geom_point() +\n  geom_smooth()\n\n\n\n\n\n\n\n\nFigure 10.6: Scatterplot with smoothed trend line and confidence interval\n\n\n\n\n\nThe geom_smooth() function adds a smoothed conditional mean with confidence interval.\n\n\nBar Charts with geom_bar()\nBar charts show counts or summaries of categorical data:\n\n\nCode\nggplot(diamonds, aes(x = cut)) +\n  geom_bar()\n\n\n\n\n\n\n\n\nFigure 10.7: Bar chart showing diamond counts by cut quality\n\n\n\n\n\nUse fill to color bars by another variable:\n\n\nCode\nggplot(diamonds, aes(x = cut, fill = clarity)) +\n  geom_bar(position = \"dodge\")\n\n\n\n\n\n\n\n\nFigure 10.8: Grouped bar chart showing diamond counts by cut and clarity\n\n\n\n\n\n\n\nHistograms with geom_histogram()\nHistograms show the distribution of a continuous variable:\n\n\nCode\nggplot(diamonds, aes(x = carat)) +\n  geom_histogram(binwidth = 0.1, fill = \"steelblue\", color = \"white\")\n\n\n\n\n\n\n\n\nFigure 10.9: Histogram showing the distribution of diamond carat weights\n\n\n\n\n\n\n\nBoxplots with geom_boxplot()\nBoxplots summarize distributions and highlight outliers:\n\n\nCode\nggplot(mpg, aes(x = class, y = hwy)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\nFigure 10.10: Boxplot comparing highway fuel efficiency across vehicle classes",
    "crumbs": [
      "Data Exploration",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "chapters/06-data-visualization.html#combining-multiple-geoms",
    "href": "chapters/06-data-visualization.html#combining-multiple-geoms",
    "title": "10  Data Visualization",
    "section": "10.7 Combining Multiple Geoms",
    "text": "10.7 Combining Multiple Geoms\nYou can layer multiple geoms to create richer visualizations:\n\n\nCode\nggplot(mpg, aes(x = displ, y = hwy)) +\n  geom_point(aes(color = class)) +\n  geom_smooth(se = FALSE, color = \"black\")\n\n\n\n\n\n\n\n\nFigure 10.11: Combining multiple geoms: points colored by class with a black trend line",
    "crumbs": [
      "Data Exploration",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "chapters/06-data-visualization.html#faceting",
    "href": "chapters/06-data-visualization.html#faceting",
    "title": "10  Data Visualization",
    "section": "10.8 Faceting",
    "text": "10.8 Faceting\nFaceting creates small multiples—separate panels for subsets of the data. This is powerful for comparing patterns across groups.\n\n\nCode\nggplot(mpg, aes(x = displ, y = hwy)) +\n  geom_point() +\n  facet_wrap(~ class, nrow = 2)\n\n\n\n\n\n\n\n\nFigure 10.12: Small multiples showing the displacement-efficiency relationship for each vehicle class\n\n\n\n\n\nUse facet_grid() for two-variable faceting:\n\n\nCode\nggplot(mpg, aes(x = displ, y = hwy)) +\n  geom_point() +\n  facet_grid(drv ~ cyl)\n\n\n\n\n\n\n\n\nFigure 10.13: Two-variable faceting by drive type (rows) and number of cylinders (columns)",
    "crumbs": [
      "Data Exploration",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "chapters/06-data-visualization.html#labels-and-titles",
    "href": "chapters/06-data-visualization.html#labels-and-titles",
    "title": "10  Data Visualization",
    "section": "10.9 Labels and Titles",
    "text": "10.9 Labels and Titles\nAdd informative labels with the labs() function:\n\n\nCode\nggplot(mpg, aes(x = displ, y = hwy, color = class)) +\n  geom_point() +\n  geom_smooth(se = FALSE) +\n  labs(\n    title = \"Fuel Efficiency Decreases with Engine Size\",\n    subtitle = \"Data from EPA fuel economy tests\",\n    caption = \"Source: fueleconomy.gov\",\n    x = \"Engine Displacement (liters)\",\n    y = \"Highway Fuel Efficiency (mpg)\",\n    color = \"Vehicle Class\"\n  )\n\n\n\n\n\n\n\n\nFigure 10.14: A well-labeled plot with title, subtitle, caption, and axis labels",
    "crumbs": [
      "Data Exploration",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "chapters/06-data-visualization.html#themes",
    "href": "chapters/06-data-visualization.html#themes",
    "title": "10  Data Visualization",
    "section": "10.10 Themes",
    "text": "10.10 Themes\nThemes control the non-data aspects of the plot—background, grid lines, fonts, etc. ggplot2 includes several built-in themes:\n\n\nCode\nggplot(mpg, aes(x = displ, y = hwy)) +\n  geom_point() +\n  theme_classic()\n\n\n\n\n\n\n\n\nFigure 10.15: Using theme_classic() for a clean, publication-ready appearance\n\n\n\n\n\nOther built-in themes include theme_minimal(), theme_bw(), theme_light(), and theme_dark(). The ggthemes package provides many additional themes.",
    "crumbs": [
      "Data Exploration",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "chapters/06-data-visualization.html#choosing-the-right-plot",
    "href": "chapters/06-data-visualization.html#choosing-the-right-plot",
    "title": "10  Data Visualization",
    "section": "10.11 Choosing the Right Plot",
    "text": "10.11 Choosing the Right Plot\nChoosing an appropriate visualization depends on the types of variables you want to display and the message you want to convey.\n\n\n\n\n\n\nFigure 10.16: Choosing the right visualization depends on your data types and question\n\n\n\nFor one categorical variable, use bar charts. For one continuous variable, use histograms or density plots. For two continuous variables, use scatterplots. For one continuous and one categorical, use boxplots or violin plots. For two categorical variables, use stacked or grouped bar charts or heat maps.",
    "crumbs": [
      "Data Exploration",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "chapters/06-data-visualization.html#principles-of-effective-visualization",
    "href": "chapters/06-data-visualization.html#principles-of-effective-visualization",
    "title": "10  Data Visualization",
    "section": "10.12 Principles of Effective Visualization",
    "text": "10.12 Principles of Effective Visualization\nEdward Tufte articulated principles of graphical excellence that remain influential: “Graphical excellence is that which gives to the viewer the greatest number of ideas in the shortest time with the least ink in the smallest space.”\nKey principles include:\nShow the data. Above all else, make the data visible. Avoid chart junk that obscures what you are trying to communicate.\nEncourage comparison. Design graphics to facilitate comparison of different groups or conditions.\nRepresent magnitudes honestly. The visual representation should be proportional to the numerical quantities being represented. Avoid truncated axes that exaggerate differences.\nMinimize clutter. Remove unnecessary grid lines, borders, and decorations. Every element should serve a purpose.\nMake displays easy to interpret. Use clear labels, appropriate colors, and logical organization.\n\nOrder Categories Meaningfully\nBy default, R orders categorical variables alphabetically, which is rarely the most informative arrangement. Use reorder() to order categories by a meaningful value:\n\n\nCode\n# Create sample data\nsample_data &lt;- tibble(\n  treatment = c(\"Control\", \"Low Dose\", \"Medium Dose\", \"High Dose\"),\n  response = c(12, 18, 25, 31)\n)\n\n# Default alphabetical order (not ideal)\nggplot(sample_data, aes(x = treatment, y = response)) +\n  geom_col(fill = \"steelblue\") +\n  labs(title = \"Default Order (Alphabetical)\",\n       x = \"Treatment\", y = \"Response\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nFigure 10.17: Bar chart with default alphabetical ordering (often not ideal)\n\n\n\n\n\n\n\nCode\n# Order by response value (more meaningful)\nsample_data |&gt;\n  mutate(treatment = reorder(treatment, response)) |&gt;\n  ggplot(aes(x = treatment, y = response)) +\n  geom_col(fill = \"steelblue\") +\n  labs(title = \"Ordered by Value (More Meaningful)\",\n       x = \"Treatment\", y = \"Response\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nFigure 10.18: Bar chart with categories ordered by value for easier comparison\n\n\n\n\n\nThe reorder() function takes a categorical variable and a numeric variable, reordering the categories by the numeric values. For horizontal bar charts (which are often easier to read), add coord_flip():\n\n\nCode\nsample_data |&gt;\n  mutate(treatment = reorder(treatment, response)) |&gt;\n  ggplot(aes(x = treatment, y = response)) +\n  geom_col(fill = \"steelblue\") +\n  coord_flip() +\n  labs(title = \"Horizontal Bars (Good for Long Labels)\",\n       x = NULL, y = \"Response\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nFigure 10.19: Horizontal bar chart with coord_flip() for easier reading of long labels",
    "crumbs": [
      "Data Exploration",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "chapters/06-data-visualization.html#visual-encoding-how-we-perceive-data",
    "href": "chapters/06-data-visualization.html#visual-encoding-how-we-perceive-data",
    "title": "10  Data Visualization",
    "section": "10.13 Visual Encoding: How We Perceive Data",
    "text": "10.13 Visual Encoding: How We Perceive Data\nEffective visualization depends on understanding how humans perceive visual information. We encode data using visual cues—properties like position, length, color, and shape. But not all visual cues are equally effective.\n\nThe Hierarchy of Visual Encoding\nResearch by Cleveland and McGill established that we perceive some visual encodings more accurately than others. From most to least accurate:\n\nPosition along a common scale (scatterplots, dot plots)\nPosition along non-aligned scales (small multiples)\nLength (bar charts)\nAngle/slope (some line charts)\nArea (bubble charts, treemaps)\nVolume (3D charts—generally avoid)\nColor saturation/hue (choropleth maps, heatmaps)\n\n\n\n\n\n\n\n\n\n\nThis hierarchy explains why bar charts work better than pie charts for comparing quantities—we judge lengths more accurately than angles or areas.\n\n\nPosition Is Most Powerful\nWhen possible, encode your most important data using position. Scatterplots, line graphs, and dot plots all use position effectively:\n\n\nCode\n# Compare a pie chart vs. bar chart for the same data\nlibrary(patchwork)\n\ncategory_data &lt;- data.frame(\n  category = c(\"Engineering\", \"Medicine\", \"Natural Sciences\", \"Social Sciences\"),\n  funding = c(35, 28, 22, 15)\n)\n\n# Bar chart - easy to compare\np_bar &lt;- ggplot(category_data, aes(x = reorder(category, funding), y = funding)) +\n  geom_col(fill = \"steelblue\") +\n  coord_flip() +\n  labs(title = \"Bar Chart: Easy Comparison\", x = \"\", y = \"Funding (%)\") +\n  theme_minimal()\n\n# Pie chart - harder to compare\np_pie &lt;- ggplot(category_data, aes(x = \"\", y = funding, fill = category)) +\n  geom_col(width = 1) +\n  coord_polar(\"y\") +\n  labs(title = \"Pie Chart: Harder to Compare\", fill = \"Category\") +\n  theme_void() +\n  theme(legend.position = \"bottom\")\n\np_bar + p_pie\n\n\n\n\n\n\n\n\nFigure 10.20: Comparing bar charts and pie charts: position-based encoding is easier to judge than angles\n\n\n\n\n\nThe bar chart makes it immediately obvious that Engineering has the most funding. With the pie chart, you must work harder to compare the slice sizes.",
    "crumbs": [
      "Data Exploration",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "chapters/06-data-visualization.html#when-should-zero-be-included",
    "href": "chapters/06-data-visualization.html#when-should-zero-be-included",
    "title": "10  Data Visualization",
    "section": "10.14 When Should Zero Be Included?",
    "text": "10.14 When Should Zero Be Included?\nA contentious issue in data visualization is whether the y-axis should always start at zero. The answer depends on the type of chart and what you’re trying to show.\n\nBar Charts: Always Include Zero\nFor bar charts, the length of the bar represents the magnitude of the value. If the axis doesn’t start at zero, the visual representation misrepresents the data:\n\n\nCode\n# Demonstration of misleading truncated axis\ngdp_data &lt;- data.frame(\n  country = c(\"A\", \"B\", \"C\"),\n  gdp = c(45000, 47000, 49000)\n)\n\np_trunc &lt;- ggplot(gdp_data, aes(x = country, y = gdp)) +\n  geom_col(fill = \"steelblue\") +\n  coord_cartesian(ylim = c(44000, 50000)) +\n  labs(title = \"Truncated Axis: Misleading\",\n       subtitle = \"Differences appear huge\",\n       y = \"GDP per capita\") +\n  theme_minimal()\n\np_full &lt;- ggplot(gdp_data, aes(x = country, y = gdp)) +\n  geom_col(fill = \"steelblue\") +\n  labs(title = \"Full Axis: Honest\",\n       subtitle = \"Differences in proper context\",\n       y = \"GDP per capita\") +\n  theme_minimal()\n\np_trunc + p_full\n\n\n\n\n\n\n\n\nFigure 10.21: Truncated axes in bar charts can mislead readers about the magnitude of differences\n\n\n\n\n\n\n\nScatterplots and Line Charts: Context Matters\nFor position-based encodings like scatterplots and line charts, zero doesn’t need to be included if it would waste space and obscure meaningful variation:\n\n\nCode\n# Temperature data - zero would be meaningless\nset.seed(42)\ntemp_data &lt;- data.frame(\n  day = 1:30,\n  temp = rnorm(30, mean = 72, sd = 5)\n)\n\np_zero &lt;- ggplot(temp_data, aes(x = day, y = temp)) +\n  geom_line(color = \"firebrick\") +\n  ylim(0, 100) +\n  labs(title = \"Including Zero: Wastes Space\", y = \"Temperature (°F)\") +\n  theme_minimal()\n\np_auto &lt;- ggplot(temp_data, aes(x = day, y = temp)) +\n  geom_line(color = \"firebrick\") +\n  labs(title = \"Natural Range: Shows Variation\", y = \"Temperature (°F)\") +\n  theme_minimal()\n\np_zero + p_auto\n\n\n\n\n\n\n\n\nFigure 10.22: For line charts, including zero depends on whether it has meaning for the variable\n\n\n\n\n\nThe key question is: what would zero mean for this variable? For temperature in Fahrenheit, zero has no special significance for daily weather data. For proportions or counts, zero is meaningful and often should be included.\n\n\n\n\n\n\nThe Zero Rule\n\n\n\nBar charts: Always include zero—the bar length represents magnitude. Line charts and scatterplots: Include zero if it’s meaningful; otherwise, show the natural range of the data. When in doubt: Ask whether excluding zero could mislead readers about the magnitude of differences.",
    "crumbs": [
      "Data Exploration",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "chapters/06-data-visualization.html#data-transformations-for-visualization",
    "href": "chapters/06-data-visualization.html#data-transformations-for-visualization",
    "title": "10  Data Visualization",
    "section": "10.15 Data Transformations for Visualization",
    "text": "10.15 Data Transformations for Visualization\nSometimes the raw data doesn’t visualize well. Transformations can reveal patterns that are hidden in the original scale.\n\nLog Transformations for Skewed Data\nMany biological variables—gene expression, population sizes, concentrations—follow approximately log-normal distributions with long right tails. Log transformation can make patterns visible:\n\n\nCode\n# Simulated gene expression data\nset.seed(123)\nexpression_data &lt;- data.frame(\n  gene_a = rlnorm(200, meanlog = 2, sdlog = 1.5),\n  gene_b = rlnorm(200, meanlog = 3, sdlog = 1.2)\n)\n\np_raw &lt;- ggplot(expression_data, aes(x = gene_a, y = gene_b)) +\n  geom_point(alpha = 0.5) +\n  labs(title = \"Original Scale\",\n       subtitle = \"Pattern obscured by outliers\",\n       x = \"Gene A Expression\", y = \"Gene B Expression\") +\n  theme_minimal()\n\np_log &lt;- ggplot(expression_data, aes(x = gene_a, y = gene_b)) +\n  geom_point(alpha = 0.5) +\n  scale_x_log10() +\n  scale_y_log10() +\n  labs(title = \"Log Scale\",\n       subtitle = \"Relationship visible\",\n       x = \"Gene A Expression (log)\", y = \"Gene B Expression (log)\") +\n  theme_minimal()\n\np_raw + p_log\n\n\n\n\n\n\n\n\nFigure 10.23: Log transformation reveals patterns in skewed data that span multiple orders of magnitude\n\n\n\n\n\n\n\nWhen to Use Log Scales\nConsider log transformation when:\n\nData span several orders of magnitude\nRelationships are multiplicative rather than additive\nDistribution is strongly right-skewed\nYou’re comparing fold changes or ratios\n\nBe sure to label axes clearly when using transformed scales, and remember that zero cannot be log-transformed.",
    "crumbs": [
      "Data Exploration",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "chapters/06-data-visualization.html#color-in-data-visualization",
    "href": "chapters/06-data-visualization.html#color-in-data-visualization",
    "title": "10  Data Visualization",
    "section": "10.16 Color in Data Visualization",
    "text": "10.16 Color in Data Visualization\nColor is a powerful but often misused encoding. Effective use of color requires understanding perception and accessibility.\n\nTypes of Color Scales\nSequential: For ordered data from low to high. Use a single hue varying in lightness.\n\n\n\n\n\n\n\n\n\nDiverging: For data with a meaningful midpoint. Two hues diverge from a neutral center.\n\n\n\n\n\n\n\n\n\nQualitative: For categorical data with no inherent order. Use distinct hues.\n\n\nColor Accessibility\nApproximately 8% of men and 0.5% of women have some form of color vision deficiency. Design for accessibility:\n\nAvoid red-green as the only distinguishing feature\nUse the viridis color scales, designed for perceptual uniformity and colorblind accessibility\nSupplement color with shape or pattern when possible\n\n\n\nCode\n# Good practice: color + shape\nggplot(mpg, aes(x = displ, y = hwy, color = drv, shape = drv)) +\n  geom_point(size = 3) +\n  scale_color_viridis_d() +\n  labs(title = \"Color + Shape: Accessible Design\",\n       x = \"Engine Displacement (L)\", y = \"Highway MPG\",\n       color = \"Drive Type\", shape = \"Drive Type\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nFigure 10.24: Using both color and shape ensures accessibility for colorblind viewers\n\n\n\n\n\n\n\n\n\n\n\nAvoid Rainbow Color Scales\n\n\n\nRainbow color scales (like the default “jet” colormap in MATLAB) have serious problems:\n\nThey’re not perceptually uniform—yellow appears brighter than blue\nThey create false boundaries where colors change dramatically\nThey’re particularly problematic for colorblind viewers\n\nUse viridis, plasma, or other perceptually uniform scales instead.",
    "crumbs": [
      "Data Exploration",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "chapters/06-data-visualization.html#the-power-of-small-multiples",
    "href": "chapters/06-data-visualization.html#the-power-of-small-multiples",
    "title": "10  Data Visualization",
    "section": "10.17 The Power of Small Multiples",
    "text": "10.17 The Power of Small Multiples\nSmall multiples—the same chart repeated for different subsets of the data—are remarkably effective for comparison. Edward Tufte called them “the best design solution for a wide range of problems in data presentation.”\n\n\nCode\n# Small multiples example\nggplot(gapminder::gapminder %&gt;%\n         filter(continent != \"Oceania\"),\n       aes(x = gdpPercap, y = lifeExp)) +\n  geom_point(alpha = 0.3, size = 0.8) +\n  geom_smooth(method = \"loess\", se = FALSE, color = \"firebrick\") +\n  scale_x_log10(labels = scales::comma) +\n  facet_grid(continent ~ cut(year, breaks = c(1950, 1970, 1990, 2010),\n                              labels = c(\"1952-1970\", \"1971-1990\", \"1991-2007\"))) +\n  labs(title = \"Life Expectancy vs. GDP Over Time by Continent\",\n       x = \"GDP per Capita (log scale)\",\n       y = \"Life Expectancy (years)\") +\n  theme_minimal() +\n  theme(strip.text = element_text(size = 9))\n\n\n\n\n\n\n\n\nFigure 10.25: Small multiples allow easy comparison of patterns across groups and time periods\n\n\n\n\n\nSmall multiples work because:\n\nThe eye can quickly scan and compare panels\nEach panel has identical axes, making comparison fair\nPatterns and outliers become visible through repetition",
    "crumbs": [
      "Data Exploration",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "chapters/06-data-visualization.html#common-visualization-mistakes",
    "href": "chapters/06-data-visualization.html#common-visualization-mistakes",
    "title": "10  Data Visualization",
    "section": "10.18 Common Visualization Mistakes",
    "text": "10.18 Common Visualization Mistakes\nBeyond the principles discussed, watch out for these common errors:\nOverplotting: Too many points obscure patterns. Use transparency, jittering, or density plots.\n\n\nCode\n# Overplotting solution\nset.seed(42)\noverplot_data &lt;- data.frame(\n  x = rnorm(5000),\n  y = rnorm(5000)\n)\n\np_over &lt;- ggplot(overplot_data, aes(x, y)) +\n  geom_point() +\n  labs(title = \"Overplotting: Points Hidden\") +\n  theme_minimal()\n\np_alpha &lt;- ggplot(overplot_data, aes(x, y)) +\n  geom_point(alpha = 0.1) +\n  labs(title = \"Transparency: Density Visible\") +\n  theme_minimal()\n\np_over + p_alpha\n\n\n\n\n\n\n\n\nFigure 10.26: Using transparency (alpha) to reveal density when many points overlap\n\n\n\n\n\nDual y-axes: These are almost always misleading. The relationship between the two scales is arbitrary and can be manipulated to show any desired pattern.\n3D effects: Three-dimensional bar charts, pie charts, and similar decorations distort perception without adding information. Avoid them.\nExcessive decoration: Gridlines, borders, backgrounds, and other “chart junk” should be minimized. Focus attention on the data.",
    "crumbs": [
      "Data Exploration",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "chapters/06-data-visualization.html#examples-of-poor-graphics",
    "href": "chapters/06-data-visualization.html#examples-of-poor-graphics",
    "title": "10  Data Visualization",
    "section": "10.19 Examples of Poor Graphics",
    "text": "10.19 Examples of Poor Graphics\nRecognizing bad graphics helps you avoid making them.\n\n\n\n\n\n\nFigure 10.27: Examples of poor graphic design that obscure rather than reveal patterns\n\n\n\nTicker-tape style displays make it hard to see patterns. Lines connecting unrelated points mislead. Pie charts make comparisons difficult because humans are poor at judging angles. Three-dimensional effects distort perception without adding information.\n\n\n\n\n\n\nFigure 10.28: Pie charts and 3D effects make comparisons difficult and distort perception",
    "crumbs": [
      "Data Exploration",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "chapters/06-data-visualization.html#a-famous-good-example",
    "href": "chapters/06-data-visualization.html#a-famous-good-example",
    "title": "10  Data Visualization",
    "section": "10.20 A Famous Good Example",
    "text": "10.20 A Famous Good Example\nCharles Minard’s 1869 map of Napoleon’s Russian campaign is often cited as one of the best statistical graphics ever made. It displays six variables: the size of the army, its location (latitude and longitude), direction of movement, temperature, and date—all in a single coherent image.\n\n\n\n\n\n\nFigure 10.29: Minard’s 1869 map of Napoleon’s Russian campaign displays six variables in a single coherent image\n\n\n\nThe graphic tells a story. You can see the army shrink as it advances, the devastating losses during the retreat, and the correlation with plummeting temperatures. No legend is needed; the meaning is immediately apparent.",
    "crumbs": [
      "Data Exploration",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "chapters/06-data-visualization.html#saving-plots",
    "href": "chapters/06-data-visualization.html#saving-plots",
    "title": "10  Data Visualization",
    "section": "10.21 Saving Plots",
    "text": "10.21 Saving Plots\nSave plots with ggsave():\n\n\nCode\n# Create and save a plot\np &lt;- ggplot(mpg, aes(x = displ, y = hwy)) +\n  geom_point()\n\nggsave(\"my_plot.png\", p, width = 8, height = 6, dpi = 300)\nggsave(\"my_plot.pdf\", p, width = 8, height = 6)\n\n\nThe function infers the format from the file extension. Specify dimensions and resolution for publication-quality output.",
    "crumbs": [
      "Data Exploration",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "chapters/06-data-visualization.html#practice-exercises",
    "href": "chapters/06-data-visualization.html#practice-exercises",
    "title": "10  Data Visualization",
    "section": "10.22 Practice Exercises",
    "text": "10.22 Practice Exercises\nThe best way to learn ggplot2 is to use it. Take a dataset you care about and try different visualizations. Experiment with aesthetics, geoms, and facets. Read error messages carefully—they often point directly to the problem.\n\nExercise V.1: Basic ggplot2\nCreate your first ggplot visualizations:\n\n\nCode\nlibrary(ggplot2)\ndata(mpg)\n\n# Basic scatterplot\nggplot(mpg, aes(x = displ, y = hwy)) +\n  geom_point()\n\n\n\nCreate a scatterplot of engine displacement vs. highway mpg\nAdd color based on vehicle class\nAdd appropriate axis labels and a title\nTry different themes (theme_minimal(), theme_classic(), theme_bw())\n\n\n\nExercise V.2: Geometric Objects\nPractice with different geoms:\n\nCreate a histogram of highway mpg using geom_histogram()\nCreate a boxplot of highway mpg by vehicle class using geom_boxplot()\nCreate a bar chart showing the count of vehicles by manufacturer using geom_bar()\nCreate a line plot (use a time series dataset or create synthetic data)\n\n\n\nExercise V.3: Aesthetic Mappings\nExplore different aesthetic mappings:\n\nMap a continuous variable to color in a scatterplot\nMap a categorical variable to shape\nSet fixed aesthetics (like size = 3) outside of aes()\nWhat is the difference between mapping a variable to an aesthetic inside aes() versus setting a fixed value outside?\n\n\n\nCode\n# Mapped aesthetic (variable determines color)\nggplot(mpg, aes(x = displ, y = hwy, color = class)) +\n  geom_point(size = 3)\n\n# Fixed aesthetic (all points same color)\nggplot(mpg, aes(x = displ, y = hwy)) +\n  geom_point(color = \"steelblue\", size = 3)\n\n\n\n\nExercise V.4: Faceting\nPractice creating small multiples:\n\nCreate a scatterplot faceted by vehicle class using facet_wrap()\nCreate a grid of plots using facet_grid() with two variables\nExperiment with the scales argument to allow different axis scales per facet\n\n\n\nCode\nggplot(mpg, aes(x = displ, y = hwy)) +\n  geom_point() +\n  facet_wrap(~ class, nrow = 2)\n\n\n\n\nExercise V.5: Combining Layers\nBuild complex visualizations by layering:\n\nCreate a scatterplot with a smoothed trend line\nAdd both points and a regression line\nUse different colors for points and the trend line\n\n\n\nCode\nggplot(mpg, aes(x = displ, y = hwy)) +\n  geom_point(aes(color = class), alpha = 0.7) +\n  geom_smooth(method = \"lm\", color = \"black\", se = TRUE) +\n  labs(\n    title = \"Engine Size vs. Fuel Efficiency\",\n    x = \"Engine Displacement (L)\",\n    y = \"Highway MPG\",\n    color = \"Vehicle Class\"\n  ) +\n  theme_minimal()\n\n\n\n\nExercise V.6: Publication-Quality Figures\nCreate a polished figure suitable for publication:\n\nChoose an appropriate chart type for your data\nAdd informative labels (title, subtitle, caption, axis labels)\nUse an appropriate color palette\nAdjust theme elements for clarity\nSave the figure using ggsave() with appropriate dimensions and resolution",
    "crumbs": [
      "Data Exploration",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "chapters/07-probability-foundations.html",
    "href": "chapters/07-probability-foundations.html",
    "title": "11  Foundations of Probability",
    "section": "",
    "text": "11.1 Why Probability Matters\nIn games of chance, probability has a very intuitive definition. We know what it means that the chance of a pair of dice coming up seven is 1 in 6. However, probability is used much more broadly today, with the word commonly appearing in everyday language. Google’s auto-complete of “What are the chances of” gives us: “having twins”, “rain today”, “getting struck by lightning”, and “getting cancer”. One goal of this chapter is to help us understand how probability is useful to understand and describe real-world events when performing data analysis.\nBecause knowing how to compute probabilities gives you an edge in games of chance, throughout history many smart individuals—including famous mathematicians such as Cardano, Fermat, and Pascal—spent time and energy thinking through the math of these games. As a result, Probability Theory was born. Probability continues to be highly useful in modern games of chance. For example, in poker, we can compute the probability of winning a hand based on the cards on the table. Casinos rely on probability theory to develop games that almost certainly guarantee a profit.\nProbability theory is useful in many other contexts and, in particular, in areas that depend on data affected by chance in some way. All of the other chapters in this part build upon probability theory. Knowledge of probability is therefore indispensable for data science.",
    "crumbs": [
      "Probability and Distributions",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Foundations of Probability</span>"
    ]
  },
  {
    "objectID": "chapters/07-probability-foundations.html#two-interpretations-of-probability",
    "href": "chapters/07-probability-foundations.html#two-interpretations-of-probability",
    "title": "11  Foundations of Probability",
    "section": "11.2 Two Interpretations of Probability",
    "text": "11.2 Two Interpretations of Probability\nThere are two main ways to think about what probability means.\nThe frequentist interpretation views probability as mathematically convenient approximations to long-run relative frequencies. If we say the probability of heads when flipping a fair coin is 0.5, we mean that if we flipped the coin many, many times, about half the flips would come up heads. This interpretation grounds probability in observable, repeatable phenomena.\nThe subjective (Bayesian) interpretation views probability as a measure of belief or uncertainty. A probability statement expresses the opinion of some individual regarding how certain an event is to occur, given their current information. This interpretation allows us to assign probabilities to one-time events and to update beliefs as we gather evidence.\nBoth interpretations have their uses, and modern statistics draws on both perspectives. For now, the frequentist interpretation provides good intuition for the concepts we will develop.",
    "crumbs": [
      "Probability and Distributions",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Foundations of Probability</span>"
    ]
  },
  {
    "objectID": "chapters/07-probability-foundations.html#random-variables-and-sample-spaces",
    "href": "chapters/07-probability-foundations.html#random-variables-and-sample-spaces",
    "title": "11  Foundations of Probability",
    "section": "11.3 Random Variables and Sample Spaces",
    "text": "11.3 Random Variables and Sample Spaces\nA random variable is a quantity that can take on different values with different probabilities. The outcome of a coin flip, the number of bacterial colonies on a plate, and the expression level of a gene are all random variables.\nThe sample space of a random variable is the set of all possible values it can take. For a coin flip, the sample space is {Heads, Tails}. For a die roll, it is {1, 2, 3, 4, 5, 6}. For the concentration of a protein, it might be any non-negative real number.\nA probability distribution describes how likely each value in the sample space is:\n\nFor discrete random variables (those that take distinct values), we use a probability mass function that gives the probability of each possible value\nFor continuous random variables (those that can take any value in a range), we use a probability density function from which probabilities are calculated by integration\n\nOne fundamental rule: the probabilities across the entire sample space must sum (or integrate) to 1. Something from the sample space must happen.",
    "crumbs": [
      "Probability and Distributions",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Foundations of Probability</span>"
    ]
  },
  {
    "objectID": "chapters/07-probability-foundations.html#discrete-probability",
    "href": "chapters/07-probability-foundations.html#discrete-probability",
    "title": "11  Foundations of Probability",
    "section": "11.4 Discrete Probability",
    "text": "11.4 Discrete Probability\nWe start by covering some basic principles related to categorical data. This subset of probability is referred to as discrete probability. It will help us understand the probability theory we will later introduce for numeric and continuous data, which is much more common in data science applications.\n\nRelative Frequency\nA precise definition of probability can be given by noting all possible outcomes and counting how many satisfy the condition for our event. For example, if we have 2 red beads and 3 blue beads inside an urn and we pick one at random, what is the probability of picking a red one?\nOur intuition tells us that the answer is 2/5 or 40%. There are five possible outcomes, of which two satisfy the condition necessary for the event “pick a red bead”. Since each of the five outcomes has the same chance of occurring, we conclude that the probability is 0.4 for red and 0.6 for blue.\nA more tangible way to think about the probability of an event is as the proportion of times the event occurs when we repeat the experiment an infinite number of times, independently, and under the same conditions.\n\n\nNotation\nWe use the notation \\(\\mbox{Pr}(A)\\) to denote the probability of event \\(A\\) happening. We use the very general term event to refer to things that can happen when something occurs by chance. In our previous example, the event was “picking a red bead”. In a political poll in which we call 100 likely voters at random, an example of an event is “calling 48 Democrats and 52 Republicans”.\nIn data science applications, we will often deal with continuous variables. These events will often be things like “is this person taller than 6 feet”. In this case, we write events in a more mathematical form: \\(X \\geq 6\\).\n\n\nProbability Distributions for Categorical Data\nIf we know the relative frequency of the different categories, defining a distribution for categorical outcomes is straightforward. We simply assign a probability to each category. In cases that can be thought of as beads in an urn, for each bead type, their proportion defines the distribution.\nIf we are randomly calling likely voters from a population that is 44% Democrat, 44% Republican, 10% undecided, and 2% Green Party, these proportions define the probability for each group:\n\n\n\nGroup\nProbability\n\n\n\n\nRepublican\n0.44\n\n\nDemocrat\n0.44\n\n\nUndecided\n0.10\n\n\nGreen\n0.02",
    "crumbs": [
      "Probability and Distributions",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Foundations of Probability</span>"
    ]
  },
  {
    "objectID": "chapters/07-probability-foundations.html#distribution-moments-and-parameters",
    "href": "chapters/07-probability-foundations.html#distribution-moments-and-parameters",
    "title": "11  Foundations of Probability",
    "section": "11.5 Distribution Moments and Parameters",
    "text": "11.5 Distribution Moments and Parameters\nProbability distributions can be characterized by their moments—metrics that describe the shape of the distribution. The first four moments correspond to important properties:\n\nMean (\\(\\mu\\)) - the center or expected value\nVariance (\\(\\sigma^2\\)) - the spread or dispersion\nSkewness - the asymmetry of the distribution\nKurtosis - the “tailedness” or peakedness\n\nFor a discrete random variable X, the expected value (mean) is:\n\\[E[X] = \\sum_{\\text{all } x} x \\cdot P(X = x) = \\mu\\]\nThe variance measures dispersion around the mean:\n\\[\\text{Var}(X) = E[(X - \\mu)^2] = \\sigma^2\\]\nThese parameters are crucial because they describe real properties of the systems we study. In biology, for example, the mean height of a population tells us about the typical value, while the variance tells us about the diversity of heights among individuals.",
    "crumbs": [
      "Probability and Distributions",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Foundations of Probability</span>"
    ]
  },
  {
    "objectID": "chapters/07-probability-foundations.html#monte-carlo-simulations",
    "href": "chapters/07-probability-foundations.html#monte-carlo-simulations",
    "title": "11  Foundations of Probability",
    "section": "11.6 Monte Carlo Simulations",
    "text": "11.6 Monte Carlo Simulations\nComputers provide a way to actually perform random experiments. Random number generators permit us to mimic the process of picking at random. An example is the sample function in R.\nFirst, we use the function rep to generate the urn:\n\n\nCode\nbeads &lt;- rep(c(\"red\", \"blue\"), times = c(2, 3))\nbeads\n\n\n[1] \"red\"  \"red\"  \"blue\" \"blue\" \"blue\"\n\n\nThen use sample to pick a bead at random:\n\n\nCode\nsample(beads, 1)\n\n\n[1] \"blue\"\n\n\nThis line of code produces one random outcome. We want to repeat this experiment a large enough number of times to make the results practically equivalent to repeating forever. This is an example of a Monte Carlo simulation.\nTo perform our first Monte Carlo simulation, we use the replicate function, which permits us to repeat the same task any number of times. Here, we repeat the random event B = 10,000 times:\n\n\nCode\nset.seed(1986)  # For reproducibility\nB &lt;- 10000\nevents &lt;- replicate(B, sample(beads, 1))\n\n\nWe can now see if our definition actually agrees with this Monte Carlo simulation approximation:\n\n\nCode\ntab &lt;- table(events)\nprop.table(tab)\n\n\nevents\n  blue    red \n0.6014 0.3986 \n\n\nThe numbers above are the estimated probabilities provided by this Monte Carlo simulation. Statistical theory tells us that as \\(B\\) gets larger, the estimates get closer to 3/5 = 0.6 and 2/5 = 0.4.\n\nWith and Without Replacement\nThe function sample has an argument that permits us to pick more than one element from the urn. By default, this selection occurs without replacement: after a bead is selected, it is not put back in the bag.\n\n\nCode\nsample(beads, 5)\n\n\n[1] \"blue\" \"red\"  \"blue\" \"red\"  \"blue\"\n\n\nThis results in rearrangements that always have three blue and two red beads because we can’t select more beads than exist.\nHowever, we can sample with replacement: return the bead back to the urn after selecting it:\n\n\nCode\nevents &lt;- sample(beads, B, replace = TRUE)\nprop.table(table(events))\n\n\nevents\n blue   red \n0.601 0.399 \n\n\nNot surprisingly, we get results very similar to those previously obtained with replicate.",
    "crumbs": [
      "Probability and Distributions",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Foundations of Probability</span>"
    ]
  },
  {
    "objectID": "chapters/07-probability-foundations.html#the-bernoulli-distribution",
    "href": "chapters/07-probability-foundations.html#the-bernoulli-distribution",
    "title": "11  Foundations of Probability",
    "section": "11.7 The Bernoulli Distribution",
    "text": "11.7 The Bernoulli Distribution\nThe simplest probability distribution describes a single event with two possible outcomes—success or failure, yes or no, heads or tails. This is the Bernoulli distribution.\nConsider flipping a fair coin once:\n\\[P(X = \\text{Head}) = \\frac{1}{2} = 0.5 = p\\]\nAnd the probability of tails is:\n\\[P(X = \\text{Tail}) = \\frac{1}{2} = 0.5 = 1 - p = q\\]\nIf the coin is not fair, \\(p\\) might differ from 0.5, but the probabilities still sum to 1:\n\\[p + (1-p) = 1\\]\nThis same framework applies to any binary outcome: whether a patient responds to treatment, whether an allele is inherited from a parent, or whether a product passes quality control.\n\n\nCode\n# Flip a coin 1000 times\nset.seed(42)\nflips &lt;- rbinom(1000, 1, 0.5)\n\nbarplot(table(flips) / 1000,\n        names.arg = c(\"Tails\", \"Heads\"),\n        ylab = \"Probability\",\n        ylim = c(0, 0.75),\n        col = \"steelblue\",\n        main = \"Estimated Bernoulli Distribution\")\n\n\n\n\n\n\n\n\nFigure 11.1: Simulated Bernoulli distribution from 1000 coin flips showing approximately equal probability for heads and tails",
    "crumbs": [
      "Probability and Distributions",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Foundations of Probability</span>"
    ]
  },
  {
    "objectID": "chapters/07-probability-foundations.html#probability-rules",
    "href": "chapters/07-probability-foundations.html#probability-rules",
    "title": "11  Foundations of Probability",
    "section": "11.8 Probability Rules",
    "text": "11.8 Probability Rules\nTwo fundamental rules govern how probabilities combine. Most probability distributions can be built up from these simple rules.\n\nThe AND Rule (Multiplication)\nThe probability that two independent events both occur is the product of their individual probabilities. If you flip a coin twice:\n\\[P(\\text{First = Head AND Second = Head}) = p \\times p = p^2\\]\nMore generally, for independent events A and B:\n\\[P(A \\text{ and } B) = P(A) \\times P(B)\\]\nFor a fair coin with \\(p = 0.5\\):\n\n\\(P(\\text{HH}) = 0.5 \\times 0.5 = 0.25\\)\n\\(P(\\text{HT}) = 0.5 \\times 0.5 = 0.25\\)\n\\(P(\\text{TH}) = 0.5 \\times 0.5 = 0.25\\)\n\\(P(\\text{TT}) = 0.5 \\times 0.5 = 0.25\\)\n\n\n\nThe OR Rule (Addition)\nThe probability that at least one of two mutually exclusive events occurs is the sum of their probabilities:\n\\[P(A \\text{ or } B) = P(A) + P(B) - P(A \\text{ and } B)\\]\nFor mutually exclusive events (those that cannot both occur), the intersection is empty:\n\\[P(A \\text{ or } B) = P(A) + P(B)\\]\nThe probability of getting exactly one head in two flips (either HT or TH):\n\\[P(\\text{one head}) = P(\\text{HT}) + P(\\text{TH}) = 0.25 + 0.25 = 0.5\\]\n\n\nMultiplication Rule Under Independence\nWhen events are independent, the multiplication rule simplifies:\n\\[P(A \\text{ and } B \\text{ and } C) = P(A) \\times P(B) \\times P(C)\\]\nBut we must be careful—assuming independence can result in very different and incorrect probability calculations when we don’t actually have independence.\n\n\n\n\n\n\nIndependence Matters\n\n\n\nImagine a court case where the suspect was described as having a mustache and a beard. The defendant has both, and the prosecution brings in an “expert” who testifies that 1/10 men have beards and 1/5 have mustaches, concluding that only \\(1/10 \\times 1/5 = 0.02\\) have both.\nBut to multiply like this we need to assume independence! If the conditional probability of a man having a mustache given that he has a beard is 0.95, then the correct probability is much higher: \\(1/10 \\times 0.95 = 0.095\\).",
    "crumbs": [
      "Probability and Distributions",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Foundations of Probability</span>"
    ]
  },
  {
    "objectID": "chapters/07-probability-foundations.html#independence",
    "href": "chapters/07-probability-foundations.html#independence",
    "title": "11  Foundations of Probability",
    "section": "11.9 Independence",
    "text": "11.9 Independence\nWe say two events are independent if the outcome of one does not affect the other. The classic example is coin tosses. Every time we toss a fair coin, the probability of seeing heads is 1/2 regardless of what previous tosses have revealed. The same is true when we pick beads from an urn with replacement.\nMany examples of events that are not independent come from card games. When we deal the first card, the probability of getting a King is 1/13 since there are thirteen possibilities. Now if we deal a King for the first card and don’t replace it into the deck, the probability of a second card being a King is only 3/51. These events are not independent: the first outcome affected the next one.\n\n\nCode\n# Demonstrate non-independence with sequential draws\nset.seed(1)\nx &lt;- sample(beads, 5)\nx[1:4]  # First four draws\n\n\n[1] \"red\"  \"blue\" \"blue\" \"blue\"\n\n\nCode\nx[5]    # If first four are blue, the fifth must be...\n\n\n[1] \"red\"\n\n\nIf you have to guess the color of the first bead, you would predict blue since blue has a 60% chance. But if you know the first four were blue, the probability of the fifth being red is now 100%, not 40%. The events are not independent, so the probabilities change.",
    "crumbs": [
      "Probability and Distributions",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Foundations of Probability</span>"
    ]
  },
  {
    "objectID": "chapters/07-probability-foundations.html#conditional-probabilities",
    "href": "chapters/07-probability-foundations.html#conditional-probabilities",
    "title": "11  Foundations of Probability",
    "section": "11.10 Conditional Probabilities",
    "text": "11.10 Conditional Probabilities\nWhen events are not independent, conditional probabilities are useful. The conditional probability \\(P(B|A)\\) is the probability of B given that A has occurred:\n\\[P(\\text{Card 2 is a King} \\mid \\text{Card 1 is a King}) = \\frac{3}{51}\\]\nWe use the \\(\\mid\\) as shorthand for “given that” or “conditional on”.\nWhen two events A and B are independent:\n\\[P(A \\mid B) = P(A)\\]\nThis is the mathematical definition of independence: the fact that B happened does not affect the probability of A happening.\nThe general multiplication rule relates joint and conditional probability:\n\\[P(A \\text{ and } B) = P(A) \\times P(B \\mid A)\\]\nThis can be extended to more events:\n\\[P(A \\text{ and } B \\text{ and } C) = P(A) \\times P(B \\mid A) \\times P(C \\mid A \\text{ and } B)\\]",
    "crumbs": [
      "Probability and Distributions",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Foundations of Probability</span>"
    ]
  },
  {
    "objectID": "chapters/07-probability-foundations.html#bayes-theorem",
    "href": "chapters/07-probability-foundations.html#bayes-theorem",
    "title": "11  Foundations of Probability",
    "section": "11.11 Bayes’ Theorem",
    "text": "11.11 Bayes’ Theorem\nRearranging the multiplication rule yields Bayes’ theorem, a cornerstone of probabilistic reasoning:\n\\[P(A|B) = \\frac{P(B|A) \\times P(A)}{P(B)}\\]\nBayes’ theorem tells us how to update our beliefs about A after observing B. In Bayesian statistics, this is written as:\n\\[P(\\theta|d) = \\frac{P(d|\\theta) \\times P(\\theta)}{P(d)}\\]\nwhere:\n\n\\(P(\\theta|d)\\) = posterior probability distribution\n\\(P(d|\\theta)\\) = likelihood function for \\(\\theta\\)\n\\(P(\\theta)\\) = prior probability distribution\n\\(P(d)\\) = marginal likelihood (normalizing constant)",
    "crumbs": [
      "Probability and Distributions",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Foundations of Probability</span>"
    ]
  },
  {
    "objectID": "chapters/07-probability-foundations.html#likelihood-vs.-probability",
    "href": "chapters/07-probability-foundations.html#likelihood-vs.-probability",
    "title": "11  Foundations of Probability",
    "section": "11.12 Likelihood vs. Probability",
    "text": "11.12 Likelihood vs. Probability\nA subtle but important distinction exists between probability and likelihood.\nProbability is the chance of observing particular data given a model or parameter value. If we know a coin has \\(p = 0.5\\), what is the probability of observing 7 heads in 10 flips?\nLikelihood is how well a parameter value explains observed data. Given that we observed 7 heads in 10 flips, how likely is it that the true probability is \\(p = 0.5\\) versus \\(p = 0.7\\)?\nMathematically, the likelihood function uses the same formula as probability, but we think of it differently:\n\\[L(\\text{parameter} | \\text{data}) = P(\\text{data} | \\text{parameter})\\]\nMaximum likelihood estimation finds the parameter value that makes the observed data most probable—the value that maximizes the likelihood function.",
    "crumbs": [
      "Probability and Distributions",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Foundations of Probability</span>"
    ]
  },
  {
    "objectID": "chapters/07-probability-foundations.html#combinations-and-permutations",
    "href": "chapters/07-probability-foundations.html#combinations-and-permutations",
    "title": "11  Foundations of Probability",
    "section": "11.13 Combinations and Permutations",
    "text": "11.13 Combinations and Permutations\nFor more complicated probability calculations, we need to count possibilities systematically. The gtools package provides useful functions.\n\nPermutations (Order Matters)\nA permutation is an arrangement where order matters. For any list of size n, the permutations function computes all different arrangements when selecting r items:\n\n\nCode\n# All ways to arrange 2 items from {1, 2, 3}\npermutations(3, 2)\n\n\n     [,1] [,2]\n[1,]    1    2\n[2,]    1    3\n[3,]    2    1\n[4,]    2    3\n[5,]    3    1\n[6,]    3    2\n\n\nNotice that order matters: 3,1 is different than 1,3. Also, (1,1), (2,2), and (3,3) do not appear because once we pick a number, it can’t appear again.\n\n\nCombinations (Order Doesn’t Matter)\nA combination is a selection where order doesn’t matter:\n\n\nCode\ncombinations(3, 2)\n\n\n     [,1] [,2]\n[1,]    1    2\n[2,]    1    3\n[3,]    2    3\n\n\nThe outcome (2,1) doesn’t appear because (1,2) already represents the same combination.\n\n\nExample: Blackjack\nLet’s compute the probability of getting a “Natural 21” in Blackjack—an Ace and a face card in the first two cards:\n\n\nCode\n# Build a deck\nsuits &lt;- c(\"Diamonds\", \"Clubs\", \"Hearts\", \"Spades\")\nnumbers &lt;- c(\"Ace\", \"Deuce\", \"Three\", \"Four\", \"Five\", \"Six\", \"Seven\",\n             \"Eight\", \"Nine\", \"Ten\", \"Jack\", \"Queen\", \"King\")\ndeck &lt;- expand.grid(number = numbers, suit = suits)\ndeck &lt;- paste(deck$number, deck$suit)\n\n# Define aces and face cards\naces &lt;- paste(\"Ace\", suits)\nfacecard &lt;- c(\"King\", \"Queen\", \"Jack\", \"Ten\")\nfacecard &lt;- expand.grid(number = facecard, suit = suits)\nfacecard &lt;- paste(facecard$number, facecard$suit)\n\n# All possible two-card hands (order doesn't matter)\nhands &lt;- combinations(52, 2, v = deck)\n\n# Probability of Natural 21\nmean((hands[,1] %in% aces & hands[,2] %in% facecard) |\n     (hands[,2] %in% aces & hands[,1] %in% facecard))\n\n\n[1] 0.04826546",
    "crumbs": [
      "Probability and Distributions",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Foundations of Probability</span>"
    ]
  },
  {
    "objectID": "chapters/07-probability-foundations.html#classic-examples",
    "href": "chapters/07-probability-foundations.html#classic-examples",
    "title": "11  Foundations of Probability",
    "section": "11.14 Classic Examples",
    "text": "11.14 Classic Examples\n\nThe Monty Hall Problem\nIn the game show “Let’s Make a Deal,” contestants pick one of three doors. Behind one door is a car; behind the others are goats. After you pick a door, Monty Hall opens one of the remaining doors to reveal a goat. Then he asks: “Do you want to switch doors?”\nIntuition suggests it shouldn’t matter—you’re choosing between two doors, so shouldn’t the probability be 50-50? Let’s use Monte Carlo simulation:\n\n\nCode\nB &lt;- 10000\nmonty_hall &lt;- function(strategy) {\n  doors &lt;- as.character(1:3)\n  prize &lt;- sample(c(\"car\", \"goat\", \"goat\"))\n  prize_door &lt;- doors[prize == \"car\"]\n  my_pick &lt;- sample(doors, 1)\n  show &lt;- sample(doors[!doors %in% c(my_pick, prize_door)], 1)\n\n  if (strategy == \"stick\") {\n    choice &lt;- my_pick\n  } else {\n    choice &lt;- doors[!doors %in% c(my_pick, show)]\n  }\n  choice == prize_door\n}\n\nstick_wins &lt;- replicate(B, monty_hall(\"stick\"))\nswitch_wins &lt;- replicate(B, monty_hall(\"switch\"))\n\ncat(\"Probability of winning when sticking:\", mean(stick_wins), \"\\n\")\n\n\nProbability of winning when sticking: 0.3365 \n\n\nCode\ncat(\"Probability of winning when switching:\", mean(switch_wins), \"\\n\")\n\n\nProbability of winning when switching: 0.6643 \n\n\nSwitching doubles your chances! The key insight: when you first pick, you have a 1/3 chance of being right. Monty’s reveal doesn’t change that. Since the probability the car is behind one of the other doors was 2/3, and Monty showed you which one doesn’t have it, switching gives you that 2/3 probability.\n\n\nThe Birthday Problem\nIn a room with 50 people, what’s the probability that at least two share a birthday?\n\n\nCode\n# Monte Carlo simulation\nB &lt;- 10000\nsame_birthday &lt;- function(n) {\n  bdays &lt;- sample(1:365, n, replace = TRUE)\n  any(duplicated(bdays))\n}\n\nresults &lt;- replicate(B, same_birthday(50))\ncat(\"Probability with 50 people:\", mean(results), \"\\n\")\n\n\nProbability with 50 people: 0.9701 \n\n\nCode\n# How does this change with group size?\ncompute_prob &lt;- function(n, B = 10000) {\n  results &lt;- replicate(B, same_birthday(n))\n  mean(results)\n}\n\nn &lt;- seq(1, 60)\nprob &lt;- sapply(n, compute_prob)\nqplot(n, prob, geom = \"line\") +\n  geom_hline(yintercept = 0.5, linetype = \"dashed\", color = \"red\") +\n  labs(x = \"Group Size\", y = \"Probability of Shared Birthday\",\n       title = \"The Birthday Problem\")\n\n\n\n\n\n\n\n\nFigure 11.2: The probability of at least two people sharing a birthday increases rapidly with group size, exceeding 50% at just 23 people\n\n\n\n\n\nWith just 23 people, there’s already a 50% chance of a shared birthday! People tend to underestimate these probabilities because they think about the probability that someone shares their birthday, not the probability that any two people share a birthday.\nWe can also compute this exactly using the multiplication rule:\n\n\nCode\n# Probability that all n people have UNIQUE birthdays\nexact_prob &lt;- function(n) {\n  prob_unique &lt;- seq(365, 365 - n + 1) / 365\n  1 - prod(prob_unique)\n}\n\neprob &lt;- sapply(n, exact_prob)\ncat(\"Exact probability with 50 people:\", exact_prob(50), \"\\n\")\n\n\nExact probability with 50 people: 0.9703736",
    "crumbs": [
      "Probability and Distributions",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Foundations of Probability</span>"
    ]
  },
  {
    "objectID": "chapters/07-probability-foundations.html#covariance-and-correlation",
    "href": "chapters/07-probability-foundations.html#covariance-and-correlation",
    "title": "11  Foundations of Probability",
    "section": "11.15 Covariance and Correlation",
    "text": "11.15 Covariance and Correlation\nWhen two variables are not independent, they share information—knowing one tells you something about the other. This shared information is quantified by covariance, a measure of how two variables vary together.\nIf high values of X tend to occur with high values of Y (and low with low), the covariance is positive. If high values of X tend to occur with low values of Y, the covariance is negative. If there is no relationship, the covariance is near zero.\nCorrelation is covariance standardized to fall between -1 and 1, making it easier to interpret. A correlation of 1 means perfect positive linear relationship; -1 means perfect negative linear relationship; 0 means no linear relationship.\nThese concepts will become central when we discuss regression and other methods for relating variables to each other.",
    "crumbs": [
      "Probability and Distributions",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Foundations of Probability</span>"
    ]
  },
  {
    "objectID": "chapters/07-probability-foundations.html#how-large-is-large-enough-for-monte-carlo",
    "href": "chapters/07-probability-foundations.html#how-large-is-large-enough-for-monte-carlo",
    "title": "11  Foundations of Probability",
    "section": "11.16 How Large is “Large Enough” for Monte Carlo?",
    "text": "11.16 How Large is “Large Enough” for Monte Carlo?\nThe theory described here requires repeating experiments over and over forever. In practice, we can’t do this. In the examples above, we used \\(B = 10,000\\) Monte Carlo experiments and it turned out to provide accurate estimates.\nOne practical approach is to check for the stability of the estimate:\n\n\nCode\nB_values &lt;- 10^seq(1, 5, len = 100)\ncompute_prob_B &lt;- function(B, n = 25) {\n  same_day &lt;- replicate(B, same_birthday(n))\n  mean(same_day)\n}\n\nprob &lt;- sapply(B_values, compute_prob_B)\nqplot(log10(B_values), prob, geom = \"line\") +\n  geom_hline(yintercept = exact_prob(25), color = \"red\", linetype = \"dashed\") +\n  labs(x = \"log10(Number of Simulations)\", y = \"Estimated Probability\",\n       title = \"Monte Carlo Convergence\")\n\n\n\n\n\n\n\n\nFigure 11.3: Monte Carlo estimates converge to the true probability (red dashed line) as the number of simulations increases\n\n\n\n\n\nThe values start to stabilize (vary less than 0.01) around 1000 simulations. The exact probability is shown in red.",
    "crumbs": [
      "Probability and Distributions",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Foundations of Probability</span>"
    ]
  },
  {
    "objectID": "chapters/07-probability-foundations.html#summary",
    "href": "chapters/07-probability-foundations.html#summary",
    "title": "11  Foundations of Probability",
    "section": "11.17 Summary",
    "text": "11.17 Summary\nThis chapter introduced the language of probability:\n\nRandom variables can take different values with different probabilities\nThe sample space contains all possible outcomes\nProbability distributions describe how likely each outcome is\nThe AND rule (multiply) and OR rule (add) combine probabilities\nIndependence means one event doesn’t affect another’s probability\nConditional probability describes probability given other information\nBayes’ theorem updates beliefs based on new evidence\nMonte Carlo simulations estimate probabilities through repeated random sampling\nClassic problems like Monty Hall and birthdays reveal counterintuitive probability results\n\nUnderstanding these foundations is essential for all statistical inference that follows.",
    "crumbs": [
      "Probability and Distributions",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Foundations of Probability</span>"
    ]
  },
  {
    "objectID": "chapters/07-probability-foundations.html#practice-exercises",
    "href": "chapters/07-probability-foundations.html#practice-exercises",
    "title": "11  Foundations of Probability",
    "section": "11.18 Practice Exercises",
    "text": "11.18 Practice Exercises\n\nExercise P.1: Simulating Coin Flips\nUse R to simulate coin flips:\n\n\nCode\nset.seed(123)\n\n\n\nSimulate 100 fair coin flips using rbinom() or sample()\nCalculate the proportion of heads\nRepeat with 1000 and 10000 flips—how does the proportion change?\nCreate a histogram of results from many simulations\n\n\n\nCode\n# Simulate coin flips\nn_flips &lt;- 1000\nflips &lt;- rbinom(n_flips, size = 1, prob = 0.5)\nmean(flips)  # Proportion of heads (1s)\n\n# Or using sample\nflips &lt;- sample(c(\"H\", \"T\"), n_flips, replace = TRUE)\nmean(flips == \"H\")\n\n\n\n\nExercise P.2: Binomial Distribution\nExplore the binomial distribution:\n\nUse rbinom() to simulate 1000 experiments, each with 20 coin flips\nCreate a histogram of the number of heads per experiment\nWhat is the most common outcome? Does this match your expectation?\nChange the probability to simulate an unfair coin\nHow does the distribution change with 200 or 2000 flips per experiment?\n\n\n\nCode\n# 1000 experiments, 20 flips each, fair coin\nset.seed(42)\nresults &lt;- rbinom(1000, size = 20, prob = 0.5)\nhist(results, breaks = 20, col = \"steelblue\",\n     main = \"Number of Heads in 20 Flips\",\n     xlab = \"Number of Heads\")\n\n\n\n\n\n\n\n\nFigure 11.4: Distribution of heads from 1000 simulated experiments of 20 coin flips each\n\n\n\n\n\n\n\nExercise P.3: The Birthday Problem\nUse Monte Carlo simulation to explore the birthday problem:\n\nWrite a function that simulates whether any two people in a group share a birthday\nEstimate the probability for groups of size 10, 23, and 50\nPlot the probability as a function of group size\nAt what group size does the probability exceed 50%?\n\n\n\nCode\n# Birthday simulation function\nsame_birthday &lt;- function(n, B = 10000) {\n  matches &lt;- replicate(B, {\n    birthdays &lt;- sample(1:365, n, replace = TRUE)\n    any(duplicated(birthdays))\n  })\n  mean(matches)\n}\n\n# Test for different group sizes\nsizes &lt;- 2:50\nprobs &lt;- sapply(sizes, same_birthday)\nplot(sizes, probs, type = \"l\",\n     xlab = \"Group Size\", ylab = \"Probability of Shared Birthday\")\nabline(h = 0.5, col = \"red\", lty = 2)\n\n\n\n\nExercise P.4: Conditional Probability\nExplore conditional probability with card simulations:\n\nCreate a virtual deck of 52 cards\nCalculate the probability of drawing a King\nGiven that the first card drawn is a King, what is the probability the second card is also a King?\nUse simulation to verify your calculation\n\n\n\nExercise P.5: The Monty Hall Problem\nSimulate the Monty Hall problem:\n\nWrite a function that simulates one round of the game\nCompare the win rate when you stick versus when you switch\nRun 10,000 simulations for each strategy\nDoes switching really double your chances?",
    "crumbs": [
      "Probability and Distributions",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Foundations of Probability</span>"
    ]
  },
  {
    "objectID": "chapters/07-probability-foundations.html#additional-resources",
    "href": "chapters/07-probability-foundations.html#additional-resources",
    "title": "11  Foundations of Probability",
    "section": "11.19 Additional Resources",
    "text": "11.19 Additional Resources\n\nIrizarry (2019) - A gitbook by a statistician with excellent introductions to key topics in statistical inference\nLogan (2010) - A comprehensive introduction to R for statistical analysis\nFor a detailed reference of common probability distributions, see Chapter 44\n\n\n\n\n\n\n\nIrizarry, Rafael A. 2019. Introduction to Data Science. Self-published. https://rafalab.github.io/dsbook/.\n\n\nLogan, Murray. 2010. Biostatistical Design and Analysis Using r. Wiley-Blackwell.",
    "crumbs": [
      "Probability and Distributions",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Foundations of Probability</span>"
    ]
  },
  {
    "objectID": "chapters/08-discrete-distributions.html",
    "href": "chapters/08-discrete-distributions.html",
    "title": "12  Discrete Probability Distributions",
    "section": "",
    "text": "12.1 What Are Discrete Distributions?\nDiscrete probability distributions describe random variables that take on distinct, countable values. The number of heads in ten coin flips, the count of bacterial colonies on a plate, and the number of defective items in a batch are all discrete random variables. Understanding these distributions allows you to model count data, calculate probabilities of specific outcomes, and perform statistical tests.",
    "crumbs": [
      "Probability and Distributions",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Discrete Probability Distributions</span>"
    ]
  },
  {
    "objectID": "chapters/08-discrete-distributions.html#the-binomial-distribution",
    "href": "chapters/08-discrete-distributions.html#the-binomial-distribution",
    "title": "12  Discrete Probability Distributions",
    "section": "12.2 The Binomial Distribution",
    "text": "12.2 The Binomial Distribution\nThe binomial distribution arises when you perform a fixed number of independent trials, each with the same probability of success. It answers questions like: If I flip a coin 20 times, what is the probability of getting exactly 12 heads?\nThe probability of observing exactly \\(k\\) successes in \\(n\\) trials, when each trial has success probability \\(p\\), is:\n\\[P(X = k) = \\binom{n}{k} p^k (1-p)^{n-k}\\]\nThe binomial coefficient \\(\\binom{n}{k}\\) counts the number of ways to arrange \\(k\\) successes among \\(n\\) trials.\nThe mean of a binomial distribution is \\(\\mu = np\\) and the variance is \\(\\sigma^2 = np(1-p)\\).\n\n\nCode\n# Simulate 1000 experiments of 20 coin flips each\nset.seed(42)\nheads &lt;- rbinom(n = 1000, size = 20, prob = 0.5)\n\nhist(heads, breaks = 0:20, col = \"steelblue\",\n     main = \"Distribution of Heads in 20 Coin Flips\",\n     xlab = \"Number of Heads\")\n\n\n\n\n\n\n\n\nFigure 12.1: Distribution of heads from 1000 simulated experiments of 20 fair coin flips\n\n\n\n\n\nWith a fair coin (\\(p = 0.5\\)) and 20 flips, we expect about 10 heads on average. The distribution is symmetric and centered at 10.\nIn R, functions for the binomial distribution include:\n\ndbinom(k, n, p) - probability of exactly k successes\npbinom(k, n, p) - probability of k or fewer successes (cumulative)\nqbinom(q, n, p) - quantile function (inverse of cumulative)\nrbinom(n, size, p) - generate random samples\n\n\n\nCode\n# Probability of exactly 10 heads in 20 flips\ndbinom(10, size = 20, prob = 0.5)\n\n\n[1] 0.1761971\n\n\nCode\n# Probability of 10 or fewer heads\npbinom(10, size = 20, prob = 0.5)\n\n\n[1] 0.5880985",
    "crumbs": [
      "Probability and Distributions",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Discrete Probability Distributions</span>"
    ]
  },
  {
    "objectID": "chapters/08-discrete-distributions.html#the-poisson-distribution",
    "href": "chapters/08-discrete-distributions.html#the-poisson-distribution",
    "title": "12  Discrete Probability Distributions",
    "section": "12.3 The Poisson Distribution",
    "text": "12.3 The Poisson Distribution\nThe Poisson distribution models the number of events occurring in a fixed interval of time or space, when events occur independently at a constant average rate. It is appropriate for count data like the number of mutations in a DNA sequence, phone calls received per hour, or organisms per quadrat in an ecological survey.\nThe probability of observing exactly \\(r\\) events when the average rate is \\(\\lambda\\) is:\n\\[P(Y = r) = \\frac{e^{-\\lambda} \\lambda^r}{r!}\\]\nA remarkable property of the Poisson distribution is that the mean and variance are both equal to \\(\\lambda\\). This provides a simple check: if your count data has variance much larger than its mean, a simple Poisson model may not be appropriate (a situation called overdispersion, common in biological data).\n\n\n\n\n\n\nFigure 12.2: The Poisson distribution models the number of events in a fixed interval\n\n\n\n\n\nCode\n# Show Poisson distributions with different lambda values\npar(mfrow = c(2, 2))\nfor (lambda in c(1, 3, 5, 10)) {\n  x &lt;- 0:20\n  plot(x, dpois(x, lambda), type = \"h\", lwd = 3, col = \"steelblue\",\n       main = paste(\"Poisson, λ =\", lambda),\n       xlab = \"Count\", ylab = \"Probability\")\n}\n\n\n\n\n\n\n\n\nFigure 12.3: Poisson distributions with different rate parameters (λ) showing how the distribution becomes more symmetric as λ increases\n\n\n\n\n\n\n\n\n\n\n\nFigure 12.4: As λ increases, the Poisson distribution approaches a normal distribution\n\n\n\nAs \\(\\lambda\\) increases, the Poisson distribution becomes more symmetric and approaches a normal distribution.\n\nA Historical Example: Horse Kick Deaths in the Prussian Army\nOne of the earliest applications of the Poisson distribution was in 1898, when it was used to model the number of soldier deaths from horse kicks in 14 different corps of the Prussian army. As shown in Figure 12.5, the Poisson distribution does a remarkable job modeling these unfortunate events.\n\n\nCode\n# Data from Ladislaus Bortkiewicz (1898)\nobserved &lt;- c(109, 65, 22, 3, 1)  # Deaths: 0, 1, 2, 3, 4\nexpected &lt;- dpois(0:4, lambda = 0.7) * 200  # 200 corps-years, estimated lambda\n\ndeaths &lt;- 0:4\nbarplot(rbind(observed, expected), beside = TRUE,\n        col = c(\"steelblue\", \"coral\"),\n        names.arg = deaths,\n        ylim = c(0, 120),\n        ylab = \"Frequency\",\n        xlab = \"Deaths per Year\",\n        main = \"Horse Kick Deaths: Observed vs. Poisson Predicted\")\nlegend(\"topright\", fill = c(\"steelblue\", \"coral\"),\n       legend = c(\"Observed\", \"Predicted from Poisson\"), bty = \"n\")\n\n\n\n\n\n\n\n\nFigure 12.5: Distribution of horse kick deaths per corps per year in the Prussian army (1875-1894). The Poisson distribution closely matches the observed data.\n\n\n\n\n\nThe Poisson distribution is particularly effective at modeling the distribution of rare, independent events like this.\n\n\nCode\n# Probability of exactly 2 events when lambda = 1\ndpois(x = 2, lambda = 1)\n\n\n[1] 0.1839397\n\n\nCode\n# Plot Poisson probabilities\nplot(dpois(x = 0:10, lambda = 3), type = \"h\", lwd = 3,\n     xlab = \"Count\", ylab = \"Probability\",\n     main = \"Poisson Distribution (λ = 3)\")",
    "crumbs": [
      "Probability and Distributions",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Discrete Probability Distributions</span>"
    ]
  },
  {
    "objectID": "chapters/08-discrete-distributions.html#the-geometric-distribution",
    "href": "chapters/08-discrete-distributions.html#the-geometric-distribution",
    "title": "12  Discrete Probability Distributions",
    "section": "12.4 The Geometric Distribution",
    "text": "12.4 The Geometric Distribution\nThe geometric distribution describes the number of trials needed to achieve the first success. If each trial has success probability \\(p\\), the probability that the first success occurs on trial \\(k\\) is:\n\\[P(X = k) = (1-p)^{k-1} p\\]\nThe mean is \\(1/p\\) and the variance is \\((1-p)/p^2\\).\nFor example, if the probability of a cell successfully transfecting is 0.1, the geometric distribution tells us how many cells we need to attempt before getting our first successful transfection.\n\n\nCode\n# Probability of first success on each trial\np &lt;- 0.1\ntrials &lt;- 1:30\nprobs &lt;- dgeom(trials - 1, prob = p)  # dgeom counts failures before first success\n\nplot(trials, probs, type = \"h\", lwd = 2, col = \"steelblue\",\n     xlab = \"Trial Number of First Success\",\n     ylab = \"Probability\",\n     main = \"Geometric Distribution (p = 0.1)\")\n\n\n\n\n\n\n\n\nFigure 12.6: The geometric distribution shows the probability of first success on each trial number\n\n\n\n\n\n\n\n\n\n\n\nFigure 12.7: The geometric distribution describes waiting time until the first success",
    "crumbs": [
      "Probability and Distributions",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Discrete Probability Distributions</span>"
    ]
  },
  {
    "objectID": "chapters/08-discrete-distributions.html#the-negative-binomial-distribution",
    "href": "chapters/08-discrete-distributions.html#the-negative-binomial-distribution",
    "title": "12  Discrete Probability Distributions",
    "section": "12.5 The Negative Binomial Distribution",
    "text": "12.5 The Negative Binomial Distribution\nThe negative binomial distribution generalizes the geometric distribution. It describes the number of trials needed to achieve \\(r\\) successes. If each trial has success probability \\(p\\), the probability that the \\(r\\)th success occurs on trial \\(k\\) is:\n\\[P(X = k) = \\binom{k-1}{r-1} p^r (1-p)^{k-r}\\]\nThe mean is \\(r/p\\) and the variance is \\(r(1-p)/p^2\\).\nConsider a predator that must capture 10 prey to reach reproductive maturity. If the daily probability of catching prey is 0.1, the negative binomial distribution describes when the predator will be ready to reproduce.\n\n\n\n\n\n\nFigure 12.8: The negative binomial distribution generalizes the geometric to waiting for r successes\n\n\n\nThe negative binomial is also commonly used to model overdispersed count data—counts with variance greater than their mean—which the simple Poisson cannot accommodate.",
    "crumbs": [
      "Probability and Distributions",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Discrete Probability Distributions</span>"
    ]
  },
  {
    "objectID": "chapters/08-discrete-distributions.html#common-pattern-in-r",
    "href": "chapters/08-discrete-distributions.html#common-pattern-in-r",
    "title": "12  Discrete Probability Distributions",
    "section": "12.6 Common Pattern in R",
    "text": "12.6 Common Pattern in R\nR uses a consistent naming convention for distribution functions:\n\n\n\nPrefix\nPurpose\nExample\n\n\n\n\nd\nProbability mass/density function\ndbinom(), dpois()\n\n\np\nCumulative distribution function\npbinom(), ppois()\n\n\nq\nQuantile function\nqbinom(), qpois()\n\n\nr\nRandom number generation\nrbinom(), rpois()\n\n\n\nThis pattern applies to all distributions in R:\n\n\n\nDistribution\nFunctions\n\n\n\n\nBinomial\ndbinom, pbinom, qbinom, rbinom\n\n\nPoisson\ndpois, ppois, qpois, rpois\n\n\nGeometric\ndgeom, pgeom, qgeom, rgeom\n\n\nNegative Binomial\ndnbinom, pnbinom, qnbinom, rnbinom",
    "crumbs": [
      "Probability and Distributions",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Discrete Probability Distributions</span>"
    ]
  },
  {
    "objectID": "chapters/08-discrete-distributions.html#choosing-the-right-distribution",
    "href": "chapters/08-discrete-distributions.html#choosing-the-right-distribution",
    "title": "12  Discrete Probability Distributions",
    "section": "12.7 Choosing the Right Distribution",
    "text": "12.7 Choosing the Right Distribution\nSelecting the appropriate distribution depends on the nature of your data and the process generating it.\nUse the binomial when you have a fixed number of independent trials with constant success probability and you are counting successes. Examples include the number of patients responding to treatment out of a fixed sample, the number of correct answers on a test, or the number of defective items in a batch.\nUse the Poisson when you are counting events in a fixed interval of time or space, events occur independently, and the average rate is constant. Examples include mutations per gene, radioactive decays per minute, or organisms per quadrat. Remember that for Poisson data, mean should approximately equal variance.\nUse the geometric when you are counting trials until the first success. Examples include the number of attempts until a successful measurement or the number of patients screened until finding one eligible for a trial.\nUse the negative binomial when counting trials until a specified number of successes, or when modeling overdispersed count data (variance exceeds mean).",
    "crumbs": [
      "Probability and Distributions",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Discrete Probability Distributions</span>"
    ]
  },
  {
    "objectID": "chapters/08-discrete-distributions.html#practice-with-simulations",
    "href": "chapters/08-discrete-distributions.html#practice-with-simulations",
    "title": "12  Discrete Probability Distributions",
    "section": "12.8 Practice with Simulations",
    "text": "12.8 Practice with Simulations\nUnderstanding distributions deepens through simulation. Generate data from each distribution, visualize it, and calculate summary statistics. Compare the theoretical mean and variance to what you observe in your simulated samples.\n\n\nCode\n# Compare theoretical and empirical properties\nset.seed(123)\n\n# Poisson with lambda = 5\npois_sample &lt;- rpois(10000, lambda = 5)\n\ncat(\"Poisson (λ = 5):\\n\")\n\n\nPoisson (λ = 5):\n\n\nCode\ncat(\"Theoretical mean:\", 5, \"  Observed:\", mean(pois_sample), \"\\n\")\n\n\nTheoretical mean: 5   Observed: 4.9746 \n\n\nCode\ncat(\"Theoretical var:\", 5, \"  Observed:\", var(pois_sample), \"\\n\\n\")\n\n\nTheoretical var: 5   Observed: 4.896444 \n\n\nCode\n# Binomial with n = 20, p = 0.3\nbinom_sample &lt;- rbinom(10000, size = 20, prob = 0.3)\n\ncat(\"Binomial (n = 20, p = 0.3):\\n\")\n\n\nBinomial (n = 20, p = 0.3):\n\n\nCode\ncat(\"Theoretical mean:\", 20 * 0.3, \"  Observed:\", mean(binom_sample), \"\\n\")\n\n\nTheoretical mean: 6   Observed: 5.9732 \n\n\nCode\ncat(\"Theoretical var:\", 20 * 0.3 * 0.7, \"  Observed:\", var(binom_sample), \"\\n\")\n\n\nTheoretical var: 4.2   Observed: 4.149097 \n\n\nThis kind of simulation-based exploration builds intuition that complements formal mathematical understanding.",
    "crumbs": [
      "Probability and Distributions",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Discrete Probability Distributions</span>"
    ]
  },
  {
    "objectID": "chapters/08-discrete-distributions.html#exercises",
    "href": "chapters/08-discrete-distributions.html#exercises",
    "title": "12  Discrete Probability Distributions",
    "section": "12.9 Exercises",
    "text": "12.9 Exercises\n\n\n\n\n\n\nExercise D.1: Binomial Probabilities in Genetics\n\n\n\nA genetic cross is expected to produce offspring with a particular trait with probability 0.25 (following Mendelian ratios). You observe 30 offspring from this cross.\n\nWhat is the expected number of offspring showing the trait?\nWhat is the probability of observing exactly 10 offspring with the trait?\nWhat is the probability of observing 10 or more offspring with the trait?\nSimulate 1000 experiments of this cross and plot the distribution of the number of offspring with the trait. Compare to the theoretical binomial distribution.\n\n\n\nCode\n# Your code here\n\n\n\n\n\n\n\n\n\n\nExercise D.2: Poisson Process in Ecology\n\n\n\nThe number of seedlings germinating per square meter follows a Poisson distribution with an average of 4.5 seedlings per m².\n\nWhat is the probability of finding exactly 5 seedlings in a randomly selected square meter?\nWhat is the probability of finding fewer than 3 seedlings?\nWhat is the variance of this distribution?\nIf the variance you observe in your actual data is 12, what does this suggest about the Poisson model?\n\n\n\nCode\n# Your code here\n\n\n\n\n\n\n\n\n\n\nExercise D.3: Comparing Distributions\n\n\n\nA researcher is counting bacterial colonies on petri dishes. In one experiment with 50 dishes, she found: - 5 dishes with 0 colonies - 12 dishes with 1 colony - 18 dishes with 2 colonies - 10 dishes with 3 colonies - 5 dishes with 4 or more colonies\n\nCalculate the mean and variance of these counts\nDoes a Poisson distribution seem appropriate? Why or why not?\nEstimate the Poisson parameter λ from the data and plot the expected vs. observed frequencies\nWhat biological processes might explain any discrepancies you observe?\n\n\n\nCode\n# Your code here\n\n\n\n\n\n\n\n\n\n\nExercise D.4: Negative Binomial Application\n\n\n\nYou are screening patients for eligibility in a clinical trial. Each patient has a 0.15 probability of meeting the eligibility criteria.\n\nWhat is the expected number of patients you need to screen to find 5 eligible patients?\nWhat is the probability that you find your 5th eligible patient on the 40th screening?\nSimulate this screening process 1000 times and create a histogram of the number of screenings needed to find 5 eligible patients\nCalculate the 95th percentile of your simulation—what does this number mean in practical terms?\n\n\n\nCode\n# Your code here\n\n\n\n\n\n\n\n\n\n\nExercise D.5: Choosing the Right Distribution\n\n\n\nFor each scenario below, identify which discrete distribution (binomial, Poisson, geometric, or negative binomial) is most appropriate and explain why:\n\nThe number of mutations in a 1000 base pair DNA sequence, where mutations occur randomly at a rate of 0.001 per base pair\nThe number of successful PCR reactions out of 20 attempts, where each attempt has a 90% success rate\nThe number of attempts needed to successfully culture a difficult cell line for the first time, when each attempt has a 5% success rate\nThe number of times you need to flip a coin until you get 3 heads\nThe number of fish caught per hour in a lake, where the average catch rate is 2.5 fish per hour\n\nFor one of these scenarios, write R code to: - Simulate the process - Calculate relevant probabilities - Create an appropriate visualization\n\n\nCode\n# Your code here",
    "crumbs": [
      "Probability and Distributions",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Discrete Probability Distributions</span>"
    ]
  },
  {
    "objectID": "chapters/09-continuous-distributions.html",
    "href": "chapters/09-continuous-distributions.html",
    "title": "13  Continuous Probability Distributions",
    "section": "",
    "text": "13.1 From Discrete to Continuous\nMany quantities we measure—weight, concentration, time, temperature—can take any value within a range, not just discrete counts. These continuous random variables require a different mathematical treatment. Instead of probability mass functions that assign probabilities to specific values, we use probability density functions (PDFs) where probabilities come from integrating over intervals.\nFor a continuous random variable, the probability that it falls within an interval \\([a, b]\\) is:\n\\[P(a \\leq X \\leq b) = \\int_a^b f(x) \\, dx\\]\nwhere \\(f(x)\\) is the probability density function. The total area under the density curve must equal 1:\n\\[\\int_{-\\infty}^{\\infty} f(x) \\, dx = 1\\]\nNote that for continuous variables, the probability of any exact value is zero—only intervals have non-zero probability.",
    "crumbs": [
      "Probability and Distributions",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Continuous Probability Distributions</span>"
    ]
  },
  {
    "objectID": "chapters/09-continuous-distributions.html#the-uniform-distribution",
    "href": "chapters/09-continuous-distributions.html#the-uniform-distribution",
    "title": "13  Continuous Probability Distributions",
    "section": "13.2 The Uniform Distribution",
    "text": "13.2 The Uniform Distribution\nThe simplest continuous distribution is the uniform distribution, where all values in an interval are equally likely. If \\(X\\) is uniformly distributed between \\(a\\) and \\(b\\):\n\\[f(x) = \\frac{1}{b-a} \\quad \\text{for } a \\leq x \\leq b\\]\nThe mean is \\((a+b)/2\\) and the variance is \\((b-a)^2/12\\).\n\n\n\n\n\n\nFigure 13.1: The uniform distribution has constant probability density across an interval\n\n\n\n\n\nCode\n# Uniform distribution between 0 and 10\nx &lt;- seq(0, 10, length.out = 100)\nplot(x, dunif(x, min = 0, max = 10), type = \"l\", lwd = 2,\n     xlab = \"x\", ylab = \"Density\",\n     main = \"Uniform Distribution (0, 10)\")\n\n\n\n\n\n\n\n\nFigure 13.2: The uniform distribution on [0, 10] showing constant density\n\n\n\n\n\nThe uniform distribution is often used to model random number generation and situations where no outcome is favored over another within a range.",
    "crumbs": [
      "Probability and Distributions",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Continuous Probability Distributions</span>"
    ]
  },
  {
    "objectID": "chapters/09-continuous-distributions.html#the-exponential-distribution",
    "href": "chapters/09-continuous-distributions.html#the-exponential-distribution",
    "title": "13  Continuous Probability Distributions",
    "section": "13.3 The Exponential Distribution",
    "text": "13.3 The Exponential Distribution\nThe exponential distribution models waiting times between events in a Poisson process—the time until the next event when events occur randomly at a constant rate \\(\\lambda\\). Its density function is:\n\\[f(x) = \\lambda e^{-\\lambda x} \\quad \\text{for } x \\geq 0\\]\nThe mean is \\(1/\\lambda\\) and the variance is \\(1/\\lambda^2\\).\n\n\n\n\n\n\nFigure 13.3: The exponential distribution models waiting times in a Poisson process\n\n\n\nIf a radioactive isotope has a decay rate of \\(\\lambda = 0.1\\) per minute, the time until the next decay follows an exponential distribution with mean 10 minutes.\n\n\nCode\n# Exponential distributions with different rates\nx &lt;- seq(0, 30, length.out = 200)\nplot(x, dexp(x, rate = 0.1), type = \"l\", lwd = 2, col = \"blue\",\n     xlab = \"Time\", ylab = \"Density\",\n     main = \"Exponential Distribution (λ = 0.1)\")\n\n\n\n\n\n\n\n\nFigure 13.4: Exponential distribution showing the characteristic right-skewed shape of waiting times\n\n\n\n\n\nA key property of the exponential distribution is memorylessness: the probability of waiting another \\(t\\) units does not depend on how long you have already waited.",
    "crumbs": [
      "Probability and Distributions",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Continuous Probability Distributions</span>"
    ]
  },
  {
    "objectID": "chapters/09-continuous-distributions.html#the-gamma-distribution",
    "href": "chapters/09-continuous-distributions.html#the-gamma-distribution",
    "title": "13  Continuous Probability Distributions",
    "section": "13.4 The Gamma Distribution",
    "text": "13.4 The Gamma Distribution\nThe gamma distribution generalizes the exponential distribution to model the waiting time until the \\(r\\)th event in a Poisson process. Its density function involves two parameters: shape \\(r\\) and rate \\(\\lambda\\):\n\\[f(x) = \\frac{\\lambda^r x^{r-1} e^{-\\lambda x}}{(r-1)!} \\quad \\text{for } x \\geq 0\\]\nThe mean is \\(r/\\lambda\\) and the variance is \\(r/\\lambda^2\\).\nWhen \\(r = 1\\), the gamma distribution reduces to the exponential. As \\(r\\) increases, the distribution becomes more symmetric and bell-shaped.",
    "crumbs": [
      "Probability and Distributions",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Continuous Probability Distributions</span>"
    ]
  },
  {
    "objectID": "chapters/09-continuous-distributions.html#the-normal-gaussian-distribution",
    "href": "chapters/09-continuous-distributions.html#the-normal-gaussian-distribution",
    "title": "13  Continuous Probability Distributions",
    "section": "13.5 The Normal (Gaussian) Distribution",
    "text": "13.5 The Normal (Gaussian) Distribution\nThe normal distribution is the most important continuous distribution in statistics. Its distinctive bell-shaped curve appears throughout nature, and the Central Limit Theorem explains why: the sum of many independent random effects tends toward normality regardless of the underlying distributions.\nThe normal distribution is characterized by two parameters: mean \\(\\mu\\) (center) and standard deviation \\(\\sigma\\) (spread):\n\\[f(x) = \\frac{1}{\\sigma\\sqrt{2\\pi}} e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}\\]\n\n\nCode\n# Normal distributions with different parameters\nx &lt;- seq(-10, 15, length.out = 200)\nplot(x, dnorm(x, mean = 0, sd = 1), type = \"l\", lwd = 2, col = \"blue\",\n     ylim = c(0, 0.5), xlab = \"x\", ylab = \"Density\",\n     main = \"Normal Distributions\")\nlines(x, dnorm(x, mean = 0, sd = 2), lwd = 2, col = \"red\")\nlines(x, dnorm(x, mean = 5, sd = 1), lwd = 2, col = \"darkgreen\")\nlegend(\"topright\",\n       legend = c(\"μ=0, σ=1\", \"μ=0, σ=2\", \"μ=5, σ=1\"),\n       col = c(\"blue\", \"red\", \"darkgreen\"), lwd = 2)\n\n\n\n\n\n\n\n\nFigure 13.5: Normal distributions with different mean (μ) and standard deviation (σ) parameters\n\n\n\n\n\n\nProperties of the Normal Distribution\nThe normal distribution is symmetric around its mean. The mean, median, and mode are all equal. About 68% of the distribution falls within one standard deviation of the mean, 95% within two standard deviations, and 99.7% within three standard deviations (the “68-95-99.7 rule”).\n\n\n\n\n\n\nFigure 13.6: The 68-95-99.7 rule for the normal distribution.\n\n\n\n\n\nEstimating Normal Parameters\nThe mean of a sample provides an estimate of the population mean:\n\\[\\bar{x} = \\frac{1}{n}\\sum_{i=1}^{n}x_i\\]\nThe sample variance estimates the population variance:\n\\[s^2 = \\frac{1}{n-1}\\sum_{i=1}^{n}(x_i - \\bar{x})^2\\]\nNote the \\(n-1\\) in the denominator (called Bessel’s correction), which provides an unbiased estimate of the population variance.\n\n\nThe Standard Normal Distribution\nWhen \\(\\mu = 0\\) and \\(\\sigma = 1\\), we have the standard normal distribution. Any normal variable can be converted to standard normal by subtracting the mean and dividing by the standard deviation:\n\\[Z = \\frac{X - \\mu}{\\sigma}\\]\nThis standardization, called computing a z-score, allows us to compare values from different normal distributions and to use tables of standard normal probabilities.\n\n\nCode\n# Probability calculations with the normal distribution\n# P(X &lt; 1.96) for standard normal\npnorm(1.96)\n\n\n[1] 0.9750021\n\n\nCode\n# P(-1.96 &lt; X &lt; 1.96)\npnorm(1.96) - pnorm(-1.96)\n\n\n[1] 0.9500042\n\n\nCode\n# What value has 97.5% of the distribution below it?\nqnorm(0.975)\n\n\n[1] 1.959964\n\n\nThe values 1.96 and -1.96 are particularly important because they bound the middle 95% of the standard normal distribution, forming the basis for 95% confidence intervals.\n\n\nZ-Scores\nA z-score is a standardized value that tells us how many standard deviations an observation is from the mean:\n\\[z_i = \\frac{x_i - \\bar{x}}{s}\\]\nZ-scores allow us to compare values from different normal distributions on a common scale. This is particularly useful when comparing measurements that have different units or very different magnitudes—for example, comparing the relative leg length of mice versus elephants.\n\n\nWhy the Normal Distribution is Special in Biology\nThe normal distribution appears throughout biology because many biological traits are influenced by numerous factors, each contributing a small effect. This is particularly evident in quantitative genetics.\n\n\n\n\n\n\nFigure 13.7: The genetic model of complex traits explains why many biological measurements are normally distributed.\n\n\n\nConsider a trait influenced by multiple genes. If we have many loci, each with a small additive effect, the distribution of trait values in a population will approximate a normal distribution—even if the contribution at each locus follows a simple Mendelian pattern.\n\n\n\n\n\n\nFigure 13.8: The distribution of genotypes in an F2 cross approaches normality as the number of contributing loci increases.\n\n\n\nThis connection between many small independent effects and the normal distribution is formalized by the Central Limit Theorem, which we explore below.\n\n\nChecking Normality\nMany statistical methods assume normally distributed data. Before applying these methods, you should check whether the assumption is reasonable.\nVisual methods include histograms and Q-Q (quantile-quantile) plots:\n\n\nCode\n# Generate some data\nset.seed(42)\nnormal_data &lt;- rnorm(200, mean = 50, sd = 10)\nskewed_data &lt;- rexp(200, rate = 0.1)\n\npar(mfrow = c(1, 2))\n\n# Q-Q plot for normal data\nqqnorm(normal_data, main = \"Normal Data\")\nqqline(normal_data, col = \"red\")\n\n# Q-Q plot for skewed data\nqqnorm(skewed_data, main = \"Skewed Data\")\nqqline(skewed_data, col = \"red\")\n\n\n\n\n\n\n\n\nFigure 13.9: Q-Q plots for assessing normality: normally distributed data (left) follows the diagonal line, while skewed data (right) deviates\n\n\n\n\n\nIn a Q-Q plot, normally distributed data should fall approximately along the diagonal line. Systematic deviations indicate non-normality.",
    "crumbs": [
      "Probability and Distributions",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Continuous Probability Distributions</span>"
    ]
  },
  {
    "objectID": "chapters/09-continuous-distributions.html#the-central-limit-theorem",
    "href": "chapters/09-continuous-distributions.html#the-central-limit-theorem",
    "title": "13  Continuous Probability Distributions",
    "section": "13.6 The Central Limit Theorem",
    "text": "13.6 The Central Limit Theorem\nThe Central Limit Theorem (CLT) states that the sampling distribution of the mean approaches normality as sample size increases, regardless of the shape of the population distribution. This is why the normal distribution appears so frequently in statistics—we often work with means or other sums of random variables.\n\n\nCode\n# Demonstrate CLT with exponential distribution\nset.seed(123)\n\n# Exponential distribution is quite skewed\npar(mfrow = c(2, 2))\n\n# Original distribution\nhist(rexp(10000, rate = 1), breaks = 50, main = \"Original: Exponential\",\n     xlab = \"x\", col = \"lightblue\")\n\n# Means of samples of size 5\nmeans_5 &lt;- replicate(10000, mean(rexp(5, rate = 1)))\nhist(means_5, breaks = 50, main = \"Means of n=5\",\n     xlab = \"Sample Mean\", col = \"lightblue\")\n\n# Means of samples of size 30\nmeans_30 &lt;- replicate(10000, mean(rexp(30, rate = 1)))\nhist(means_30, breaks = 50, main = \"Means of n=30\",\n     xlab = \"Sample Mean\", col = \"lightblue\")\n\n# Means of samples of size 100\nmeans_100 &lt;- replicate(10000, mean(rexp(100, rate = 1)))\nhist(means_100, breaks = 50, main = \"Means of n=100\",\n     xlab = \"Sample Mean\", col = \"lightblue\")\n\n\n\n\n\n\n\n\nFigure 13.10: The Central Limit Theorem: sampling distributions of means become normal regardless of the population distribution as sample size increases\n\n\n\n\n\nEven though the exponential distribution is strongly right-skewed, the distribution of sample means becomes increasingly normal as sample size grows. This is the Central Limit Theorem in action.",
    "crumbs": [
      "Probability and Distributions",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Continuous Probability Distributions</span>"
    ]
  },
  {
    "objectID": "chapters/09-continuous-distributions.html#summary-of-distribution-functions-in-r",
    "href": "chapters/09-continuous-distributions.html#summary-of-distribution-functions-in-r",
    "title": "13  Continuous Probability Distributions",
    "section": "13.7 Summary of Distribution Functions in R",
    "text": "13.7 Summary of Distribution Functions in R\nR provides consistent functions for all distributions:\n\n\n\n\n\n\n\n\n\n\nDistribution\nd (density)\np (cumulative)\nq (quantile)\nr (random)\n\n\n\n\nUniform\ndunif\npunif\nqunif\nrunif\n\n\nExponential\ndexp\npexp\nqexp\nrexp\n\n\nNormal\ndnorm\npnorm\nqnorm\nrnorm\n\n\nGamma\ndgamma\npgamma\nqgamma\nrgamma\n\n\n\nUnderstanding these distributions and their properties prepares you for statistical inference, where we use sampling distributions to make probabilistic statements about population parameters.",
    "crumbs": [
      "Probability and Distributions",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Continuous Probability Distributions</span>"
    ]
  },
  {
    "objectID": "chapters/09-continuous-distributions.html#exercises",
    "href": "chapters/09-continuous-distributions.html#exercises",
    "title": "13  Continuous Probability Distributions",
    "section": "13.8 Exercises",
    "text": "13.8 Exercises\n\n\n\n\n\n\nExercise C.1: Normal Distribution Calculations\n\n\n\nHeights of adult males in a population are normally distributed with mean 175 cm and standard deviation 7 cm.\n\nWhat proportion of males are taller than 185 cm?\nWhat proportion are between 170 and 180 cm?\nWhat height represents the 90th percentile?\nIf you randomly sample 4 males, what is the probability that their average height exceeds 180 cm? (Hint: use the sampling distribution of the mean)\n\n\n\nCode\n# Your code here\n\n\n\n\n\n\n\n\n\n\nExercise C.2: Exponential Waiting Times\n\n\n\nThe time between arrivals of patients at an emergency room follows an exponential distribution with mean 12 minutes.\n\nWhat is the rate parameter λ?\nWhat is the probability that the next patient arrives within 5 minutes?\nWhat is the probability that more than 20 minutes elapse before the next patient?\nSimulate 1000 inter-arrival times and compare the empirical distribution to the theoretical exponential distribution\n\n\n\nCode\n# Your code here\n\n\n\n\n\n\n\n\n\n\nExercise C.3: Central Limit Theorem Exploration\n\n\n\nConsider a highly skewed distribution: the chi-square distribution with 2 degrees of freedom.\n\nGenerate and plot 10,000 values from this distribution to visualize its shape\nNow repeatedly sample n=5 observations from this distribution, calculate their mean, and repeat 10,000 times. Plot the distribution of these means.\nRepeat part (b) with sample sizes of n=10, n=30, and n=50\nFor each sample size, calculate the mean and standard deviation of your sample means. Compare to the theoretical values predicted by the CLT.\nAt what sample size does the distribution of means look approximately normal?\n\n\n\nCode\n# Your code here\n\n\n\n\n\n\n\n\n\n\nExercise C.4: Q-Q Plots for Assessing Normality\n\n\n\nYou have collected the following measurements of protein concentration (mg/mL):\ndata &lt;- c(23.1, 24.5, 22.8, 25.3, 26.1, 24.9, 23.7, 25.8, 24.2, 26.5,\n          23.9, 25.1, 24.6, 23.4, 25.9, 24.3, 26.2, 23.6, 25.4, 24.8)\n\nCreate a histogram of this data\nCreate a Q-Q plot to assess normality\nPerform a Shapiro-Wilk test for normality (shapiro.test())\nBased on these assessments, does the normality assumption seem reasonable?\nNow add two outliers to the data (values of 30 and 19) and repeat parts (b) and (c). How do outliers affect the normality assessment?\n\n\n\nCode\n# Your code here\n\n\n\n\n\n\n\n\n\n\nExercise C.5: Comparing Uniform and Normal\n\n\n\n\nGenerate 1000 random values from a uniform distribution on the interval [0, 10]\nGenerate 1000 random values from a normal distribution with mean 5 and standard deviation chosen so that approximately 95% of values fall between 0 and 10\nCreate side-by-side histograms of both distributions\nCalculate and compare: the mean, median, standard deviation, and IQR for both distributions\nFor each distribution, what proportion of values fall within one standard deviation of the mean? How does this compare to the theoretical value for the normal distribution (68%)?\nExplain why the uniform distribution does or does not follow the 68-95-99.7 rule\n\n\n\nCode\n# Your code here",
    "crumbs": [
      "Probability and Distributions",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Continuous Probability Distributions</span>"
    ]
  },
  {
    "objectID": "chapters/09-continuous-distributions.html#additional-resources",
    "href": "chapters/09-continuous-distributions.html#additional-resources",
    "title": "13  Continuous Probability Distributions",
    "section": "13.9 Additional Resources",
    "text": "13.9 Additional Resources\n\nIrizarry (2019) - Excellent chapters on probability distributions and the Central Limit Theorem\nLogan (2010) - Comprehensive treatment of distributions in the context of biological statistics\n\n\n\n\n\n\n\nIrizarry, Rafael A. 2019. Introduction to Data Science. Self-published. https://rafalab.github.io/dsbook/.\n\n\nLogan, Murray. 2010. Biostatistical Design and Analysis Using r. Wiley-Blackwell.",
    "crumbs": [
      "Probability and Distributions",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Continuous Probability Distributions</span>"
    ]
  },
  {
    "objectID": "chapters/10-sampling-estimation.html",
    "href": "chapters/10-sampling-estimation.html",
    "title": "14  Sampling and Parameter Estimation",
    "section": "",
    "text": "14.1 The Problem of Inference\nScience often works by measuring samples to learn about populations. We cannot measure every protein in a cell, every patient with a disease, or every fish in the ocean. Instead, we take samples and use statistical inference to draw conclusions about the larger populations from which they came.\nThis creates a fundamental challenge: sample statistics vary from sample to sample, even when samples come from the same population. If you take two different random samples from a population and calculate their means, you will almost certainly get two different values. How, then, can we say anything reliable about the population?\nThe answer lies in understanding the sampling distribution—the distribution of a statistic across all possible samples of a given size.",
    "crumbs": [
      "Probability and Distributions",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Sampling and Parameter Estimation</span>"
    ]
  },
  {
    "objectID": "chapters/10-sampling-estimation.html#parameters-and-statistics",
    "href": "chapters/10-sampling-estimation.html#parameters-and-statistics",
    "title": "14  Sampling and Parameter Estimation",
    "section": "14.2 Parameters and Statistics",
    "text": "14.2 Parameters and Statistics\nA parameter is a numerical characteristic of a population—the true population mean \\(\\mu\\), the true population standard deviation \\(\\sigma\\), the true proportion \\(p\\). Parameters are typically fixed but unknown.\nA statistic is a numerical characteristic of a sample—the sample mean \\(\\bar{x}\\), the sample standard deviation \\(s\\), the sample proportion \\(\\hat{p}\\). Statistics are calculated from data and vary from sample to sample.\nWe use statistics to estimate parameters. The sample mean \\(\\bar{x}\\) estimates the population mean \\(\\mu\\). The sample standard deviation \\(s\\) estimates the population standard deviation \\(\\sigma\\). These estimates will rarely equal the true parameter values exactly, but we can quantify how close they are likely to be.",
    "crumbs": [
      "Probability and Distributions",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Sampling and Parameter Estimation</span>"
    ]
  },
  {
    "objectID": "chapters/10-sampling-estimation.html#point-estimates",
    "href": "chapters/10-sampling-estimation.html#point-estimates",
    "title": "14  Sampling and Parameter Estimation",
    "section": "14.3 Point Estimates",
    "text": "14.3 Point Estimates\nA point estimate is a single number used as our best guess for a parameter. The sample mean is a natural point estimate for the population mean:\n\\[\\bar{x} = \\frac{1}{n} \\sum_{i=1}^{n} x_i\\]\nWhat makes a good estimator? Ideally, an estimator should be:\nUnbiased: On average, across many samples, the estimator equals the true parameter. The sample mean is an unbiased estimator of the population mean.\nEfficient: Among unbiased estimators, it has the smallest variance. The sample mean is the most efficient estimator of a normal mean.\nConsistent: As sample size increases, the estimator converges to the true parameter value.",
    "crumbs": [
      "Probability and Distributions",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Sampling and Parameter Estimation</span>"
    ]
  },
  {
    "objectID": "chapters/10-sampling-estimation.html#the-sampling-distribution-of-the-mean",
    "href": "chapters/10-sampling-estimation.html#the-sampling-distribution-of-the-mean",
    "title": "14  Sampling and Parameter Estimation",
    "section": "14.4 The Sampling Distribution of the Mean",
    "text": "14.4 The Sampling Distribution of the Mean\nImagine drawing all possible samples of size \\(n\\) from a population and calculating the mean of each. The distribution of these means is the sampling distribution of the mean.\nThe sampling distribution has remarkable properties:\n\nIts mean equals the population mean: \\(E[\\bar{X}] = \\mu\\)\nIts standard deviation (the standard error) equals: \\(SE = \\frac{\\sigma}{\\sqrt{n}}\\)\nFor large samples, it is approximately normal (Central Limit Theorem)\n\n\n\nCode\n# Demonstrate sampling distribution\nset.seed(32)\n\n# Create a population\ntrue_pop &lt;- rpois(n = 10000, lambda = 3)\npop_mean &lt;- mean(true_pop)\npop_sd &lt;- sd(true_pop)\n\n# Take many samples and compute their means\nsample_sizes &lt;- c(5, 20, 50, 200)\npar(mfrow = c(2, 2))\n\nfor (n in sample_sizes) {\n  sample_means &lt;- replicate(1000, mean(sample(true_pop, n)))\n  hist(sample_means, breaks = 30, main = paste(\"n =\", n),\n       xlab = \"Sample Mean\", col = \"steelblue\",\n       xlim = c(1, 5))\n  abline(v = pop_mean, col = \"red\", lwd = 2)\n}\n\n\n\n\n\n\n\n\nFigure 14.1: Sampling distributions of the mean become narrower and more normal as sample size increases\n\n\n\n\n\nAs sample size increases, the sampling distribution becomes narrower (smaller standard error) and more normal in shape. This is why larger samples give more precise estimates.",
    "crumbs": [
      "Probability and Distributions",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Sampling and Parameter Estimation</span>"
    ]
  },
  {
    "objectID": "chapters/10-sampling-estimation.html#standard-error",
    "href": "chapters/10-sampling-estimation.html#standard-error",
    "title": "14  Sampling and Parameter Estimation",
    "section": "14.5 Standard Error",
    "text": "14.5 Standard Error\nThe standard error (SE) measures the variability of a statistic across samples. For the sample mean:\n\\[SE_{\\bar{x}} = \\frac{\\sigma}{\\sqrt{n}}\\]\nSince we usually do not know \\(\\sigma\\), we estimate the standard error using the sample standard deviation:\n\\[\\widehat{SE}_{\\bar{x}} = \\frac{s}{\\sqrt{n}}\\]\nThe standard error shrinks as sample size increases, but following a square root relationship. To halve the standard error, you need to quadruple the sample size.\n\n\nCode\n# Demonstrate how SE changes with sample size\nset.seed(32)\ntrue_pop &lt;- rpois(n = 1000, lambda = 5)\n\n# Sample size of 5\nsamps_5 &lt;- replicate(n = 50, sample(true_pop, size = 5))\nmeans_5 &lt;- apply(samps_5, 2, mean)\nse_5 &lt;- sd(means_5)\n\n# Sample size of 50\nsamps_50 &lt;- replicate(n = 50, sample(true_pop, size = 50))\nmeans_50 &lt;- apply(samps_50, 2, mean)\nse_50 &lt;- sd(means_50)\n\ncat(\"Standard error with n=5:\", round(se_5, 3), \"\\n\")\n\n\nStandard error with n=5: 0.919 \n\n\nCode\ncat(\"Standard error with n=50:\", round(se_50, 3), \"\\n\")\n\n\nStandard error with n=50: 0.305 \n\n\nCode\ncat(\"Ratio:\", round(se_5/se_50, 2), \"(theoretical: √10 =\", round(sqrt(10), 2), \")\\n\")\n\n\nRatio: 3.01 (theoretical: √10 = 3.16 )\n\n\n\n\n\n\n\n\nFigure 14.2: The standard error decreases as sample size increases",
    "crumbs": [
      "Probability and Distributions",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Sampling and Parameter Estimation</span>"
    ]
  },
  {
    "objectID": "chapters/10-sampling-estimation.html#confidence-intervals",
    "href": "chapters/10-sampling-estimation.html#confidence-intervals",
    "title": "14  Sampling and Parameter Estimation",
    "section": "14.6 Confidence Intervals",
    "text": "14.6 Confidence Intervals\nA point estimate tells us our best guess, but not how uncertain we are. A confidence interval provides a range of plausible values for the parameter along with a measure of confidence.\nA 95% confidence interval for the population mean, when the population is normally distributed or the sample is large, is:\n\\[\\bar{x} \\pm t_{\\alpha/2} \\times \\frac{s}{\\sqrt{n}}\\]\nwhere \\(t_{\\alpha/2}\\) is the critical value from the t-distribution with \\(n-1\\) degrees of freedom.\n\n\n\n\n\n\nFigure 14.3: Confidence intervals provide a range of plausible values for the parameter\n\n\n\nThe interpretation requires care: a 95% confidence interval means that if we repeated this procedure many times, 95% of the resulting intervals would contain the true parameter. Any particular interval either does or does not contain the true value—we just don’t know which.\n\n\nCode\n# Calculate a confidence interval\nset.seed(42)\nsample_data &lt;- rnorm(30, mean = 100, sd = 15)\n\nsample_mean &lt;- mean(sample_data)\nsample_se &lt;- sd(sample_data) / sqrt(length(sample_data))\nt_crit &lt;- qt(0.975, df = length(sample_data) - 1)\n\nlower &lt;- sample_mean - t_crit * sample_se\nupper &lt;- sample_mean + t_crit * sample_se\n\ncat(\"Sample mean:\", round(sample_mean, 2), \"\\n\")\n\n\nSample mean: 101.03 \n\n\nCode\ncat(\"95% CI: [\", round(lower, 2), \",\", round(upper, 2), \"]\\n\")\n\n\n95% CI: [ 94 , 108.06 ]\n\n\nCode\n# Or use t.test directly\nt.test(sample_data)$conf.int\n\n\n[1]  93.99927 108.05833\nattr(,\"conf.level\")\n[1] 0.95\n\n\n\n\n\n\n\n\nFigure 14.4: The interpretation of confidence intervals requires understanding what is random",
    "crumbs": [
      "Probability and Distributions",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Sampling and Parameter Estimation</span>"
    ]
  },
  {
    "objectID": "chapters/10-sampling-estimation.html#the-central-limit-theorem-in-practice",
    "href": "chapters/10-sampling-estimation.html#the-central-limit-theorem-in-practice",
    "title": "14  Sampling and Parameter Estimation",
    "section": "14.7 The Central Limit Theorem in Practice",
    "text": "14.7 The Central Limit Theorem in Practice\nThe Central Limit Theorem (CLT) is one of the most remarkable results in statistics. It states that the sampling distribution of the mean approaches a normal distribution as sample size increases, regardless of the shape of the underlying population distribution.\nThis is why confidence intervals work. This is why the normal distribution appears everywhere in statistics. And understanding the CLT in practice—through examples like polling and surveys—builds deep intuition for statistical inference.\n\nThe CLT Demonstrated\nLet’s see the CLT in action with a highly non-normal distribution. We’ll sample from an exponential distribution (heavily right-skewed) and watch how the sampling distribution of the mean becomes normal:\n\n\nCode\nset.seed(42)\n\n# Exponential distribution (very skewed)\nrate &lt;- 0.5\npop_mean &lt;- 1/rate  # True mean = 2\n\npar(mfrow = c(2, 2))\n\n# Original population\nx &lt;- rexp(10000, rate = rate)\nhist(x, breaks = 50, main = \"Original Distribution (Exponential)\",\n     xlab = \"Value\", col = \"gray80\", probability = TRUE)\ncurve(dexp(x, rate = rate), add = TRUE, col = \"red\", lwd = 2)\n\n# Sample means for different n\nfor (n in c(5, 30, 100)) {\n  sample_means &lt;- replicate(5000, mean(rexp(n, rate = rate)))\n  hist(sample_means, breaks = 50, probability = TRUE,\n       main = paste(\"Sampling Distribution (n =\", n, \")\"),\n       xlab = \"Sample Mean\", col = \"steelblue\")\n\n  # Overlay normal curve with theoretical SE\n  theoretical_se &lt;- (1/rate) / sqrt(n)\n  curve(dnorm(x, mean = pop_mean, sd = theoretical_se),\n        add = TRUE, col = \"red\", lwd = 2)\n}\n\n\n\n\n\n\n\n\nFigure 14.5: The Central Limit Theorem in action: sampling distributions of means from a skewed exponential population become normal\n\n\n\n\n\nEven with n = 30, the sampling distribution of the mean is remarkably close to normal, despite the exponential distribution being extremely skewed. By n = 100, the approximation is nearly perfect.\n\n\nMargin of Error: Quantifying Precision\nThe margin of error (MOE) is a practical expression of sampling uncertainty. For a confidence interval around a proportion or mean, the margin of error is the half-width of the interval:\n\\[\\text{Margin of Error} = z^* \\times SE\\]\nFor a 95% confidence interval, \\(z^* \\approx 1.96 \\approx 2\\), giving us the approximation:\n\\[\\text{95% MOE} \\approx 2 \\times SE\\]\nFor proportions, the standard error is \\(SE = \\sqrt{\\frac{p(1-p)}{n}}\\), so:\n\\[\\text{95% MOE} \\approx 2 \\times \\sqrt{\\frac{p(1-p)}{n}}\\]\nThe maximum standard error for a proportion occurs when \\(p = 0.5\\), giving the conservative formula used in sample size planning:\n\\[\\text{Maximum 95% MOE} \\approx \\frac{1}{\\sqrt{n}}\\]\n\n\nPolling as a Statistical Inference Problem\nPolitical polling provides an excellent practical example of sampling and the CLT. When a poll reports “48% of voters support Candidate A, with a margin of error of ±3%,” they are making a statistical inference from a sample to a population.\nLet’s simulate an election poll:\n\n\nCode\nset.seed(123)\n\n# True population proportion (unknown in real life)\ntrue_proportion &lt;- 0.52  # 52% actually support the candidate\n\n# Simulate a poll of 1000 likely voters\nn &lt;- 1000\npoll_sample &lt;- rbinom(1, size = n, prob = true_proportion) / n\n\n# Calculate margin of error\nse &lt;- sqrt(poll_sample * (1 - poll_sample) / n)\nmoe &lt;- 1.96 * se\n\ncat(\"Poll result:\", round(poll_sample * 100, 1), \"%\\n\")\n\n\nPoll result: 53.2 %\n\n\nCode\ncat(\"Margin of error: ±\", round(moe * 100, 1), \"%\\n\")\n\n\nMargin of error: ± 3.1 %\n\n\nCode\ncat(\"95% CI: [\", round((poll_sample - moe) * 100, 1), \"%, \",\n    round((poll_sample + moe) * 100, 1), \"%]\\n\")\n\n\n95% CI: [ 50.1 %,  56.3 %]\n\n\nCode\ncat(\"True proportion:\", true_proportion * 100, \"%\\n\")\n\n\nTrue proportion: 52 %\n\n\n\n\nHow Sample Size Affects Margin of Error\nThe margin of error is inversely proportional to \\(\\sqrt{n}\\). This has important practical implications:\n\n\nCode\n# Margin of error as a function of sample size\nsample_sizes &lt;- c(100, 400, 1000, 2000, 4000, 10000)\np &lt;- 0.5  # Conservative estimate\n\nmoe_values &lt;- 1.96 * sqrt(p * (1 - p) / sample_sizes) * 100\n\nplot(sample_sizes, moe_values, type = \"b\", pch = 19,\n     xlab = \"Sample Size\", ylab = \"Margin of Error (%)\",\n     main = \"95% Margin of Error vs. Sample Size\",\n     log = \"x\", col = \"steelblue\", lwd = 2)\ngrid()\n\n# Add labels\ntext(sample_sizes, moe_values + 0.5,\n     paste0(\"±\", round(moe_values, 1), \"%\"),\n     cex = 0.8)\n\n\n\n\n\n\n\n\nFigure 14.6: Margin of error decreases with sample size, but with diminishing returns\n\n\n\n\n\nNotice the diminishing returns: going from n = 100 to n = 400 cuts the margin of error in half (from ±10% to ±5%). But to halve it again (to ±2.5%), you need n = 1,600—four times as many respondents. This is why most national polls use samples of 1,000–2,000: it provides reasonable precision (±2-3%) at manageable cost.\n\n\nInterpreting Polls: Uncertainty in Action\nWhen two candidates are within the margin of error of each other, the poll results are consistent with either candidate leading. This is not a flaw of polling—it reflects genuine uncertainty.\n\n\nCode\nset.seed(456)\n\n# Simulate many polls from the same population\ntrue_p &lt;- 0.48  # Candidate A's true support\nn_poll &lt;- 1000\nn_polls &lt;- 100\n\npolls &lt;- replicate(n_polls, {\n  sample_p &lt;- rbinom(1, n_poll, true_p) / n_poll\n  se &lt;- sqrt(sample_p * (1 - sample_p) / n_poll)\n  c(estimate = sample_p, lower = sample_p - 1.96*se, upper = sample_p + 1.96*se)\n})\n\npolls_df &lt;- data.frame(\n  poll = 1:n_polls,\n  estimate = polls[\"estimate\", ],\n  lower = polls[\"lower\", ],\n  upper = polls[\"upper\", ]\n)\n\n# How many CIs contain the true value?\ncoverage &lt;- mean(polls_df$lower &lt;= true_p & polls_df$upper &gt;= true_p)\n\n# Plot first 30 polls\nggplot(polls_df[1:30, ], aes(x = poll, y = estimate)) +\n  geom_hline(yintercept = true_p, color = \"red\", linetype = \"dashed\", linewidth = 1) +\n  geom_hline(yintercept = 0.5, color = \"gray50\", linetype = \"dotted\") +\n  geom_errorbar(aes(ymin = lower, ymax = upper), width = 0.3, alpha = 0.6) +\n  geom_point(size = 2) +\n  labs(\n    title = \"95% Confidence Intervals from 30 Polls\",\n    subtitle = paste0(\"True proportion = \", true_p*100, \"% | \", round(coverage*100),\n                     \"% of all CIs contain true value\"),\n    x = \"Poll Number\",\n    y = \"Estimated Proportion\"\n  ) +\n  scale_y_continuous(labels = scales::percent_format()) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nFigure 14.7: Simulated polls showing how confidence intervals vary: about 95% contain the true proportion\n\n\n\n\n\nIn repeated sampling, approximately 95% of confidence intervals contain the true value—but any single interval either does or does not contain it. This is the frequentist interpretation of confidence intervals.\n\n\nCorrect Language for Confidence Intervals\nConfidence intervals are frequently misinterpreted. The key insight is understanding what is random before and after data collection:\n\n\n\n\n\n\nWhat Is Random?\n\n\n\nBefore data collection: The confidence interval is random—we don’t yet know where it will fall. The population parameter \\(\\mu\\) is fixed (though unknown).\nAfter data collection: The confidence interval is now fixed—it’s just two numbers like [48.2, 52.1]. It either contains the true parameter or it doesn’t. There is no probability involved anymore.\n\n\nConsider this analogy: Before flipping a coin, there’s a 50% probability of heads. After flipping, the coin shows what it shows—there’s no longer any probability, just an outcome.\nThe correct interpretation of a 95% confidence interval is:\n\n“If we repeated this sampling procedure many times, 95% of the resulting intervals would contain the true population parameter.”\n\nIncorrect interpretations (which are very common):\n\n❌ “There is a 95% probability that \\(\\mu\\) is in this interval”\n❌ “We are 95% confident that \\(\\mu\\) is between these values”\n❌ “95% of the data falls within this interval”\n\nThe first two statements incorrectly assign probability to the parameter, which is fixed. The third confuses a confidence interval for a reference range.\n\n\nCode\n# Visual demonstration: Monte Carlo simulation of CI coverage\nset.seed(2024)\n\n# True population parameters\ntrue_mean &lt;- 50\ntrue_sd &lt;- 10\nn &lt;- 25  # Sample size\nn_simulations &lt;- 100\n\n# Storage for confidence intervals\nci_data &lt;- data.frame(\n  sim = 1:n_simulations,\n  lower = numeric(n_simulations),\n  upper = numeric(n_simulations),\n  contains_true = logical(n_simulations)\n)\n\n# Generate 100 confidence intervals\nfor (i in 1:n_simulations) {\n  sample_data &lt;- rnorm(n, mean = true_mean, sd = true_sd)\n  ci &lt;- t.test(sample_data)$conf.int\n  ci_data$lower[i] &lt;- ci[1]\n  ci_data$upper[i] &lt;- ci[2]\n  ci_data$contains_true[i] &lt;- (ci[1] &lt;= true_mean) & (ci[2] &gt;= true_mean)\n}\n\n# Count how many contain the true mean\ncoverage &lt;- mean(ci_data$contains_true)\n\n# Plot\nplot(NULL, xlim = c(42, 58), ylim = c(0, n_simulations + 1),\n     xlab = \"Value\", ylab = \"Simulation Number\",\n     main = paste0(\"100 Confidence Intervals (\", round(coverage*100), \"% contain true mean)\"))\n\n# Draw intervals\nfor (i in 1:n_simulations) {\n  color &lt;- ifelse(ci_data$contains_true[i], \"steelblue\", \"red\")\n  segments(ci_data$lower[i], i, ci_data$upper[i], i, col = color, lwd = 1.5)\n}\n\n# True mean line\nabline(v = true_mean, col = \"darkgreen\", lwd = 2, lty = 2)\nlegend(\"topright\",\n       legend = c(\"Contains true mean\", \"Misses true mean\", \"True mean (μ = 50)\"),\n       col = c(\"steelblue\", \"red\", \"darkgreen\"),\n       lwd = c(2, 2, 2), lty = c(1, 1, 2))\n\n\n\n\n\n\n\n\nFigure 14.8: Monte Carlo demonstration of confidence interval coverage: about 95% of intervals (blue) contain the true mean\n\n\n\n\n\nIn this simulation, each horizontal line represents a 95% confidence interval from a different random sample. The red intervals are the ~5% that failed to capture the true mean. Once calculated, each interval either contains the true value (blue) or doesn’t (red)—there’s no probability about it anymore.\n\n\nThe Practical Value of Margin of Error\nUnderstanding margin of error helps you:\n\nInterpret reported results: A poll showing 52% vs. 48% with ±3% MOE does not clearly favor either candidate\nPlan studies: Use \\(n \\approx \\frac{1}{\\text{MOE}^2}\\) for proportions near 0.5 to achieve a desired margin of error\nCommunicate uncertainty: Always report intervals, not just point estimates\nMake decisions: Consider whether differences are within or beyond the margin of error\n\nFor sample size planning, if you want a specific margin of error:\n\n\nCode\n# Required sample size for different margins of error (95% CI)\ndesired_moe &lt;- c(0.10, 0.05, 0.03, 0.02, 0.01)\nrequired_n &lt;- ceiling((1.96 / desired_moe)^2 * 0.25)  # 0.25 = p(1-p) at p=0.5\n\ndata.frame(\n  `Margin of Error` = paste0(\"±\", desired_moe * 100, \"%\"),\n  `Required n` = required_n\n)\n\n\n  Margin.of.Error Required.n\n1            ±10%         97\n2             ±5%        385\n3             ±3%       1068\n4             ±2%       2401\n5             ±1%       9604\n\n\nThese calculations assume simple random sampling. Real-world surveys often use complex sampling designs that affect the effective sample size.",
    "crumbs": [
      "Probability and Distributions",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Sampling and Parameter Estimation</span>"
    ]
  },
  {
    "objectID": "chapters/10-sampling-estimation.html#coefficient-of-variation",
    "href": "chapters/10-sampling-estimation.html#coefficient-of-variation",
    "title": "14  Sampling and Parameter Estimation",
    "section": "14.8 Coefficient of Variation",
    "text": "14.8 Coefficient of Variation\nWhen comparing variability across groups with different means, the standard deviation alone can be misleading. The coefficient of variation (CV) standardizes variability relative to the mean:\n\\[CV = \\frac{s}{\\bar{x}} \\times 100\\%\\]\nA CV of 10% means the standard deviation is 10% of the mean. This allows meaningful comparisons between groups or measurements on different scales.",
    "crumbs": [
      "Probability and Distributions",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Sampling and Parameter Estimation</span>"
    ]
  },
  {
    "objectID": "chapters/10-sampling-estimation.html#percentiles-and-quantiles",
    "href": "chapters/10-sampling-estimation.html#percentiles-and-quantiles",
    "title": "14  Sampling and Parameter Estimation",
    "section": "14.9 Percentiles and Quantiles",
    "text": "14.9 Percentiles and Quantiles\nPercentiles describe the relative position of values within a distribution. The \\(p\\)th percentile is the value below which \\(p\\)% of the data falls. The 50th percentile is the median, the 25th percentile is the first quartile, and the 75th percentile is the third quartile.\nQuantiles divide data into equal parts. Quartiles divide into four parts, deciles into ten parts, percentiles into one hundred parts.\n\n\nCode\n# Calculate percentiles\ndata &lt;- c(12, 15, 18, 22, 25, 28, 32, 35, 40, 45)\n\nquantile(data, probs = c(0.25, 0.5, 0.75))\n\n\n  25%   50%   75% \n19.00 26.50 34.25 \n\n\nCode\nsummary(data)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  12.00   19.00   26.50   27.20   34.25   45.00 \n\n\nQuantiles form the basis for many statistical procedures, including constructing confidence intervals and calculating p-values.",
    "crumbs": [
      "Probability and Distributions",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Sampling and Parameter Estimation</span>"
    ]
  },
  {
    "objectID": "chapters/10-sampling-estimation.html#bias-and-variability",
    "href": "chapters/10-sampling-estimation.html#bias-and-variability",
    "title": "14  Sampling and Parameter Estimation",
    "section": "14.10 Bias and Variability",
    "text": "14.10 Bias and Variability\nTwo distinct types of error affect estimates:\nBias is systematic error—the tendency for an estimator to consistently over- or underestimate the true parameter. An unbiased estimator has zero bias: its average value across all possible samples equals the true parameter.\nVariability is random error—the spread of estimates around their average value. Low variability means estimates cluster tightly together.\nThe ideal estimator has both low bias and low variability. Sometimes there is a tradeoff: a slightly biased estimator might have much lower variability, resulting in estimates that are closer to the truth on average.\nThe mean squared error (MSE) combines both sources of error:\n\\[MSE = Bias^2 + Variance\\]",
    "crumbs": [
      "Probability and Distributions",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Sampling and Parameter Estimation</span>"
    ]
  },
  {
    "objectID": "chapters/10-sampling-estimation.html#the-bootstrap-resampling-for-estimation",
    "href": "chapters/10-sampling-estimation.html#the-bootstrap-resampling-for-estimation",
    "title": "14  Sampling and Parameter Estimation",
    "section": "14.11 The Bootstrap: Resampling for Estimation",
    "text": "14.11 The Bootstrap: Resampling for Estimation\nThe bootstrap, introduced by Bradley Efron in 1979, is a powerful resampling method for estimating standard errors and constructing confidence intervals when analytical formulas are unavailable or assumptions are questionable (Efron 1979).\nThe key insight is elegant: we can estimate the sampling distribution of a statistic by repeatedly resampling from our observed data. If our sample is representative of the population, then samples drawn from our sample (with replacement) should behave like samples drawn from the population.\n\nThe Bootstrap Algorithm\n\nTake a random sample with replacement from your data (same size as original)\nCalculate the statistic of interest on this resampled data\nRepeat steps 1-2 many times (typically 1000-10000)\nThe distribution of bootstrap statistics approximates the sampling distribution\n\n\n\nCode\n# Bootstrap estimation of the sampling distribution of the mean\nset.seed(123)\n\n# Our observed sample\noriginal_sample &lt;- c(23, 31, 28, 35, 42, 29, 33, 27, 38, 31)\nn &lt;- length(original_sample)\n\n# Bootstrap: resample with replacement\nn_bootstrap &lt;- 5000\nbootstrap_means &lt;- numeric(n_bootstrap)\n\nfor (i in 1:n_bootstrap) {\n  boot_sample &lt;- sample(original_sample, size = n, replace = TRUE)\n  bootstrap_means[i] &lt;- mean(boot_sample)\n}\n\n# Compare bootstrap distribution to observed statistics\nhist(bootstrap_means, breaks = 40, col = \"steelblue\",\n     main = \"Bootstrap Distribution of the Mean\",\n     xlab = \"Sample Mean\")\nabline(v = mean(original_sample), col = \"red\", lwd = 2)\nabline(v = quantile(bootstrap_means, c(0.025, 0.975)), col = \"darkgreen\", lwd = 2, lty = 2)\n\ncat(\"Original sample mean:\", mean(original_sample), \"\\n\")\n\n\nOriginal sample mean: 31.7 \n\n\nCode\ncat(\"Bootstrap SE:\", sd(bootstrap_means), \"\\n\")\n\n\nBootstrap SE: 1.635207 \n\n\nCode\ncat(\"95% Bootstrap CI:\", quantile(bootstrap_means, c(0.025, 0.975)), \"\\n\")\n\n\n95% Bootstrap CI: 28.7 35.0025 \n\n\n\n\n\n\n\n\nFigure 14.9: Bootstrap distribution of the sample mean with 95% confidence interval bounds\n\n\n\n\n\n\n\nBootstrap Confidence Intervals\nThe bootstrap provides several methods for constructing confidence intervals:\nPercentile method: Use the 2.5th and 97.5th percentiles of the bootstrap distribution as the 95% CI bounds. This is the simplest approach shown above.\nBasic (reverse percentile) method: Reflects the percentiles around the original estimate to correct for certain types of bias.\nBCa (bias-corrected and accelerated): A more sophisticated method that adjusts for both bias and skewness in the bootstrap distribution. This is often preferred when the sampling distribution is not symmetric.\n\n\nCode\n# Using the boot package for more sophisticated bootstrap CI\nlibrary(boot)\n\n# Define statistic function\nmean_stat &lt;- function(data, indices) {\n  mean(data[indices])\n}\n\n# Run bootstrap\nboot_result &lt;- boot(original_sample, mean_stat, R = 5000)\n\n# Different CI methods\nboot.ci(boot_result, type = c(\"perc\", \"basic\", \"bca\"))\n\n\nBOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS\nBased on 5000 bootstrap replicates\n\nCALL : \nboot.ci(boot.out = boot_result, type = c(\"perc\", \"basic\", \"bca\"))\n\nIntervals : \nLevel      Basic              Percentile            BCa          \n95%   (28.3, 34.9 )   (28.5, 35.1 )   (28.5, 35.1 )  \nCalculations and Intervals on Original Scale\n\n\n\n\nWhen to Use the Bootstrap\nThe bootstrap is particularly valuable when:\n\nThe sampling distribution of your statistic is unknown or complex\nSample sizes are small and normality assumptions are questionable\nYou are estimating something other than a mean (e.g., median, correlation, regression coefficients)\nAnalytical formulas for standard errors do not exist\n\nHowever, the bootstrap has limitations. It assumes your sample is representative of the population and works poorly with very small samples (n &lt; 10-15) or when estimating extreme quantiles.",
    "crumbs": [
      "Probability and Distributions",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Sampling and Parameter Estimation</span>"
    ]
  },
  {
    "objectID": "chapters/10-sampling-estimation.html#maximum-likelihood-estimation",
    "href": "chapters/10-sampling-estimation.html#maximum-likelihood-estimation",
    "title": "14  Sampling and Parameter Estimation",
    "section": "14.12 Maximum Likelihood Estimation",
    "text": "14.12 Maximum Likelihood Estimation\nMaximum likelihood estimation (MLE) provides a principled framework for parameter estimation. The likelihood function measures how probable the observed data would be for different parameter values. MLE finds the parameter values that make the observed data most probable.\nFor a sample \\(x_1, x_2, \\ldots, x_n\\) from a distribution with parameter \\(\\theta\\), the likelihood function is:\n\\[L(\\theta | x_1, \\ldots, x_n) = \\prod_{i=1}^{n} f(x_i | \\theta)\\]\nwhere \\(f(x_i | \\theta)\\) is the probability density (or mass) function. We typically work with the log-likelihood for computational convenience:\n\\[\\ell(\\theta) = \\sum_{i=1}^{n} \\log f(x_i | \\theta)\\]\nThe MLE \\(\\hat{\\theta}\\) is the value that maximizes \\(\\ell(\\theta)\\).\n\n\nCode\n# MLE example: estimating the rate parameter of an exponential distribution\nset.seed(42)\nexp_data &lt;- rexp(100, rate = 0.5)  # True rate is 0.5\n\n# Log-likelihood function for exponential\nlog_likelihood &lt;- function(rate, data) {\n  sum(dexp(data, rate = rate, log = TRUE))\n}\n\n# Find MLE\nrates &lt;- seq(0.1, 1, by = 0.01)\nll_values &lt;- sapply(rates, log_likelihood, data = exp_data)\n\n# Plot likelihood surface\nplot(rates, ll_values, type = \"l\", lwd = 2,\n     xlab = \"Rate parameter\", ylab = \"Log-likelihood\",\n     main = \"Log-likelihood for Exponential Rate\")\nabline(v = 1/mean(exp_data), col = \"red\", lwd = 2)  # MLE = 1/mean for exponential\n\n\n\n\n\n\n\n\n\nCode\ncat(\"True rate: 0.5\\n\")\n\n\nTrue rate: 0.5\n\n\nCode\ncat(\"MLE estimate:\", round(1/mean(exp_data), 3), \"\\n\")\n\n\nMLE estimate: 0.445 \n\n\nMLEs have desirable properties: they are consistent (converge to true values as n increases) and asymptotically efficient (achieve the smallest possible variance for large samples).",
    "crumbs": [
      "Probability and Distributions",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Sampling and Parameter Estimation</span>"
    ]
  },
  {
    "objectID": "chapters/10-sampling-estimation.html#simulation-based-understanding-of-estimation",
    "href": "chapters/10-sampling-estimation.html#simulation-based-understanding-of-estimation",
    "title": "14  Sampling and Parameter Estimation",
    "section": "14.13 Simulation-Based Understanding of Estimation",
    "text": "14.13 Simulation-Based Understanding of Estimation\nSimulation provides powerful intuition for statistical concepts. By repeatedly sampling from known populations, we can directly observe sampling distributions.\n\n\nCode\n# Exploring properties of estimators through simulation\nset.seed(456)\n\n# True population parameters\npop_mean &lt;- 50\npop_sd &lt;- 10\n\n# Simulate many samples and compute estimates\nn_samples &lt;- 2000\nsample_sizes &lt;- c(5, 15, 50)\n\npar(mfrow = c(2, 3))\n\nfor (n in sample_sizes) {\n  # Collect sample means\n  sample_means &lt;- replicate(n_samples, {\n    samp &lt;- rnorm(n, mean = pop_mean, sd = pop_sd)\n    mean(samp)\n  })\n\n  # Plot distribution of sample means\n  hist(sample_means, breaks = 40, col = \"steelblue\",\n       main = paste(\"Sample Means (n =\", n, \")\"),\n       xlab = \"Sample Mean\", xlim = c(35, 65))\n  abline(v = pop_mean, col = \"red\", lwd = 2)\n\n  # Calculate actual SE vs theoretical\n  actual_se &lt;- sd(sample_means)\n  theoretical_se &lt;- pop_sd / sqrt(n)\n\n  legend(\"topright\", bty = \"n\", cex = 0.8,\n         legend = c(paste(\"Actual SE:\", round(actual_se, 2)),\n                    paste(\"Theoretical:\", round(theoretical_se, 2))))\n}\n\n# Now do the same for sample standard deviations\nfor (n in sample_sizes) {\n  # Collect sample SDs\n  sample_sds &lt;- replicate(n_samples, {\n    samp &lt;- rnorm(n, mean = pop_mean, sd = pop_sd)\n    sd(samp)\n  })\n\n  hist(sample_sds, breaks = 40, col = \"coral\",\n       main = paste(\"Sample SDs (n =\", n, \")\"),\n       xlab = \"Sample SD\")\n  abline(v = pop_sd, col = \"red\", lwd = 2)\n}\n\n\n\n\n\n\n\n\nFigure 14.10: Properties of estimators through simulation: sample means are unbiased and their variability decreases with n\n\n\n\n\n\nThis simulation reveals several important facts: 1. Sample means are unbiased (centered on the population mean) 2. The spread of sample means decreases as \\(\\sqrt{n}\\) 3. Sample standard deviations are slightly biased for small samples but become unbiased as n increases",
    "crumbs": [
      "Probability and Distributions",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Sampling and Parameter Estimation</span>"
    ]
  },
  {
    "objectID": "chapters/10-sampling-estimation.html#principles-of-experimental-design",
    "href": "chapters/10-sampling-estimation.html#principles-of-experimental-design",
    "title": "14  Sampling and Parameter Estimation",
    "section": "14.14 Principles of Experimental Design",
    "text": "14.14 Principles of Experimental Design\nGood statistical analysis cannot rescue a poorly designed study. The way you collect data fundamentally determines what conclusions you can draw. Understanding key design principles is essential for planning experiments that yield valid, interpretable results.\n\nRandomization\nRandomization assigns subjects to treatment groups by chance, ensuring that treatment groups are comparable. Without randomization, systematic differences between groups (confounders) can bias results.\n\n\nCode\n# Randomly assign 20 subjects to treatment or control\nset.seed(42)\nsubjects &lt;- 1:20\ntreatment_group &lt;- sample(subjects, size = 10)\ncontrol_group &lt;- setdiff(subjects, treatment_group)\n\ncat(\"Treatment group:\", treatment_group, \"\\n\")\n\n\nTreatment group: 17 5 1 10 4 2 20 18 8 7 \n\n\nCode\ncat(\"Control group:\", control_group, \"\\n\")\n\n\nControl group: 3 6 9 11 12 13 14 15 16 19 \n\n\nRandomization provides the foundation for causal inference. Without it, we can only establish associations, not causation.\n\n\nControls\nEvery experiment needs controls—groups that differ from treatment groups only in the variable of interest:\n\nNegative control: Receives no treatment; establishes baseline\nPositive control: Receives a treatment known to work; confirms the experimental system is functioning\nProcedural control: Receives all procedures except the active ingredient (e.g., sham surgery, vehicle-only injection)\n\nWithout proper controls, you cannot determine whether observed effects are due to your treatment or some other factor.\n\n\nBlinding\nBlinding prevents knowledge of group assignment from influencing results:\n\nSingle-blind: Subjects do not know which treatment they receive\nDouble-blind: Neither subjects nor experimenters know group assignments\nTriple-blind: Subjects, experimenters, and data analysts are all blinded\n\nBlinding prevents both placebo effects (subjects’ expectations influencing outcomes) and experimenter bias (conscious or unconscious influence on measurements).\n\n\nReplication\nReplication means having multiple independent observations in each treatment group. Replication is essential because it:\n\nProvides estimates of variability\nEnables statistical inference\nIncreases precision of estimates\nAllows detection of real effects\n\nThe unit of replication must match the unit of treatment. If you treat tanks with different water temperatures and measure multiple fish per tank, your replicates are tanks, not fish.\n\n\n\n\n\n\nTechnical vs. Biological Replicates\n\n\n\n\nTechnical replicates: Repeated measurements of the same sample (e.g., running the same sample through a machine twice)\nBiological replicates: Independent samples from different individuals or experimental units\n\nTechnical replicates measure precision of the measurement process. Biological replicates measure biological variability and enable inference to the population. Do not confuse them!\n\n\n\n\nBlocking\nBlocking groups experimental units that are similar to each other, then applies all treatments within each block. This reduces variability by accounting for known sources of heterogeneity.\nCommon blocking factors: - Time (experimental batches, days) - Location (different incubators, growth chambers) - Individual (paired designs, repeated measures)\n\n\nCode\n# Randomized complete block design\n# 4 treatments applied within each of 3 blocks\nset.seed(123)\nblocks &lt;- 1:3\ntreatments &lt;- c(\"A\", \"B\", \"C\", \"D\")\n\ndesign &lt;- expand.grid(Block = blocks, Treatment = treatments)\ndesign$Order &lt;- NA\n\nfor (b in blocks) {\n  block_rows &lt;- design$Block == b\n  design$Order[block_rows] &lt;- sample(1:4)\n}\n\ndesign[order(design$Block, design$Order), ]\n\n\n   Block Treatment Order\n7      1         C     1\n10     1         D     2\n1      1         A     3\n4      1         B     4\n11     2         D     1\n5      2         B     2\n2      2         A     3\n8      2         C     4\n6      3         B     1\n9      3         C     2\n3      3         A     3\n12     3         D     4\n\n\nBlocking is particularly valuable when blocks correspond to major sources of variation (e.g., different labs, experimental days, genetic backgrounds).\n\n\nSample Size Considerations\nDetermining appropriate sample size before collecting data is crucial. Too few samples waste resources by producing inconclusive results; too many waste resources by studying effects that were detectable with smaller samples.\nSample size depends on: - Effect size: The minimum meaningful difference you want to detect - Variability: The expected noise in your measurements - Significance level (\\(\\alpha\\)): Usually 0.05 - Power: Usually 0.80 (80% chance of detecting a real effect)\nPower analysis (covered in detail in a later chapter) formalizes these considerations.",
    "crumbs": [
      "Probability and Distributions",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Sampling and Parameter Estimation</span>"
    ]
  },
  {
    "objectID": "chapters/10-sampling-estimation.html#key-takeaways",
    "href": "chapters/10-sampling-estimation.html#key-takeaways",
    "title": "14  Sampling and Parameter Estimation",
    "section": "14.15 Key Takeaways",
    "text": "14.15 Key Takeaways\nUnderstanding sampling distributions and estimation is fundamental to statistical inference. Key points to remember:\n\nStatistics vary from sample to sample; this variability is quantified by the standard error\nLarger samples give more precise estimates (smaller standard errors)\nConfidence intervals quantify uncertainty about parameter estimates\nThe Central Limit Theorem explains why the normal distribution appears so frequently\nBoth bias and variability affect the quality of estimates\nThe bootstrap provides a flexible, computer-intensive approach to estimation when analytical methods are limited\nMaximum likelihood provides a principled framework for parameter estimation\nGood experimental design—randomization, controls, blinding, proper replication—is essential for valid inference\n\nThese concepts provide the foundation for hypothesis testing and the statistical inference methods we develop in subsequent chapters.",
    "crumbs": [
      "Probability and Distributions",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Sampling and Parameter Estimation</span>"
    ]
  },
  {
    "objectID": "chapters/10-sampling-estimation.html#exercises",
    "href": "chapters/10-sampling-estimation.html#exercises",
    "title": "14  Sampling and Parameter Estimation",
    "section": "14.16 Exercises",
    "text": "14.16 Exercises\n\n\n\n\n\n\nExercise S.1: Standard Error and Sample Size\n\n\n\nYou are measuring the concentration of a specific protein in blood samples. From previous studies, you know the population standard deviation is approximately 15 mg/dL.\n\nIf you collect 25 samples, what is the standard error of the mean?\nHow many samples would you need to reduce the standard error to 2 mg/dL?\nIf you want to halve the standard error from part (a), how many times larger must your sample be?\nSimulate this scenario: create 1000 samples of size n=25 from a normal distribution with mean 100 and SD 15, calculate the mean of each sample, and verify that the standard deviation of these means is approximately equal to the theoretical standard error\n\n\n\nCode\n# Your code here\n\n\n\n\n\n\n\n\n\n\nExercise S.2: Confidence Intervals and Interpretation\n\n\n\nA researcher measures the wing length (mm) of 20 monarch butterflies and obtains:\nwing_lengths &lt;- c(48.2, 50.1, 49.3, 51.2, 47.8, 50.5, 49.1, 48.9,\n                  51.0, 49.7, 50.3, 48.5, 49.9, 50.8, 49.4,\n                  48.7, 50.2, 49.5, 51.1, 49.0)\n\nCalculate the 95% confidence interval for the mean wing length\nInterpret this confidence interval in words (use correct statistical language)\nIf you were to repeat this study 100 times, approximately how many of the resulting 95% confidence intervals would you expect to contain the true population mean?\nCalculate a 99% confidence interval. How does it compare to the 95% CI and why?\n\n\n\nCode\n# Your code here\n\n\n\n\n\n\n\n\n\n\nExercise S.3: Bootstrap Confidence Intervals\n\n\n\nConsider the following reaction times (milliseconds) for a cognitive task:\nreaction_times &lt;- c(245, 289, 267, 301, 278, 256, 312, 298,\n                    271, 288, 305, 281, 293, 266, 310)\n\nCalculate the median reaction time and a 95% confidence interval using the t-test approach (treating median as mean for this comparison)\nUse the bootstrap (with at least 5000 resamples) to construct a 95% confidence interval for the median\nCompare the two intervals. Which method is more appropriate for estimating the median, and why?\nCreate a histogram of your bootstrap distribution of medians\nUse the bootstrap to estimate the standard error of the median\n\n\n\nCode\n# Your code here\n\n\n\n\n\n\n\n\n\n\nExercise S.4: Polling and Margin of Error\n\n\n\nYou are conducting a survey to estimate the proportion of biology students who plan to attend graduate school. In your random sample of 400 students, 256 indicate they plan to attend graduate school.\n\nCalculate the sample proportion and its standard error\nConstruct a 95% confidence interval for the true proportion\nCalculate the margin of error for your estimate\nA colleague claims that the true proportion is 70%. Is this claim consistent with your data? Explain.\nHow large a sample would you need to achieve a margin of error of ±2% (at 95% confidence)?\n\n\n\nCode\n# Your code here\n\n\n\n\n\n\n\n\n\n\nExercise S.5: Experimental Design Critique\n\n\n\nRead the following experimental description and identify problems with the design:\n\n“A researcher wants to test whether a new drug reduces blood pressure. She recruits 40 volunteers and assigns the first 20 who sign up to receive the drug, and the remaining 20 to receive a placebo. The researcher measures each participant’s blood pressure and tells them which group they are in. The researcher’s assistant, who knows which participants received the drug, measures blood pressure again after 4 weeks. The average reduction in the drug group was 8 mmHg compared to 3 mmHg in the placebo group.”\n\nFor this design:\n\nList at least three major problems with how this experiment was conducted\nFor each problem you identified, explain how it could bias the results\nRedesign this experiment to address the problems you identified\nWould technical replicates (measuring each person’s blood pressure multiple times) or biological replicates (measuring more people) be more valuable for this study? Explain.\n\n\n\n\n\n\n\n\n\nExercise S.6: Simulation Study of Coverage\n\n\n\nThe true “95% confidence interval” should contain the true parameter 95% of the time in repeated sampling. Let’s verify this through simulation.\n\nSet up a simulation where you:\n\nChoose true population parameters (e.g., mean = 50, SD = 10)\nTake a random sample of size n = 20\nCalculate a 95% confidence interval\nCheck if the interval contains the true mean\nRepeat this process 1000 times\n\nWhat proportion of your confidence intervals contained the true mean?\nVisualize your results by plotting the first 100 confidence intervals, color-coding them by whether they contain the true mean\nRepeat the simulation with n = 50 and n = 100. How does sample size affect the width of confidence intervals?\nNow try a 90% confidence interval. What proportion of intervals contain the true mean?\n\n\n\nCode\n# Your code here",
    "crumbs": [
      "Probability and Distributions",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Sampling and Parameter Estimation</span>"
    ]
  },
  {
    "objectID": "chapters/10-sampling-estimation.html#additional-resources",
    "href": "chapters/10-sampling-estimation.html#additional-resources",
    "title": "14  Sampling and Parameter Estimation",
    "section": "14.17 Additional Resources",
    "text": "14.17 Additional Resources\n\nEfron (1979) - The original bootstrap paper, a landmark in modern statistics\nIrizarry (2019) - Excellent chapters on sampling and estimation with R examples\n\n\n\n\n\n\n\nEfron, Bradley. 1979. “Bootstrap Methods: Another Look at the Jackknife.” The Annals of Statistics 7 (1): 1–26.\n\n\nIrizarry, Rafael A. 2019. Introduction to Data Science. Self-published. https://rafalab.github.io/dsbook/.",
    "crumbs": [
      "Probability and Distributions",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Sampling and Parameter Estimation</span>"
    ]
  },
  {
    "objectID": "chapters/11-experimental-design.html",
    "href": "chapters/11-experimental-design.html",
    "title": "15  Experimental Design Principles",
    "section": "",
    "text": "15.1 Why Design Matters\nThe quality of your conclusions depends fundamentally on the quality of your experimental design. No amount of sophisticated statistical analysis can rescue a poorly designed experiment. As the saying goes, “to consult the statistician after an experiment is finished is often merely to ask him to conduct a post mortem examination. He can perhaps say what the experiment died of.”\nGood experimental design determines what questions you can answer, what conclusions you can draw, and how confidently you can draw them. A well-designed experiment maximizes the information gained from available resources while minimizing bias and unwanted variation.\nIn bioengineering, experimental design is particularly critical because:\nThis chapter covers the fundamental principles that underlie all good experimental design, from laboratory studies to clinical trials. These principles apply whether you are testing a new biomaterial, characterizing cellular responses, or evaluating a medical device.",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Experimental Design Principles</span>"
    ]
  },
  {
    "objectID": "chapters/11-experimental-design.html#why-design-matters",
    "href": "chapters/11-experimental-design.html#why-design-matters",
    "title": "15  Experimental Design Principles",
    "section": "",
    "text": "Biological systems are inherently variable\nResources (reagents, subjects, time) are often limited and expensive\nEthical considerations constrain what manipulations are permissible\nRegulatory requirements for medical devices and therapies demand rigorous evidence",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Experimental Design Principles</span>"
    ]
  },
  {
    "objectID": "chapters/11-experimental-design.html#key-principles-of-experimental-design",
    "href": "chapters/11-experimental-design.html#key-principles-of-experimental-design",
    "title": "15  Experimental Design Principles",
    "section": "15.2 Key Principles of Experimental Design",
    "text": "15.2 Key Principles of Experimental Design\n\nReplication\nReplication means repeating measurements on independent experimental units. True replication provides independent estimates of the effect you are studying and allows you to quantify variability.\n\nTrue Replication vs Pseudoreplication\nTrue replication involves independent experimental units—separate subjects, cultures, or measurements that could, in principle, have different outcomes.\nPseudoreplication occurs when you treat non-independent measurements as if they were independent. This is one of the most common and serious errors in experimental design.\nExample of pseudoreplication:\nSuppose you want to test whether a new growth factor increases cell proliferation. You prepare one culture dish with the growth factor and one control dish, then count cells in 10 different fields of view from each dish.\n\nSample size: NOT n = 10. You have n = 1 per treatment.\nProblem: All 10 measurements from the same dish are not independent. If that particular dish has contamination or was handled differently, all measurements are affected.\nSolution: Prepare multiple independent culture dishes (e.g., 5 treatment, 5 control) and treat the dish as the experimental unit.\n\n\n\n\n\n\n\nCommon Sources of Pseudoreplication\n\n\n\n\nMeasuring multiple locations on the same specimen\nMultiple measurements on the same subject at the same time point\nMultiple samples from the same culture or batch\nSubsampling from pooled samples\nMultiple tests on the same tissue section\n\nAlways ask: Are these measurements truly independent, or could they be more similar to each other than to measurements from a different experimental unit?\n\n\n\n\nHow Much Replication?\nMore replication increases statistical power and precision, but with diminishing returns. The relationship is governed by the standard error:\n\\[SE = \\frac{\\sigma}{\\sqrt{n}}\\]\nDoubling sample size reduces standard error by only 41% (since \\(\\sqrt{2} \\approx 1.41\\)). To halve the standard error requires quadrupling the sample size.\nPractical considerations:\n\nUse power analysis to determine minimum sample size (Chapter 24)\nPilot studies help estimate expected effect sizes and variability\nConsider practical constraints (cost, time, ethical limits)\nPlan for attrition or exclusions (add 10-20% buffer)\n\n\n\nCode\n# Relationship between sample size and standard error\nn &lt;- 1:100\nse_relative &lt;- 1/sqrt(n)\n\nggplot(data.frame(n = n, se = se_relative), aes(x = n, y = se)) +\n  geom_line(color = \"steelblue\", size = 1.2) +\n  geom_vline(xintercept = c(10, 40), linetype = \"dashed\", alpha = 0.5) +\n  annotate(\"text\", x = 10, y = 0.8, label = \"n=10\", hjust = -0.2) +\n  annotate(\"text\", x = 40, y = 0.5, label = \"4x samples\\n= 1/2 SE\", hjust = -0.2) +\n  labs(title = \"Diminishing Returns of Increased Replication\",\n       x = \"Sample Size (n)\",\n       y = \"Relative Standard Error\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nFigure 15.1: Standard error decreases with the square root of sample size, showing diminishing returns from additional replication\n\n\n\n\n\n\n\n\nRandomization\nRandomization is the use of a random process to assign experimental units to treatments or to determine the order of operations. Randomization serves two critical purposes:\n\nEliminates systematic bias that could arise from assignment procedures\nProvides the basis for statistical inference by ensuring treatment groups are comparable\n\nWithout randomization, systematic differences between groups could confound your results. If you always process treatment samples in the morning and controls in the afternoon, time-of-day effects become confounded with treatment effects.\n\nHow to Randomize\nUse a random number generator, not human judgment. Humans are terrible at generating random sequences—we create patterns even when trying not to.\n\n\nCode\n# Example: Randomize 20 subjects to two treatment groups\nset.seed(42)  # For reproducibility\nsubjects &lt;- paste0(\"Subject_\", 1:20)\n\n# Create random assignment\nassignments &lt;- data.frame(\n  subject = subjects,\n  treatment = sample(rep(c(\"Control\", \"Treatment\"), each = 10))\n)\n\n# View first few assignments\nhead(assignments)\n\n\n    subject treatment\n1 Subject_1 Treatment\n2 Subject_2   Control\n3 Subject_3   Control\n4 Subject_4   Control\n5 Subject_5   Control\n6 Subject_6   Control\n\n\nFor more complex designs:\n\n\nCode\n# Randomize with unequal allocation (2:1 treatment:control)\nsubjects &lt;- paste0(\"Subject_\", 1:30)\nassignments &lt;- data.frame(\n  subject = subjects,\n  treatment = sample(rep(c(\"Treatment\", \"Control\"), times = c(20, 10)))\n)\n\ntable(assignments$treatment)\n\n\n\n  Control Treatment \n       10        20 \n\n\n\n\n\n\n\n\nRestricted Randomization\n\n\n\nSometimes complete randomization can lead to imbalanced groups by chance. Permuted block randomization ensures balance:\n\n\nCode\n# Block randomization: ensure equal allocation every 4 subjects\nlibrary(blockrand)\n\n\nError in `library()`:\n! there is no package called 'blockrand'\n\n\nCode\nblockrand(n = 20, num.levels = 2, levels = c(\"Control\", \"Treatment\"),\n          block.sizes = c(4))\n\n\nError in `blockrand()`:\n! could not find function \"blockrand\"\n\n\nThis approach is common in clinical trials to prevent severe imbalance.\n\n\n\n\n\nBlocking\nBlocking is grouping experimental units into homogeneous sets (blocks) and ensuring each treatment appears within each block. Blocking controls for known sources of variation, increasing the precision of comparisons.\n\nWhen to Block\nBlock when you can identify sources of variation that:\n\nAre not of primary interest but affect the response\nCan be controlled during experimental design\nAre substantial relative to experimental error\n\nCommon blocking factors in bioengineering:\n\nTime: Day of experiment, batch, season\nLocation: Incubator, bench position, laboratory\nBiological variation: Litter, donor, cell passage number\nTechnical factors: Operator, instrument, reagent lot\n\n\n\nExample: Blocked Design\nTesting three culture conditions (treatments A, B, C) across different days:\nWithout blocking (completely randomized): - Day 1: A, A, A, B, B, B - Day 2: C, C, C\nDay effects are confounded with treatment effects!\nWith blocking: - Day 1 (Block 1): A, B, C - Day 2 (Block 2): A, B, C - Day 3 (Block 3): A, B, C\nEach treatment appears once per block. Day-to-day variation is removed from the treatment comparison.\n\n\nCode\n# Generate blocked design\ntreatments &lt;- c(\"A\", \"B\", \"C\")\nblocks &lt;- 5\n\nblocked_design &lt;- expand.grid(\n  Block = 1:blocks,\n  Treatment = treatments\n) %&gt;%\n  mutate(Run_Order = sample(1:n()))  # Randomize order within blocks\n\nblocked_design %&gt;%\n  arrange(Block, Run_Order) %&gt;%\n  head(9)\n\n\n  Block Treatment Run_Order\n1     1         B         4\n2     1         C         8\n3     1         A        10\n4     2         A         2\n5     2         B         3\n6     2         C         7\n7     3         C         1\n8     3         A         6\n9     3         B        12\n\n\n\n\nAnalysis of Blocked Designs\nInclude the blocking factor in your statistical model:\n\n\nCode\n# ANOVA with blocking\nmodel &lt;- lm(response ~ treatment + block, data = my_data)\nanova(model)\n\n\nThe blocking factor accounts for block-to-block variation, increasing power to detect treatment effects.\n\n\n\nControls\nControls are reference conditions used to establish baseline performance and validate experimental procedures. Different types of controls serve different purposes.\n\nTypes of Controls\nNegative Controls demonstrate what happens in the absence of treatment:\n\nUntreated cells in a drug study\nBuffer-only injection\nEmpty vector in a transfection experiment\nSham surgery (all procedures except the intervention)\n\nNegative controls establish baseline and show that observed effects are due to treatment, not handling or measurement.\nPositive Controls demonstrate that the system can respond:\n\nKnown active compound at established dose\nSuccessfully transfected cells with a reporter gene\nCalibration standards\nPrevious validated batch\n\nPositive controls verify that experimental conditions are suitable for detecting an effect. If positive controls fail, the experiment cannot distinguish true negatives from technical failures.\nVehicle Controls account for delivery method:\n\nCells treated with DMSO at the same concentration used to dissolve drugs\nSaline injection to match treatment injection volume\nEmpty nanoparticles without payload\n\nVehicle controls distinguish drug effects from effects of the carrier or solvent.\nInternal Controls provide within-sample references:\n\nHousekeeping genes in qPCR\nLoading controls in Western blots\nUntreated wells on the same plate\n\nInternal controls account for sample-to-sample variation in technical factors.\n\n\n\n\n\n\nControls in Bioengineering Applications\n\n\n\nWhen testing a new biomaterial: - Negative control: Standard tissue culture plastic or untreated tissue - Positive control: Established biomaterial known to support cell growth - Vehicle control: Scaffold material without bioactive modification\nWhen evaluating gene delivery: - Negative control: Cells without transfection - Positive control: Transfection with established reporter - Vehicle control: Empty vector or delivery vehicle alone",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Experimental Design Principles</span>"
    ]
  },
  {
    "objectID": "chapters/11-experimental-design.html#types-of-studies",
    "href": "chapters/11-experimental-design.html#types-of-studies",
    "title": "15  Experimental Design Principles",
    "section": "15.3 Types of Studies",
    "text": "15.3 Types of Studies\nUnderstanding study types helps you choose appropriate designs and recognize their limitations.\n\nObservational vs Experimental Studies\nObservational studies observe subjects without intervention:\n\nResearchers do not assign treatments\nCannot control confounding variables\nCan identify associations but not prove causation\n\nExamples: - Cohort studies following patients with different genetic variants - Surveys of device use patterns - Analysis of patient records\nExperimental studies involve researcher-controlled interventions:\n\nResearchers assign treatments\nCan include randomization and controls\nProvide strongest evidence for causation\n\nExamples: - Randomized controlled trials of new therapies - Laboratory experiments testing biomaterial properties - Studies comparing surgical techniques\n\n\n\n\n\n\nBradford Hill Criteria for Causation\n\n\n\nWhen experimental studies are not feasible (ethical or practical constraints), observational evidence can support causal claims if it shows:\n\nStrength: Strong associations are more likely causal\nConsistency: Reproducible across studies and populations\nSpecificity: Specific exposure linked to specific outcome\nTemporality: Exposure precedes outcome\nDose-response: Greater exposure leads to greater effect\nBiological plausibility: Mechanism makes sense\nCoherence: Consistent with existing knowledge\nExperimental evidence: Intervention changes outcome (when available)\nAnalogy: Similar cause-effect relationships exist\n\nThese criteria help evaluate observational findings, but randomized experiments remain the gold standard.\n\n\n\n\nProspective vs Retrospective Studies\nProspective studies follow subjects forward in time:\n\nDefine groups and measurements before data collection\nCollect data specifically for the research question\nBetter control over data quality and completeness\nMore expensive and time-consuming\n\nRetrospective studies look backward using existing data:\n\nUse previously collected data (medical records, databases)\nFaster and less expensive\nLimited to available variables\nMore prone to missing data and measurement inconsistencies\n\n\n\nCross-sectional vs Longitudinal Studies\nCross-sectional studies measure each subject once:\n\nSnapshot at a single time point\nCompare different subjects in different conditions\nCannot assess within-subject change\nFaster and less expensive\n\nLongitudinal studies measure subjects repeatedly over time:\n\nTrack change within individuals\nCan assess temporal dynamics\nRequire more complex statistical methods\nSubject to dropout and attrition\nMore expensive but more powerful for detecting change\n\nExample: Testing a new wound dressing\n\nCross-sectional: Compare healing rates in patients using new vs standard dressing at 1 week post-surgery\nLongitudinal: Measure wound size at days 0, 3, 7, 14 in patients randomized to new vs standard dressing\n\nThe longitudinal design provides more information about healing trajectories and requires fewer total subjects.",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Experimental Design Principles</span>"
    ]
  },
  {
    "objectID": "chapters/11-experimental-design.html#clinical-trial-design",
    "href": "chapters/11-experimental-design.html#clinical-trial-design",
    "title": "15  Experimental Design Principles",
    "section": "15.4 Clinical Trial Design",
    "text": "15.4 Clinical Trial Design\nClinical trials evaluate medical interventions in human subjects. Regulatory agencies (FDA, EMA) require specific design features to ensure valid and ethical evidence.\n\nPhases of Clinical Trials\nClinical trials progress through sequential phases, each with distinct objectives:\n\nPhase I: Safety and Dosing\n\nPrimary goal: Assess safety and determine appropriate dosing\nSubjects: Small number (20-80) of healthy volunteers (exceptions for cancer/severe disease)\nDesign: Dose escalation, often starting far below expected therapeutic dose\nDuration: Months\nOutcomes: Adverse events, pharmacokinetics, tolerability\n\n\n\nPhase II: Efficacy and Side Effects\n\nPrimary goal: Evaluate whether intervention shows promise of efficacy\nSubjects: Moderate number (100-300) of patients with target condition\nDesign: May include randomization and controls, often comparing doses\nDuration: Months to 2 years\nOutcomes: Preliminary efficacy, additional safety data, optimal dosing\n\n\n\nPhase III: Confirmatory Efficacy\n\nPrimary goal: Definitively establish efficacy and safety\nSubjects: Large number (300-3000+) of patients\nDesign: Randomized controlled trials with appropriate controls\nDuration: 1-4 years\nOutcomes: Primary efficacy endpoint, safety profile, comparison to standard of care\n\nSuccessful Phase III trials support regulatory approval (FDA, EMA).\n\n\nPhase IV: Post-Market Surveillance\n\nPrimary goal: Monitor long-term effects and rare adverse events\nSubjects: Thousands to millions of patients in real-world use\nDesign: Observational studies, registries, electronic health record analyses\nDuration: Ongoing after approval\nOutcomes: Rare adverse events, long-term outcomes, effectiveness in broader populations\n\n\n\n\n\n\n\nDevice vs Drug Trials\n\n\n\nMedical devices often follow different regulatory pathways than pharmaceuticals:\n\n510(k) clearance: Demonstrate substantial equivalence to existing device (no clinical trial required)\nPremarket Approval (PMA): High-risk devices require clinical evidence similar to Phase III drug trials\nBreakthrough Device Program: Expedited review for devices treating life-threatening conditions\n\nDevice trials may emphasize performance and safety over traditional efficacy endpoints.\n\n\n\n\n\nRandomized Controlled Trials (RCTs)\nThe randomized controlled trial is the gold standard for evaluating interventions:\nKey features:\n\nRandomization: Subjects randomly assigned to treatment groups\nControl group: Comparison group receiving standard care or placebo\nProspective: Follow subjects forward in time\nPre-specified outcomes: Primary endpoint defined before data collection\n\nAdvantages:\n\nRandomization eliminates systematic bias\nAllows causal inference\nProvides strongest evidence for regulatory approval\n\nLimitations:\n\nExpensive and time-consuming\nMay exclude important patient subgroups (strict eligibility criteria)\nControlled conditions may not reflect real-world practice\nEthical concerns about withholding potentially beneficial treatment\n\n\n\nBlinding\nBlinding (also called masking) prevents knowledge of treatment assignment from influencing outcomes or their assessment.\n\nSingle-Blind Studies\nSubjects do not know their treatment assignment, but researchers do.\n\nPrevents subject expectations from influencing outcomes (placebo effects)\nResearchers might unconsciously bias assessments\nCommon when blinding investigators is impractical\n\nExample: Surgical trial where surgeons must know procedure\n\n\nDouble-Blind Studies\nNeither subjects nor researchers know treatment assignments during the trial.\n\nEliminates bias from both subject expectations and researcher assessments\nRequires careful procedures to maintain blinding (identical packaging, etc.)\nGold standard for drug trials\n\nExample: Pill vs placebo trial with identical capsules\n\n\nTriple-Blind Studies\nSubjects, researchers, and data analysts remain blinded until analysis is complete.\n\nPrevents selective analysis or interpretation based on treatment groups\nMaximizes protection against bias\nMost rigorous design\n\n\n\n\n\n\n\nBlinding in Device Trials\n\n\n\nBlinding is often impossible in device studies (e.g., surgical procedures, implants). Strategies to reduce bias:\n\nBlind outcome assessors: Even if surgeons and patients know treatment, those measuring outcomes do not\nObjective outcomes: Use outcomes less susceptible to bias (mortality, device failure) rather than subjective assessments\nSham procedures: In some surgical trials, control group undergoes incision and closure without the intervention (ethically controversial)\n\n\n\n\n\n\nPlacebo Controls and Ethical Considerations\nPlacebo is an inactive treatment designed to appear indistinguishable from active treatment.\nPlacebo effects are real—subjects improve simply from believing they received treatment. Placebo-controlled trials isolate the specific treatment effect from these non-specific effects.\n\nEthical Issues with Placebos\nUsing placebos raises ethical concerns when:\n\nEffective standard treatments exist (withholding effective care)\nCondition is serious or life-threatening\nDelay in treatment could cause harm\n\nEthical placebo use: - ✓ No established effective treatment exists - ✓ Condition is minor or self-limiting - ✓ Placebo group receives standard care plus placebo - ✓ Stopping rules allow early termination if treatment clearly superior\nEthically problematic: - ✗ Withholding established effective treatment for serious disease - ✗ Placebo group receives no treatment when effective treatment exists\nSolution: Add-on designs where both groups receive standard care, and the comparison is standard care + placebo vs standard care + new treatment.\n\n\n\nIntention-to-Treat Analysis\nIntention-to-treat (ITT) analysis includes all randomized subjects in the groups to which they were assigned, regardless of:\n\nWhether they received the treatment\nWhether they completed the study\nWhether they switched treatments\nWhether they adhered to the protocol\n\n\nWhy ITT?\nPreserves randomization: Excluding subjects breaks the randomization and reintroduces bias.\nExample of bias from excluding subjects:\nSuppose a drug trial excludes patients who stopped treatment due to side effects. The drug group now contains only patients who tolerated it, while the placebo group includes all randomized patients. This makes the drug appear safer and more effective than it actually is.\nConservative estimate: ITT provides a realistic estimate of effectiveness when patients may not perfectly adhere to treatment (real-world conditions).\n\n\nITT vs Per-Protocol\nPer-protocol analysis includes only subjects who completed the study according to protocol.\n\nMay better reflect treatment efficacy under ideal conditions\nIntroduces bias by excluding subjects non-randomly\nUsed as secondary analysis but not primary\n\nModern approach: Report both ITT (primary) and per-protocol (sensitivity analysis).",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Experimental Design Principles</span>"
    ]
  },
  {
    "objectID": "chapters/11-experimental-design.html#common-experimental-designs",
    "href": "chapters/11-experimental-design.html#common-experimental-designs",
    "title": "15  Experimental Design Principles",
    "section": "15.5 Common Experimental Designs",
    "text": "15.5 Common Experimental Designs\n\nCompletely Randomized Design\nStructure: Experimental units randomly assigned to treatments without restrictions.\nWhen to use: - Experimental units are homogeneous - No known sources of variation to block - Simple to implement and analyze\nExample: Testing three drug doses on cell cultures, randomly assigning 15 cultures to each dose.\n\n\nCode\n# Completely randomized design\nset.seed(42)\nn_per_group &lt;- 15\ndoses &lt;- c(\"Low\", \"Medium\", \"High\")\n\ncrd_design &lt;- data.frame(\n  culture_id = 1:(n_per_group * 3),\n  dose = sample(rep(doses, each = n_per_group))\n)\n\ntable(crd_design$dose)\n\n\n\n  High    Low Medium \n    15     15     15 \n\n\nAnalysis: One-way ANOVA or t-test\n\n\nCode\n# Analysis\nmodel &lt;- lm(response ~ dose, data = crd_design)\nanova(model)\n\n\n\n\nRandomized Block Design\nStructure: Experimental units grouped into homogeneous blocks; all treatments appear in each block.\nWhen to use: - Identifiable source of variation to control - Cannot make all units homogeneous - Increases precision of treatment comparisons\nExample: Testing three biomaterials using cells from different donors (block by donor).\n\n\nCode\n# Randomized block design\ndonors &lt;- 8\nmaterials &lt;- c(\"Material_A\", \"Material_B\", \"Material_C\")\n\nrbd_design &lt;- expand.grid(\n  donor = paste0(\"Donor_\", 1:donors),\n  material = materials\n) %&gt;%\n  arrange(donor) %&gt;%\n  group_by(donor) %&gt;%\n  mutate(run_order = sample(1:n())) %&gt;%\n  ungroup()\n\nhead(rbd_design, 9)\n\n\n# A tibble: 9 × 3\n  donor   material   run_order\n  &lt;fct&gt;   &lt;fct&gt;          &lt;int&gt;\n1 Donor_1 Material_A         2\n2 Donor_1 Material_B         3\n3 Donor_1 Material_C         1\n4 Donor_2 Material_A         3\n5 Donor_2 Material_B         2\n6 Donor_2 Material_C         1\n7 Donor_3 Material_A         2\n8 Donor_3 Material_B         1\n9 Donor_3 Material_C         3\n\n\nAnalysis: Two-way ANOVA with block effect\n\n\nCode\n# Analysis accounting for blocking\nmodel &lt;- lm(response ~ material + donor, data = rbd_design)\nanova(model)\n\n\nBlocking can dramatically increase power when block effects are substantial.\n\n\nFactorial Designs\nStructure: Simultaneously study two or more factors, including all combinations of factor levels.\nAdvantages: - Efficient: Test multiple factors in one experiment - Detect interactions: Does the effect of Factor A depend on the level of Factor B? - More realistic: Real systems involve multiple factors\nExample: 2 × 3 factorial design testing two materials (A, B) and three drug concentrations (Low, Medium, High).\n\n\nCode\n# 2x3 factorial design\nmaterials &lt;- c(\"Material_A\", \"Material_B\")\ndrug_conc &lt;- c(\"Low\", \"Medium\", \"High\")\nn_reps &lt;- 5\n\nfactorial_design &lt;- expand.grid(\n  material = materials,\n  drug = drug_conc,\n  replicate = 1:n_reps\n) %&gt;%\n  mutate(sample_id = 1:n())\n\n# 6 treatment combinations, 5 replicates each = 30 samples\nhead(factorial_design, 12)\n\n\n     material   drug replicate sample_id\n1  Material_A    Low         1         1\n2  Material_B    Low         1         2\n3  Material_A Medium         1         3\n4  Material_B Medium         1         4\n5  Material_A   High         1         5\n6  Material_B   High         1         6\n7  Material_A    Low         2         7\n8  Material_B    Low         2         8\n9  Material_A Medium         2         9\n10 Material_B Medium         2        10\n11 Material_A   High         2        11\n12 Material_B   High         2        12\n\n\nAnalysis: Two-way ANOVA with interaction\n\n\nCode\n# Analysis including interaction\nmodel &lt;- lm(response ~ material * drug, data = factorial_design)\nanova(model)\n\n\nThe interaction term (material:drug) tests whether drug effects differ between materials.\nInterpreting interactions:\n\nNo interaction: Effects of each factor are additive\nInteraction present: Effect of one factor depends on the other\n\nMaterial A responds to drug, Material B does not\nDrug is beneficial at low concentration but harmful at high concentration on Material A\n\n\n\n\nCode\n# Simulated data showing interaction\nset.seed(42)\nsim_data &lt;- expand.grid(\n  material = c(\"Material_A\", \"Material_B\"),\n  drug = c(\"Low\", \"Medium\", \"High\"),\n  replicate = 1:10\n) %&gt;%\n  mutate(\n    response = case_when(\n      material == \"Material_A\" & drug == \"Low\" ~ rnorm(n(), 10, 2),\n      material == \"Material_A\" & drug == \"Medium\" ~ rnorm(n(), 15, 2),\n      material == \"Material_A\" & drug == \"High\" ~ rnorm(n(), 25, 2),\n      material == \"Material_B\" & drug == \"Low\" ~ rnorm(n(), 12, 2),\n      material == \"Material_B\" & drug == \"Medium\" ~ rnorm(n(), 13, 2),\n      material == \"Material_B\" & drug == \"High\" ~ rnorm(n(), 14, 2)\n    )\n  )\n\nsim_data %&gt;%\n  group_by(material, drug) %&gt;%\n  summarize(mean_response = mean(response), .groups = \"drop\") %&gt;%\n  ggplot(aes(x = drug, y = mean_response, color = material, group = material)) +\n  geom_line(size = 1.2) +\n  geom_point(size = 3) +\n  labs(title = \"Interaction Between Material and Drug Concentration\",\n       x = \"Drug Concentration\",\n       y = \"Mean Response\",\n       color = \"Material\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nFigure 15.2: Example of interaction effect: drug concentration has different effects on the two materials\n\n\n\n\n\nMaterial A shows strong dose-response; Material B shows minimal response. This is an interaction.\n\n\nCrossover Designs\nStructure: Each subject receives multiple treatments in sequence, with washout periods between treatments.\nAdvantages: - Each subject serves as their own control - Reduces between-subject variability - Requires fewer subjects than parallel-group design\nDisadvantages: - Requires washout periods (treatment effects must be reversible) - Carryover effects: First treatment influences response to second - Order effects: Subjects may change over time - Not suitable for irreversible interventions\nExample: Two-period crossover comparing drugs A and B:\n\nGroup 1: Receive A in period 1, then B in period 2\nGroup 2: Receive B in period 1, then A in period 2\n\nRandomizing the sequence controls for order effects.\n\n\nCode\n# Two-period crossover design\nsubjects &lt;- 20\ncrossover_design &lt;- data.frame(\n  subject = rep(1:subjects, each = 2),\n  period = rep(1:2, times = subjects)\n) %&gt;%\n  mutate(\n    sequence = rep(sample(rep(c(\"AB\", \"BA\"), each = subjects/2)), each = 2),\n    treatment = ifelse(\n      (sequence == \"AB\" & period == 1) | (sequence == \"BA\" & period == 2),\n      \"Drug_A\", \"Drug_B\"\n    )\n  )\n\nhead(crossover_design, 8)\n\n\n  subject period sequence treatment\n1       1      1       BA    Drug_B\n2       1      2       BA    Drug_A\n3       2      1       AB    Drug_A\n4       2      2       AB    Drug_B\n5       3      1       AB    Drug_A\n6       3      2       AB    Drug_B\n7       4      1       AB    Drug_A\n8       4      2       AB    Drug_B\n\n\nAnalysis: Include period and sequence effects\n\n\nCode\n# Mixed effects model for crossover\nlibrary(lme4)\nmodel &lt;- lmer(response ~ treatment + period + (1|subject), data = crossover_design)\n\n\n\n\n\n\n\n\nWhen NOT to Use Crossover Designs\n\n\n\nAvoid crossover designs when:\n\nTreatment has permanent effects (surgery, gene therapy)\nCondition changes irreversibly over time (progressive disease)\nWashout is impractical (long-lasting drugs)\nCarryover effects are likely and cannot be measured",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Experimental Design Principles</span>"
    ]
  },
  {
    "objectID": "chapters/11-experimental-design.html#sample-size-considerations",
    "href": "chapters/11-experimental-design.html#sample-size-considerations",
    "title": "15  Experimental Design Principles",
    "section": "15.6 Sample Size Considerations",
    "text": "15.6 Sample Size Considerations\nDetermining appropriate sample size is one of the most important design decisions. Too few subjects wastes resources and may miss real effects. Too many subjects exposes more individuals to experimental procedures than necessary and wastes resources.\nSample size depends on:\n\nExpected effect size: Larger effects require fewer subjects\nDesired statistical power: Typically 80% or 90%\nSignificance level: Typically α = 0.05\nVariability: More variable data requires larger samples\nStudy design: Paired designs require fewer subjects than independent groups\n\nSee Chapter 24 for detailed coverage of power analysis and sample size calculation.\nQuick reference for two-group comparisons:\n\n\nCode\n# Sample size for detecting different effect sizes (Cohen's d)\neffect_sizes &lt;- c(0.2, 0.5, 0.8)\nsample_sizes &lt;- sapply(effect_sizes, function(d) {\n  ceiling(pwr.t.test(d = d, sig.level = 0.05, power = 0.80,\n                     type = \"two.sample\")$n)\n})\n\ndata.frame(\n  Effect_Size = c(\"Small (0.2)\", \"Medium (0.5)\", \"Large (0.8)\"),\n  Cohen_d = effect_sizes,\n  Subjects_per_Group = sample_sizes,\n  Total_Subjects = sample_sizes * 2\n)\n\n\n   Effect_Size Cohen_d Subjects_per_Group Total_Subjects\n1  Small (0.2)     0.2                394            788\n2 Medium (0.5)     0.5                 64            128\n3  Large (0.8)     0.8                 26             52\n\n\n\n\n\n\n\n\nPilot Studies\n\n\n\nWhen effect sizes are unknown, pilot studies (small preliminary experiments) can provide estimates:\n\nRun 5-10 subjects per group\nEstimate means and standard deviations\nCalculate sample size needed for full study\nBe conservative—pilot estimates are often optimistic\n\nAdd 10-20% buffer to account for dropout and exclusions.",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Experimental Design Principles</span>"
    ]
  },
  {
    "objectID": "chapters/11-experimental-design.html#common-design-pitfalls-to-avoid",
    "href": "chapters/11-experimental-design.html#common-design-pitfalls-to-avoid",
    "title": "15  Experimental Design Principles",
    "section": "15.7 Common Design Pitfalls to Avoid",
    "text": "15.7 Common Design Pitfalls to Avoid\n\n1. Pseudoreplication\nProblem: Treating non-independent measurements as independent replicates.\nExample: Measuring multiple cells from one culture dish and treating each cell as n = 1.\nSolution: Identify the true experimental unit (the dish) and replicate at that level.\n\n\n2. Confounding\nProblem: Treatment effects are inseparable from other factors.\nExample: Testing new material only with experienced technician and control only with novice technician. Technician skill is confounded with material.\nSolution: Randomization and counterbalancing. Both technicians handle both materials.\n\n\n3. Selection Bias\nProblem: Non-random selection or assignment of subjects.\nExample: Letting patients choose their treatment or assigning sicker patients to experimental treatment.\nSolution: Randomization and clear eligibility criteria applied before assignment.\n\n\n4. Measurement Bias\nProblem: Systematic errors in outcome assessment.\nExample: Unblinded researcher measures outcomes, unconsciously recording better results for preferred treatment.\nSolution: Blinding of outcome assessors, objective measurements, standardized protocols.\n\n\n5. Inadequate Power\nProblem: Sample size too small to detect realistic effect sizes.\nExample: Testing new drug with n = 5 per group when n = 64 per group is needed.\nSolution: A priori power analysis before data collection.\n\n\n6. P-Hacking and Multiple Comparisons\nProblem: Testing many hypotheses and reporting only significant results, or analyzing data multiple ways until significance is found.\nExample: Collecting data, running t-test, not significant, trying different transformations, removing “outliers,” testing subgroups, until p &lt; 0.05.\nSolution: Pre-specify primary hypotheses and analysis plan. Adjust for multiple comparisons. Register studies prospectively.\n\n\n7. Regression to the Mean\nProblem: Extreme values tend to be closer to average on re-measurement simply due to random variation.\nExample: Selecting subjects with highest blood pressure, then observing decrease without treatment. Appears treatment worked, but values would decrease anyway.\nSolution: Include control group not selected on extreme values, or compare to expected distribution.\n\n\n8. Attrition Bias\nProblem: Differential dropout between groups.\nExample: Side effects cause dropout in treatment group; only tolerant subjects remain. Makes treatment appear safer than it is.\nSolution: Intention-to-treat analysis, minimize dropout, analyze characteristics of dropouts.\n\n\n9. Failure to Validate Assumptions\nProblem: Applying statistical methods without checking whether assumptions hold.\nExample: Using t-test on heavily skewed data with outliers.\nSolution: Check assumptions, use diagnostic plots, consider robust or non-parametric alternatives.\n\n\n10. Ignoring Biological Variability\nProblem: Failing to account for known sources of variation.\nExample: Running all treatment samples on Monday and all controls on Friday.\nSolution: Blocking, randomization of run order, including batch effects in analysis.",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Experimental Design Principles</span>"
    ]
  },
  {
    "objectID": "chapters/11-experimental-design.html#summary",
    "href": "chapters/11-experimental-design.html#summary",
    "title": "15  Experimental Design Principles",
    "section": "15.8 Summary",
    "text": "15.8 Summary\nGood experimental design is the foundation of good science:\n\nReplication provides independent estimates and quantifies variability\nRandomization eliminates systematic bias and enables inference\nBlocking controls for known sources of variation\nControls establish baselines and validate methods\n\nDifferent study types suit different questions:\n\nExperimental studies provide strongest causal evidence\nObservational studies identify associations when experiments are not feasible\nClinical trials require rigorous design for regulatory approval\n\nCommon designs include:\n\nCompletely randomized: Simple, assumes homogeneous units\nRandomized block: Controls for known variation\nFactorial: Tests multiple factors and interactions efficiently\nCrossover: Uses subjects as their own controls\n\nAvoid common pitfalls:\n\nPseudoreplication\nConfounding\nSelection bias\nInadequate power\nP-hacking\nIgnoring assumptions\n\nBefore collecting data:\n\nDefine clear research questions and hypotheses\nChoose appropriate design for your question\nCalculate required sample size (power analysis)\nPlan randomization and blinding procedures\nPre-specify primary outcomes and analysis methods\nPrepare data collection forms and protocols\n\nTime invested in design pays enormous dividends in the quality and interpretability of results.",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Experimental Design Principles</span>"
    ]
  },
  {
    "objectID": "chapters/11-experimental-design.html#practice-exercises",
    "href": "chapters/11-experimental-design.html#practice-exercises",
    "title": "15  Experimental Design Principles",
    "section": "15.9 Practice Exercises",
    "text": "15.9 Practice Exercises\n\nExercise 11.1: Identifying Pseudoreplication\nFor each scenario, identify whether pseudoreplication is present and explain why:\n\nTesting cell viability: Prepare 3 culture plates per treatment, count 100 cells per plate, analyze with n = 300 per treatment.\nTesting biomaterial: Create 5 scaffolds per material type, cut each scaffold into 4 pieces for mechanical testing, analyze with n = 20 per material.\nClinical trial: Randomize 50 patients per treatment, measure blood pressure 3 times at each visit, average the 3 measurements for each patient, analyze with n = 50 per treatment.\nGene expression: Extract RNA from 6 independent cell cultures per treatment, run each sample in qPCR with technical triplicates, analyze with n = 18 per treatment.\n\n\n\nExercise 11.2: Designing a Blocked Experiment\nYou are testing three drug formulations (A, B, C) on cell proliferation. You have cells from 5 different donors, and you know donor-to-donor variability is substantial.\n\nDesign a randomized block experiment for this scenario\nWrite R code to generate the randomized design\nWhat are the advantages of blocking on donor compared to a completely randomized design?\nHow would you analyze data from this design?\n\n\n\nCode\n# Your code here\n\n\n\n\nExercise 11.3: Factorial Design\nDesign a 2 × 2 factorial experiment testing: - Factor A: Two biomaterials (Material 1, Material 2) - Factor B: Two oxygen levels (Normoxia 21%, Hypoxia 5%)\n\nList all treatment combinations\nCreate the experimental design with 6 replicates per combination\nWrite R code to generate and randomize the design\nDraw a hypothetical interaction plot showing an interaction between material and oxygen\nExplain what this interaction would mean biologically\n\n\n\nCode\n# Your code here\n\n\n\n\nExercise 11.4: Sample Size Calculation\nYou are planning a study comparing a new wound dressing to standard care. Previous studies suggest: - Standard care: mean healing time = 14 days, SD = 4 days - Expected improvement: 2-3 days faster healing\n\nCalculate the effect size (Cohen’s d) if the new dressing reduces healing time by 2 days\nDetermine the sample size needed per group for 80% power at α = 0.05\nRepeat for 90% power\nHow many subjects would you actually recruit (accounting for 15% dropout)?\n\n\n\nCode\nlibrary(pwr)\n# Your code here\n\n\n\n\nExercise 11.5: Choosing a Design\nFor each research question, recommend an appropriate experimental design and justify your choice:\n\nQuestion: Does a new hydrogel support better cell attachment than existing hydrogels?\n\nFactors to consider: You have 3 hydrogels to test and cells from 4 donors\n\nQuestion: Do two different sterilization methods affect biomaterial properties?\n\nFactors to consider: You want to test both methods on the same samples if possible\n\nQuestion: Does exercise improve outcomes in patients receiving a new implant?\n\nFactors to consider: Implants are expensive, patient recruitment is slow\n\nQuestion: What combination of growth factors produces optimal tissue engineering outcomes?\n\nFactors to consider: Testing EGF (0, 10, 50 ng/mL) and VEGF (0, 20, 100 ng/mL)\n\n\n\n\nExercise 11.6: Critique a Design\nA researcher wants to test whether a new scaffold improves bone regeneration. They propose:\n\nCreate 2 scaffolds with new design, 2 scaffolds with standard design\nImplant in 4 rabbits (1 scaffold per rabbit)\nAfter 8 weeks, measure bone formation\nCompare new vs standard using t-test\n\nQuestions:\n\nWhat is the true sample size? Is this adequate?\nWhat sources of variation are uncontrolled?\nHow could randomization be incorporated?\nHow could blocking be used to improve the design?\nPropose an improved design with explicit justification for changes\n\n\n\nExercise 11.7: Simulation Study\nUse simulation to demonstrate the value of blocking:\n\nSimulate data from a blocked design with 3 treatments and 5 blocks, where:\n\nBlock effects are substantial (SD = 5)\nTreatment effect exists (mean difference = 3)\nWithin-block error is moderate (SD = 2)\n\nAnalyze the data TWO ways:\n\nIgnoring blocking: lm(response ~ treatment)\nIncluding blocking: lm(response ~ treatment + block)\n\nCompare the p-values for the treatment effect\nRepeat 1000 times and calculate power for each approach\n\n\n\nCode\nset.seed(42)\n\n# Simulation parameters\nn_sims &lt;- 1000\nn_blocks &lt;- 5\ntreatments &lt;- c(\"A\", \"B\", \"C\")\ntreatment_effects &lt;- c(0, 3, 1.5)  # A is reference, B = +3, C = +1.5\nblock_sd &lt;- 5\nerror_sd &lt;- 2\n\n# Storage for p-values\np_blocked &lt;- numeric(n_sims)\np_unblocked &lt;- numeric(n_sims)\n\nfor (i in 1:n_sims) {\n  # Your simulation code here\n}\n\n# Calculate power (proportion of p-values &lt; 0.05)\npower_blocked &lt;- mean(p_blocked &lt; 0.05)\npower_unblocked &lt;- mean(p_unblocked &lt; 0.05)\n\ncat(\"Power with blocking:\", power_blocked, \"\\n\")\ncat(\"Power without blocking:\", power_unblocked, \"\\n\")\n\n\n\n\nExercise 11.8: Clinical Trial Design\nYou are designing a Phase III clinical trial for a new drug-eluting stent:\nBackground: - Target: Patients with coronary artery disease requiring stent - Primary outcome: Target vessel revascularization at 12 months - Standard stent TVR rate: 8% - New stent expected TVR rate: 4% (50% relative reduction)\nTasks:\n\nWhat type of control group would be appropriate? Why?\nShould the trial be blinded? If so, single, double, or triple blind?\nCalculate sample size for 80% power and 90% power (use pwr.2p.test for two proportions)\nWhat inclusion/exclusion criteria would you set?\nWhat safety monitoring would you implement?\nHow would you handle patients who are lost to follow-up before 12 months?\n\n\n\nCode\nlibrary(pwr)\n# Sample size calculation for two proportions",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Experimental Design Principles</span>"
    ]
  },
  {
    "objectID": "chapters/11-experimental-design.html#additional-resources",
    "href": "chapters/11-experimental-design.html#additional-resources",
    "title": "15  Experimental Design Principles",
    "section": "15.10 Additional Resources",
    "text": "15.10 Additional Resources\n\nFisher (1935) - Classic text on experimental design principles\nMontgomery (2017) - Comprehensive modern treatment of experimental design\nFriedman et al. (2015) - Clinical trial design and conduct\nPocock (2004) - Statistical methods for clinical trials\nRubin (2008) - Observational studies and causal inference\n\n\n\n\n\n\n\nFisher, Ronald A. 1935. The Design of Experiments. Edinburgh: Oliver; Boyd.\n\n\nFriedman, Lawrence M., Curt D. Furberg, David L. DeMets, David M. Reboussin, and Christopher B. Granger. 2015. Fundamentals of Clinical Trials. 5th ed. New York: Springer.\n\n\nMontgomery, Douglas C. 2017. Design and Analysis of Experiments. 9th ed. Hoboken, NJ: Wiley.\n\n\nPocock, Stuart J. 2004. Clinical Trials: A Practical Approach. Chichester: Wiley.\n\n\nRubin, Donald B. 2008. Matched Sampling for Causal Effects. Cambridge: Cambridge University Press.",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Experimental Design Principles</span>"
    ]
  },
  {
    "objectID": "chapters/11-hypothesis-testing.html",
    "href": "chapters/11-hypothesis-testing.html",
    "title": "16  Hypothesis Testing",
    "section": "",
    "text": "16.1 What is a Hypothesis?\nA hypothesis is a statement of belief about the world—a claim that can be evaluated with data. In statistics, we formalize hypothesis testing as a framework for using data to decide between competing claims.\nThe null hypothesis (\\(H_0\\)) represents a default position, typically stating that there is no effect, no difference, or no relationship. The alternative hypothesis (\\(H_A\\)) represents what we would conclude if we reject the null—typically that there is an effect, difference, or relationship.\nFor example, consider testing whether an amino acid substitution changes the catalytic rate of an enzyme:\nThe alternative hypothesis might be directional (the substitution increases the rate) or non-directional (the substitution changes the rate, in either direction). This distinction affects how we calculate p-values.",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "chapters/11-hypothesis-testing.html#what-is-a-hypothesis",
    "href": "chapters/11-hypothesis-testing.html#what-is-a-hypothesis",
    "title": "16  Hypothesis Testing",
    "section": "",
    "text": "\\(H_0\\): The substitution does not change the catalytic rate\n\\(H_A\\): The substitution does change the catalytic rate",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "chapters/11-hypothesis-testing.html#the-logic-of-hypothesis-testing",
    "href": "chapters/11-hypothesis-testing.html#the-logic-of-hypothesis-testing",
    "title": "16  Hypothesis Testing",
    "section": "16.2 The Logic of Hypothesis Testing",
    "text": "16.2 The Logic of Hypothesis Testing\nHypothesis testing follows a specific logic. We assume the null hypothesis is true and ask: how likely would we be to observe data as extreme as what we actually observed? If this probability is very small, we conclude that the null hypothesis is unlikely to be true and reject it in favor of the alternative.\nThis framework naturally raises several key questions. What is the probability that we might reject a true null hypothesis, committing a false positive error? Conversely, what is the probability that we might fail to reject a false null hypothesis, missing a real effect? How do we decide when the evidence is strong enough to reject the null hypothesis? And perhaps most subtly, what can we legitimately conclude when we fail to reject the null—does this mean we’ve proven it true, or merely that we lack sufficient evidence against it?",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "chapters/11-hypothesis-testing.html#type-i-and-type-ii-errors",
    "href": "chapters/11-hypothesis-testing.html#type-i-and-type-ii-errors",
    "title": "16  Hypothesis Testing",
    "section": "16.3 Type I and Type II Errors",
    "text": "16.3 Type I and Type II Errors\nTwo types of mistakes are possible in hypothesis testing.\n\n\n\n\n\n\nFigure 16.1: Type I and Type II errors in hypothesis testing\n\n\n\nA Type I error occurs when we reject a true null hypothesis—concluding there is an effect when there is not. The probability of a Type I error is denoted \\(\\alpha\\) and is called the significance level. By convention, \\(\\alpha\\) is often set to 0.05, meaning we accept a 5% chance of falsely rejecting a true null.\nA Type II error occurs when we fail to reject a false null hypothesis—concluding there is no effect when there actually is one. The probability of a Type II error is denoted \\(\\beta\\).\nPower is the probability of correctly rejecting a false null hypothesis: Power = \\(1 - \\beta\\). Power depends on the effect size (how big the true effect is), sample size, significance level, and variability in the data.\n\n\n\n\n\\(H_0\\) True\n\\(H_0\\) False\n\n\n\n\nReject \\(H_0\\)\nType I Error (\\(\\alpha\\))\nCorrect Decision (Power)\n\n\nFail to Reject \\(H_0\\)\nCorrect Decision\nType II Error (\\(\\beta\\))\n\n\n\n\nUnderstanding Statistical Power\nPower analysis is essential for designing experiments that can actually detect effects of interest. A study with low power is unlikely to find real effects even when they exist—wasting resources and potentially leading to incorrect conclusions.\nSeveral key factors determine statistical power. Effect size matters fundamentally—larger effects are easier to detect than subtle ones. Sample size directly affects power because more data provides more information about the true state of nature. The chosen significance level (\\(\\alpha\\)) creates a trade-off: higher \\(\\alpha\\) increases power to detect true effects but also increases the Type I error rate. Finally, variability in the data affects how clearly we can see signals; less noise makes true effects easier to detect, while high variability obscures even substantial effects.\nFor a two-sample t-test, the relationship between these factors can be expressed approximately as:\n\\[\\text{Power} \\approx \\Phi\\left(\\frac{|\\mu_1 - \\mu_2|}{\\sigma}\\sqrt{\\frac{n}{2}} - z_{1-\\alpha/2}\\right)\\]\nwhere \\(\\Phi\\) is the standard normal CDF, and \\(z_{1-\\alpha/2}\\) is the critical value.\n\n\nCode\n# Visualize how power depends on effect size and sample size\nlibrary(pwr)\n\n# Power curves for different effect sizes\neffect_sizes &lt;- c(0.2, 0.5, 0.8)  # Cohen's d: small, medium, large\nsample_sizes &lt;- seq(5, 100, by = 5)\n\npar(mfrow = c(1, 1))\ncolors &lt;- c(\"blue\", \"darkgreen\", \"red\")\n\nplot(NULL, xlim = c(5, 100), ylim = c(0, 1),\n     xlab = \"Sample Size (per group)\", ylab = \"Power\",\n     main = \"Power Curves for Two-Sample t-Test\")\nabline(h = 0.8, lty = 2, col = \"gray\")\n\nfor (i in 1:3) {\n  powers &lt;- sapply(sample_sizes, function(n) {\n    pwr.t.test(n = n, d = effect_sizes[i], sig.level = 0.05, type = \"two.sample\")$power\n  })\n  lines(sample_sizes, powers, col = colors[i], lwd = 2)\n}\n\nlegend(\"bottomright\",\n       legend = c(\"Small (d=0.2)\", \"Medium (d=0.5)\", \"Large (d=0.8)\"),\n       col = colors, lwd = 2)\n\n\n\n\n\n\n\n\nFigure 16.2: Statistical power increases with sample size and effect size; the dashed line indicates the common 80% power threshold\n\n\n\n\n\n\n\nA Priori Power Analysis\nBefore conducting an experiment, power analysis helps determine the necessary sample size. The question is: “How many observations do I need to have an 80% chance of detecting an effect of a given size?”\n\n\nCode\n# Sample size calculation for 80% power\n# Detecting a medium effect (d = 0.5) with alpha = 0.05\npower_result &lt;- pwr.t.test(d = 0.5, sig.level = 0.05, power = 0.8, type = \"two.sample\")\ncat(\"Required sample size per group:\", ceiling(power_result$n), \"\\n\")\n\n\nRequired sample size per group: 64 \n\n\nCode\n# For a small effect (d = 0.2)\npower_small &lt;- pwr.t.test(d = 0.2, sig.level = 0.05, power = 0.8, type = \"two.sample\")\ncat(\"For small effect (d=0.2):\", ceiling(power_small$n), \"per group\\n\")\n\n\nFor small effect (d=0.2): 394 per group\n\n\nNotice how detecting small effects requires substantially larger samples. This is why pilot studies and literature-based effect size estimates are valuable for planning.",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "chapters/11-hypothesis-testing.html#p-values",
    "href": "chapters/11-hypothesis-testing.html#p-values",
    "title": "16  Hypothesis Testing",
    "section": "16.4 P-Values",
    "text": "16.4 P-Values\nThe p-value is the probability of observing a test statistic as extreme or more extreme than the one calculated from the data, assuming the null hypothesis is true.\nA small p-value indicates that the observed data would be unlikely if the null hypothesis were true, providing evidence against the null. A large p-value indicates that the data are consistent with the null hypothesis.\nThe p-value is NOT the probability that the null hypothesis is true. It is the probability of the data (or more extreme data) given the null hypothesis, not the probability of the null hypothesis given the data.",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "chapters/11-hypothesis-testing.html#significance-level-and-decision-rules",
    "href": "chapters/11-hypothesis-testing.html#significance-level-and-decision-rules",
    "title": "16  Hypothesis Testing",
    "section": "16.5 Significance Level and Decision Rules",
    "text": "16.5 Significance Level and Decision Rules\nThe significance level \\(\\alpha\\) is the threshold below which we reject the null hypothesis. If \\(p &lt; \\alpha\\), we reject \\(H_0\\). If \\(p \\geq \\alpha\\), we fail to reject \\(H_0\\).\n\n\n\n\n\n\nFigure 16.3: The significance level α determines the rejection region for hypothesis testing\n\n\n\nThe conventional choice of \\(\\alpha = 0.05\\) is arbitrary but widely used. In contexts where Type I errors are particularly costly (e.g., approving an ineffective drug), smaller \\(\\alpha\\) values may be appropriate. In exploratory research, larger \\(\\alpha\\) values might be acceptable.\nImportant: “fail to reject” is not the same as “accept.” Failing to reject the null hypothesis means the data did not provide sufficient evidence against it, not that the null hypothesis is true.",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "chapters/11-hypothesis-testing.html#test-statistics-and-statistical-distributions",
    "href": "chapters/11-hypothesis-testing.html#test-statistics-and-statistical-distributions",
    "title": "16  Hypothesis Testing",
    "section": "16.6 Test Statistics and Statistical Distributions",
    "text": "16.6 Test Statistics and Statistical Distributions\nA test statistic summarizes the data in a way that allows comparison to a known distribution under the null hypothesis. Different tests use different statistics: the t-statistic for t-tests, the F-statistic for ANOVA, the chi-squared statistic for contingency tables.\nJust like raw data, test statistics are random variables with their own sampling distributions. Under the null hypothesis, we know what distribution the test statistic should follow. We can then calculate how unusual our observed statistic is under this distribution.\n\n\n\n\n\n\nFigure 16.4: Test statistics follow known distributions under the null hypothesis",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "chapters/11-hypothesis-testing.html#one-tailed-vs.-two-tailed-tests",
    "href": "chapters/11-hypothesis-testing.html#one-tailed-vs.-two-tailed-tests",
    "title": "16  Hypothesis Testing",
    "section": "16.7 One-Tailed vs. Two-Tailed Tests",
    "text": "16.7 One-Tailed vs. Two-Tailed Tests\nA two-tailed test considers extreme values in both directions. The alternative hypothesis is non-directional: \\(H_A: \\mu \\neq \\mu_0\\). Extreme values in either tail of the distribution count as evidence against the null.\nA one-tailed test considers extreme values in only one direction. The alternative hypothesis is directional: \\(H_A: \\mu &gt; \\mu_0\\) or \\(H_A: \\mu &lt; \\mu_0\\). Only extreme values in the specified direction count as evidence against the null.\n\n\n\n\n\n\nFigure 16.5: One-tailed vs. two-tailed tests: the alternative hypothesis determines which tail(s) to consider\n\n\n\nTwo-tailed tests are more conservative and are appropriate when you do not have a strong prior expectation about the direction of an effect. One-tailed tests have more power to detect effects in the specified direction but will miss effects in the opposite direction.",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "chapters/11-hypothesis-testing.html#multiple-testing",
    "href": "chapters/11-hypothesis-testing.html#multiple-testing",
    "title": "16  Hypothesis Testing",
    "section": "16.8 Multiple Testing",
    "text": "16.8 Multiple Testing\nWhen you perform many hypothesis tests, the probability of at least one Type I error increases. If you test 20 independent hypotheses at \\(\\alpha = 0.05\\), you expect about one false positive even when all null hypotheses are true.\nSeveral approaches address multiple testing:\nBonferroni correction divides \\(\\alpha\\) by the number of tests. For 20 tests, use \\(\\alpha = 0.05/20 = 0.0025\\). This is conservative and may miss true effects.\nFalse Discovery Rate (FDR) control allows some false positives but controls their proportion among rejected hypotheses. This is less conservative than Bonferroni and widely used in genomics and other high-throughput applications.",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "chapters/11-hypothesis-testing.html#practical-vs.-statistical-significance",
    "href": "chapters/11-hypothesis-testing.html#practical-vs.-statistical-significance",
    "title": "16  Hypothesis Testing",
    "section": "16.9 Practical vs. Statistical Significance",
    "text": "16.9 Practical vs. Statistical Significance\nStatistical significance does not imply practical importance. With a large enough sample, even trivially small effects become statistically significant. Conversely, practically important effects may not reach statistical significance with small samples.\nAlways consider effect sizes alongside p-values. Report confidence intervals, which convey both the magnitude of an effect and the uncertainty about it. A 95% confidence interval that excludes zero is equivalent to statistical significance at \\(\\alpha = 0.05\\), but also shows the range of plausible effect sizes.\n\nThe Connection Between P-Values and Confidence Intervals\nP-values and confidence intervals are mathematically linked. Understanding this connection deepens your grasp of both concepts.\nFor testing whether a parameter equals some null value \\(\\theta_0\\) (e.g., testing if \\(\\mu = 0\\) or \\(\\mu_1 - \\mu_2 = 0\\)):\n\n\n\n\n\n\nThe P-Value / CI Duality\n\n\n\n\nIf the \\((1-\\alpha)\\) confidence interval excludes \\(\\theta_0\\), then \\(p &lt; \\alpha\\)\nIf the \\((1-\\alpha)\\) confidence interval includes \\(\\theta_0\\), then \\(p \\geq \\alpha\\)\n\nA 95% CI that doesn’t contain zero corresponds to p &lt; 0.05 for a two-tailed test.\n\n\nThis relationship makes sense when you think about it: the confidence interval represents the range of parameter values that are “compatible” with the data. If the null value falls outside this range, the data provide evidence against it (small p-value). If the null value falls inside, the data are consistent with it (large p-value).\n\n\nCode\n# Demonstrate the CI-pvalue relationship\nset.seed(42)\n\n# Three scenarios\nscenarios &lt;- list(\n  list(true_mean = 5, label = \"CI excludes 0\"),\n  list(true_mean = 2, label = \"CI barely excludes 0\"),\n  list(true_mean = 0.5, label = \"CI includes 0\")\n)\n\npar(mfrow = c(1, 3))\n\nfor (scenario in scenarios) {\n  sample_data &lt;- rnorm(30, mean = scenario$true_mean, sd = 5)\n  result &lt;- t.test(sample_data)\n\n  # Plot CI\n  ci &lt;- result$conf.int\n  mean_est &lt;- result$estimate\n\n  plot(1, mean_est, xlim = c(0.5, 1.5), ylim = c(-4, 12),\n       pch = 19, cex = 1.5, xaxt = \"n\", xlab = \"\",\n       ylab = \"Estimated Mean\",\n       main = paste0(scenario$label, \"\\np = \", round(result$p.value, 4)))\n\n  arrows(1, ci[1], 1, ci[2], angle = 90, code = 3, length = 0.1, lwd = 2)\n  abline(h = 0, col = \"red\", lty = 2, lwd = 2)\n}\n\n\n\n\n\n\n\n\nFigure 16.6: The relationship between confidence intervals and p-values: CIs that exclude zero correspond to small p-values\n\n\n\n\n\nThe beauty of confidence intervals is that they provide more information than p-values alone:\n\nDirection: You see whether the effect is positive or negative\nMagnitude: You see the estimated size of the effect\nPrecision: The width shows your uncertainty\nSignificance: Whether zero is included tells you if p &lt; 0.05\n\nThis is why many statisticians advocate for reporting confidence intervals as the primary summary, with p-values as secondary.\n\n\nStandardized Effect Sizes\nEffect sizes quantify the magnitude of an effect in a standardized way, allowing comparison across studies.\nCohen’s d measures the difference between two means in standard deviation units:\n\\[d = \\frac{\\bar{x}_1 - \\bar{x}_2}{s_{\\text{pooled}}}\\]\nwhere \\(s_{\\text{pooled}} = \\sqrt{\\frac{(n_1-1)s_1^2 + (n_2-1)s_2^2}{n_1 + n_2 - 2}}\\)\nCohen’s guidelines suggest \\(d = 0.2\\) is small, \\(d = 0.5\\) is medium, and \\(d = 0.8\\) is large, though these benchmarks should be interpreted in context (Cohen 1988).\n\n\nCode\n# Calculate Cohen's d\ngroup1 &lt;- c(23, 25, 28, 31, 27, 29)\ngroup2 &lt;- c(18, 20, 22, 19, 21, 23)\n\n# Pooled standard deviation\nn1 &lt;- length(group1)\nn2 &lt;- length(group2)\ns_pooled &lt;- sqrt(((n1-1)*var(group1) + (n2-1)*var(group2)) / (n1 + n2 - 2))\n\n# Cohen's d\ncohens_d &lt;- (mean(group1) - mean(group2)) / s_pooled\ncat(\"Cohen's d:\", round(cohens_d, 3), \"\\n\")\n\n\nCohen's d: 2.76 \n\n\nOther effect sizes include: - Pearson’s r: Correlation coefficient (-1 to 1) - \\(\\eta^2\\) (eta-squared): Proportion of variance explained in ANOVA - Odds ratio: Effect size for binary outcomes",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "chapters/11-hypothesis-testing.html#critiques-of-nhst",
    "href": "chapters/11-hypothesis-testing.html#critiques-of-nhst",
    "title": "16  Hypothesis Testing",
    "section": "16.10 Critiques of NHST",
    "text": "16.10 Critiques of NHST\nThe Null Hypothesis Significance Testing (NHST) framework, while widely used, has important limitations that researchers should understand.\n\n\n\n\n\n\nLimitations of P-Values\n\n\n\n\nP-values do not measure effect size: A tiny, meaningless effect can have p &lt; 0.001 with enough data\nP-values do not measure probability that \\(H_0\\) is true: This is a common misinterpretation\nThe 0.05 threshold is arbitrary: There is nothing magical about \\(\\alpha = 0.05\\)\nDichotomous thinking: Treating p = 0.049 and p = 0.051 as fundamentally different is misleading\nPublication bias: Studies with p &lt; 0.05 are more likely to be published, distorting the literature\n\n\n\n\nAlternatives and Complements to NHST\nSeveral approaches complement or provide alternatives to traditional null hypothesis significance testing. Confidence intervals offer richer information than p-values alone, communicating both the estimated effect size and the uncertainty surrounding it in a single, interpretable summary. Effect sizes quantify the practical magnitude of results in standardized units, allowing comparison across studies and helping distinguish statistical significance from practical importance. Bayesian methods reframe inference entirely, providing direct probability statements about hypotheses themselves rather than about the data conditional on a hypothesis. Finally, equivalence testing allows researchers to demonstrate that an effect is negligibly small—actively supporting the conclusion that groups are practically equivalent rather than merely failing to find a significant difference.\nThe American Statistical Association’s 2016 statement on p-values emphasizes that p-values should not be used in isolation and that scientific conclusions should not be based solely on whether a p-value crosses a threshold.",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "chapters/11-hypothesis-testing.html#example-null-distribution-via-randomization",
    "href": "chapters/11-hypothesis-testing.html#example-null-distribution-via-randomization",
    "title": "16  Hypothesis Testing",
    "section": "16.11 Example: Null Distribution via Randomization",
    "text": "16.11 Example: Null Distribution via Randomization\nWe can create empirical null distributions through randomization, providing an alternative to parametric assumptions.\n\n\nCode\n# Two groups to compare\nset.seed(56)\npop_1 &lt;- rnorm(n = 50, mean = 20.1, sd = 2)\npop_2 &lt;- rnorm(n = 50, mean = 19.3, sd = 2)\n\n# Observed t-statistic\nt_obs &lt;- t.test(x = pop_1, y = pop_2, alternative = \"greater\")$statistic\n\n# Create null distribution by randomization\npops_comb &lt;- c(pop_1, pop_2)\n\nt_rand &lt;- replicate(1000, {\n  pops_shuf &lt;- sample(pops_comb)\n  t.test(x = pops_shuf[1:50], y = pops_shuf[51:100], alternative = \"greater\")$statistic\n})\n\n# Plot null distribution\nhist(t_rand, breaks = 30, main = \"Randomization Null Distribution\",\n     xlab = \"t-statistic\", col = \"lightblue\")\nabline(v = t_obs, col = \"red\", lwd = 2)\n\n\n\n\n\n\n\n\nFigure 16.7: Randomization creates an empirical null distribution; the red line shows the observed test statistic\n\n\n\n\n\n\n\nCode\n# Calculate p-value\np_value &lt;- sum(t_rand &gt;= t_obs) / 1000\ncat(\"Observed t:\", round(t_obs, 3), \"\\n\")\n\n\nObserved t: 2.211 \n\n\nCode\ncat(\"P-value:\", p_value, \"\\n\")\n\n\nP-value: 0.016 \n\n\n\n\n\n\n\n\nFigure 16.8: Randomization tests provide a distribution-free alternative to parametric tests",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "chapters/11-hypothesis-testing.html#summary",
    "href": "chapters/11-hypothesis-testing.html#summary",
    "title": "16  Hypothesis Testing",
    "section": "16.12 Summary",
    "text": "16.12 Summary\nHypothesis testing provides a framework for using data to evaluate claims about populations. Key concepts include:\n\nNull and alternative hypotheses formalize competing claims\nType I errors (false positives) and Type II errors (false negatives) represent the two ways we can be wrong\nP-values quantify evidence against the null hypothesis\nSignificance levels set thresholds for decision-making\nMultiple testing requires adjustment to control error rates\nStatistical significance does not imply practical importance\n\nIn the following chapters, we apply this framework to specific tests: t-tests for comparing means, chi-squared tests for categorical data, and nonparametric alternatives when assumptions are violated.",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "chapters/11-hypothesis-testing.html#exercises",
    "href": "chapters/11-hypothesis-testing.html#exercises",
    "title": "16  Hypothesis Testing",
    "section": "16.13 Exercises",
    "text": "16.13 Exercises\n\n\n\n\n\n\nExercise H.1: Understanding Type I and Type II Errors\n\n\n\nConsider a diagnostic test for a rare disease that affects 1% of the population. The test has a false positive rate (Type I error) of 5% and a false negative rate (Type II error) of 10%.\n\nDefine the null and alternative hypotheses for this diagnostic test\nWhat does a Type I error mean in this context? What are the consequences?\nWhat does a Type II error mean in this context? What are the consequences?\nIf 10,000 people are tested, how many false positives and false negatives would you expect?\nOf the people who test positive, what proportion actually have the disease? (This is related to the positive predictive value)\n\n\n\nCode\n# Your code here\n\n\n\n\n\n\n\n\n\n\nExercise H.2: P-Values and Interpretation\n\n\n\nFor each statement below, indicate whether it is a correct interpretation of p = 0.03:\n\nThe probability that the null hypothesis is true is 0.03\nThe probability of observing data this extreme or more extreme, given that the null hypothesis is true, is 0.03\nThe probability that the result occurred by chance is 0.03\nThere is a 97% probability that the alternative hypothesis is true\nIf we repeated this experiment many times, we would expect to see results this extreme about 3% of the time if the null hypothesis were true\n\nFor the correct interpretation(s), explain why they are correct. For the incorrect ones, explain the mistake.\n\n\n\n\n\n\n\n\nExercise H.3: Power Analysis\n\n\n\nYou are planning a study to compare the effectiveness of two fertilizers on plant growth. Based on pilot data, you estimate the standard deviation of plant heights to be 12 cm. You want to detect a difference of 8 cm between treatments with 80% power at α = 0.05.\n\nCalculate the required sample size per group using the pwr.t.test() function\nCreate a power curve showing how power changes with sample size for this effect size\nHow much would the required sample size change if you wanted 90% power instead of 80%?\nIf your budget only allows for 20 plants per group, what is the smallest effect size (in Cohen’s d units) you could detect with 80% power?\nDiscuss the trade-offs between sample size, power, and minimum detectable effect size\n\n\n\nCode\n# Your code here\nlibrary(pwr)\n\n\n\n\n\n\n\n\n\n\nExercise H.4: Multiple Testing Correction\n\n\n\nA genomics study tests 20,000 genes for differential expression between two conditions, using α = 0.05 for each test.\n\nIf all null hypotheses are true (no genes are differentially expressed), how many false positives (Type I errors) would you expect on average?\nApply the Bonferroni correction. What significance threshold should be used for each individual test?\nWhy might the Bonferroni correction be too conservative in this context?\nSimulate a scenario where 100 genes are truly differentially expressed (with effect size d = 0.8, n = 10 per group) and 19,900 are not. How many true positives and false positives do you get with:\n\nNo correction (α = 0.05)\nBonferroni correction\nUse the p.adjust() function with method “fdr” for False Discovery Rate control\n\n\n\n\nCode\n# Your code here\n\n\n\n\n\n\n\n\n\n\nExercise H.5: Confidence Intervals vs. P-Values\n\n\n\nYou conduct an experiment measuring the effect of a drug on blood glucose levels. You measure 15 patients before and after treatment and find: - Mean difference: -12.3 mg/dL - 95% CI: [-22.1, -2.5] - p-value: 0.018\n\nBased on the confidence interval alone, what can you conclude about the null hypothesis that the drug has no effect?\nExplain the relationship between the confidence interval and the p-value in this example\nIf you calculated a 99% confidence interval instead, would it include zero? Would the p-value be less than 0.01? Explain your reasoning.\nGenerate simulated data consistent with these results and verify the relationship between the CI and p-value\n\n\n\nCode\n# Your code here\n\n\n\n\n\n\n\n\n\n\nExercise H.6: Randomization Test\n\n\n\nYou have two groups of seedlings grown under different light conditions:\ngroup_A &lt;- c(12.3, 14.1, 13.5, 15.2, 13.8, 14.9, 13.2, 14.5)\ngroup_B &lt;- c(10.8, 11.5, 12.1, 11.2, 10.9, 12.4, 11.8, 11.3)\n\nCalculate the observed difference in means (group A - group B)\nPerform a randomization test with 10,000 permutations to generate a null distribution\nCalculate the p-value from your randomization test\nCompare this to the p-value from a two-sample t-test\nCreate a histogram of the null distribution with a vertical line showing the observed difference\nUnder what circumstances might the randomization test be preferable to the t-test?\n\n\n\nCode\n# Your code here",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "chapters/11-hypothesis-testing.html#additional-resources",
    "href": "chapters/11-hypothesis-testing.html#additional-resources",
    "title": "16  Hypothesis Testing",
    "section": "16.14 Additional Resources",
    "text": "16.14 Additional Resources\n\nCohen (1988) - The classic reference on statistical power analysis\nLogan (2010) - Comprehensive treatment of hypothesis testing in biological contexts\n\n\n\n\n\n\n\nCohen, Jacob. 1988. Statistical Power Analysis for the Behavioral Sciences. 2nd ed. Lawrence Erlbaum Associates.\n\n\nLogan, Murray. 2010. Biostatistical Design and Analysis Using r. Wiley-Blackwell.",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "chapters/12-t-tests.html",
    "href": "chapters/12-t-tests.html",
    "title": "17  T-Tests",
    "section": "",
    "text": "17.1 Comparing Means\nOne of the most common questions in data analysis is whether two groups differ. Is the mean expression level different between treatment and control? Does the new material have different strength than the standard? Do patients on drug A have different outcomes than patients on drug B?\nThe t-test is the classic method for comparing means. It compares the observed difference between groups to the variability expected by chance, producing a test statistic that follows a t-distribution under the null hypothesis of no difference.",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>T-Tests</span>"
    ]
  },
  {
    "objectID": "chapters/12-t-tests.html#the-t-distribution",
    "href": "chapters/12-t-tests.html#the-t-distribution",
    "title": "17  T-Tests",
    "section": "17.2 The T-Distribution",
    "text": "17.2 The T-Distribution\nThe t-distribution, introduced by William Sealy Gosset writing under the pseudonym “Student” (Student 1908), resembles the normal distribution but has heavier tails. This accounts for the extra uncertainty that comes from estimating the population standard deviation from sample data.\nThe t-distribution is characterized by its degrees of freedom (df). As df increases, the t-distribution approaches the normal distribution. For small samples, the heavier tails mean that extreme values are more likely, leading to wider confidence intervals and more conservative tests.\n\n\nCode\n# Compare t-distributions with different df\nx &lt;- seq(-4, 4, length.out = 200)\nplot(x, dnorm(x), type = \"l\", lwd = 2, col = \"black\",\n     xlab = \"x\", ylab = \"Density\",\n     main = \"T-distributions vs. Normal\")\nlines(x, dt(x, df = 3), lwd = 2, col = \"red\")\nlines(x, dt(x, df = 10), lwd = 2, col = \"blue\")\nlegend(\"topright\",\n       legend = c(\"Normal\", \"t (df=3)\", \"t (df=10)\"),\n       col = c(\"black\", \"red\", \"blue\"), lwd = 2)\n\n\n\n\n\n\n\n\nFigure 17.1: The t-distribution has heavier tails than the normal, especially with low degrees of freedom\n\n\n\n\n\n\nWhy Heavier Tails Matter in Practice\nThe heavier tails of the t-distribution have real practical consequences. When you estimate the standard deviation from a small sample, you might underestimate or overestimate the true value. The t-distribution accounts for this uncertainty by assigning more probability to extreme values.\nConsider this concrete example: suppose you’re estimating voter support from a poll of 25 likely voters. With the true population proportion unknown and estimated from the sample, how wide should your confidence interval be?\n\n\nCode\n# Demonstrate the practical difference between normal and t-based intervals\nset.seed(2016)\n\n# Simulate: true support is 48.5%, poll 25 people\ntrue_support &lt;- 0.485\nn_poll &lt;- 25\n\n# One poll result\npoll_result &lt;- rbinom(1, n_poll, true_support) / n_poll\npoll_se &lt;- sqrt(poll_result * (1 - poll_result) / n_poll)\n\n# Compare critical values\nz_crit &lt;- qnorm(0.975)      # Normal: 1.96\nt_crit &lt;- qt(0.975, df = n_poll - 1)  # t with 24 df: 2.06\n\n# Calculate intervals\nnormal_ci &lt;- c(poll_result - z_crit * poll_se, poll_result + z_crit * poll_se)\nt_ci &lt;- c(poll_result - t_crit * poll_se, poll_result + t_crit * poll_se)\n\ncat(\"Poll result:\", round(poll_result * 100, 1), \"%\\n\")\n\n\nPoll result: 40 %\n\n\nCode\ncat(\"Standard error:\", round(poll_se * 100, 1), \"%\\n\")\n\n\nStandard error: 9.8 %\n\n\nCode\ncat(\"\\nNormal-based 95% CI: [\", round(normal_ci[1]*100, 1), \"%, \",\n    round(normal_ci[2]*100, 1), \"%]\\n\")\n\n\n\nNormal-based 95% CI: [ 20.8 %,  59.2 %]\n\n\nCode\ncat(\"t-based 95% CI:      [\", round(t_ci[1]*100, 1), \"%, \",\n    round(t_ci[2]*100, 1), \"%]\\n\")\n\n\nt-based 95% CI:      [ 19.8 %,  60.2 %]\n\n\nCode\ncat(\"\\nDifference in width:\", round((t_ci[2] - t_ci[1] - (normal_ci[2] - normal_ci[1]))*100, 2),\n    \"percentage points\\n\")\n\n\n\nDifference in width: 2.04 percentage points\n\n\nWith only 25 observations, the t-distribution gives a critical value of about 2.06 instead of 1.96. This ~5% wider interval provides better coverage when the sample standard deviation might deviate substantially from the population value.\nThe difference matters most in the tails. For extreme values (like being 2.5+ standard errors away from the mean), the t-distribution assigns noticeably more probability:\n\n\nCode\n# Probability of being more than 2.5 SE from the mean\nprob_extreme_normal &lt;- 2 * pnorm(-2.5)\nprob_extreme_t &lt;- 2 * pt(-2.5, df = 24)\n\ncat(\"P(|Z| &gt; 2.5) with normal distribution:\", round(prob_extreme_normal, 4), \"\\n\")\n\n\nP(|Z| &gt; 2.5) with normal distribution: 0.0124 \n\n\nCode\ncat(\"P(|T| &gt; 2.5) with t(df=24):\", round(prob_extreme_t, 4), \"\\n\")\n\n\nP(|T| &gt; 2.5) with t(df=24): 0.0197 \n\n\nCode\ncat(\"The t-distribution gives\", round(prob_extreme_t/prob_extreme_normal, 1),\n    \"times higher probability to extreme values\\n\")\n\n\nThe t-distribution gives 1.6 times higher probability to extreme values\n\n\nThis is why using the normal distribution instead of the t-distribution for small samples leads to confidence intervals that are too narrow and p-values that are too small—both resulting in overconfident conclusions.",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>T-Tests</span>"
    ]
  },
  {
    "objectID": "chapters/12-t-tests.html#one-sample-t-test",
    "href": "chapters/12-t-tests.html#one-sample-t-test",
    "title": "17  T-Tests",
    "section": "17.3 One-Sample T-Test",
    "text": "17.3 One-Sample T-Test\nThe one-sample t-test compares a sample mean to a hypothesized population value. The null hypothesis is that the population mean equals the specified value: \\(H_0: \\mu = \\mu_0\\).\nThe test statistic is:\n\\[t = \\frac{\\bar{x} - \\mu_0}{s / \\sqrt{n}}\\]\nThis is the difference between the sample mean and hypothesized value, divided by the standard error of the mean. Under the null hypothesis, this statistic follows a t-distribution with \\(n-1\\) degrees of freedom.\n\n\nCode\n# One-sample t-test example\n# Does this sample come from a population with mean = 100?\nset.seed(42)\nsample_data &lt;- rnorm(25, mean = 105, sd = 15)\n\nt.test(sample_data, mu = 100)\n\n\n\n    One Sample t-test\n\ndata:  sample_data\nt = 1.9936, df = 24, p-value = 0.05768\nalternative hypothesis: true mean is not equal to 100\n95 percent confidence interval:\n  99.72443 115.90166\nsample estimates:\nmean of x \n  107.813 \n\n\nThe output shows the t-statistic, degrees of freedom, p-value, confidence interval, and sample mean. The small p-value indicates evidence that the true mean differs from 100.",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>T-Tests</span>"
    ]
  },
  {
    "objectID": "chapters/12-t-tests.html#two-sample-t-test",
    "href": "chapters/12-t-tests.html#two-sample-t-test",
    "title": "17  T-Tests",
    "section": "17.4 Two-Sample T-Test",
    "text": "17.4 Two-Sample T-Test\nThe two-sample (independent samples) t-test compares means from two independent groups. The null hypothesis is that the population means are equal: \\(H_0: \\mu_1 = \\mu_2\\).\nThe test assumes: - Independence of observations within and between groups - Normally distributed populations (or large samples) - Equal variances in both groups (for the standard version)\n\n\nCode\n# Two-sample t-test example\nset.seed(518)\ntreatment &lt;- rnorm(n = 30, mean = 12, sd = 3)\ncontrol &lt;- rnorm(n = 30, mean = 10, sd = 3)\n\n# Visualize the data\npar(mfrow = c(1, 2))\nboxplot(treatment, control, names = c(\"Treatment\", \"Control\"),\n        col = c(\"lightblue\", \"lightgreen\"), main = \"Boxplot\")\n\n# Combined histogram\nhist(treatment, col = rgb(0, 0, 1, 0.5), xlim = c(0, 20),\n     main = \"Histograms\", xlab = \"Value\")\nhist(control, col = rgb(0, 1, 0, 0.5), add = TRUE)\nlegend(\"topright\", legend = c(\"Treatment\", \"Control\"),\n       fill = c(rgb(0, 0, 1, 0.5), rgb(0, 1, 0, 0.5)))\n\n\n\n\n\n\n\n\nFigure 17.2: Visualization of two groups before performing a two-sample t-test\n\n\n\n\n\n\n\nCode\n# Perform the t-test\nt.test(treatment, control)\n\n\n\n    Welch Two Sample t-test\n\ndata:  treatment and control\nt = 1.3224, df = 57.98, p-value = 0.1912\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -0.5256045  2.5718411\nsample estimates:\nmean of x mean of y \n 11.08437  10.06125",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>T-Tests</span>"
    ]
  },
  {
    "objectID": "chapters/12-t-tests.html#welchs-t-test",
    "href": "chapters/12-t-tests.html#welchs-t-test",
    "title": "17  T-Tests",
    "section": "17.5 Welch’s T-Test",
    "text": "17.5 Welch’s T-Test\nThe classic two-sample t-test assumes equal variances. When this assumption is violated, Welch’s t-test provides a better alternative. It adjusts the degrees of freedom to account for unequal variances.\nR’s t.test() function uses Welch’s test by default. To use the equal-variance version, set var.equal = TRUE.\n\n\nCode\n# When variances are unequal\nset.seed(42)\ngroup1 &lt;- rnorm(30, mean = 50, sd = 5)\ngroup2 &lt;- rnorm(30, mean = 52, sd = 15)\n\n# Welch's test (default)\nt.test(group1, group2)\n\n\n\n    Welch Two Sample t-test\n\ndata:  group1 and group2\nt = 0.055423, df = 37.98, p-value = 0.9561\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -6.095093  6.438216\nsample estimates:\nmean of x mean of y \n 50.34293  50.17137 \n\n\nCode\n# Equal variance assumed\nt.test(group1, group2, var.equal = TRUE)\n\n\n\n    Two Sample t-test\n\ndata:  group1 and group2\nt = 0.055423, df = 58, p-value = 0.956\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -6.024786  6.367910\nsample estimates:\nmean of x mean of y \n 50.34293  50.17137",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>T-Tests</span>"
    ]
  },
  {
    "objectID": "chapters/12-t-tests.html#paired-t-test",
    "href": "chapters/12-t-tests.html#paired-t-test",
    "title": "17  T-Tests",
    "section": "17.6 Paired T-Test",
    "text": "17.6 Paired T-Test\nWhen observations in two groups are naturally paired—the same subjects measured twice, matched pairs, or before-and-after measurements—the paired t-test is more appropriate. It tests whether the mean difference within pairs is zero.\nThe paired t-test is more powerful than the two-sample test when pairs are positively correlated, because it removes between-subject variability.\n\n\nCode\n# Paired t-test example: before and after treatment\nset.seed(123)\nn &lt;- 20\nbefore &lt;- rnorm(n, mean = 100, sd = 15)\n# After measurements are correlated with before\nafter &lt;- before + rnorm(n, mean = 5, sd = 5)\n\n# Paired test (correct for this data)\nt.test(after, before, paired = TRUE)\n\n\n\n    Paired t-test\n\ndata:  after and before\nt = 5.1123, df = 19, p-value = 6.19e-05\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n 2.801598 6.685830\nsample estimates:\nmean difference \n       4.743714 \n\n\nCode\n# Compare to unpaired (less power)\nt.test(after, before, paired = FALSE)\n\n\n\n    Welch Two Sample t-test\n\ndata:  after and before\nt = 1.0209, df = 37.992, p-value = 0.3138\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -4.663231 14.150660\nsample estimates:\nmean of x mean of y \n 106.8681  102.1244 \n\n\nNotice that the paired test produces a smaller p-value because it accounts for the correlation between measurements on the same subject.",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>T-Tests</span>"
    ]
  },
  {
    "objectID": "chapters/12-t-tests.html#one-tailed-vs.-two-tailed-tests",
    "href": "chapters/12-t-tests.html#one-tailed-vs.-two-tailed-tests",
    "title": "17  T-Tests",
    "section": "17.7 One-Tailed vs. Two-Tailed Tests",
    "text": "17.7 One-Tailed vs. Two-Tailed Tests\nBy default, t.test() performs a two-tailed test. For a one-tailed test, specify the alternative hypothesis:\n\n\nCode\n# Two-tailed (default): H_A: treatment ≠ control\nt.test(treatment, control, alternative = \"two.sided\")$p.value\n\n\n[1] 0.1912327\n\n\nCode\n# One-tailed: H_A: treatment &gt; control\nt.test(treatment, control, alternative = \"greater\")$p.value\n\n\n[1] 0.09561633\n\n\nCode\n# One-tailed: H_A: treatment &lt; control\nt.test(treatment, control, alternative = \"less\")$p.value\n\n\n[1] 0.9043837\n\n\nUse one-tailed tests only when you have a strong prior reason to expect an effect in a specific direction and would not act on an effect in the opposite direction.",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>T-Tests</span>"
    ]
  },
  {
    "objectID": "chapters/12-t-tests.html#checking-assumptions",
    "href": "chapters/12-t-tests.html#checking-assumptions",
    "title": "17  T-Tests",
    "section": "17.8 Checking Assumptions",
    "text": "17.8 Checking Assumptions\nT-tests assume normally distributed data (or large samples) and, for the standard two-sample test, equal variances. Check these assumptions before interpreting results.\nNormality: Use histograms, Q-Q plots, or formal tests like Shapiro-Wilk.\n\n\nCode\n# Check normality with Q-Q plot\nqqnorm(treatment)\nqqline(treatment, col = \"red\")\n\n\n\n\n\n\n\n\nFigure 17.3: Q-Q plot for assessing normality: points following the line suggest approximate normality\n\n\n\n\n\n\n\nCode\n# Shapiro-Wilk test for normality\nshapiro.test(treatment)\n\n\n\n    Shapiro-Wilk normality test\n\ndata:  treatment\nW = 0.9115, p-value = 0.01624\n\n\nA non-significant Shapiro-Wilk test suggests the data are consistent with normality. However, this test has low power for small samples and may reject normality for trivial deviations with large samples.\nEqual variances: Compare standard deviations or use Levene’s test.\n\n\nCode\n# Compare standard deviations\nsd(treatment)\n\n\n[1] 3.024138\n\n\nCode\nsd(control)\n\n\n[1] 2.968592\n\n\nCode\n# Levene's test (from car package)\n# car::leveneTest(c(treatment, control), \n#                 factor(rep(c(\"treatment\", \"control\"), each = 30)))",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>T-Tests</span>"
    ]
  },
  {
    "objectID": "chapters/12-t-tests.html#effect-size-cohens-d",
    "href": "chapters/12-t-tests.html#effect-size-cohens-d",
    "title": "17  T-Tests",
    "section": "17.9 Effect Size: Cohen’s d",
    "text": "17.9 Effect Size: Cohen’s d\nStatistical significance does not tell you how large an effect is. Cohen’s d measures effect size as the standardized difference between means:\n\\[d = \\frac{\\bar{x}_1 - \\bar{x}_2}{s_{pooled}}\\]\nwhere \\(s_{pooled}\\) is the pooled standard deviation.\nConventional interpretations: \\(|d| = 0.2\\) is small, \\(|d| = 0.5\\) is medium, \\(|d| = 0.8\\) is large. However, context matters—a small d might be practically important in some fields.\n\n\nCode\n# Calculate Cohen's d\nmean_diff &lt;- mean(treatment) - mean(control)\ns_pooled &lt;- sqrt((var(treatment) + var(control)) / 2)\ncohens_d &lt;- mean_diff / s_pooled\n\ncat(\"Cohen's d:\", round(cohens_d, 2), \"\\n\")\n\n\nCohen's d: 0.34",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>T-Tests</span>"
    ]
  },
  {
    "objectID": "chapters/12-t-tests.html#practical-example",
    "href": "chapters/12-t-tests.html#practical-example",
    "title": "17  T-Tests",
    "section": "17.10 Practical Example",
    "text": "17.10 Practical Example\nLet’s work through a complete analysis comparing two groups:\n\n\nCode\n# Simulated drug trial data\nset.seed(999)\ndrug &lt;- rnorm(40, mean = 75, sd = 12)\nplacebo &lt;- rnorm(40, mean = 70, sd = 12)\n\n# Step 1: Visualize\npar(mfrow = c(2, 2))\nboxplot(drug, placebo, names = c(\"Drug\", \"Placebo\"),\n        col = c(\"coral\", \"lightblue\"), main = \"Response by Group\")\n\n# Step 2: Check normality\nqqnorm(drug, main = \"Q-Q Plot: Drug\")\nqqline(drug, col = \"red\")\nqqnorm(placebo, main = \"Q-Q Plot: Placebo\")\nqqline(placebo, col = \"red\")\n\n# Combined histogram\nhist(drug, col = rgb(1, 0.5, 0.5, 0.5), xlim = c(40, 110),\n     main = \"Distribution Comparison\", xlab = \"Response\")\nhist(placebo, col = rgb(0.5, 0.5, 1, 0.5), add = TRUE)\n\n\n\n\n\n\n\n\nFigure 17.4: Complete analysis workflow: visualization and assumption checking before the t-test\n\n\n\n\n\n\n\nCode\n# Step 3: Perform t-test\nresult &lt;- t.test(drug, placebo)\nprint(result)\n\n\n\n    Welch Two Sample t-test\n\ndata:  drug and placebo\nt = 1.2147, df = 75.923, p-value = 0.2282\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -1.990367  8.213525\nsample estimates:\nmean of x mean of y \n 72.26982  69.15824 \n\n\nCode\n# Step 4: Calculate effect size\ncohens_d &lt;- (mean(drug) - mean(placebo)) / \n            sqrt((var(drug) + var(placebo)) / 2)\ncat(\"\\nCohen's d:\", round(cohens_d, 2), \"\\n\")\n\n\n\nCohen's d: 0.27 \n\n\nThe t-test shows a significant difference (p &lt; 0.05), and Cohen’s d indicates a medium effect size. We can conclude that the drug group shows higher response than the placebo group, with the mean difference being about 0.4 standard deviations.",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>T-Tests</span>"
    ]
  },
  {
    "objectID": "chapters/12-t-tests.html#randomization-tests-as-an-alternative",
    "href": "chapters/12-t-tests.html#randomization-tests-as-an-alternative",
    "title": "17  T-Tests",
    "section": "17.11 Randomization Tests as an Alternative",
    "text": "17.11 Randomization Tests as an Alternative\nWhen normality assumptions are questionable and sample sizes are small, randomization (permutation) tests provide a non-parametric alternative to the t-test. The logic is elegant: if there is no difference between groups, then the group labels are arbitrary and could be shuffled without affecting the distribution of the test statistic.\n\n\nCode\n# Randomization test example\nset.seed(42)\ngroup_A &lt;- c(23, 25, 28, 31, 35, 29)\ngroup_B &lt;- c(18, 20, 22, 19, 21, 23)\n\n# Observed difference\nobs_diff &lt;- mean(group_A) - mean(group_B)\n\n# Combine all observations\nall_data &lt;- c(group_A, group_B)\nn_A &lt;- length(group_A)\nn_B &lt;- length(group_B)\n\n# Generate null distribution by permutation\nn_perms &lt;- 10000\nperm_diffs &lt;- numeric(n_perms)\n\nfor (i in 1:n_perms) {\n  shuffled &lt;- sample(all_data)\n  perm_diffs[i] &lt;- mean(shuffled[1:n_A]) - mean(shuffled[(n_A+1):(n_A+n_B)])\n}\n\n# Plot null distribution\nhist(perm_diffs, breaks = 50, col = \"lightblue\",\n     main = \"Randomization Null Distribution\",\n     xlab = \"Difference in Means\")\nabline(v = obs_diff, col = \"red\", lwd = 2)\nabline(v = -obs_diff, col = \"red\", lwd = 2, lty = 2)\n\n# Two-tailed p-value\np_value &lt;- mean(abs(perm_diffs) &gt;= abs(obs_diff))\ncat(\"Observed difference:\", round(obs_diff, 2), \"\\n\")\n\n\nObserved difference: 8 \n\n\nCode\ncat(\"Permutation p-value:\", p_value, \"\\n\")\n\n\nPermutation p-value: 0.0039 \n\n\n\n\n\n\n\n\nFigure 17.5: Permutation test null distribution with observed difference (red line) for comparison\n\n\n\n\n\nThe randomization test makes no assumptions about the underlying distribution—it only assumes that observations are exchangeable under the null hypothesis. This makes it robust to non-normality and outliers.\n\n\n\n\n\n\nWhen to Use Randomization Tests\n\n\n\n\nSample sizes are small (n &lt; 20 per group)\nData are clearly non-normal or contain outliers\nYou want to avoid distributional assumptions\nAs a sensitivity analysis to complement parametric results",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>T-Tests</span>"
    ]
  },
  {
    "objectID": "chapters/12-t-tests.html#choosing-the-right-t-test",
    "href": "chapters/12-t-tests.html#choosing-the-right-t-test",
    "title": "17  T-Tests",
    "section": "17.12 Choosing the Right T-Test",
    "text": "17.12 Choosing the Right T-Test\n\n\n\n\n\n\n\n\nScenario\nTest\nR Function\n\n\n\n\nCompare sample mean to known value\nOne-sample\nt.test(x, mu = value)\n\n\nCompare two independent groups\nTwo-sample (Welch’s)\nt.test(x, y)\n\n\nCompare two independent groups (equal variance)\nTwo-sample (Student’s)\nt.test(x, y, var.equal = TRUE)\n\n\nCompare paired measurements\nPaired\nt.test(x, y, paired = TRUE)\n\n\n\nDecision guidelines:\n\nIf comparing to a fixed, known value: one-sample t-test\nIf observations in groups are naturally paired: paired t-test\nIf groups are independent with potentially unequal variances: Welch’s t-test (the default)\nIf groups are independent and you have strong evidence of equal variances: Student’s t-test\n\nWhen in doubt, use Welch’s t-test—it performs nearly as well as Student’s t-test when variances are equal and much better when they are not.",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>T-Tests</span>"
    ]
  },
  {
    "objectID": "chapters/12-t-tests.html#summary",
    "href": "chapters/12-t-tests.html#summary",
    "title": "17  T-Tests",
    "section": "17.13 Summary",
    "text": "17.13 Summary\nThe t-test family provides essential tools for comparing means:\n\nOne-sample tests compare a sample to a hypothesized value\nTwo-sample tests compare independent groups\nPaired tests compare matched or repeated measurements\nWelch’s version handles unequal variances (recommended default)\nRandomization tests provide a distribution-free alternative\n\nAlways visualize your data, check assumptions, and report effect sizes alongside p-values. A statistically significant result is only meaningful if the underlying assumptions are reasonable and the effect size is practically relevant.",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>T-Tests</span>"
    ]
  },
  {
    "objectID": "chapters/12-t-tests.html#practice-exercises",
    "href": "chapters/12-t-tests.html#practice-exercises",
    "title": "17  T-Tests",
    "section": "17.14 Practice Exercises",
    "text": "17.14 Practice Exercises\n\nExercise H.1: One-Sample t-test\n\nGenerate a sample of 30 observations from a normal distribution with mean 105 and SD 15\nTest whether the mean differs significantly from 100\nInterpret the p-value and confidence interval\nWhat happens to the p-value when you increase the sample size?\n\n\n\nCode\nset.seed(42)\nsample_data &lt;- rnorm(30, mean = 105, sd = 15)\nt.test(sample_data, mu = 100)\n\n\n\n\nExercise H.2: Two-Sample t-test\nCreate a dummy dataset with one continuous and one categorical variable:\n\nDraw samples of 100 observations from two normal distributions with slightly different means but equal standard deviations\nPerform a two-sample t-test\nVisualize the data with a boxplot\nRepeat with sample sizes of 10, 100, and 1000—how does sample size affect the results?\nWhat happens when you make the means more different?\n\n\n\nCode\nset.seed(42)\ngroup_a &lt;- rnorm(100, mean = 10, sd = 2)\ngroup_b &lt;- rnorm(100, mean = 11, sd = 2)\n\n# Combine into data frame\ndata &lt;- data.frame(\n  value = c(group_a, group_b),\n  group = rep(c(\"A\", \"B\"), each = 100)\n)\n\n# t-test\nt.test(value ~ group, data = data)\n\n# Visualization\nboxplot(value ~ group, data = data)\n\n\n\n\nExercise H.3: Chi-Square Test for Hardy-Weinberg Equilibrium\nTest whether a population is in Hardy-Weinberg equilibrium:\n\n\nCode\n# Observed genotype counts\nAA_counts &lt;- 50\nAa_counts &lt;- 40\naa_counts &lt;- 10\n\n# Calculate allele frequencies\ntotal &lt;- AA_counts + Aa_counts + aa_counts\np &lt;- (2*AA_counts + Aa_counts) / (2*total)\nq &lt;- 1 - p\n\n# Expected counts under HWE\nexpected &lt;- c(p^2, 2*p*q, q^2) * total\n\n# Chi-square test\nobserved &lt;- c(AA_counts, Aa_counts, aa_counts)\nchisq.test(observed, p = c(p^2, 2*p*q, q^2))\n\n\n\n    Chi-squared test for given probabilities\n\ndata:  observed\nX-squared = 0.22676, df = 2, p-value = 0.8928\n\n\n\nModify the observed counts and see how it affects the test result\nWhat genotype frequencies would indicate strong departure from HWE?\n\n\n\nExercise H.4: Effect Size and Power\n\nUsing the two-sample t-test from Exercise H.2, calculate Cohen’s d effect size\nHow does effect size change when you increase the difference between means?\nHow does effect size change when you increase the standard deviation?",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>T-Tests</span>"
    ]
  },
  {
    "objectID": "chapters/12-t-tests.html#additional-resources",
    "href": "chapters/12-t-tests.html#additional-resources",
    "title": "17  T-Tests",
    "section": "17.15 Additional Resources",
    "text": "17.15 Additional Resources\n\nLogan (2010) - Detailed coverage of t-tests with biological examples\nIrizarry (2019) - Excellent treatment of randomization and permutation methods\n\n\n\n\n\n\n\nIrizarry, Rafael A. 2019. Introduction to Data Science. Self-published. https://rafalab.github.io/dsbook/.\n\n\nLogan, Murray. 2010. Biostatistical Design and Analysis Using r. Wiley-Blackwell.\n\n\nStudent. 1908. “The Probable Error of a Mean.” Biometrika 6 (1): 1–25.",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>T-Tests</span>"
    ]
  },
  {
    "objectID": "chapters/13-nonparametric-tests.html",
    "href": "chapters/13-nonparametric-tests.html",
    "title": "18  Nonparametric Tests",
    "section": "",
    "text": "18.1 When Assumptions Fail\nParametric tests like the t-test make assumptions about the underlying data distribution—typically that data are normally distributed with equal variances across groups. When these assumptions are violated, the tests may give misleading results. Nonparametric tests provide alternatives that make fewer assumptions about the data.\nNonparametric methods are sometimes called distribution-free methods because they do not assume a specific probability distribution. Instead, they typically work with ranks or signs of data rather than the raw values. This makes them robust to outliers and applicable to ordinal data where parametric methods would be inappropriate.",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Nonparametric Tests</span>"
    ]
  },
  {
    "objectID": "chapters/13-nonparametric-tests.html#the-mann-whitney-u-test",
    "href": "chapters/13-nonparametric-tests.html#the-mann-whitney-u-test",
    "title": "18  Nonparametric Tests",
    "section": "18.2 The Mann-Whitney U Test",
    "text": "18.2 The Mann-Whitney U Test\nThe Mann-Whitney U test (Mann and Whitney 1947) (also called the Wilcoxon rank-sum test) is the nonparametric equivalent of the two-sample t-test. It tests whether two independent groups tend to have different values, based on comparing the ranks of observations rather than the observations themselves.\nThe null hypothesis is that the distributions of the two groups are identical. The alternative is that one group tends to have larger values than the other.\n\n\nCode\n# Generate data with non-normal distributions\nset.seed(518)\ngroup1 &lt;- sample(rnorm(n = 10000, mean = 2, sd = 0.5), size = 100)\ngroup2 &lt;- sample(rnorm(n = 10000, mean = 5, sd = 1.5), size = 100)\n\n# Mann-Whitney U test\nwilcox.test(group1, group2)\n\n\n\n    Wilcoxon rank sum test with continuity correction\n\ndata:  group1 and group2\nW = 440, p-value &lt; 2.2e-16\nalternative hypothesis: true location shift is not equal to 0\n\n\nThe test works by combining all observations, ranking them, and comparing the sum of ranks in each group. If one group tends to have higher values, its rank sum will be larger than expected by chance.",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Nonparametric Tests</span>"
    ]
  },
  {
    "objectID": "chapters/13-nonparametric-tests.html#wilcoxon-signed-rank-test",
    "href": "chapters/13-nonparametric-tests.html#wilcoxon-signed-rank-test",
    "title": "18  Nonparametric Tests",
    "section": "18.3 Wilcoxon Signed-Rank Test",
    "text": "18.3 Wilcoxon Signed-Rank Test\nFor paired data, the Wilcoxon signed-rank test (Wilcoxon 1945) is the nonparametric alternative to the paired t-test. It tests whether the median difference between pairs is zero.\n\n\nCode\n# Paired data example\nset.seed(123)\nbefore &lt;- rnorm(20, mean = 100, sd = 15)\nafter &lt;- before + rexp(20, rate = 0.2)  # Skewed improvement\n\nwilcox.test(after, before, paired = TRUE)\n\n\n\n    Wilcoxon signed rank exact test\n\ndata:  after and before\nV = 210, p-value = 1.907e-06\nalternative hypothesis: true location shift is not equal to 0\n\n\nThe test calculates the differences between pairs, ranks their absolute values, and considers the signs of the differences. Under the null hypothesis, positive and negative differences should be equally likely and of similar magnitude.",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Nonparametric Tests</span>"
    ]
  },
  {
    "objectID": "chapters/13-nonparametric-tests.html#kruskal-wallis-test",
    "href": "chapters/13-nonparametric-tests.html#kruskal-wallis-test",
    "title": "18  Nonparametric Tests",
    "section": "18.4 Kruskal-Wallis Test",
    "text": "18.4 Kruskal-Wallis Test\nThe Kruskal-Wallis test extends the Mann-Whitney U test to more than two groups, serving as a nonparametric alternative to one-way ANOVA. It tests whether at least one group tends to have different values from the others.\n\n\nCode\n# Example with three groups\nset.seed(42)\ndata &lt;- data.frame(\n  value = c(rexp(30, 0.1), rexp(30, 0.15), rexp(30, 0.2)),\n  group = factor(rep(c(\"A\", \"B\", \"C\"), each = 30))\n)\n\nkruskal.test(value ~ group, data = data)\n\n\n\n    Kruskal-Wallis rank sum test\n\ndata:  value by group\nKruskal-Wallis chi-squared = 9.3507, df = 2, p-value = 0.009322\n\n\nLike ANOVA, a significant Kruskal-Wallis test tells you that groups differ but not which specific groups differ from which others. Post-hoc pairwise comparisons can follow up on a significant result.",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Nonparametric Tests</span>"
    ]
  },
  {
    "objectID": "chapters/13-nonparametric-tests.html#advantages-and-limitations",
    "href": "chapters/13-nonparametric-tests.html#advantages-and-limitations",
    "title": "18  Nonparametric Tests",
    "section": "18.5 Advantages and Limitations",
    "text": "18.5 Advantages and Limitations\nAdvantages of nonparametric tests:\nNonparametric tests do not require normally distributed data. They are robust to outliers since they work with ranks rather than raw values. They can be applied to ordinal data where the assumption of interval-level measurement would be violated. They often have good power relative to parametric tests even when parametric assumptions are met.\nLimitations:\nWhen parametric assumptions are met, nonparametric tests are slightly less powerful than their parametric counterparts. They test hypotheses about distributions or medians rather than means, which may not always align with research questions. They can be more difficult to extend to complex designs with multiple factors or covariates.",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Nonparametric Tests</span>"
    ]
  },
  {
    "objectID": "chapters/13-nonparametric-tests.html#choosing-between-parametric-and-nonparametric",
    "href": "chapters/13-nonparametric-tests.html#choosing-between-parametric-and-nonparametric",
    "title": "18  Nonparametric Tests",
    "section": "18.6 Choosing Between Parametric and Nonparametric",
    "text": "18.6 Choosing Between Parametric and Nonparametric\nThe choice depends on your data and research question. If your data are reasonably normal (or your sample is large enough for the Central Limit Theorem to apply) and you care about means, parametric tests are appropriate and efficient. If your data are severely non-normal, contain outliers, or are ordinal in nature, nonparametric tests provide a safer alternative.\nWith large samples, the Central Limit Theorem ensures that parametric tests are robust to non-normality, so the choice matters less. With small samples, checking assumptions becomes more important.",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Nonparametric Tests</span>"
    ]
  },
  {
    "objectID": "chapters/13-nonparametric-tests.html#frequency-analysis-chi-square-tests",
    "href": "chapters/13-nonparametric-tests.html#frequency-analysis-chi-square-tests",
    "title": "18  Nonparametric Tests",
    "section": "18.7 Frequency Analysis: Chi-Square Tests",
    "text": "18.7 Frequency Analysis: Chi-Square Tests\nWhen data consist of counts in categories rather than continuous measurements, we need tests designed for categorical data. The chi-square (\\(\\chi^2\\)) test compares observed frequencies to expected frequencies under a null hypothesis.\n\nGoodness-of-Fit Test\nThe chi-square goodness-of-fit test asks whether observed frequencies match expected proportions. For example, do offspring genotypes follow expected Mendelian ratios?\n\n\nCode\n# Test whether observed counts match expected 3:1 ratio\nobserved &lt;- c(75, 25)  # Dominant, Recessive phenotypes\nexpected_ratio &lt;- c(3, 1)\nexpected &lt;- sum(observed) * expected_ratio / sum(expected_ratio)\n\n# Chi-square test\nchisq.test(observed, p = expected_ratio / sum(expected_ratio))\n\n\n\n    Chi-squared test for given probabilities\n\ndata:  observed\nX-squared = 0, df = 1, p-value = 1\n\n\nThe test statistic is:\n\\[\\chi^2 = \\sum \\frac{(O_i - E_i)^2}{E_i}\\]\nwhere \\(O_i\\) are observed counts and \\(E_i\\) are expected counts. Under the null hypothesis (observed = expected), this follows a chi-square distribution with \\(k-1\\) degrees of freedom, where \\(k\\) is the number of categories.\n\n\nTests of Independence: Contingency Tables\nWhen we have counts cross-classified by two categorical variables, a contingency table displays the frequencies. The chi-square test of independence asks whether the two variables are associated.\n\n\nCode\n# Example: Is treatment outcome associated with gender?\ntreatment_data &lt;- matrix(c(\n  45, 35,   # Males: Success, Failure\n  55, 15    # Females: Success, Failure\n), nrow = 2, byrow = TRUE)\nrownames(treatment_data) &lt;- c(\"Male\", \"Female\")\ncolnames(treatment_data) &lt;- c(\"Success\", \"Failure\")\n\ntreatment_data\n\n\n       Success Failure\nMale        45      35\nFemale      55      15\n\n\nCode\n# Chi-square test of independence\nchisq.test(treatment_data)\n\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  treatment_data\nX-squared = 7.3962, df = 1, p-value = 0.006536\n\n\nExpected counts under independence are calculated as:\n\\[E_{ij} = \\frac{(\\text{Row Total}_i) \\times (\\text{Column Total}_j)}{\\text{Grand Total}}\\]\n\n\n\n\n\n\nAssumptions of Chi-Square Tests\n\n\n\n\nObservations must be independent\nExpected counts should be at least 5 in each cell (some sources say 80% of cells should have expected counts ≥ 5)\nFor 2×2 tables with small expected counts, use Fisher’s exact test instead\n\n\n\n\n\nFisher’s Exact Test\nWhen sample sizes are small, Fisher’s exact test provides exact p-values rather than relying on the chi-square approximation:\n\n\nCode\n# Small sample example\nsmall_table &lt;- matrix(c(3, 1, 1, 3), nrow = 2)\nfisher.test(small_table)\n\n\n\n    Fisher's Exact Test for Count Data\n\ndata:  small_table\np-value = 0.4857\nalternative hypothesis: true odds ratio is not equal to 1\n95 percent confidence interval:\n   0.2117329 621.9337505\nsample estimates:\nodds ratio \n  6.408309 \n\n\n\n\nG-Test (Likelihood Ratio Test)\nThe G-test is an alternative to the chi-square test based on the likelihood ratio. It has better theoretical properties and is preferred by some statisticians:\n\\[G = 2 \\sum O_i \\ln\\left(\\frac{O_i}{E_i}\\right)\\]\n\n\nCode\n# G-test for the treatment data\n# Using observed and expected from chi-square\ntest_result &lt;- chisq.test(treatment_data)\nobserved_counts &lt;- as.vector(treatment_data)\nexpected_counts &lt;- as.vector(test_result$expected)\n\nG &lt;- 2 * sum(observed_counts * log(observed_counts / expected_counts))\np_value &lt;- 1 - pchisq(G, df = 1)\n\ncat(\"G statistic:\", round(G, 3), \"\\n\")\n\n\nG statistic: 8.563 \n\n\nCode\ncat(\"p-value:\", round(p_value, 4), \"\\n\")\n\n\np-value: 0.0034 \n\n\n\n\nOdds Ratios\nFor 2×2 tables, the odds ratio quantifies the strength of association between two binary variables:\n\\[OR = \\frac{a/b}{c/d} = \\frac{ad}{bc}\\]\nwhere the table is:\n\n\n\n\nOutcome+\nOutcome-\n\n\n\n\nExposure+\na\nb\n\n\nExposure-\nc\nd\n\n\n\nAn odds ratio of 1 indicates no association. OR &gt; 1 indicates positive association; OR &lt; 1 indicates negative association.\n\n\nCode\n# Calculate odds ratio for treatment data\na &lt;- treatment_data[1, 1]  # Male, Success\nb &lt;- treatment_data[1, 2]  # Male, Failure\nc &lt;- treatment_data[2, 1]  # Female, Success\nd &lt;- treatment_data[2, 2]  # Female, Failure\n\nodds_ratio &lt;- (a * d) / (b * c)\ncat(\"Odds ratio:\", round(odds_ratio, 3), \"\\n\")\n\n\nOdds ratio: 0.351 \n\n\nCode\n# Using fisher.test to get OR with confidence interval\nfisher.test(treatment_data)\n\n\n\n    Fisher's Exact Test for Count Data\n\ndata:  treatment_data\np-value = 0.005273\nalternative hypothesis: true odds ratio is not equal to 1\n95 percent confidence interval:\n 0.1579843 0.7609357\nsample estimates:\nodds ratio \n 0.3531441 \n\n\nAn odds ratio of 0.35 indicates that males have lower odds of success compared to females in this example.\n\n\nMcNemar’s Test for Paired Data\nWhen categorical data are paired (e.g., before/after measurements on the same subjects), McNemar’s test is appropriate:\n\n\nCode\n# Before/after treatment: did opinion change?\nbefore_after &lt;- matrix(c(\n  40, 10,  # Agree before: Agree after, Disagree after\n  25, 25   # Disagree before: Agree after, Disagree after\n), nrow = 2, byrow = TRUE)\n\nmcnemar.test(before_after)\n\n\n\n    McNemar's Chi-squared test with continuity correction\n\ndata:  before_after\nMcNemar's chi-squared = 5.6, df = 1, p-value = 0.01796\n\n\nThe test focuses on the discordant pairs—cases where the response changed—and asks whether changes in one direction are more common than changes in the other direction.",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Nonparametric Tests</span>"
    ]
  },
  {
    "objectID": "chapters/13-nonparametric-tests.html#summary",
    "href": "chapters/13-nonparametric-tests.html#summary",
    "title": "18  Nonparametric Tests",
    "section": "18.8 Summary",
    "text": "18.8 Summary\nNonparametric and frequency-based tests provide alternatives when parametric assumptions fail or data are categorical:\n\nMann-Whitney U and Kruskal-Wallis for comparing groups with non-normal data\nWilcoxon signed-rank for paired non-normal data\nChi-square tests for categorical data (goodness-of-fit and independence)\nFisher’s exact test for small samples\nOdds ratios to quantify association strength\nMcNemar’s test for paired categorical data",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Nonparametric Tests</span>"
    ]
  },
  {
    "objectID": "chapters/13-nonparametric-tests.html#exercises",
    "href": "chapters/13-nonparametric-tests.html#exercises",
    "title": "18  Nonparametric Tests",
    "section": "18.9 Exercises",
    "text": "18.9 Exercises\n\n\n\n\n\n\nExercise N.1: Mann-Whitney U Test\n\n\n\nYou measure enzyme activity (in arbitrary units) in two groups of transgenic plants:\ncontrol &lt;- c(23.1, 25.4, 22.8, 26.3, 24.1, 25.9, 23.7, 24.8, 22.5, 26.1)\ntransgenic &lt;- c(28.4, 31.2, 29.6, 30.1, 27.8, 32.3, 29.9, 28.7, 30.5, 31.8)\n\nCreate side-by-side boxplots to visualize the two groups\nAssess whether the data appear normally distributed (use Q-Q plots or Shapiro-Wilk test)\nPerform a Mann-Whitney U test to compare the two groups\nPerform a two-sample t-test for comparison\nWhich test is more appropriate for these data and why?\nCalculate the median and IQR for each group and report these along with your test results\n\n\n\nCode\n# Your code here\n\n\n\n\n\n\n\n\n\n\nExercise N.2: Wilcoxon Signed-Rank Test\n\n\n\nA researcher measures blood pressure before and after a stress reduction intervention in 12 participants:\nbefore &lt;- c(142, 138, 155, 148, 162, 140, 151, 139, 156, 145, 149, 158)\nafter &lt;- c(136, 132, 149, 142, 150, 138, 145, 135, 151, 140, 143, 152)\n\nCalculate the paired differences (after - before) and visualize their distribution\nTest whether the differences are normally distributed\nPerform a Wilcoxon signed-rank test\nPerform a paired t-test for comparison\nCalculate the median reduction in blood pressure and construct an approximate 95% CI using a bootstrap approach\nReport your conclusions about the intervention’s effectiveness\n\n\n\nCode\n# Your code here\n\n\n\n\n\n\n\n\n\n\nExercise N.3: Chi-Square Goodness-of-Fit\n\n\n\nA genetics experiment produces offspring with four phenotypes. According to Mendelian theory, these should occur in a 9:3:3:1 ratio. You observe the following counts:\nobserved &lt;- c(315, 108, 101, 32)  # Phenotypes: AB, Ab, aB, ab\n\nCalculate the expected counts based on the 9:3:3:1 ratio\nPerform a chi-square goodness-of-fit test\nVisualize the observed vs. expected counts with a barplot\nCalculate the contribution of each category to the total chi-square statistic\nDo the data support the theoretical prediction? Explain your reasoning\n\n\n\nCode\n# Your code here\n\n\n\n\n\n\n\n\n\n\nExercise N.4: Chi-Square Test of Independence\n\n\n\nA clinical trial tests a new treatment for bacterial infections. Results are classified by treatment group and outcome:\n\n\n\n\nCured\nNot Cured\n\n\n\n\nTreatment\n48\n12\n\n\nControl\n32\n28\n\n\n\n\nCreate this 2×2 table in R\nPerform a chi-square test of independence\nCalculate and interpret the odds ratio\nUse Fisher’s exact test and compare the p-value to the chi-square test\nCheck the expected cell counts—is the chi-square approximation appropriate?\nReport your conclusions about treatment effectiveness\n\n\n\nCode\n# Your code here\n\n\n\n\n\n\n\n\n\n\nExercise N.5: Choosing the Right Test\n\n\n\nFor each scenario below, indicate which test is most appropriate (Mann-Whitney U, Wilcoxon signed-rank, Kruskal-Wallis, chi-square, Fisher’s exact, or McNemar’s test) and explain why:\n\nComparing survival times of patients on three different chemotherapy regimens (n = 15 per group, data are heavily right-skewed)\nTesting whether a coin is fair after observing 65 heads in 100 flips\nComparing patient satisfaction ratings (on a scale of 1-5) before and after a hospital redesign\nTesting whether smoking status (yes/no) is associated with lung disease (yes/no) in a sample of 30 patients\nComparing memory test scores in four age groups (n = 50 per group, scores range 0-100 but are not normally distributed)\nAnalyzing a before/after survey where respondents indicate support (yes/no) for a policy\n\nFor two of these scenarios, write complete R code to perform the analysis with simulated or provided data.\n\n\nCode\n# Your code here",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Nonparametric Tests</span>"
    ]
  },
  {
    "objectID": "chapters/13-nonparametric-tests.html#additional-resources",
    "href": "chapters/13-nonparametric-tests.html#additional-resources",
    "title": "18  Nonparametric Tests",
    "section": "18.10 Additional Resources",
    "text": "18.10 Additional Resources\n\nLogan (2010) - Comprehensive coverage of nonparametric and categorical data analysis\nCrawley (2007) - Detailed treatment of chi-square and contingency table methods in R\n\n\n\n\n\n\n\nCrawley, Michael J. 2007. The r Book. John Wiley & Sons.\n\n\nLogan, Murray. 2010. Biostatistical Design and Analysis Using r. Wiley-Blackwell.\n\n\nMann, Henry B., and Donald R. Whitney. 1947. “On a Test of Whether One of Two Random Variables Is Stochastically Larger Than the Other.” The Annals of Mathematical Statistics 18 (1): 50–60.\n\n\nWilcoxon, Frank. 1945. “Individual Comparisons by Ranking Methods.” Biometrics Bulletin 1 (6): 80–83.",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Nonparametric Tests</span>"
    ]
  },
  {
    "objectID": "chapters/14-bootstrapping.html",
    "href": "chapters/14-bootstrapping.html",
    "title": "19  Resampling Methods",
    "section": "",
    "text": "19.1 The Bootstrap Idea\nFor the sample mean, we have elegant formulas for standard errors and confidence intervals derived from probability theory. But what about other statistics—the median, a correlation coefficient, the ratio of two means? For many estimators, no convenient formula exists.\nThe bootstrap (Efron 1979), invented by Bradley Efron in 1979, provides a general solution. The key insight is that we can learn about the sampling distribution of a statistic by resampling from our data. If our sample is representative of the population, then samples drawn from our sample (with replacement) mimic what we would get from repeated sampling from the population.",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Resampling Methods</span>"
    ]
  },
  {
    "objectID": "chapters/14-bootstrapping.html#the-bootstrap-idea",
    "href": "chapters/14-bootstrapping.html#the-bootstrap-idea",
    "title": "19  Resampling Methods",
    "section": "",
    "text": "Figure 19.1: Bootstrap resampling procedure showing how multiple samples are drawn with replacement from the original data",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Resampling Methods</span>"
    ]
  },
  {
    "objectID": "chapters/14-bootstrapping.html#why-the-bootstrap-works",
    "href": "chapters/14-bootstrapping.html#why-the-bootstrap-works",
    "title": "19  Resampling Methods",
    "section": "19.2 Why the Bootstrap Works",
    "text": "19.2 Why the Bootstrap Works\nThe bootstrap treats the observed sample as if it were the population. By drawing many samples with replacement from this “population,” we create a distribution of the statistic of interest. This bootstrap distribution approximates the true sampling distribution.\nThe bootstrap standard error is the standard deviation of the bootstrap distribution. Bootstrap confidence intervals can be constructed from the percentiles of the bootstrap distribution—the 2.5th and 97.5th percentiles give an approximate 95% confidence interval.",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Resampling Methods</span>"
    ]
  },
  {
    "objectID": "chapters/14-bootstrapping.html#bootstrap-procedure",
    "href": "chapters/14-bootstrapping.html#bootstrap-procedure",
    "title": "19  Resampling Methods",
    "section": "19.3 Bootstrap Procedure",
    "text": "19.3 Bootstrap Procedure\nThe basic algorithm is straightforward:\n\nDraw a random sample of size n from your data with replacement (the bootstrap sample)\nCalculate the statistic of interest from this bootstrap sample\nRepeat steps 1 and 2 many times (1000 or more)\nUse the distribution of bootstrap statistics to estimate standard error or confidence intervals\n\n\n\nCode\n# Bootstrap example: estimating standard error of median\nset.seed(42)\noriginal_data &lt;- rexp(50, rate = 0.1)  # Skewed distribution\n\n# Observed median\nobserved_median &lt;- median(original_data)\n\n# Bootstrap\nn_boot &lt;- 1000\nboot_medians &lt;- replicate(n_boot, {\n  boot_sample &lt;- sample(original_data, replace = TRUE)\n  median(boot_sample)\n})\n\n# Bootstrap standard error\nboot_se &lt;- sd(boot_medians)\n\n# Bootstrap confidence interval (percentile method)\nboot_ci &lt;- quantile(boot_medians, c(0.025, 0.975))\n\ncat(\"Observed median:\", round(observed_median, 2), \"\\n\")\n\n\nObserved median: 6.59 \n\n\nCode\ncat(\"Bootstrap SE:\", round(boot_se, 2), \"\\n\")\n\n\nBootstrap SE: 1.46 \n\n\nCode\ncat(\"95% CI:\", round(boot_ci, 2), \"\\n\")\n\n\n95% CI: 4.39 11.92 \n\n\nCode\nhist(boot_medians, breaks = 30, main = \"Bootstrap Distribution of Median\",\n     xlab = \"Median\", col = \"lightblue\")\nabline(v = observed_median, col = \"red\", lwd = 2)\nabline(v = boot_ci, col = \"blue\", lwd = 2, lty = 2)\n\n\n\n\n\n\n\n\nFigure 19.2: Bootstrap distribution of the median from 1000 resamples, showing the estimated sampling distribution with 95% confidence intervals",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Resampling Methods</span>"
    ]
  },
  {
    "objectID": "chapters/14-bootstrapping.html#advantages-of-the-bootstrap",
    "href": "chapters/14-bootstrapping.html#advantages-of-the-bootstrap",
    "title": "19  Resampling Methods",
    "section": "19.4 Advantages of the Bootstrap",
    "text": "19.4 Advantages of the Bootstrap\nThe bootstrap is remarkably versatile. It can be applied to almost any statistic—means, medians, correlations, regression coefficients, eigenvalues, and more. It works when no formula for standard errors exists. It is nonparametric, making no assumptions about the underlying distribution. It handles complex sampling designs and calculations that would be intractable analytically.\nThe bootstrap is widely used for assessing confidence in phylogenetic trees, where the complexity of tree-building algorithms makes analytical approaches impractical. In machine learning, bootstrap aggregating (bagging) improves prediction accuracy by combining models trained on bootstrap samples.",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Resampling Methods</span>"
    ]
  },
  {
    "objectID": "chapters/14-bootstrapping.html#when-the-bootstrap-fails",
    "href": "chapters/14-bootstrapping.html#when-the-bootstrap-fails",
    "title": "19  Resampling Methods",
    "section": "19.5 When the Bootstrap Fails",
    "text": "19.5 When the Bootstrap Fails\nThe bootstrap is not a magic solution to all problems. It requires that the original sample be representative of the population—a biased sample produces biased bootstrap estimates. It can struggle with very small samples where the original data may not adequately represent the population.\nCertain statistics, like the maximum of a sample, are poorly estimated by the bootstrap because the bootstrap distribution is bounded by the observed data. The bootstrap also assumes that observations are independent; for dependent data (like time series), specialized bootstrap methods are needed.",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Resampling Methods</span>"
    ]
  },
  {
    "objectID": "chapters/14-bootstrapping.html#bootstrap-confidence-intervals",
    "href": "chapters/14-bootstrapping.html#bootstrap-confidence-intervals",
    "title": "19  Resampling Methods",
    "section": "19.6 Bootstrap Confidence Intervals",
    "text": "19.6 Bootstrap Confidence Intervals\nSeveral methods exist for constructing bootstrap confidence intervals. The percentile method uses the quantiles of the bootstrap distribution directly. The basic bootstrap method reflects the bootstrap distribution around the observed estimate. The BCa (bias-corrected and accelerated) method adjusts for bias and skewness in the bootstrap distribution.\n\n\nCode\n# Different bootstrap CI methods\nlibrary(boot)\n\n# Define statistic function\nmedian_fun &lt;- function(data, indices) {\n  median(data[indices])\n}\n\n# Run bootstrap\nboot_result &lt;- boot(original_data, median_fun, R = 1000)\n\n# Different CI methods\nboot.ci(boot_result, type = c(\"perc\", \"basic\", \"bca\"))\n\n\nBOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS\nBased on 1000 bootstrap replicates\n\nCALL : \nboot.ci(boot.out = boot_result, type = c(\"perc\", \"basic\", \"bca\"))\n\nIntervals : \nLevel      Basic              Percentile            BCa          \n95%   ( 1.256,  8.841 )   ( 4.331, 11.916 )   ( 4.244, 11.582 )  \nCalculations and Intervals on Original Scale\n\n\nThe BCa method is generally preferred when computationally feasible, as it provides better coverage in many situations.",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Resampling Methods</span>"
    ]
  },
  {
    "objectID": "chapters/14-bootstrapping.html#practical-recommendations",
    "href": "chapters/14-bootstrapping.html#practical-recommendations",
    "title": "19  Resampling Methods",
    "section": "19.7 Practical Recommendations",
    "text": "19.7 Practical Recommendations\nFor most applications, 1000 bootstrap replications provide adequate precision for standard errors. For confidence intervals, especially when using the BCa method, 10,000 replications may be preferable. Always set a random seed for reproducibility.\nRemember that the bootstrap estimates sampling variability—it cannot fix problems with biased samples or invalid measurements. Use it as a tool for understanding uncertainty, not as a cure for poor data quality.",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Resampling Methods</span>"
    ]
  },
  {
    "objectID": "chapters/14-bootstrapping.html#permutation-tests",
    "href": "chapters/14-bootstrapping.html#permutation-tests",
    "title": "19  Resampling Methods",
    "section": "19.8 Permutation Tests",
    "text": "19.8 Permutation Tests\nWhile the bootstrap estimates sampling variability by resampling with replacement, permutation tests (also called randomization tests) address a different question: they test the null hypothesis by resampling without replacement. Permutation tests are among the oldest statistical tests, predating many parametric methods.\n\nThe Permutation Idea\nA permutation test asks: “If there were truly no difference between groups, how likely would we be to see a difference as large as the one we observed?”\nUnder the null hypothesis of no group difference, group labels are arbitrary—the data could have been assigned to either group. A permutation test generates the null distribution by repeatedly shuffling group labels and recalculating the test statistic. The p-value is the proportion of permuted statistics as extreme as the observed statistic.\n\n\nCode\n# Example: Two-sample permutation test\nset.seed(42)\n\n# Generate two groups with different means\ngroup_A &lt;- rnorm(15, mean = 10, sd = 2)\ngroup_B &lt;- rnorm(15, mean = 12, sd = 2)\n\n# Observed difference in means\nobserved_diff &lt;- mean(group_B) - mean(group_A)\ncat(\"Observed difference:\", round(observed_diff, 3), \"\\n\")\n\n\nObserved difference: 0.337 \n\n\nCode\n# Combined data for permutation\ncombined &lt;- c(group_A, group_B)\nn_A &lt;- length(group_A)\nn_B &lt;- length(group_B)\nn_total &lt;- n_A + n_B\n\n# Permutation test\nn_perm &lt;- 10000\nperm_diffs &lt;- replicate(n_perm, {\n  shuffled &lt;- sample(combined)  # Shuffle without replacement\n  mean(shuffled[(n_A + 1):n_total]) - mean(shuffled[1:n_A])\n})\n\n# Visualize the permutation distribution\nhist(perm_diffs, breaks = 50, col = \"lightblue\",\n     main = \"Permutation Distribution of Mean Difference\",\n     xlab = \"Difference in Means\")\nabline(v = observed_diff, col = \"red\", lwd = 2)\nabline(v = -observed_diff, col = \"red\", lwd = 2, lty = 2)\nlegend(\"topright\", c(\"Observed\", \"Mirror (for two-tailed)\"),\n       col = \"red\", lty = c(1, 2), lwd = 2)\n\n\n\n\n\n\n\n\nFigure 19.3: Permutation test concept: if group labels don’t matter, shuffling them should produce similar results\n\n\n\n\n\n\n\nCalculating the P-Value\n\n\nCode\n# Two-tailed p-value: proportion of permuted differences as extreme as observed\np_value &lt;- mean(abs(perm_diffs) &gt;= abs(observed_diff))\ncat(\"Permutation p-value:\", p_value, \"\\n\")\n\n\nPermutation p-value: 0.6947 \n\n\nCode\n# Compare to t-test\nt_test &lt;- t.test(group_B, group_A)\ncat(\"t-test p-value:\", round(t_test$p.value, 4), \"\\n\")\n\n\nt-test p-value: 0.704 \n\n\nThe permutation p-value and parametric p-value are often similar when parametric assumptions are met. The permutation test is exact—it gives the correct p-value regardless of the underlying distribution.\n\n\nPermutation Test for Correlation\nPermutation tests work for any test statistic. Here’s an example testing whether a correlation is significantly different from zero:\n\n\nCode\n# Test correlation significance\nset.seed(123)\nx &lt;- rnorm(25)\ny &lt;- 0.5 * x + rnorm(25, sd = 0.8)  # True correlation exists\n\n# Observed correlation\nobserved_cor &lt;- cor(x, y)\ncat(\"Observed correlation:\", round(observed_cor, 3), \"\\n\")\n\n\nObserved correlation: 0.653 \n\n\nCode\n# Permutation test: shuffle one variable to break association\nn_perm &lt;- 10000\nperm_cors &lt;- replicate(n_perm, {\n  cor(x, sample(y))  # Shuffle y, keeping x fixed\n})\n\n# Permutation p-value\np_value_cor &lt;- mean(abs(perm_cors) &gt;= abs(observed_cor))\ncat(\"Permutation p-value:\", p_value_cor, \"\\n\")\n\n\nPermutation p-value: 1e-04 \n\n\nCode\n# Visualize\nhist(perm_cors, breaks = 50, col = \"lightblue\",\n     main = \"Permutation Distribution of Correlation\",\n     xlab = \"Correlation Coefficient\")\nabline(v = observed_cor, col = \"red\", lwd = 2)\nabline(v = -observed_cor, col = \"red\", lwd = 2, lty = 2)\n\n\n\n\n\n\n\n\nFigure 19.4: Permutation test for correlation: null distribution generated by breaking the X-Y pairing\n\n\n\n\n\n\n\nBootstrap vs Permutation\nThe bootstrap and permutation tests answer different questions:\n\n\n\n\n\n\n\n\nFeature\nBootstrap\nPermutation Test\n\n\n\n\nQuestion\nWhat is the sampling variability of my estimate?\nIs the observed effect real or due to chance?\n\n\nResampling\nWith replacement\nWithout replacement\n\n\nOutput\nConfidence intervals, standard errors\nP-value\n\n\nNull hypothesis\nNone assumed\nTests specific null (e.g., no group difference)\n\n\nAssumptions\nSample is representative\nObservations are exchangeable under null\n\n\n\nUse bootstrap when: - You want confidence intervals for any statistic - No convenient formula for standard error exists - You need to assess uncertainty\nUse permutation tests when: - You want to test a null hypothesis - Parametric assumptions may be violated - You want an exact test without distributional assumptions\n\n\nPermutation Test for Paired Data\nFor paired designs (like before/after measurements), permute the sign of differences rather than shuffling between groups:\n\n\nCode\n# Paired permutation test\nset.seed(456)\nbefore &lt;- rnorm(20, mean = 100, sd = 15)\nafter &lt;- before + rnorm(20, mean = 5, sd = 8)  # Treatment adds ~5 units\n\n# Observed mean difference\ndifferences &lt;- after - before\nobserved_mean_diff &lt;- mean(differences)\ncat(\"Observed mean difference:\", round(observed_mean_diff, 3), \"\\n\")\n\n\nObserved mean difference: 3.076 \n\n\nCode\n# Permutation: randomly flip signs of differences\nn_perm &lt;- 10000\nperm_means &lt;- replicate(n_perm, {\n  signs &lt;- sample(c(-1, 1), length(differences), replace = TRUE)\n  mean(differences * signs)\n})\n\n# P-value\np_value_paired &lt;- mean(abs(perm_means) &gt;= abs(observed_mean_diff))\ncat(\"Permutation p-value:\", p_value_paired, \"\\n\")\n\n\nPermutation p-value: 0.0954 \n\n\nCode\n# Compare to paired t-test\npaired_t &lt;- t.test(after, before, paired = TRUE)\ncat(\"Paired t-test p-value:\", round(paired_t$p.value, 4), \"\\n\")\n\n\nPaired t-test p-value: 0.0915 \n\n\n\n\nWhen Permutation Tests Excel\nPermutation tests are particularly valuable when:\n\nSample sizes are small: Parametric tests may not be reliable\nDistributions are non-normal: Especially with skewed or heavy-tailed data\nData are ranks or ordinal: No parametric distribution applies\nComplex test statistics: Custom statistics without known distributions\nYou want exact inference: No approximation error from asymptotic theory\n\n\n\n\n\n\n\nPractical Considerations\n\n\n\n\nNumber of permutations: 10,000 is often sufficient; for publishable results, 100,000 gives more precision\nComputation: Permutation tests can be slow for large datasets; consider parallel computing\nSmall samples: With very small samples, the number of unique permutations is limited\nExact vs. Monte Carlo: For small samples, you can enumerate all permutations exactly; for larger samples, random sampling (Monte Carlo) approximates the permutation distribution\n\n\n\n\n\nImplementation with coin Package\nThe coin package provides efficient, well-tested permutation tests:\n\n\nCode\nlibrary(coin)\n\n# Two-sample permutation test\ntest_data &lt;- data.frame(\n  value = c(group_A, group_B),\n  group = factor(rep(c(\"A\", \"B\"), c(length(group_A), length(group_B))))\n)\n\n# Exact permutation test (or Monte Carlo approximation for larger samples)\noneway_test(value ~ group, data = test_data, distribution = \"approximate\")\n\n\n\n    Approximative Two-Sample Fisher-Pitman Permutation Test\n\ndata:  value by group (A, B)\nZ = -0.38996, p-value = 0.6972\nalternative hypothesis: true mu is not equal to 0",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Resampling Methods</span>"
    ]
  },
  {
    "objectID": "chapters/14-bootstrapping.html#summary",
    "href": "chapters/14-bootstrapping.html#summary",
    "title": "19  Resampling Methods",
    "section": "19.9 Summary",
    "text": "19.9 Summary\nResampling methods provide powerful, flexible tools for statistical inference:\n\nBootstrap estimates sampling variability by resampling with replacement from observed data\nPermutation tests test null hypotheses by resampling without replacement to generate null distributions\nBoth methods make minimal distributional assumptions\nBootstrap excels at constructing confidence intervals; permutation tests excel at hypothesis testing\nUse 1,000+ replications for bootstrap standard errors; 10,000+ for confidence intervals and p-values\nThese methods complement rather than replace traditional parametric approaches",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Resampling Methods</span>"
    ]
  },
  {
    "objectID": "chapters/14-bootstrapping.html#exercises",
    "href": "chapters/14-bootstrapping.html#exercises",
    "title": "19  Resampling Methods",
    "section": "19.10 Exercises",
    "text": "19.10 Exercises\n\n\n\n\n\n\nExercise B.1: Bootstrap Standard Errors\n\n\n\nYou measure the half-life of a radioactive isotope in 15 independent experiments:\nhalf_lives &lt;- c(5.2, 5.8, 5.1, 6.2, 5.9, 5.4, 6.1, 5.7, 5.3, 6.0, 5.5, 5.9, 5.6, 6.3, 5.4)\n\nCalculate the mean and median half-life\nUse the bootstrap (with 5000 replications) to estimate the standard error of the mean\nUse the bootstrap to estimate the standard error of the median\nCompare the bootstrap SE of the mean to the analytical formula SE = s/√n\nCreate histograms of the bootstrap distributions for both the mean and median\nWhich statistic (mean or median) has greater sampling variability for these data?\n\n\n\nCode\n# Your code here\n\n\n\n\n\n\n\n\n\n\nExercise B.2: Bootstrap Confidence Intervals\n\n\n\nYou measure reaction times (in milliseconds) for a cognitive task:\nreaction_times &lt;- c(234, 245, 267, 289, 256, 298, 312, 245, 278, 301, 267, 289, 234, 256, 278)\n\nCalculate 95% confidence intervals for the median using three methods:\n\nPercentile method (manual calculation)\nUsing the boot package with percentile, basic, and BCa methods\n\nCompare the widths and endpoints of these different confidence intervals\nWhich method would you prefer for this dataset and why?\nBootstrap the 90th percentile and construct a 95% CI for it\n\n\n\nCode\n# Your code here\nlibrary(boot)\n\n\n\n\n\n\n\n\n\n\nExercise B.3: Bootstrap for Correlation\n\n\n\nYou have measurements of two physiological variables:\nx &lt;- c(12.3, 14.5, 11.8, 15.2, 13.1, 14.8, 12.9, 15.5, 13.6, 14.1)\ny &lt;- c(98, 105, 95, 110, 101, 107, 99, 112, 103, 106)\n\nCalculate Pearson’s correlation coefficient\nCalculate the 95% CI for the correlation using the traditional Fisher z-transformation method (cor.test())\nUse the bootstrap (10000 replications) to construct a 95% CI for the correlation\nCompare the two confidence intervals—are they similar?\nCreate a scatterplot with the data and report the correlation with its bootstrap CI\n\n\n\nCode\n# Your code here\n\n\n\n\n\n\n\n\n\n\nExercise B.4: Permutation Test vs. t-test\n\n\n\nYou compare growth rates of bacteria cultured in two different media:\nmedium_A &lt;- c(0.42, 0.38, 0.45, 0.41, 0.39, 0.44, 0.40, 0.43)\nmedium_B &lt;- c(0.51, 0.48, 0.55, 0.49, 0.52, 0.47, 0.54, 0.50)\n\nVisualize the two groups with boxplots\nPerform a two-sample t-test\nImplement a permutation test with 10,000 permutations to test for a difference in means\nCompare the p-values from the t-test and permutation test\nCalculate the effect size (Cohen’s d)\nCreatehistogram of the permutation distribution with the observed difference marked\n\n\n\nCode\n# Your code here\n\n\n\n\n\n\n\n\n\n\nExercise B.5: Paired Permutation Test\n\n\n\nYou measure expression levels of a gene before and after heat stress in 10 plant samples:\nbefore_stress &lt;- c(125, 142, 138, 151, 129, 145, 133, 148, 136, 141)\nafter_stress &lt;- c(189, 207, 195, 218, 182, 203, 191, 214, 198, 206)\n\nCalculate the paired differences and perform a paired t-test\nImplement a paired permutation test by randomly flipping the signs of differences (10,000 permutations)\nCompare the p-values from both methods\nUse the bootstrap to construct a 95% CI for the mean difference\nVisualize the paired data with a before-after plot (connecting lines between paired points)\n\n\n\nCode\n# Your code here\n\n\n\n\n\n\n\n\n\n\nExercise B.6: Bootstrap vs. Permutation—Understanding the Difference\n\n\n\nConsider the following dataset:\ngroup1 &lt;- c(23, 25, 28, 31, 27, 29, 26, 30)\ngroup2 &lt;- c(35, 38, 33, 40, 36, 37, 34, 39)\n\nUse the bootstrap (sampling with replacement from each group separately) to construct 95% CIs for the mean of group1 and the mean of group2\nUse a permutation test to test whether the two groups have different means\nExplain in your own words:\n\nWhat question does the bootstrap answer?\nWhat question does the permutation test answer?\nWhy do we resample WITH replacement for bootstrap but WITHOUT replacement for permutation tests?\n\nCreate visualizations demonstrating both the bootstrap distribution (for one of the means) and the permutation distribution (for the difference in means)\n\n\n\nCode\n# Your code here\n\n\n\n\n\n\n\n\n\n\nEfron, Bradley. 1979. “Bootstrap Methods: Another Look at the Jackknife.” The Annals of Statistics 7 (1): 1–26.",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Resampling Methods</span>"
    ]
  },
  {
    "objectID": "chapters/30-what-are-models.html",
    "href": "chapters/30-what-are-models.html",
    "title": "20  What are Models?",
    "section": "",
    "text": "20.1 The Essence of Modeling\nA model is a simplified representation of reality that helps us understand, explain, or predict phenomena. In statistics and data science, models are mathematical relationships between variables that capture patterns in data while ignoring irrelevant details.\nThis famous quote captures the fundamental truth of modeling: no model perfectly represents reality, but a good model can still provide valuable insights and predictions.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>What are Models?</span>"
    ]
  },
  {
    "objectID": "chapters/30-what-are-models.html#the-essence-of-modeling",
    "href": "chapters/30-what-are-models.html#the-essence-of-modeling",
    "title": "20  What are Models?",
    "section": "",
    "text": "“All models are wrong, but some are useful.” — George Box",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>What are Models?</span>"
    ]
  },
  {
    "objectID": "chapters/30-what-are-models.html#two-cultures-of-statistical-modeling",
    "href": "chapters/30-what-are-models.html#two-cultures-of-statistical-modeling",
    "title": "20  What are Models?",
    "section": "20.2 Two Cultures of Statistical Modeling",
    "text": "20.2 Two Cultures of Statistical Modeling\nIn his influential 2001 paper, Leo Breiman identified two cultures in statistical modeling:\nThe Data Modeling Culture (traditional statistics):\n\nAssumes data are generated by a specific stochastic model\nFocus on estimating parameters and testing hypotheses\nEmphasis on interpretability and understanding mechanisms\nExamples: linear regression, ANOVA, generalized linear models\n\nThe Algorithmic Modeling Culture (machine learning):\n\nTreats the data-generating mechanism as unknown\nFocus on predictive accuracy\nEmphasis on performance over interpretability\nExamples: random forests, neural networks, boosting\n\nBoth approaches have value. Traditional models excel at inference and explanation; algorithmic approaches often produce better predictions. The choice depends on whether your goal is understanding or prediction.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>What are Models?</span>"
    ]
  },
  {
    "objectID": "chapters/30-what-are-models.html#the-general-linear-model-framework",
    "href": "chapters/30-what-are-models.html#the-general-linear-model-framework",
    "title": "20  What are Models?",
    "section": "20.3 The General Linear Model Framework",
    "text": "20.3 The General Linear Model Framework\nMost statistical models you encounter are special cases of the General Linear Model (GLM):\n\\[Y = X\\beta + \\epsilon\\]\nwhere:\n\n\\(Y\\) is the response variable (what we want to predict/explain)\n\\(X\\) is the design matrix of predictor variables\n\\(\\beta\\) are coefficients we estimate\n\\(\\epsilon\\) is random error\n\nThis framework unifies many methods:\n\n\n\nMethod\nResponse Type\nPredictors\n\n\n\n\nOne-sample t-test\nContinuous\nNone (intercept only)\n\n\nTwo-sample t-test\nContinuous\nOne categorical (2 levels)\n\n\nANOVA\nContinuous\nOne or more categorical\n\n\nSimple regression\nContinuous\nOne continuous\n\n\nMultiple regression\nContinuous\nMultiple (any type)\n\n\nANCOVA\nContinuous\nMixed categorical and continuous\n\n\n\nThe beauty of this unified framework is that once you understand regression, you understand the entire family of linear models.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>What are Models?</span>"
    ]
  },
  {
    "objectID": "chapters/30-what-are-models.html#components-of-a-statistical-model",
    "href": "chapters/30-what-are-models.html#components-of-a-statistical-model",
    "title": "20  What are Models?",
    "section": "20.4 Components of a Statistical Model",
    "text": "20.4 Components of a Statistical Model\nEvery statistical model specifies:\n\nResponse variable: What we want to predict or explain\nPredictor variables: Information we use to make predictions\nFunctional form: How predictors relate to the response (linear, polynomial, etc.)\nError structure: Assumptions about variability (normally distributed, etc.)\n\n\n\nCode\n# Visualizing a simple linear model\nset.seed(42)\nx &lt;- 1:50\ny &lt;- 2 + 0.5*x + rnorm(50, sd = 3)\n\nplot(x, y, pch = 19, col = \"steelblue\",\n     main = \"Components of a Linear Model\",\n     xlab = \"Predictor (X)\", ylab = \"Response (Y)\")\n\n# Fitted line (the model)\nmodel &lt;- lm(y ~ x)\nabline(model, col = \"red\", lwd = 2)\n\n# Show residuals for a few points\nsegments(x[c(10,25,40)], y[c(10,25,40)],\n         x[c(10,25,40)], fitted(model)[c(10,25,40)],\n         col = \"darkgreen\", lwd = 2)\ntext(x[25] + 3, (y[25] + fitted(model)[25])/2,\n     \"ε (residual)\", col = \"darkgreen\")\n\nlegend(\"topleft\",\n       c(\"Data points\", \"Model (E[Y|X])\", \"Residuals (ε)\"),\n       pch = c(19, NA, NA), lty = c(NA, 1, 1),\n       col = c(\"steelblue\", \"red\", \"darkgreen\"), lwd = 2)\n\n\n\n\n\n\n\n\nFigure 20.1: Components of a linear model showing data points, the fitted model line representing E[Y|X], and residuals (ε) as vertical deviations from the line",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>What are Models?</span>"
    ]
  },
  {
    "objectID": "chapters/30-what-are-models.html#model-fitting-finding-the-best-parameters",
    "href": "chapters/30-what-are-models.html#model-fitting-finding-the-best-parameters",
    "title": "20  What are Models?",
    "section": "20.5 Model Fitting: Finding the Best Parameters",
    "text": "20.5 Model Fitting: Finding the Best Parameters\nModel fitting is the process of finding parameter values that make the model best explain the observed data.\n\nLeast Squares\nFor linear models, least squares minimizes the sum of squared residuals:\n\\[\\text{minimize} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\\]\nThis produces the best linear unbiased estimators (BLUE) under certain conditions.\n\n\nMaximum Likelihood\nMaximum likelihood estimation (MLE) finds parameters that maximize the probability of observing the data:\n\\[\\hat{\\theta} = \\arg\\max_{\\theta} L(\\theta | \\text{data}) = \\arg\\max_{\\theta} \\prod_{i=1}^{n} f(y_i | \\theta)\\]\nFor normally distributed errors, least squares and MLE give identical results. MLE extends to non-normal distributions and complex models.\n\n\nCode\n# Visualize likelihood for estimating a mean\nset.seed(123)\ndata &lt;- rnorm(20, mean = 5, sd = 2)\n\n# Calculate log-likelihood for different values of mu\nmu_values &lt;- seq(3, 7, length.out = 100)\nlog_lik &lt;- sapply(mu_values, function(mu) {\n  sum(dnorm(data, mean = mu, sd = 2, log = TRUE))\n})\n\npar(mfrow = c(1, 2))\n\n# Data histogram\nhist(data, breaks = 10, main = \"Sample Data\", xlab = \"Value\",\n     col = \"lightblue\", border = \"white\")\nabline(v = mean(data), col = \"red\", lwd = 2)\n\n# Log-likelihood curve\nplot(mu_values, log_lik, type = \"l\", lwd = 2, col = \"blue\",\n     xlab = expression(mu), ylab = \"Log-likelihood\",\n     main = \"Maximum Likelihood Estimation\")\nabline(v = mu_values[which.max(log_lik)], col = \"red\", lwd = 2, lty = 2)\ntext(mu_values[which.max(log_lik)], min(log_lik) + 2,\n     paste(\"MLE =\", round(mu_values[which.max(log_lik)], 2)))\n\n\n\n\n\n\n\n\nFigure 20.2: Maximum likelihood estimation for the mean of a normal distribution, showing sample data (left) and the log-likelihood curve (right) with maximum at the MLE",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>What are Models?</span>"
    ]
  },
  {
    "objectID": "chapters/30-what-are-models.html#overfitting-when-models-learn-too-much",
    "href": "chapters/30-what-are-models.html#overfitting-when-models-learn-too-much",
    "title": "20  What are Models?",
    "section": "20.6 Overfitting: When Models Learn Too Much",
    "text": "20.6 Overfitting: When Models Learn Too Much\nOverfitting occurs when a model captures noise rather than signal—it fits the training data extremely well but fails to generalize to new data.\n\n\nCode\n# Demonstrate overfitting with polynomial regression\nset.seed(42)\nn &lt;- 20\nx &lt;- seq(0, 1, length.out = n)\ny_true &lt;- sin(2*pi*x)\ny &lt;- y_true + rnorm(n, sd = 0.3)\n\npar(mfrow = c(1, 2))\n\n# Underfitting (too simple)\nplot(x, y, pch = 19, main = \"Underfitting (degree 1)\")\nabline(lm(y ~ x), col = \"red\", lwd = 2)\n\n# Overfitting (too complex)\nplot(x, y, pch = 19, main = \"Overfitting (degree 15)\")\nx_new &lt;- seq(0, 1, length.out = 100)\nlines(x_new, predict(lm(y ~ poly(x, 15)),\n                     newdata = data.frame(x = x_new)),\n      col = \"red\", lwd = 2)\n\n\n\n\n\n\n\n\nFigure 20.3: Demonstration of underfitting (degree 1 polynomial, left) and overfitting (degree 15 polynomial, right) in regression models\n\n\n\n\n\nSigns of overfitting:\n\nModel fits training data nearly perfectly\nPredictions on new data are poor\nCoefficients are extremely large or unstable\nSmall changes in data produce very different models\n\n\nThe Bias-Variance Tradeoff\nPrediction error has two sources:\nBias: Error from oversimplifying—missing important patterns\nVariance: Error from oversensitivity—fitting noise\n\\[\\text{Total Error} = \\text{Bias}^2 + \\text{Variance} + \\text{Irreducible Error}\\]\nSimple models have high bias but low variance. Complex models have low bias but high variance. The goal is to find the sweet spot.\n\n\nCode\n# Conceptual bias-variance plot\ncomplexity &lt;- 1:20\nbias_sq &lt;- 10 / complexity\nvariance &lt;- 0.5 * complexity\ntotal &lt;- bias_sq + variance + 2  # irreducible error = 2\n\nplot(complexity, total, type = \"l\", lwd = 2,\n     ylim = c(0, max(total) + 1),\n     xlab = \"Model Complexity\", ylab = \"Error\",\n     main = \"Bias-Variance Tradeoff\")\nlines(complexity, bias_sq, col = \"blue\", lwd = 2, lty = 2)\nlines(complexity, variance, col = \"red\", lwd = 2, lty = 2)\nabline(h = 2, col = \"gray\", lty = 3)\n\n# Optimal complexity\noptimal &lt;- which.min(total)\npoints(optimal, total[optimal], pch = 19, cex = 1.5, col = \"darkgreen\")\n\nlegend(\"topright\",\n       c(\"Total Error\", \"Bias²\", \"Variance\", \"Irreducible\"),\n       col = c(\"black\", \"blue\", \"red\", \"gray\"),\n       lty = c(1, 2, 2, 3), lwd = 2)\n\n\n\n\n\n\n\n\nFigure 20.4: The bias-variance tradeoff showing how total prediction error decomposes into bias squared, variance, and irreducible error, with optimal model complexity at the minimum total error",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>What are Models?</span>"
    ]
  },
  {
    "objectID": "chapters/30-what-are-models.html#feature-engineering-and-transformations",
    "href": "chapters/30-what-are-models.html#feature-engineering-and-transformations",
    "title": "20  What are Models?",
    "section": "20.7 Feature Engineering and Transformations",
    "text": "20.7 Feature Engineering and Transformations\nThe raw predictor variables may not capture the true relationship. Feature engineering creates new variables that better represent the underlying patterns.\nCommon transformations:\n\nPolynomial terms: \\(x^2\\), \\(x^3\\) for curved relationships\nLog transform: \\(\\log(x)\\) for multiplicative relationships\nInteractions: \\(x_1 \\times x_2\\) when effects depend on each other\nCategorical encoding: Converting categories to numbers\n\n\n\nCode\n# Example: log transformation\nset.seed(42)\nx &lt;- runif(50, 1, 100)\ny &lt;- 2 * log(x) + rnorm(50, sd = 0.5)\n\npar(mfrow = c(1, 2))\n\n# Raw scale - looks nonlinear\nplot(x, y, pch = 19, main = \"Original Scale\",\n     xlab = \"X\", ylab = \"Y\")\nabline(lm(y ~ x), col = \"red\", lwd = 2)\n\n# Log scale - linear\nplot(log(x), y, pch = 19, main = \"After Log Transform\",\n     xlab = \"log(X)\", ylab = \"Y\")\nabline(lm(y ~ log(x)), col = \"red\", lwd = 2)\n\n\n\n\n\n\n\n\nFigure 20.5: Effect of log transformation on a nonlinear relationship, showing curved pattern on original scale (left) that becomes linear after log transformation (right)",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>What are Models?</span>"
    ]
  },
  {
    "objectID": "chapters/30-what-are-models.html#model-selection",
    "href": "chapters/30-what-are-models.html#model-selection",
    "title": "20  What are Models?",
    "section": "20.8 Model Selection",
    "text": "20.8 Model Selection\nWhen multiple models are possible, how do we choose? Several criteria exist:\nAdjusted R²: Penalizes for additional parameters \\[R^2_{adj} = 1 - \\frac{(1-R^2)(n-1)}{n-p-1}\\]\nAIC (Akaike Information Criterion): Balances fit and complexity \\[AIC = -2\\ln(L) + 2k\\]\nBIC (Bayesian Information Criterion): Heavier penalty for complexity \\[BIC = -2\\ln(L) + k\\ln(n)\\]\nLower AIC/BIC values indicate better models (balancing fit and parsimony).\n\n\nCode\n# Model selection example\ndata(mtcars)\n\n# Compare models of increasing complexity\nm1 &lt;- lm(mpg ~ wt, data = mtcars)\nm2 &lt;- lm(mpg ~ wt + hp, data = mtcars)\nm3 &lt;- lm(mpg ~ wt + hp + disp, data = mtcars)\nm4 &lt;- lm(mpg ~ wt + hp + disp + drat + qsec, data = mtcars)\n\n# Compare using AIC\nmodels &lt;- list(m1, m2, m3, m4)\ncomparison &lt;- data.frame(\n  Model = c(\"mpg ~ wt\", \"mpg ~ wt + hp\",\n            \"mpg ~ wt + hp + disp\",\n            \"mpg ~ wt + hp + disp + drat + qsec\"),\n  R_squared = sapply(models, function(m) summary(m)$r.squared),\n  Adj_R_squared = sapply(models, function(m) summary(m)$adj.r.squared),\n  AIC = sapply(models, AIC),\n  BIC = sapply(models, BIC)\n)\n\nknitr::kable(comparison, digits = 2)\n\n\n\n\n\n\n\n\n\n\n\n\nModel\nR_squared\nAdj_R_squared\nAIC\nBIC\n\n\n\n\nmpg ~ wt\n0.75\n0.74\n166.03\n170.43\n\n\nmpg ~ wt + hp\n0.83\n0.81\n156.65\n162.52\n\n\nmpg ~ wt + hp + disp\n0.83\n0.81\n158.64\n165.97\n\n\nmpg ~ wt + hp + disp + drat + qsec\n0.85\n0.82\n158.28\n168.54",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>What are Models?</span>"
    ]
  },
  {
    "objectID": "chapters/30-what-are-models.html#cross-validation-for-model-assessment",
    "href": "chapters/30-what-are-models.html#cross-validation-for-model-assessment",
    "title": "20  What are Models?",
    "section": "20.9 Cross-Validation for Model Assessment",
    "text": "20.9 Cross-Validation for Model Assessment\nThe gold standard for evaluating predictive performance is cross-validation: testing the model on data it hasn’t seen.\nK-fold cross-validation: 1. Split data into K parts 2. For each part: train on the other K-1 parts, test on the held-out part 3. Average performance across all K tests\n\n\nCode\nlibrary(boot)\n\n# Compare polynomial degrees using cross-validation\nset.seed(42)\nn &lt;- 100\nx &lt;- seq(0, 4*pi, length.out = n)\ny &lt;- sin(x) + rnorm(n, sd = 0.5)\ndata_cv &lt;- data.frame(x, y)\n\n# Calculate CV error for different polynomial degrees\ndegrees &lt;- 1:15\ncv_errors &lt;- sapply(degrees, function(d) {\n  model &lt;- glm(y ~ poly(x, d), data = data_cv)\n  cv.glm(data_cv, model, K = 10)$delta[1]\n})\n\nplot(degrees, cv_errors, type = \"b\", pch = 19,\n     xlab = \"Polynomial Degree\", ylab = \"CV Error\",\n     main = \"Cross-Validation for Model Selection\")\nabline(v = which.min(cv_errors), col = \"red\", lty = 2)\n\n\n\n\n\n\n\n\nFigure 20.6: Cross-validation error for polynomial models of different degrees, showing optimal model complexity at minimum CV error",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>What are Models?</span>"
    ]
  },
  {
    "objectID": "chapters/30-what-are-models.html#prediction-vs.-explanation",
    "href": "chapters/30-what-are-models.html#prediction-vs.-explanation",
    "title": "20  What are Models?",
    "section": "20.10 Prediction vs. Explanation",
    "text": "20.10 Prediction vs. Explanation\nDifferent goals require different approaches:\nFor Explanation:\n\nSimpler models are often preferable\nFocus on coefficient interpretation\nStatistical significance matters\nUnderstand which variables drive the relationship\n\nFor Prediction:\n\nModel complexity can be higher if it helps\nFocus on out-of-sample performance\nAccuracy metrics matter most\nUnderstanding why is secondary\n\nIn biology and bioengineering, we often want both—models that predict well AND provide mechanistic insight. This tension shapes many modeling decisions.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>What are Models?</span>"
    ]
  },
  {
    "objectID": "chapters/30-what-are-models.html#practical-modeling-workflow",
    "href": "chapters/30-what-are-models.html#practical-modeling-workflow",
    "title": "20  What are Models?",
    "section": "20.11 Practical Modeling Workflow",
    "text": "20.11 Practical Modeling Workflow\n\nDefine the question: What are you trying to learn or predict?\nExplore the data: Visualize relationships, check distributions, identify issues\nChoose candidate models: Based on data type, assumptions, and goals\nFit and evaluate: Use appropriate metrics and validation\nCheck assumptions: Residual analysis, diagnostic plots\nIterate: Refine based on diagnostics\nReport honestly: Including limitations and uncertainty",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>What are Models?</span>"
    ]
  },
  {
    "objectID": "chapters/30-what-are-models.html#summary",
    "href": "chapters/30-what-are-models.html#summary",
    "title": "20  What are Models?",
    "section": "20.12 Summary",
    "text": "20.12 Summary\n\nModels are simplified representations of reality that help us understand and predict\nThe general linear model framework unifies many common statistical methods\nModel fitting finds parameters that best explain the data (least squares, MLE)\nOverfitting occurs when models learn noise instead of signal\nThe bias-variance tradeoff governs model complexity choices\nFeature engineering can improve model performance\nCross-validation provides honest estimates of predictive performance\nDifferent goals (prediction vs. explanation) may favor different approaches",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>What are Models?</span>"
    ]
  },
  {
    "objectID": "chapters/30-what-are-models.html#additional-resources",
    "href": "chapters/30-what-are-models.html#additional-resources",
    "title": "20  What are Models?",
    "section": "20.13 Additional Resources",
    "text": "20.13 Additional Resources\n\nJames et al. (2023) - Comprehensive introduction to statistical learning concepts\nCrawley (2007) - Practical guide to statistical modeling in R\nBreiman, L. (2001). Statistical Modeling: The Two Cultures. Statistical Science, 16(3), 199-231.\n\n\n\n\n\n\n\nCrawley, Michael J. 2007. The r Book. John Wiley & Sons.\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2023. An Introduction to Statistical Learning with Applications in r. 2nd ed. Springer. https://www.statlearning.com.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>What are Models?</span>"
    ]
  },
  {
    "objectID": "chapters/15-correlation.html",
    "href": "chapters/15-correlation.html",
    "title": "21  Correlation",
    "section": "",
    "text": "21.1 Measuring Association\nWhen two variables vary together, we say they are correlated. Understanding whether and how variables are related is fundamental to science—it helps us identify potential causal relationships, make predictions, and understand systems.\nCorrelation quantifies the strength and direction of the linear relationship between two variables. A positive correlation means that high values of one variable tend to occur with high values of the other. A negative correlation means that high values of one variable tend to occur with low values of the other.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Correlation</span>"
    ]
  },
  {
    "objectID": "chapters/15-correlation.html#covariance",
    "href": "chapters/15-correlation.html#covariance",
    "title": "21  Correlation",
    "section": "21.2 Covariance",
    "text": "21.2 Covariance\nThe covariance measures how two variables vary together. If X and Y tend to be above their means at the same time (and below their means at the same time), the covariance is positive. If one tends to be above its mean when the other is below, the covariance is negative.\n\\[Cov(X, Y) = \\frac{\\sum_{i=1}^{n}(x_i - \\bar{x})(y_i - \\bar{y})}{n-1}\\]\n\nUnderstanding Covariance Visually\nThe covariance formula involves products of deviations from the mean. Consider each point in a scatterplot:\n\nIf a point is in the upper-right quadrant (both X and Y above their means), the product of deviations is positive\nIf a point is in the lower-left quadrant (both below their means), the product is also positive\nIf a point is in the upper-left or lower-right (one above, one below), the product is negative\n\n\n\nCode\n# Visualizing covariance with quadrants\nset.seed(42)\nx &lt;- rnorm(50, mean = 10, sd = 2)\ny &lt;- 0.8 * x + rnorm(50, sd = 1.5)\n\n# Calculate means\nmean_x &lt;- mean(x)\nmean_y &lt;- mean(y)\n\n# Plot with quadrants\nplot(x, y, pch = 19, col = \"steelblue\",\n     xlab = \"X\", ylab = \"Y\",\n     main = \"Covariance: Products of Deviations from Means\")\nabline(v = mean_x, h = mean_y, col = \"red\", lty = 2, lwd = 2)\n\n# Add quadrant labels\ntext(max(x) - 0.5, max(y) - 0.5, \"(+)(+) = +\", col = \"darkgreen\", cex = 0.9)\ntext(min(x) + 0.5, min(y) + 0.5, \"(-)(−) = +\", col = \"darkgreen\", cex = 0.9)\ntext(max(x) - 0.5, min(y) + 0.5, \"(+)(−) = −\", col = \"darkred\", cex = 0.9)\ntext(min(x) + 0.5, max(y) - 0.5, \"(−)(+) = −\", col = \"darkred\", cex = 0.9)\n\n# Report covariance\ncat(\"Covariance:\", round(cov(x, y), 3), \"\\n\")\n\n\nCovariance: 3.479 \n\n\n\n\n\n\n\n\nFigure 21.1: Visualization of covariance showing how products of deviations from means contribute to the overall covariance, with positive products in upper-right and lower-left quadrants\n\n\n\n\n\nWhen points cluster in the positive quadrants (upper-right and lower-left), the covariance is positive. When points cluster in the negative quadrants, the covariance is negative. When points are evenly distributed, covariance is near zero.\nThe problem with covariance is that its magnitude depends on the scales of X and Y, making it hard to interpret. Is a covariance of 100 strong or weak? It depends entirely on the units of measurement.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Correlation</span>"
    ]
  },
  {
    "objectID": "chapters/15-correlation.html#pearsons-correlation-coefficient",
    "href": "chapters/15-correlation.html#pearsons-correlation-coefficient",
    "title": "21  Correlation",
    "section": "21.3 Pearson’s Correlation Coefficient",
    "text": "21.3 Pearson’s Correlation Coefficient\nThe Pearson correlation coefficient standardizes covariance by dividing by the product of the standard deviations:\n\\[r = \\frac{Cov(X, Y)}{s_X s_Y} = \\frac{\\sum_{i=1}^{n}(x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum(x_i - \\bar{x})^2 \\sum(y_i - \\bar{y})^2}}\\]\nThis produces a value between -1 and +1:\n\n\\(r = 1\\): perfect positive linear relationship\n\\(r = -1\\): perfect negative linear relationship\n\n\\(r = 0\\): no linear relationship\n\n\n\n\n\n\n\nFigure 21.2: Examples of scatterplots with different correlation coefficients ranging from strong positive to strong negative\n\n\n\n\n\nCode\n# Examples of different correlations\nset.seed(42)\nn &lt;- 100\n\npar(mfrow = c(2, 2))\n\n# Strong positive\nx1 &lt;- rnorm(n)\ny1 &lt;- 0.9 * x1 + rnorm(n, sd = 0.4)\nplot(x1, y1, main = paste(\"r =\", round(cor(x1, y1), 2)), pch = 19, col = \"blue\")\n\n# Moderate negative\ny2 &lt;- -0.6 * x1 + rnorm(n, sd = 0.8)\nplot(x1, y2, main = paste(\"r =\", round(cor(x1, y2), 2)), pch = 19, col = \"red\")\n\n# No correlation\ny3 &lt;- rnorm(n)\nplot(x1, y3, main = paste(\"r =\", round(cor(x1, y3), 2)), pch = 19, col = \"gray\")\n\n# Non-linear relationship (correlation misleading)\nx4 &lt;- runif(n, -3, 3)\ny4 &lt;- x4^2 + rnorm(n, sd = 0.5)\nplot(x4, y4, main = paste(\"r =\", round(cor(x4, y4), 2), \"(non-linear!)\"), \n     pch = 19, col = \"purple\")\n\n\n\n\n\n\n\n\nFigure 21.3: Four scatterplots demonstrating different correlation patterns: strong positive, moderate negative, no correlation, and a non-linear relationship where correlation is misleading",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Correlation</span>"
    ]
  },
  {
    "objectID": "chapters/15-correlation.html#anscombes-quartet",
    "href": "chapters/15-correlation.html#anscombes-quartet",
    "title": "21  Correlation",
    "section": "21.4 Anscombe’s Quartet",
    "text": "21.4 Anscombe’s Quartet\nFrancis Anscombe created a famous set of four datasets that all have nearly identical statistical properties—same means, variances, correlations, and regression lines—yet look completely different when plotted. This demonstrates why visualization is essential.\n\n\n\n\n\n\nFigure 21.4: Anscombe’s Quartet showing four datasets with identical statistical properties but very different patterns when visualized\n\n\n\nAlways plot your data before calculating correlations. The correlation coefficient captures only linear relationships and can be misleading for non-linear patterns.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Correlation</span>"
    ]
  },
  {
    "objectID": "chapters/15-correlation.html#testing-correlation",
    "href": "chapters/15-correlation.html#testing-correlation",
    "title": "21  Correlation",
    "section": "21.5 Testing Correlation",
    "text": "21.5 Testing Correlation\nThe cor.test() function tests whether a correlation is significantly different from zero:\n\n\nCode\n# Example: zebrafish length and weight\nset.seed(123)\nlength &lt;- rnorm(50, mean = 2.5, sd = 0.5)\nweight &lt;- 10 * length^2 + rnorm(50, sd = 5)\n\ncor.test(length, weight)\n\n\n\n    Pearson's product-moment correlation\n\ndata:  length and weight\nt = 29.857, df = 48, p-value &lt; 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.9546054 0.9853063\nsample estimates:\n     cor \n0.974118 \n\n\nThe null hypothesis is that the population correlation is zero (\\(H_0: \\rho = 0\\)). A small p-value indicates evidence of a non-zero correlation.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Correlation</span>"
    ]
  },
  {
    "objectID": "chapters/15-correlation.html#sample-correlation-as-a-random-variable",
    "href": "chapters/15-correlation.html#sample-correlation-as-a-random-variable",
    "title": "21  Correlation",
    "section": "21.6 Sample Correlation as a Random Variable",
    "text": "21.6 Sample Correlation as a Random Variable\nJust like the sample mean, the sample correlation coefficient is a random variable—it varies from sample to sample. If we could repeatedly draw samples from the same population and compute r for each, we would get a distribution of r values centered around the true population correlation \\(\\rho\\).\nThis sampling variability has important implications for interpretation. A sample correlation of r = 0.3 from a small study might arise even when the true correlation is zero (or is actually 0.5). Understanding this uncertainty is essential for proper inference.\n\n\nCode\n# Demonstrate sampling variability of correlation\nset.seed(42)\n\n# True population parameters\nrho_true &lt;- 0.5  # True population correlation\nn_small &lt;- 20\nn_large &lt;- 100\nn_reps &lt;- 1000\n\n# Function to generate correlated data\ngenerate_correlated_data &lt;- function(n, rho) {\n  x &lt;- rnorm(n)\n  y &lt;- rho * x + sqrt(1 - rho^2) * rnorm(n)\n  return(cor(x, y))\n}\n\n# Generate sampling distributions for different sample sizes\nr_small &lt;- replicate(n_reps, generate_correlated_data(n_small, rho_true))\nr_large &lt;- replicate(n_reps, generate_correlated_data(n_large, rho_true))\n\n# Plot sampling distributions\npar(mfrow = c(1, 2))\n\nhist(r_small, breaks = 30, col = \"lightblue\",\n     main = paste(\"Sampling Distribution of r\\n(n =\", n_small, \")\"),\n     xlab = \"Sample Correlation\", xlim = c(-0.2, 1))\nabline(v = rho_true, col = \"red\", lwd = 2, lty = 2)\nabline(v = mean(r_small), col = \"blue\", lwd = 2)\nlegend(\"topleft\", legend = c(paste(\"True ρ =\", rho_true),\n                              paste(\"Mean r =\", round(mean(r_small), 3))),\n       col = c(\"red\", \"blue\"), lty = c(2, 1), lwd = 2, cex = 0.8)\n\nhist(r_large, breaks = 30, col = \"lightgreen\",\n     main = paste(\"Sampling Distribution of r\\n(n =\", n_large, \")\"),\n     xlab = \"Sample Correlation\", xlim = c(-0.2, 1))\nabline(v = rho_true, col = \"red\", lwd = 2, lty = 2)\nabline(v = mean(r_large), col = \"blue\", lwd = 2)\nlegend(\"topleft\", legend = c(paste(\"True ρ =\", rho_true),\n                              paste(\"Mean r =\", round(mean(r_large), 3))),\n       col = c(\"red\", \"blue\"), lty = c(2, 1), lwd = 2, cex = 0.8)\n\n\n\n\n\n\n\n\nFigure 21.5: Sampling distributions of correlation coefficient r for small (n=20) and large (n=100) samples, demonstrating how sample size affects the reliability of correlation estimates\n\n\n\n\n\nNotice the dramatic difference in variability. With n = 20, sample correlations range widely around the true value—sometimes even appearing negative when the true correlation is 0.5! With n = 100, the distribution is much tighter, and our sample r is a more reliable estimate of \\(\\rho\\).\n\n\nCode\n# Quantify the variability\ncat(\"True population correlation: ρ =\", rho_true, \"\\n\\n\")\n\n\nTrue population correlation: ρ = 0.5 \n\n\nCode\ncat(\"With n =\", n_small, \":\\n\")\n\n\nWith n = 20 :\n\n\nCode\ncat(\"  Mean of sample r:\", round(mean(r_small), 3), \"\\n\")\n\n\n  Mean of sample r: 0.483 \n\n\nCode\ncat(\"  SD of sample r:\", round(sd(r_small), 3), \"\\n\")\n\n\n  SD of sample r: 0.181 \n\n\nCode\ncat(\"  95% of samples give r between\", round(quantile(r_small, 0.025), 3),\n    \"and\", round(quantile(r_small, 0.975), 3), \"\\n\\n\")\n\n\n  95% of samples give r between 0.056 and 0.781 \n\n\nCode\ncat(\"With n =\", n_large, \":\\n\")\n\n\nWith n = 100 :\n\n\nCode\ncat(\"  Mean of sample r:\", round(mean(r_large), 3), \"\\n\")\n\n\n  Mean of sample r: 0.497 \n\n\nCode\ncat(\"  SD of sample r:\", round(sd(r_large), 3), \"\\n\")\n\n\n  SD of sample r: 0.075 \n\n\nCode\ncat(\"  95% of samples give r between\", round(quantile(r_large, 0.025), 3),\n    \"and\", round(quantile(r_large, 0.975), 3), \"\\n\")\n\n\n  95% of samples give r between 0.347 and 0.634 \n\n\n\n\n\n\n\n\nPractical Implications\n\n\n\n\nSmall samples yield unreliable correlations: With n &lt; 30, sample r can differ substantially from \\(\\rho\\)\nConfidence intervals are essential: Report CIs to communicate uncertainty, not just the point estimate\nReplication matters: A single study with r = 0.4 (n = 25) is consistent with true correlations anywhere from near-zero to quite strong\nPublication bias distorts the literature: Studies with “significant” correlations are more likely to be published, inflating effect sizes in the literature\n\n\n\n\nFisher’s Z-Transformation\nThe sampling distribution of r is not symmetric, especially when \\(\\rho\\) is far from zero. Fisher’s z-transformation stabilizes the variance and makes the distribution approximately normal:\n\\[z = \\frac{1}{2} \\ln\\left(\\frac{1 + r}{1 - r}\\right) = \\text{arctanh}(r)\\]\nThe standard error of z is approximately \\(\\frac{1}{\\sqrt{n-3}}\\), which depends only on sample size—not on the true correlation. This transformation is used to construct confidence intervals for correlation and to compare correlations across groups.\n\n\nCode\n# Fisher's z-transformation for confidence interval\nr_observed &lt;- 0.6\nn &lt;- 50\n\n# Transform to z\nz &lt;- atanh(r_observed)  # Same as 0.5 * log((1 + r) / (1 - r))\nse_z &lt;- 1 / sqrt(n - 3)\n\n# 95% CI in z scale\nz_lower &lt;- z - 1.96 * se_z\nz_upper &lt;- z + 1.96 * se_z\n\n# Transform back to r scale\nr_lower &lt;- tanh(z_lower)\nr_upper &lt;- tanh(z_upper)\n\ncat(\"Observed r:\", r_observed, \"with n =\", n, \"\\n\")\n\n\nObserved r: 0.6 with n = 50 \n\n\nCode\ncat(\"95% CI for ρ: [\", round(r_lower, 3), \",\", round(r_upper, 3), \"]\\n\")\n\n\n95% CI for ρ: [ 0.386 , 0.753 ]",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Correlation</span>"
    ]
  },
  {
    "objectID": "chapters/15-correlation.html#parametric-assumptions",
    "href": "chapters/15-correlation.html#parametric-assumptions",
    "title": "21  Correlation",
    "section": "21.7 Parametric Assumptions",
    "text": "21.7 Parametric Assumptions\nPearson’s correlation assumes that both variables are normally distributed (or at least that the relationship is linear and homoscedastic). When these assumptions are violated, nonparametric alternatives may be more appropriate.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Correlation</span>"
    ]
  },
  {
    "objectID": "chapters/15-correlation.html#nonparametric-correlation",
    "href": "chapters/15-correlation.html#nonparametric-correlation",
    "title": "21  Correlation",
    "section": "21.8 Nonparametric Correlation",
    "text": "21.8 Nonparametric Correlation\nSpearman’s rank correlation replaces values with their ranks before calculating correlation. It measures monotonic (consistently increasing or decreasing) rather than strictly linear relationships and is robust to outliers.\nKendall’s tau is another rank-based measure that counts concordant and discordant pairs. It is particularly appropriate for small samples or data with many ties.\n\n\nCode\n# Compare methods on non-normal data\nset.seed(42)\nx &lt;- rexp(30, rate = 0.1)\ny &lt;- x + rexp(30, rate = 0.2)\n\ncat(\"Pearson:\", round(cor(x, y, method = \"pearson\"), 3), \"\\n\")\n\n\nPearson: 0.764 \n\n\nCode\ncat(\"Spearman:\", round(cor(x, y, method = \"spearman\"), 3), \"\\n\")\n\n\nSpearman: 0.808 \n\n\nCode\ncat(\"Kendall:\", round(cor(x, y, method = \"kendall\"), 3), \"\\n\")\n\n\nKendall: 0.623",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Correlation</span>"
    ]
  },
  {
    "objectID": "chapters/15-correlation.html#correlation-is-not-causation",
    "href": "chapters/15-correlation.html#correlation-is-not-causation",
    "title": "21  Correlation",
    "section": "21.9 Correlation Is Not Causation",
    "text": "21.9 Correlation Is Not Causation\nA correlation between X and Y might arise because X causes Y, because Y causes X, because a third variable Z causes both, or simply by chance. Correlation alone cannot distinguish these possibilities.\nTo establish causation, you need experimental manipulation (changing X and observing Y), temporal precedence (X occurs before Y), and ruling out confounding variables. Observational correlations are valuable for generating hypotheses but insufficient for establishing causal relationships.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Correlation</span>"
    ]
  },
  {
    "objectID": "chapters/15-correlation.html#interpreting-correlation-magnitude",
    "href": "chapters/15-correlation.html#interpreting-correlation-magnitude",
    "title": "21  Correlation",
    "section": "21.10 Interpreting Correlation Magnitude",
    "text": "21.10 Interpreting Correlation Magnitude\nWhile there are no universal standards, these guidelines provide rough interpretation:\n\n\n\n\nr\n\n\n\n\n0.00 - 0.19\nNegligible\n\n\n0.20 - 0.39\nWeak\n\n\n0.40 - 0.59\nModerate\n\n\n0.60 - 0.79\nStrong\n\n\n0.80 - 1.00\nVery strong\n\n\n\nContext matters greatly. In physics, correlations below 0.99 might be disappointing. In psychology or ecology, correlations of 0.3 can be considered meaningful.\n\nCoefficient of Determination\nThe square of the correlation coefficient, \\(r^2\\), is called the coefficient of determination. It represents the proportion of variance in one variable that is explained by its linear relationship with the other.\nIf \\(r = 0.7\\), then \\(r^2 = 0.49\\), meaning about 49% of the variance in Y is explained by its relationship with X. This leaves 51% unexplained—due to other factors, measurement error, or the relationship not being perfectly linear.\n\n\nCode\n# Visualize explained vs unexplained variance\nset.seed(123)\nx &lt;- 1:50\ny &lt;- 2 * x + rnorm(50, sd = 15)\n\nr &lt;- cor(x, y)\nr_squared &lt;- r^2\n\ncat(\"Correlation (r):\", round(r, 3), \"\\n\")\n\n\nCorrelation (r): 0.901 \n\n\nCode\ncat(\"R-squared (r²):\", round(r_squared, 3), \"\\n\")\n\n\nR-squared (r²): 0.812 \n\n\nCode\ncat(\"Variance explained:\", round(r_squared * 100, 1), \"%\\n\")\n\n\nVariance explained: 81.2 %",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Correlation</span>"
    ]
  },
  {
    "objectID": "chapters/15-correlation.html#common-pitfalls-with-correlation",
    "href": "chapters/15-correlation.html#common-pitfalls-with-correlation",
    "title": "21  Correlation",
    "section": "21.11 Common Pitfalls with Correlation",
    "text": "21.11 Common Pitfalls with Correlation\n\n\n\n\n\n\nWatch Out For These Mistakes\n\n\n\n\nRestricted range: If you only sample part of the range of X or Y, correlation will appear weaker than it truly is\nOutliers: A single extreme point can dramatically inflate or deflate the correlation\nNon-linearity: Correlation only measures linear relationships; a perfect curved relationship can have r ≈ 0\nAggregation effects: Correlations computed on grouped data (e.g., country averages) are often much stronger than correlations on individual data (ecological fallacy)\nConfounding: A third variable may create a spurious correlation between X and Y\n\n\n\n\n\nCode\n# Demonstrating the effect of outliers\nset.seed(42)\nx_base &lt;- rnorm(30)\ny_base &lt;- rnorm(30)\n\npar(mfrow = c(1, 2))\n\n# Without outlier\nplot(x_base, y_base, pch = 19, col = \"blue\",\n     main = paste(\"Without outlier: r =\", round(cor(x_base, y_base), 3)),\n     xlab = \"X\", ylab = \"Y\", xlim = c(-3, 5), ylim = c(-3, 5))\n\n# With outlier\nx_out &lt;- c(x_base, 4)\ny_out &lt;- c(y_base, 4)\nplot(x_out, y_out, pch = 19, col = c(rep(\"blue\", 30), \"red\"),\n     main = paste(\"With outlier: r =\", round(cor(x_out, y_out), 3)),\n     xlab = \"X\", ylab = \"Y\", xlim = c(-3, 5), ylim = c(-3, 5))\n\n\n\n\n\n\n\n\nFigure 21.6: Effect of a single outlier on correlation, showing how r changes from near zero to moderately positive with the addition of one extreme point\n\n\n\n\n\nA single outlier has shifted the correlation from near zero to moderately positive. Always visualize your data and consider robust alternatives like Spearman’s correlation when outliers are present.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Correlation</span>"
    ]
  },
  {
    "objectID": "chapters/15-correlation.html#summary",
    "href": "chapters/15-correlation.html#summary",
    "title": "21  Correlation",
    "section": "21.12 Summary",
    "text": "21.12 Summary\nCorrelation quantifies the strength and direction of linear relationships between variables:\n\nCovariance measures how variables move together, but depends on units\nPearson’s r standardizes covariance to range from -1 to +1\nSpearman and Kendall provide robust rank-based alternatives\nCorrelation does not imply causation\nAlways visualize data and check for non-linearity and outliers",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Correlation</span>"
    ]
  },
  {
    "objectID": "chapters/15-correlation.html#practice-exercises",
    "href": "chapters/15-correlation.html#practice-exercises",
    "title": "21  Correlation",
    "section": "21.13 Practice Exercises",
    "text": "21.13 Practice Exercises\n\n\n\n\n\n\nExercise L.1: Correlation\n\n\n\n\nCreate two correlated variables using simulation\nCalculate the Pearson correlation coefficient\nCreate a scatterplot and add the correlation value\nWhat happens to the correlation when you add outliers?\n\n\n\nCode\nset.seed(42)\nx &lt;- rnorm(50)\ny &lt;- 0.7*x + rnorm(50, sd = 0.5)  # Correlated with x\n\ncor(x, y)\n\n\n[1] 0.8400841\n\n\nCode\nplot(x, y, main = paste(\"Correlation:\", round(cor(x, y), 2)))\nabline(lm(y ~ x), col = \"red\")\n\n\n\n\n\n\n\n\nFigure 21.7: Scatterplot of two correlated variables with fitted regression line\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise L.2: Pearson vs. Spearman\n\n\n\nCompare Pearson and Spearman correlations:\n\nGenerate data with a monotonic but non-linear relationship\nCalculate both Pearson and Spearman correlations\nWhich is more appropriate for your data and why?\n\n\n\nCode\n# Monotonic but non-linear\nset.seed(42)\nx &lt;- 1:50\ny &lt;- log(x) + rnorm(50, sd = 0.3)\n\ncor(x, y, method = \"pearson\")\ncor(x, y, method = \"spearman\")\n\n\n\n\n\n\n\n\n\n\nExercise L.3: Effect of Outliers\n\n\n\nExplore how outliers affect correlation:\n\nGenerate correlated data with no outliers—calculate r\nAdd one extreme outlier—recalculate r\nUse Spearman’s correlation on the data with the outlier\nWhat do you conclude about robustness?\n\n\n\nCode\n# Your code here\n\n\n\n\n\n\n\n\n\n\nExercise L.4: Partial Correlation\n\n\n\nYou have three variables: temperature (T), enzyme activity (E), and pH (P). Both temperature and pH affect enzyme activity.\ntemperature &lt;- c(20, 25, 30, 35, 40, 45, 50, 55, 60)\npH &lt;- c(5.5, 6.0, 6.5, 7.0, 7.5, 8.0, 7.5, 7.0, 6.5)\nenzyme_activity &lt;- c(12, 18, 25, 35, 42, 38, 28, 20, 15)\n\nCalculate the correlation between temperature and enzyme activity\nCalculate the correlation between pH and enzyme activity\nCalculate the correlation between temperature and pH\nFit a multiple regression model predicting enzyme activity from both temperature and pH\nCalculate the partial correlation between temperature and enzyme activity (controlling for pH) using the ppcor package or manually from residuals\nInterpret the difference between the simple and partial correlations\n\n\n\nCode\n# Your code here\n# install.packages(\"ppcor\")  # if needed\nlibrary(ppcor)\n\n\n\n\n\n\n\n\n\n\nExercise L.5: Correlation Matrix and Visualization\n\n\n\nYou have measurements of four morphological traits in a sample of fish:\nset.seed(123)\nn &lt;- 50\nbody_length &lt;- rnorm(n, mean = 150, sd = 20)\nbody_weight &lt;- 0.001 * body_length^3 + rnorm(n, sd = 50)\nfin_length &lt;- 0.15 * body_length + rnorm(n, sd = 3)\neye_diameter &lt;- 8 + 0.05 * body_length + rnorm(n, sd = 1)\n\nfish_data &lt;- data.frame(body_length, body_weight, fin_length, eye_diameter)\n\nCalculate the correlation matrix for all four variables\nCreate a pairs plot (scatterplot matrix) to visualize all pairwise relationships\nUse the corrplot package to create a correlation heatmap\nWhich pairs of variables are most strongly correlated?\nFor the pair with the strongest correlation, test whether it is significantly different from zero\nAre any of the relationships clearly non-linear? How does this affect the correlation coefficient?\n\n\n\nCode\n# Your code here\n# install.packages(\"corrplot\")  # if needed\nlibrary(corrplot)",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Correlation</span>"
    ]
  },
  {
    "objectID": "chapters/15-correlation.html#additional-resources",
    "href": "chapters/15-correlation.html#additional-resources",
    "title": "21  Correlation",
    "section": "21.14 Additional Resources",
    "text": "21.14 Additional Resources\n\nLogan (2010) - Detailed coverage of correlation with biological examples\nJames et al. (2023) - Excellent discussion of correlation in the context of statistical learning\n\n\n\n\n\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2023. An Introduction to Statistical Learning with Applications in r. 2nd ed. Springer. https://www.statlearning.com.\n\n\nLogan, Murray. 2010. Biostatistical Design and Analysis Using r. Wiley-Blackwell.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Correlation</span>"
    ]
  },
  {
    "objectID": "chapters/16-simple-linear-regression.html",
    "href": "chapters/16-simple-linear-regression.html",
    "title": "22  Simple Linear Regression",
    "section": "",
    "text": "22.1 From Correlation to Prediction\nCorrelation tells us that two variables are related, but it does not allow us to predict one from the other or to describe the nature of that relationship. Linear regression goes further—it models the relationship between variables, allowing us to make predictions and to quantify how changes in one variable are associated with changes in another.\nIn simple linear regression, we model a response variable Y as a linear function of a predictor variable X:\n\\[y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i\\]\nHere \\(\\beta_0\\) is the intercept (the expected value of Y when X equals zero), \\(\\beta_1\\) is the slope (how much Y changes for a one-unit change in X), and \\(\\epsilon_i\\) represents the random error—the part of Y not explained by X.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "chapters/16-simple-linear-regression.html#from-correlation-to-prediction",
    "href": "chapters/16-simple-linear-regression.html#from-correlation-to-prediction",
    "title": "22  Simple Linear Regression",
    "section": "",
    "text": "Figure 22.1: Conceptual diagram of simple linear regression showing the relationship between predictor X and response Y with the linear model equation",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "chapters/16-simple-linear-regression.html#origins-of-the-term-regression",
    "href": "chapters/16-simple-linear-regression.html#origins-of-the-term-regression",
    "title": "22  Simple Linear Regression",
    "section": "22.2 Origins of the Term “Regression”",
    "text": "22.2 Origins of the Term “Regression”\nThe term “regression” comes from Francis Galton’s studies of heredity in the 1880s. He observed that tall parents tended to have children who were tall, but not as extremely tall as the parents—children’s heights “regressed” toward the population mean. This phenomenon, now called regression to the mean, is a statistical artifact that appears whenever two variables are imperfectly correlated.\n\n\n\n\n\n\nFigure 22.2: Historical illustration of Galton’s regression to the mean, showing how offspring heights regress toward the population mean",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "chapters/16-simple-linear-regression.html#the-regression-fallacy",
    "href": "chapters/16-simple-linear-regression.html#the-regression-fallacy",
    "title": "22  Simple Linear Regression",
    "section": "22.3 The Regression Fallacy",
    "text": "22.3 The Regression Fallacy\nUnderstanding regression to the mean is crucial because ignoring it leads to a common error in reasoning called the regression fallacy. This occurs when we attribute regression to the mean to some other cause—typically claiming credit for improvement that was simply statistical regression.\nThe most famous example is the “Sophomore Slump” in baseball. The player who wins Rookie of the Year typically performs worse in their second season. Sportswriters often blame this on pressure, complacency, or teams “figuring out” the player. But much of this decline is simply regression to the mean.\nConsider why: to win Rookie of the Year, a player typically needs exceptional performance—often their personal best. This outstanding season likely involved skill plus some good luck (favorable conditions, timely hits, etc.). In the following season, luck averages out, and performance regresses toward the player’s true ability level.\n\n\nCode\n# Simulating the Sophomore Slump\nset.seed(42)\nn_players &lt;- 500\n\n# True talent level for each player (varies between players)\ntrue_talent &lt;- rnorm(n_players, mean = 0.265, sd = 0.020)\n\n# Season 1: observed performance = true talent + luck\nluck_season1 &lt;- rnorm(n_players, mean = 0, sd = 0.025)\nbatting_avg_yr1 &lt;- true_talent + luck_season1\n\n# Season 2: observed performance = same true talent + different luck\nluck_season2 &lt;- rnorm(n_players, mean = 0, sd = 0.025)\nbatting_avg_yr2 &lt;- true_talent + luck_season2\n\n# Find the \"Rookie of the Year\" - best performer in year 1\nroy_idx &lt;- which.max(batting_avg_yr1)\n\n# Look at top 10 performers in year 1\ntop_10 &lt;- order(batting_avg_yr1, decreasing = TRUE)[1:10]\n\ncat(\"Top 10 performers in Year 1 vs Year 2:\\n\")\n\n\nTop 10 performers in Year 1 vs Year 2:\n\n\nCode\ncat(\"========================================\\n\")\n\n\n========================================\n\n\nCode\nfor (i in 1:10) {\n  idx &lt;- top_10[i]\n  change &lt;- batting_avg_yr2[idx] - batting_avg_yr1[idx]\n  cat(sprintf(\"Player %d: Yr1 = %.3f, Yr2 = %.3f, Change = %+.3f\\n\",\n              i, batting_avg_yr1[idx], batting_avg_yr2[idx], change))\n}\n\n\nPlayer 1: Yr1 = 0.384, Yr2 = 0.291, Change = -0.093\nPlayer 2: Yr1 = 0.366, Yr2 = 0.272, Change = -0.094\nPlayer 3: Yr1 = 0.357, Yr2 = 0.298, Change = -0.059\nPlayer 4: Yr1 = 0.352, Yr2 = 0.240, Change = -0.112\nPlayer 5: Yr1 = 0.349, Yr2 = 0.328, Change = -0.021\nPlayer 6: Yr1 = 0.343, Yr2 = 0.301, Change = -0.042\nPlayer 7: Yr1 = 0.337, Yr2 = 0.299, Change = -0.038\nPlayer 8: Yr1 = 0.334, Yr2 = 0.325, Change = -0.009\nPlayer 9: Yr1 = 0.329, Yr2 = 0.242, Change = -0.087\nPlayer 10: Yr1 = 0.329, Yr2 = 0.283, Change = -0.046\n\n\nCode\n# How many of top 10 declined?\ndeclines &lt;- sum(batting_avg_yr2[top_10] &lt; batting_avg_yr1[top_10])\ncat(sprintf(\"\\n%d of top 10 performers showed a decline (the 'slump')\\n\", declines))\n\n\n\n10 of top 10 performers showed a decline (the 'slump')\n\n\nNotice that nearly all of the top performers declined—not because of anything about being a sophomore, but because extreme initial performance tends to be followed by more average performance.\n\n\nCode\n# Visualize regression to the mean\nplot(batting_avg_yr1, batting_avg_yr2,\n     pch = 19, col = rgb(0, 0, 0, 0.3),\n     xlab = \"Year 1 Batting Average\",\n     ylab = \"Year 2 Batting Average\",\n     main = \"Regression to the Mean: The Sophomore Slump\",\n     xlim = c(0.20, 0.35), ylim = c(0.20, 0.35))\n\n# Add y = x line (what we'd see with no regression)\nabline(a = 0, b = 1, col = \"gray\", lty = 2, lwd = 2)\n\n# Add regression line\nreg_line &lt;- lm(batting_avg_yr2 ~ batting_avg_yr1)\nabline(reg_line, col = \"red\", lwd = 2)\n\n# Highlight top performers\npoints(batting_avg_yr1[top_10], batting_avg_yr2[top_10],\n       pch = 19, col = \"blue\", cex = 1.5)\n\n# Mark the \"Rookie of the Year\"\npoints(batting_avg_yr1[roy_idx], batting_avg_yr2[roy_idx],\n       pch = 17, col = \"red\", cex = 2)\n\nlegend(\"topleft\",\n       legend = c(\"All players\", \"Top 10 Year 1\", \"Best performer\",\n                  \"No regression (y=x)\", \"Regression line\"),\n       pch = c(19, 19, 17, NA, NA),\n       lty = c(NA, NA, NA, 2, 1),\n       col = c(rgb(0,0,0,0.3), \"blue\", \"red\", \"gray\", \"red\"),\n       lwd = c(NA, NA, NA, 2, 2))\n\n\n\n\n\n\n\n\nFigure 22.3: Visualization of regression to the mean in baseball batting averages, showing how top Year 1 performers tend to decline in Year 2, not due to a sophomore slump but statistical regression\n\n\n\n\n\nThe dashed line shows what we would see if Year 1 perfectly predicted Year 2 (no regression to the mean). The red regression line shows reality—it’s flatter, meaning extreme Year 1 performers tend to move toward the center in Year 2.\n\n\n\n\n\n\nAvoiding the Regression Fallacy\n\n\n\nThe regression fallacy appears in many contexts:\n\nMedical treatments: Patients seek treatment when symptoms are worst; subsequent improvement may be regression, not treatment effect\nPerformance management: Workers reprimanded for poor performance often improve; those praised for good performance often decline—both may be regression\nEducational interventions: Students identified as struggling (tested at their worst) often improve regardless of intervention\n\nWhen evaluating any intervention applied to extreme cases, always consider whether observed changes might simply be regression to the mean.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "chapters/16-simple-linear-regression.html#fitting-the-model-ordinary-least-squares",
    "href": "chapters/16-simple-linear-regression.html#fitting-the-model-ordinary-least-squares",
    "title": "22  Simple Linear Regression",
    "section": "22.4 Fitting the Model: Ordinary Least Squares",
    "text": "22.4 Fitting the Model: Ordinary Least Squares\nThe most common method for fitting a regression line is ordinary least squares (OLS). OLS finds the line that minimizes the sum of squared residuals—the squared vertical distances between observed points and the fitted line.\n\n\n\n\n\n\nFigure 22.4: Illustration of ordinary least squares showing how the regression line minimizes the sum of squared vertical distances (residuals) from points to the line\n\n\n\nThe OLS estimates for the slope and intercept are:\n\\[\\hat{\\beta}_1 = \\frac{\\sum(x_i - \\bar{x})(y_i - \\bar{y})}{\\sum(x_i - \\bar{x})^2} = r \\frac{s_y}{s_x}\\]\n\\[\\hat{\\beta}_0 = \\bar{y} - \\hat{\\beta}_1 \\bar{x}\\]\nNotice that the slope equals the correlation coefficient times the ratio of standard deviations. This makes clear the connection between correlation and regression.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "chapters/16-simple-linear-regression.html#linear-regression-in-r",
    "href": "chapters/16-simple-linear-regression.html#linear-regression-in-r",
    "title": "22  Simple Linear Regression",
    "section": "22.5 Linear Regression in R",
    "text": "22.5 Linear Regression in R\n\n\nCode\n# Example: zebrafish size data\nset.seed(42)\nn &lt;- 100\nlength_cm &lt;- runif(n, 0.5, 3.5)\nweight_mg &lt;- 15 * length_cm^2 + rnorm(n, sd = 10)\n\n# Fit the model\nfish_lm &lt;- lm(weight_mg ~ length_cm)\nsummary(fish_lm)\n\n\n\nCall:\nlm(formula = weight_mg ~ length_cm)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-39.277  -9.033  -0.432   9.998  29.934 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  -51.429      3.579  -14.37   &lt;2e-16 ***\nlength_cm     61.660      1.583   38.95   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 14.27 on 98 degrees of freedom\nMultiple R-squared:  0.9393,    Adjusted R-squared:  0.9387 \nF-statistic:  1517 on 1 and 98 DF,  p-value: &lt; 2.2e-16\n\n\nThe output shows the estimated coefficients with their standard errors, t-statistics, and p-values. The Multiple R-squared indicates how much of the variance in Y is explained by X.\n\n\nCode\n# Visualize the fit\nplot(length_cm, weight_mg, pch = 19, col = \"blue\",\n     xlab = \"Length (cm)\", ylab = \"Weight (mg)\",\n     main = \"Linear Regression: Weight vs Length\")\nabline(fish_lm, col = \"red\", lwd = 2)\n\n\n\n\n\n\n\n\nFigure 22.5: Linear regression of zebrafish weight versus length showing fitted regression line",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "chapters/16-simple-linear-regression.html#interpretation-of-coefficients",
    "href": "chapters/16-simple-linear-regression.html#interpretation-of-coefficients",
    "title": "22  Simple Linear Regression",
    "section": "22.6 Interpretation of Coefficients",
    "text": "22.6 Interpretation of Coefficients\nThe intercept \\(\\hat{\\beta}_0\\) is the predicted value of Y when X equals zero. This may or may not be meaningful depending on whether X = 0 makes sense in your context.\nThe slope \\(\\hat{\\beta}_1\\) is the predicted change in Y for a one-unit increase in X. If the slope is 15, then each additional unit of X is associated with 15 more units of Y on average.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "chapters/16-simple-linear-regression.html#hypothesis-testing-in-regression",
    "href": "chapters/16-simple-linear-regression.html#hypothesis-testing-in-regression",
    "title": "22  Simple Linear Regression",
    "section": "22.7 Hypothesis Testing in Regression",
    "text": "22.7 Hypothesis Testing in Regression\nThe hypothesis test for the slope asks whether there is evidence of a relationship between X and Y:\n\\[H_0: \\beta_1 = 0 \\quad \\text{(no relationship)}\\] \\[H_A: \\beta_1 \\neq 0 \\quad \\text{(relationship exists)}\\]\nThe test uses a t-statistic, comparing the estimated slope to its standard error. The p-value indicates the probability of observing a slope this far from zero if the true slope were zero.\n\n\n\n\n\n\nFigure 22.6: Hypothesis testing in regression showing the t-distribution for testing whether the slope coefficient differs from zero",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "chapters/16-simple-linear-regression.html#r-squared-measuring-model-fit",
    "href": "chapters/16-simple-linear-regression.html#r-squared-measuring-model-fit",
    "title": "22  Simple Linear Regression",
    "section": "22.8 R-Squared: Measuring Model Fit",
    "text": "22.8 R-Squared: Measuring Model Fit\nR-squared (\\(R^2\\)) measures the proportion of variance in Y explained by the model:\n\\[R^2 = 1 - \\frac{SS_{error}}{SS_{total}} = \\frac{SS_{model}}{SS_{total}}\\]\nIn simple linear regression, \\(R^2\\) equals the square of the correlation coefficient. An \\(R^2\\) of 0.7 means the model explains 70% of the variance in Y; the remaining 30% is unexplained.\nBe cautious with \\(R^2\\)—it always increases when you add predictors, even useless ones. Adjusted \\(R^2\\) penalizes for model complexity.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "chapters/16-simple-linear-regression.html#making-predictions",
    "href": "chapters/16-simple-linear-regression.html#making-predictions",
    "title": "22  Simple Linear Regression",
    "section": "22.9 Making Predictions",
    "text": "22.9 Making Predictions\nOnce you have a fitted model, you can predict Y for new values of X:\n\n\nCode\n# Predict weight for new lengths\nnew_lengths &lt;- data.frame(length_cm = c(1.0, 2.0, 3.0))\npredict(fish_lm, newdata = new_lengths, interval = \"confidence\")\n\n\n        fit        lwr       upr\n1  10.23108   5.828292  14.63387\n2  71.89135  69.050838  74.73186\n3 133.55162 129.491293 137.61194\n\n\nThe confidence interval indicates uncertainty about the mean Y at each X value. A prediction interval (using interval = \"prediction\") would be wider, accounting for individual variability around that mean.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "chapters/16-simple-linear-regression.html#tidyverse-approach-the-broom-package",
    "href": "chapters/16-simple-linear-regression.html#tidyverse-approach-the-broom-package",
    "title": "22  Simple Linear Regression",
    "section": "22.10 Tidyverse Approach: The broom Package",
    "text": "22.10 Tidyverse Approach: The broom Package\nThe summary() output from lm() is informative but awkward to work with programmatically. The broom package provides three functions that convert model output into tidy tibbles, making it easy to use regression results in tidyverse workflows.\n\ntidy(): Extract Coefficient Statistics\nThe tidy() function extracts coefficient estimates, standard errors, test statistics, and p-values as a tibble:\n\n\nCode\nlibrary(broom)\n\n# Standard summary output (not tidy)\nsummary(fish_lm)$coefficients\n\n\n             Estimate Std. Error   t value     Pr(&gt;|t|)\n(Intercept) -51.42918   3.578653 -14.37110 7.241174e-26\nlength_cm    61.66027   1.582874  38.95464 1.914443e-61\n\n\nCode\n# Tidy output - much easier to work with\ntidy(fish_lm)\n\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)    -51.4      3.58     -14.4 7.24e-26\n2 length_cm       61.7      1.58      39.0 1.91e-61\n\n\nThe tidy format makes it easy to filter, arrange, or visualize coefficients:\n\n\nCode\n# Extract just significant coefficients\ntidy(fish_lm) |&gt;\n  filter(p.value &lt; 0.05)\n\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)    -51.4      3.58     -14.4 7.24e-26\n2 length_cm       61.7      1.58      39.0 1.91e-61\n\n\nCode\n# Add confidence intervals\ntidy(fish_lm, conf.int = TRUE, conf.level = 0.95)\n\n\n# A tibble: 2 × 7\n  term        estimate std.error statistic  p.value conf.low conf.high\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)    -51.4      3.58     -14.4 7.24e-26    -58.5     -44.3\n2 length_cm       61.7      1.58      39.0 1.91e-61     58.5      64.8\n\n\n\n\nglance(): Model-Level Statistics\nThe glance() function extracts model-level summaries—R-squared, AIC, BIC, and other fit statistics—as a single-row tibble:\n\n\nCode\nglance(fish_lm)\n\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic  p.value    df logLik   AIC   BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.939         0.939  14.3     1517. 1.91e-61     1  -407.  819.  827.\n# ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\n\nThis is particularly useful when comparing multiple models:\n\n\nCode\n# Compare models with different predictors\nfish_lm_simple &lt;- lm(weight_mg ~ length_cm)\nfish_lm_quad &lt;- lm(weight_mg ~ length_cm + I(length_cm^2))\n\n# Combine model summaries\nbind_rows(\n  glance(fish_lm_simple) |&gt; mutate(model = \"linear\"),\n  glance(fish_lm_quad) |&gt; mutate(model = \"quadratic\")\n) |&gt;\n  select(model, r.squared, adj.r.squared, AIC, BIC)\n\n\n# A tibble: 2 × 5\n  model     r.squared adj.r.squared   AIC   BIC\n  &lt;chr&gt;         &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 linear        0.939         0.939  819.  827.\n2 quadratic     0.974         0.974  735.  745.\n\n\n\n\naugment(): Add Fitted Values and Residuals\nThe augment() function adds fitted values, residuals, and diagnostic measures to your original data:\n\n\nCode\n# Add model diagnostics to the data\nfish_augmented &lt;- augment(fish_lm)\nhead(fish_augmented)\n\n\n# A tibble: 6 × 8\n  weight_mg length_cm .fitted .resid   .hat .sigma .cooksd .std.resid\n      &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;      &lt;dbl&gt;\n1     161.       3.24   149.   12.5  0.0269   14.3 0.0109       0.888\n2     157.       3.31   153.    3.88 0.0289   14.3 0.00113      0.276\n3      43.4      1.36    32.3  11.1  0.0163   14.3 0.00510      0.785\n4     141.       2.99   133.    7.63 0.0204   14.3 0.00304      0.541\n5      89.1      2.43    98.1  -8.99 0.0115   14.3 0.00234     -0.634\n6      66.3      2.06    75.4  -9.17 0.0100   14.3 0.00211     -0.646\n\n\nThe augmented data includes:\n\n.fitted: Predicted values\n.resid: Residuals\n.hat: Leverage values\n.cooksd: Cook’s distance (influence measure)\n.std.resid: Standardized residuals\n\nThis makes ggplot-based diagnostic plots straightforward:\n\n\nCode\n# Diagnostic plots with ggplot\nlibrary(patchwork)\n\np1 &lt;- ggplot(fish_augmented, aes(.fitted, .resid)) +\n  geom_point(alpha = 0.6) +\n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"red\") +\n  geom_smooth(se = FALSE, color = \"blue\") +\n  labs(x = \"Fitted values\", y = \"Residuals\",\n       title = \"Residuals vs Fitted\")\n\np2 &lt;- ggplot(fish_augmented, aes(sample = .std.resid)) +\n  stat_qq() +\n  stat_qq_line(color = \"red\") +\n  labs(x = \"Theoretical quantiles\", y = \"Standardized residuals\",\n       title = \"Q-Q Plot\")\n\np1 + p2\n\n\n\n\n\n\n\n\nFigure 22.7: Residual diagnostic plots created using augmented data from the broom package\n\n\n\n\n\nYou can also use augment() to add predictions for new data:\n\n\nCode\n# Predict for new lengths\nnew_fish &lt;- tibble(length_cm = c(1.0, 2.0, 3.0, 4.0))\naugment(fish_lm, newdata = new_fish, interval = \"confidence\")\n\n\n# A tibble: 4 × 4\n  length_cm .fitted .lower .upper\n      &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1         1    10.2   5.83   14.6\n2         2    71.9  69.1    74.7\n3         3   134.  129.    138. \n4         4   195.  189.    202. \n\n\n\n\n\n\n\n\nWhy Use broom?\n\n\n\nThe broom package integrates regression analysis into tidyverse workflows:\n\nReproducibility: Tidy output is easier to save, share, and version control\nVisualization: Augmented data works directly with ggplot2\nIteration: Compare many models using map() and bind_rows()\nConsistency: Same functions work for lm(), glm(), t.test(), and 100+ model types\n\n\n\nCode\n# Example: Fit models to multiple groups\nmtcars |&gt;\n  group_by(cyl) |&gt;\n  group_modify(~ tidy(lm(mpg ~ wt, data = .x)))",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "chapters/16-simple-linear-regression.html#model-assumptions",
    "href": "chapters/16-simple-linear-regression.html#model-assumptions",
    "title": "22  Simple Linear Regression",
    "section": "22.11 Model Assumptions",
    "text": "22.11 Model Assumptions\nLinear regression assumptions include:\n\nLinearity: The relationship between X and Y is linear\nIndependence: Observations are independent of each other\nNormality: Residuals are normally distributed\nHomoscedasticity: Residuals have constant variance across X\n\nThese assumptions should be checked through residual analysis.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "chapters/16-simple-linear-regression.html#residual-analysis",
    "href": "chapters/16-simple-linear-regression.html#residual-analysis",
    "title": "22  Simple Linear Regression",
    "section": "22.12 Residual Analysis",
    "text": "22.12 Residual Analysis\nResiduals are the differences between observed and fitted values: \\(e_i = y_i - \\hat{y}_i\\). Examining residuals reveals whether model assumptions are satisfied.\n\n\nCode\n# Residual diagnostic plots\npar(mfrow = c(2, 2))\nplot(fish_lm)\n\n\n\n\n\n\n\n\nFigure 22.8: Standard residual diagnostic plots for linear regression including residuals vs fitted, Q-Q plot, scale-location, and residuals vs leverage\n\n\n\n\n\nKey diagnostic plots:\n\nResiduals vs Fitted: Should show random scatter around zero. Patterns suggest non-linearity or heteroscedasticity.\nQ-Q Plot: Residuals should fall along the diagonal line if normally distributed. Deviations at the tails indicate non-normality.\nScale-Location: Should show constant spread. A funnel shape indicates heteroscedasticity.\nResiduals vs Leverage: Identifies influential points. Points with high leverage and large residuals may unduly influence the fit.\n\n\n\nCode\n# Example of problematic residuals\nset.seed(123)\nx_prob &lt;- seq(1, 10, length.out = 50)\ny_prob &lt;- x_prob^2 + rnorm(50, sd = 5)  # Quadratic relationship\n\nlm_prob &lt;- lm(y_prob ~ x_prob)\n\npar(mfrow = c(1, 2))\nplot(x_prob, y_prob, pch = 19, main = \"Data with Non-linear Pattern\")\nabline(lm_prob, col = \"red\", lwd = 2)\n\nplot(fitted(lm_prob), residuals(lm_prob), pch = 19,\n     xlab = \"Fitted values\", ylab = \"Residuals\",\n     main = \"Residuals Show Clear Pattern\")\nabline(h = 0, col = \"red\", lty = 2)\n\n\n\n\n\n\n\n\nFigure 22.9: Example of problematic residuals from fitting a linear model to nonlinear data, showing curved pattern in residual plot\n\n\n\n\n\nThe curved pattern in the residuals reveals that a linear model is inappropriate—the true relationship is non-linear.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "chapters/16-simple-linear-regression.html#model-i-vs-model-ii-regression",
    "href": "chapters/16-simple-linear-regression.html#model-i-vs-model-ii-regression",
    "title": "22  Simple Linear Regression",
    "section": "22.13 Model I vs Model II Regression",
    "text": "22.13 Model I vs Model II Regression\nStandard OLS regression (Model I) assumes X is measured without error and minimizes vertical distances to the line. This is appropriate when:\n\nX is fixed by the experimenter (controlled variable)\nX is measured with negligible error compared to Y\nThe goal is prediction of Y from X\n\nWhen both variables are measured with error (common in observational studies), Model II regression may be more appropriate. Model II methods include:\n\nMajor Axis (MA) regression: Minimizes perpendicular distances to the line\nReduced Major Axis (RMA): Often preferred when both variables have similar measurement error\n\n\n\nCode\n# Model I slope estimate\nslope_model1 &lt;- coef(fish_lm)[2]\n\n# Reduced Major Axis slope estimate (ratio of standard deviations)\nslope_rma &lt;- sd(weight_mg) / sd(length_cm) * sign(cor(length_cm, weight_mg))\n\ncat(\"Model I (OLS) slope:\", round(slope_model1, 3), \"\\n\")\n\n\nModel I (OLS) slope: 61.66 \n\n\nCode\ncat(\"Model II (RMA) slope:\", round(slope_rma, 3), \"\\n\")\n\n\nModel II (RMA) slope: 63.62 \n\n\n\n\n\n\n\n\nWhen to Use Model II Regression\n\n\n\nUse Model II regression when: - Both X and Y are random variables measured with error - You want to describe the relationship rather than predict Y from X - You need to compare slopes across groups (e.g., allometric scaling)\nThe lmodel2 package in R provides Model II regression methods.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "chapters/16-simple-linear-regression.html#extrapolation-warning",
    "href": "chapters/16-simple-linear-regression.html#extrapolation-warning",
    "title": "22  Simple Linear Regression",
    "section": "22.14 Extrapolation Warning",
    "text": "22.14 Extrapolation Warning\nRegression models should only be used to make predictions within the range of observed X values. Extrapolation—predicting beyond this range—is risky because the linear relationship may not hold.\n\n\nCode\n# Danger of extrapolation\nplot(length_cm, weight_mg, pch = 19, col = \"blue\",\n     xlim = c(0, 6), ylim = c(-50, 600),\n     xlab = \"Length (cm)\", ylab = \"Weight (mg)\",\n     main = \"Extrapolation Risk\")\nabline(fish_lm, col = \"red\", lwd = 2)\nabline(v = c(min(length_cm), max(length_cm)), col = \"gray\", lty = 2)\n\n# Mark extrapolation zone\nrect(max(length_cm), -50, 6, 600, col = rgb(1, 0, 0, 0.1), border = NA)\ntext(5, 100, \"Extrapolation\\nzone\", col = \"red\")\n\n\n\n\n\n\n\n\nFigure 22.10: Illustration of extrapolation risk showing how predictions beyond the observed data range (shaded zone) may be unreliable",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "chapters/16-simple-linear-regression.html#summary",
    "href": "chapters/16-simple-linear-regression.html#summary",
    "title": "22  Simple Linear Regression",
    "section": "22.15 Summary",
    "text": "22.15 Summary\nSimple linear regression models the relationship between a predictor and response variable:\n\nOLS finds the line minimizing squared residuals\nThe slope indicates how Y changes per unit change in X\nR-squared measures proportion of variance explained\nResidual analysis checks model assumptions\nModel II regression is appropriate when both variables have measurement error\nAvoid extrapolating beyond the range of observed data",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "chapters/16-simple-linear-regression.html#practice-exercises",
    "href": "chapters/16-simple-linear-regression.html#practice-exercises",
    "title": "22  Simple Linear Regression",
    "section": "22.16 Practice Exercises",
    "text": "22.16 Practice Exercises\n\nExercise R.1: Simple Linear Regression\n\nUsing a dataset of your choice (or the built-in mtcars), fit a linear model with lm()\nExamine the model summary\nCreate a scatterplot with the regression line\nPlot the residuals—do they appear randomly distributed?\n\n\n\nCode\n# Example with built-in data\ndata(mtcars)\nmodel &lt;- lm(mpg ~ wt, data = mtcars)\nsummary(model)\n\n# Regression plot\nplot(mpg ~ wt, data = mtcars)\nabline(model, col = \"red\")\n\n# Residual plot\nplot(model$fitted.values, model$residuals)\nabline(h = 0, col = \"red\", lty = 2)\n\n\n\n\nExercise R.2: Interpreting Coefficients\nUsing your model from Exercise R.1:\n\nWhat does the intercept represent?\nWhat does the slope represent?\nWhat is the R-squared value and what does it mean?\nIs the relationship statistically significant?\n\n\n\nExercise R.3: Model Diagnostics\nCheck the assumptions of your linear model:\n\nUse plot(model) to generate diagnostic plots\nAre the residuals normally distributed? (Q-Q plot)\nIs there constant variance? (Residuals vs. Fitted)\nAre there influential points? (Cook’s distance)\n\n\n\nExercise R.4: Prediction\nUsing your fitted model:\n\nPredict the response for a new observation\nCalculate the confidence interval for the prediction\nWhat happens when you extrapolate beyond the range of your data?\n\n\n\nCode\n# Predict for a new value\nnew_data &lt;- data.frame(wt = 3.5)\npredict(model, newdata = new_data, interval = \"confidence\")",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "chapters/16-simple-linear-regression.html#additional-resources",
    "href": "chapters/16-simple-linear-regression.html#additional-resources",
    "title": "22  Simple Linear Regression",
    "section": "22.17 Additional Resources",
    "text": "22.17 Additional Resources\n\nJames et al. (2023) - Excellent introduction to regression in the context of statistical learning\nLogan (2010) - Detailed coverage of regression with biological applications\n\n\n\n\n\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2023. An Introduction to Statistical Learning with Applications in r. 2nd ed. Springer. https://www.statlearning.com.\n\n\nLogan, Murray. 2010. Biostatistical Design and Analysis Using r. Wiley-Blackwell.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "chapters/17-residual-analysis.html",
    "href": "chapters/17-residual-analysis.html",
    "title": "23  Residual Analysis",
    "section": "",
    "text": "23.1 What Are Residuals?\nResiduals are the differences between observed values and values predicted by the model:\n\\[e_i = y_i - \\hat{y}_i\\]\nThey represent the part of the data not explained by the model—the “leftover” variation. Analyzing residuals helps us check whether the assumptions of our model are met and identify potential problems.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Residual Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/17-residual-analysis.html#what-are-residuals",
    "href": "chapters/17-residual-analysis.html#what-are-residuals",
    "title": "23  Residual Analysis",
    "section": "",
    "text": "Figure 23.1: Conceptual illustration of residuals as the vertical distances between observed data points and the fitted regression line",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Residual Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/17-residual-analysis.html#why-residual-analysis-matters",
    "href": "chapters/17-residual-analysis.html#why-residual-analysis-matters",
    "title": "23  Residual Analysis",
    "section": "23.2 Why Residual Analysis Matters",
    "text": "23.2 Why Residual Analysis Matters\nA regression model might fit the data well according to R-squared while still being inappropriate. The model might capture the wrong pattern, miss non-linear relationships, or be unduly influenced by outliers. Residual analysis reveals these problems.\nRemember Anscombe’s Quartet—four datasets with identical regression lines but very different patterns. Looking only at the regression output would miss these differences entirely.\n\n\n\n\n\n\nFigure 23.2: Anscombe’s Quartet demonstrating why residual analysis is essential, showing four datasets with identical regression statistics but different patterns",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Residual Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/17-residual-analysis.html#checking-assumptions",
    "href": "chapters/17-residual-analysis.html#checking-assumptions",
    "title": "23  Residual Analysis",
    "section": "23.3 Checking Assumptions",
    "text": "23.3 Checking Assumptions\n\nLinearity\nIf the relationship is truly linear, residuals should show no systematic pattern when plotted against fitted values or the predictor variable. A curved pattern suggests non-linearity.\n\n\nCode\n# Create data with non-linear relationship\nset.seed(42)\nx &lt;- seq(0, 10, length.out = 100)\ny &lt;- 2 + 0.5 * x + 0.1 * x^2 + rnorm(100, sd = 1)\n\nmodel &lt;- lm(y ~ x)\n\npar(mfrow = c(1, 2))\nplot(x, y, main = \"Data with Quadratic Pattern\")\nabline(model, col = \"red\")\n\nplot(fitted(model), residuals(model), \n     main = \"Residuals vs Fitted\",\n     xlab = \"Fitted values\", ylab = \"Residuals\")\nabline(h = 0, col = \"red\", lty = 2)\n\n\n\n\n\n\n\n\nFigure 23.3: Checking linearity assumption with quadratic data, showing how a curved pattern in residuals vs fitted values reveals nonlinearity\n\n\n\n\n\nThe curved pattern in the residual plot reveals that a linear model is inadequate.\n\n\nNormality\nResiduals should be approximately normally distributed. Check with a histogram or Q-Q plot:\n\n\nCode\n# Good model for comparison\nx2 &lt;- rnorm(100)\ny2 &lt;- 2 + 3 * x2 + rnorm(100)\ngood_model &lt;- lm(y2 ~ x2)\n\npar(mfrow = c(1, 2))\nhist(residuals(good_model), breaks = 20, main = \"Histogram of Residuals\",\n     xlab = \"Residuals\", col = \"lightblue\")\nqqnorm(residuals(good_model))\nqqline(residuals(good_model), col = \"red\")\n\n\n\n\n\n\n\n\nFigure 23.4: Checking normality assumption using histogram and Q-Q plot of residuals from a well-fitting model\n\n\n\n\n\nPoints on the Q-Q plot should fall approximately along the diagonal line. Systematic departures indicate non-normality.\n\n\nHomoscedasticity\nResiduals should have constant variance across the range of fitted values. A fan or cone shape indicates heteroscedasticity (unequal variance).\n\n\n\n\n\n\nFigure 23.5: Examples of homoscedasticity (constant variance) and heteroscedasticity (non-constant variance) in residual plots\n\n\n\n\n\nIndependence\nResiduals should be independent of each other. This is hard to check visually but is violated when observations are related (e.g., repeated measurements on the same subjects, or time series data).",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Residual Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/17-residual-analysis.html#diagnostic-plots-in-r",
    "href": "chapters/17-residual-analysis.html#diagnostic-plots-in-r",
    "title": "23  Residual Analysis",
    "section": "23.4 Diagnostic Plots in R",
    "text": "23.4 Diagnostic Plots in R\nR provides built-in diagnostic plots for linear models:\n\n\nCode\n# Standard diagnostic plots\npar(mfrow = c(2, 2))\nplot(good_model)\n\n\n\n\n\n\n\n\nFigure 23.6: Complete set of standard diagnostic plots for regression model assessment including residuals vs fitted, Q-Q plot, scale-location, and residuals vs leverage\n\n\n\n\n\nThese four plots show: 1. Residuals vs Fitted: Check for linearity and homoscedasticity 2. Q-Q Plot: Check for normality 3. Scale-Location: Check for homoscedasticity 4. Residuals vs Leverage: Identify influential points",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Residual Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/17-residual-analysis.html#leverage-and-influence",
    "href": "chapters/17-residual-analysis.html#leverage-and-influence",
    "title": "23  Residual Analysis",
    "section": "23.5 Leverage and Influence",
    "text": "23.5 Leverage and Influence\nNot all observations affect the regression equally. Leverage measures how unusual an observation’s X value is—points with extreme X values have more potential to influence the fitted line.\nCook’s Distance measures how much the regression would change if an observation were removed. High Cook’s D values indicate influential points that merit closer examination.\n\n\n\n\n\n\nFigure 23.7: Illustration of leverage and influence showing how high-leverage points with large residuals can strongly affect the fitted regression line\n\n\n\n\n\nCode\n# Check for influential points\ninfluence.measures(good_model)$is.inf[1:5,]  # First 5 observations\n\n\n  dfb.1_ dfb.x2 dffit cov.r cook.d   hat\n1  FALSE  FALSE FALSE FALSE  FALSE FALSE\n2  FALSE  FALSE FALSE FALSE  FALSE FALSE\n3  FALSE  FALSE FALSE FALSE  FALSE FALSE\n4  FALSE  FALSE  TRUE FALSE  FALSE FALSE\n5  FALSE  FALSE FALSE FALSE  FALSE FALSE",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Residual Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/17-residual-analysis.html#handling-violations",
    "href": "chapters/17-residual-analysis.html#handling-violations",
    "title": "23  Residual Analysis",
    "section": "23.6 Handling Violations",
    "text": "23.6 Handling Violations\nWhen assumptions are violated, several approaches may help:\nTransform the data: Log, square root, or other transformations can stabilize variance and improve linearity.\nUse robust regression: Methods like rlm() from the MASS package down-weight influential observations.\nTry a different model: Non-linear regression, generalized linear models, or generalized additive models may be more appropriate.\nRemove outliers: Only if you have substantive reasons—never simply to improve fit.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Residual Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/17-residual-analysis.html#residual-analysis-workflow",
    "href": "chapters/17-residual-analysis.html#residual-analysis-workflow",
    "title": "23  Residual Analysis",
    "section": "23.7 Residual Analysis Workflow",
    "text": "23.7 Residual Analysis Workflow\nA systematic approach to residual analysis:\n\nFit the model\nGenerate diagnostic plots\nCheck for patterns in residuals vs. fitted values\nExamine the Q-Q plot for normality\nLook for influential points\nIf problems exist, consider transformations or alternative models\nRe-check diagnostics after any changes\n\nResidual analysis is not optional—it is an essential part of any regression analysis. Models that look good on paper may tell misleading stories if their assumptions are violated.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Residual Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/18-statistical-power.html",
    "href": "chapters/18-statistical-power.html",
    "title": "24  Statistical Power",
    "section": "",
    "text": "24.1 What is Statistical Power?\nPower is the probability of correctly rejecting a false null hypothesis—the probability of detecting an effect when one truly exists. If the true effect size is non-zero, power tells us how likely our study is to find it.\nPower = 1 - \\(\\beta\\), where \\(\\beta\\) is the probability of a Type II error (failing to reject a false null hypothesis). We typically aim for power of at least 80%, meaning we accept a 20% chance of missing a true effect.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Statistical Power</span>"
    ]
  },
  {
    "objectID": "chapters/18-statistical-power.html#what-is-statistical-power",
    "href": "chapters/18-statistical-power.html#what-is-statistical-power",
    "title": "24  Statistical Power",
    "section": "",
    "text": "Figure 24.1: Illustration of statistical power showing the relationship between Type I error (alpha), Type II error (beta), and power in hypothesis testing",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Statistical Power</span>"
    ]
  },
  {
    "objectID": "chapters/18-statistical-power.html#why-power-matters",
    "href": "chapters/18-statistical-power.html#why-power-matters",
    "title": "24  Statistical Power",
    "section": "24.2 Why Power Matters",
    "text": "24.2 Why Power Matters\nA study with low power has poor chances of detecting true effects. Even if an effect exists, the study may fail to find statistical significance. Worse, significant results from underpowered studies tend to overestimate effect sizes—a phenomenon called the “winner’s curse.”\nUnderstanding power helps us interpret results appropriately. If we fail to reject the null hypothesis, was it because no effect exists, or because our study lacked the power to detect it?",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Statistical Power</span>"
    ]
  },
  {
    "objectID": "chapters/18-statistical-power.html#determinants-of-power",
    "href": "chapters/18-statistical-power.html#determinants-of-power",
    "title": "24  Statistical Power",
    "section": "24.3 Determinants of Power",
    "text": "24.3 Determinants of Power\nPower depends on four factors that are mathematically related:\n\\[\\text{Power} \\propto \\frac{(\\text{Effect Size}) \\times (\\alpha) \\times (\\sqrt{n})}{\\sigma}\\]\nEffect Size: Larger effects are easier to detect. Effect size can be measured in original units or standardized (like Cohen’s d).\nSample Size (n): Larger samples provide more information and higher power.\nSignificance Level (\\(\\alpha\\)): Using a more lenient alpha (e.g., 0.10 instead of 0.05) increases power but also increases Type I error risk.\nVariability (\\(\\sigma\\)): Less variable data makes effects easier to detect.\n\n\n\n\n\n\nFigure 24.2: Visual representation of the four factors that determine statistical power: effect size, sample size, significance level, and variability",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Statistical Power</span>"
    ]
  },
  {
    "objectID": "chapters/18-statistical-power.html#cohens-d-standardized-effect-size",
    "href": "chapters/18-statistical-power.html#cohens-d-standardized-effect-size",
    "title": "24  Statistical Power",
    "section": "24.4 Cohen’s d: Standardized Effect Size",
    "text": "24.4 Cohen’s d: Standardized Effect Size\nCohen’s d expresses the difference between means in standard deviation units:\n\\[d = \\frac{\\mu_1 - \\mu_2}{s_{pooled}}\\]\nConventional benchmarks (Cohen, 1988): - d = 0.2: small effect - d = 0.5: medium effect - d = 0.8: large effect\nThese benchmarks are only guidelines—what counts as “small” depends on the research context.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Statistical Power</span>"
    ]
  },
  {
    "objectID": "chapters/18-statistical-power.html#a-priori-power-analysis",
    "href": "chapters/18-statistical-power.html#a-priori-power-analysis",
    "title": "24  Statistical Power",
    "section": "24.5 A Priori Power Analysis",
    "text": "24.5 A Priori Power Analysis\nBefore collecting data, power analysis helps determine the sample size needed to detect effects of interest. This requires specifying:\n\nThe expected effect size\nThe desired power (typically 0.80)\nThe significance level (typically 0.05)\nThe statistical test to be used\n\n\n\nCode\n# How many subjects needed to detect d = 0.5 with 80% power?\npwr.t.test(d = 0.5, sig.level = 0.05, power = 0.80, type = \"two.sample\")\n\n\n\n     Two-sample t test power calculation \n\n              n = 63.76561\n              d = 0.5\n      sig.level = 0.05\n          power = 0.8\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n\n\nAbout 64 subjects per group are needed to detect a medium effect with 80% power using a two-sample t-test.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Statistical Power</span>"
    ]
  },
  {
    "objectID": "chapters/18-statistical-power.html#power-curves",
    "href": "chapters/18-statistical-power.html#power-curves",
    "title": "24  Statistical Power",
    "section": "24.6 Power Curves",
    "text": "24.6 Power Curves\nPower curves show how power changes with sample size or effect size:\n\n\nCode\n# Power curve for different effect sizes\nsample_sizes &lt;- seq(10, 200, by = 5)\neffect_sizes &lt;- c(0.2, 0.5, 0.8)\n\npower_data &lt;- expand.grid(n = sample_sizes, d = effect_sizes)\npower_data$power &lt;- mapply(function(n, d) {\n  pwr.t.test(n = n, d = d, sig.level = 0.05, type = \"two.sample\")$power\n}, power_data$n, power_data$d)\n\nggplot(power_data, aes(x = n, y = power, color = factor(d))) +\n  geom_line(size = 1.2) +\n  geom_hline(yintercept = 0.8, linetype = \"dashed\") +\n  labs(title = \"Power Curves for Two-Sample t-Test\",\n       x = \"Sample Size per Group\",\n       y = \"Power\",\n       color = \"Effect Size (d)\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nFigure 24.3: Power curves showing how statistical power increases with sample size for different effect sizes (Cohen’s d = 0.2, 0.5, 0.8) in a two-sample t-test",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Statistical Power</span>"
    ]
  },
  {
    "objectID": "chapters/18-statistical-power.html#power-for-anova",
    "href": "chapters/18-statistical-power.html#power-for-anova",
    "title": "24  Statistical Power",
    "section": "24.7 Power for ANOVA",
    "text": "24.7 Power for ANOVA\nFor ANOVA, effect size is measured by Cohen’s f:\n\\[f = \\frac{\\sigma_{between}}{\\sigma_{within}}\\]\nBenchmarks: f = 0.10 (small), f = 0.25 (medium), f = 0.40 (large).\n\n\nCode\n# Sample size for ANOVA with 3 groups\npwr.anova.test(k = 3, f = 0.25, sig.level = 0.05, power = 0.80)\n\n\n\n     Balanced one-way analysis of variance power calculation \n\n              k = 3\n              n = 52.3966\n              f = 0.25\n      sig.level = 0.05\n          power = 0.8\n\nNOTE: n is number in each group",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Statistical Power</span>"
    ]
  },
  {
    "objectID": "chapters/18-statistical-power.html#simulation-based-power-analysis",
    "href": "chapters/18-statistical-power.html#simulation-based-power-analysis",
    "title": "24  Statistical Power",
    "section": "24.8 Simulation-Based Power Analysis",
    "text": "24.8 Simulation-Based Power Analysis\nFor complex designs, simulation provides a flexible approach:\n\n\nCode\n# Simulation-based power for comparing two Poisson distributions\nset.seed(42)\n\npower_sim &lt;- function(n, lambda1, lambda2, n_sims = 1000) {\n  significant &lt;- replicate(n_sims, {\n    x1 &lt;- rpois(n, lambda1)\n    x2 &lt;- rpois(n, lambda2)\n    t.test(x1, x2)$p.value &lt; 0.05\n  })\n  mean(significant)\n}\n\n# Power for different sample sizes\nsample_sizes &lt;- seq(10, 100, by = 10)\npowers &lt;- sapply(sample_sizes, power_sim, lambda1 = 10, lambda2 = 12)\n\nplot(sample_sizes, powers, type = \"b\", pch = 19,\n     xlab = \"Sample Size per Group\", ylab = \"Power\",\n     main = \"Simulated Power (λ1=10 vs λ2=12)\")\nabline(h = 0.8, lty = 2, col = \"red\")\n\n\n\n\n\n\n\n\nFigure 24.4: Simulation-based power analysis for comparing two Poisson distributions, showing power as a function of sample size",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Statistical Power</span>"
    ]
  },
  {
    "objectID": "chapters/18-statistical-power.html#post-hoc-power-analysis",
    "href": "chapters/18-statistical-power.html#post-hoc-power-analysis",
    "title": "24  Statistical Power",
    "section": "24.9 Post Hoc Power Analysis",
    "text": "24.9 Post Hoc Power Analysis\nCalculating power after a study is completed is controversial. Post hoc power calculated from observed effect sizes is mathematically determined by the p-value and adds no new information. It cannot tell you whether a non-significant result reflects a true null or insufficient power.\nIf you want to understand what your study could detect, specify effect sizes based on scientific considerations, not observed results.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Statistical Power</span>"
    ]
  },
  {
    "objectID": "chapters/18-statistical-power.html#practical-recommendations",
    "href": "chapters/18-statistical-power.html#practical-recommendations",
    "title": "24  Statistical Power",
    "section": "24.10 Practical Recommendations",
    "text": "24.10 Practical Recommendations\nAlways conduct power analysis before data collection. Use realistic effect size estimates based on pilot data or previous literature. Consider what effect size would be practically meaningful, not just what you think exists.\nBe conservative—effects are often smaller than expected. Plan for some attrition or missing data. When in doubt, collect more data if feasible.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Statistical Power</span>"
    ]
  },
  {
    "objectID": "chapters/20-multiple-regression.html",
    "href": "chapters/20-multiple-regression.html",
    "title": "25  Multiple Regression",
    "section": "",
    "text": "25.1 Beyond One Predictor\nSimple linear regression uses a single predictor. But the response variable often depends on multiple factors. A patient’s blood pressure might depend on age, weight, sodium intake, and medication. Gene expression might depend on temperature, time, and treatment condition.\nMultiple regression extends linear regression to multiple predictors:\n\\[y_i = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\ldots + \\beta_p x_p + \\epsilon_i\\]",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Multiple Regression</span>"
    ]
  },
  {
    "objectID": "chapters/20-multiple-regression.html#beyond-one-predictor",
    "href": "chapters/20-multiple-regression.html#beyond-one-predictor",
    "title": "25  Multiple Regression",
    "section": "",
    "text": "Figure 25.1: Multiple regression extends linear regression to multiple predictors in multidimensional space",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Multiple Regression</span>"
    ]
  },
  {
    "objectID": "chapters/20-multiple-regression.html#goals-of-multiple-regression",
    "href": "chapters/20-multiple-regression.html#goals-of-multiple-regression",
    "title": "25  Multiple Regression",
    "section": "25.2 Goals of Multiple Regression",
    "text": "25.2 Goals of Multiple Regression\nMultiple regression serves two main purposes. First, it often improves prediction by incorporating multiple sources of information. Second, it allows us to investigate the effect of each predictor while controlling for the others—the effect of X1 “holding X2 constant.”\nThis second goal is powerful but requires caution. In observational data, controlling for variables statistically is not the same as controlling them experimentally. Confounding variables you do not measure cannot be controlled.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Multiple Regression</span>"
    ]
  },
  {
    "objectID": "chapters/20-multiple-regression.html#understanding-confounding-through-stratification",
    "href": "chapters/20-multiple-regression.html#understanding-confounding-through-stratification",
    "title": "25  Multiple Regression",
    "section": "25.3 Understanding Confounding Through Stratification",
    "text": "25.3 Understanding Confounding Through Stratification\nA powerful way to understand confounding—and why multiple regression is necessary—is through stratification. When two predictors are correlated, their apparent individual effects can be misleading.\nConsider a biological example: suppose we measure both body size and metabolic rate in animals, and we want to know if each independently predicts lifespan. If larger animals have both higher metabolic rates AND longer lifespans (because both correlate with species type), we might see a spurious positive relationship between metabolic rate and lifespan when the true relationship is negative within any given body size.\nThe solution is to stratify by the confounding variable. If we group animals by body size and look at the relationship between metabolic rate and lifespan within each group, we see the true (negative) relationship.\n\n\nCode\n# Simulated confounding example\nset.seed(42)\nn &lt;- 200\n\n# Body size (the confounder)\nbody_size &lt;- runif(n, 1, 10)\n\n# Metabolic rate increases with body size (positive correlation with confounder)\nmetabolic_rate &lt;- 2 * body_size + rnorm(n, sd = 2)\n\n# Lifespan: increases with body size, DECREASES with metabolic rate\n# But without controlling for body size, it appears metabolic rate increases lifespan!\nlifespan &lt;- 5 + 3 * body_size - 0.5 * metabolic_rate + rnorm(n, sd = 2)\n\nconfound_data &lt;- data.frame(body_size, metabolic_rate, lifespan)\n\n# Naive regression (ignoring confounder)\nnaive_fit &lt;- lm(lifespan ~ metabolic_rate, data = confound_data)\n\n# Proper multiple regression (controlling for body size)\nproper_fit &lt;- lm(lifespan ~ metabolic_rate + body_size, data = confound_data)\n\ncat(\"Naive model (ignoring body size):\\n\")\n\n\nNaive model (ignoring body size):\n\n\nCode\ncat(\"Metabolic rate coefficient:\", round(coef(naive_fit)[2], 3), \"\\n\\n\")\n\n\nMetabolic rate coefficient: 0.787 \n\n\nCode\ncat(\"Multiple regression (controlling for body size):\\n\")\n\n\nMultiple regression (controlling for body size):\n\n\nCode\ncat(\"Metabolic rate coefficient:\", round(coef(proper_fit)[2], 3), \"\\n\")\n\n\nMetabolic rate coefficient: -0.466 \n\n\nCode\ncat(\"Body size coefficient:\", round(coef(proper_fit)[3], 3), \"\\n\")\n\n\nBody size coefficient: 2.844 \n\n\nThe naive model shows a positive relationship between metabolic rate and lifespan. But once we control for body size, we see the true negative relationship—higher metabolic rate is associated with shorter lifespan, as biological theory predicts.\nWe can visualize this confounding through stratification:\n\n\nCode\n# Stratify by body size\nconfound_data$size_strata &lt;- cut(confound_data$body_size,\n                                  breaks = quantile(body_size, c(0, 0.33, 0.67, 1)),\n                                  labels = c(\"Small\", \"Medium\", \"Large\"),\n                                  include.lowest = TRUE)\n\n# Plot relationship within each stratum\nggplot(confound_data, aes(x = metabolic_rate, y = lifespan)) +\n  geom_point(alpha = 0.5) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  facet_wrap(~ size_strata) +\n  labs(title = \"Stratification Reveals True Relationship\",\n       subtitle = \"Within each body size group, higher metabolic rate predicts shorter lifespan\",\n       x = \"Metabolic Rate\", y = \"Lifespan\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nFigure 25.2: Stratification by body size reveals the true negative relationship between metabolic rate and lifespan\n\n\n\n\n\nWithin each stratum (holding body size approximately constant), we see the true negative relationship. The slopes within strata approximate what multiple regression gives us.\n\n\n\n\n\n\nSimpson’s Paradox\n\n\n\nThis phenomenon—where a trend reverses or disappears when data are stratified by a confounding variable—is known as Simpson’s Paradox. In our example, metabolic rate appears positively associated with lifespan in the aggregate data, but is negatively associated within each body size group.\nSimpson’s Paradox is not a statistical anomaly; it’s a consequence of confounding. Famous examples include:\n\nUC Berkeley gender bias case (1973): Aggregate admission rates appeared to favor men, but within most departments, women were admitted at equal or higher rates. The paradox arose because women disproportionately applied to more competitive departments.\nKidney stone treatments: Treatment A appeared less effective overall, but was more effective for both small and large stones. The paradox occurred because Treatment A was preferentially used for more severe cases.\n\nThe resolution is always the same: aggregate data can be misleading when a confounding variable affects both the predictor and outcome. Stratification (or multiple regression) reveals the relationship within levels of the confounder.\n\n\n\n\n\n\n\n\nWhy This Matters\n\n\n\nWhen predictors are correlated, simple regression coefficients can be misleading—even showing the wrong sign! Multiple regression “adjusts” for confounders, revealing relationships that are closer to (though not necessarily equal to) causal effects. However, you can only adjust for confounders you measure. Unmeasured confounders remain a threat to causal interpretation.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Multiple Regression</span>"
    ]
  },
  {
    "objectID": "chapters/20-multiple-regression.html#additive-vs.-multiplicative-models",
    "href": "chapters/20-multiple-regression.html#additive-vs.-multiplicative-models",
    "title": "25  Multiple Regression",
    "section": "25.4 Additive vs. Multiplicative Models",
    "text": "25.4 Additive vs. Multiplicative Models\nAn additive model assumes predictors contribute independently:\n\\[y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\epsilon\\]\nA multiplicative model includes interactions—the effect of one predictor depends on the value of another:\n\\[y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_1 x_2 + \\epsilon\\]\n\n\nCode\n# Example with two predictors\nset.seed(42)\nn &lt;- 100\nx1 &lt;- rnorm(n)\nx2 &lt;- rnorm(n)\ny &lt;- 2 + 3*x1 + 2*x2 + 1.5*x1*x2 + rnorm(n)\n\n# Additive model\nadd_model &lt;- lm(y ~ x1 + x2)\n\n# Model with interaction\nint_model &lt;- lm(y ~ x1 * x2)\n\nsummary(int_model)\n\n\n\nCall:\nlm(formula = y ~ x1 * x2)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.55125 -0.69885 -0.03771  0.56441  2.42157 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  2.00219    0.10108   19.81   &lt;2e-16 ***\nx1           2.84494    0.09734   29.23   &lt;2e-16 ***\nx2           2.04126    0.11512   17.73   &lt;2e-16 ***\nx1:x2        1.35163    0.09228   14.65   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.005 on 96 degrees of freedom\nMultiple R-squared:  0.9289,    Adjusted R-squared:  0.9267 \nF-statistic:   418 on 3 and 96 DF,  p-value: &lt; 2.2e-16",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Multiple Regression</span>"
    ]
  },
  {
    "objectID": "chapters/20-multiple-regression.html#interpretation-of-coefficients",
    "href": "chapters/20-multiple-regression.html#interpretation-of-coefficients",
    "title": "25  Multiple Regression",
    "section": "25.5 Interpretation of Coefficients",
    "text": "25.5 Interpretation of Coefficients\nIn multiple regression, each coefficient represents the expected change in Y for a one-unit change in that predictor, holding all other predictors constant.\nThis “holding constant” interpretation makes the coefficients different from what you would get from separate simple regressions. The coefficient for X1 in multiple regression represents the unique contribution of X1 after accounting for X2.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Multiple Regression</span>"
    ]
  },
  {
    "objectID": "chapters/20-multiple-regression.html#multicollinearity",
    "href": "chapters/20-multiple-regression.html#multicollinearity",
    "title": "25  Multiple Regression",
    "section": "25.6 Multicollinearity",
    "text": "25.6 Multicollinearity\nWhen predictors are correlated with each other, interpreting individual coefficients becomes problematic. This multicollinearity inflates standard errors and can make coefficients unstable.\n\n\nCode\n# Check for multicollinearity visually\nlibrary(car)\npairs(~ x1 + x2, main = \"Scatterplot Matrix\")\n\n\n\n\n\n\n\n\nFigure 25.3: Scatterplot matrix for checking multicollinearity between predictors\n\n\n\n\n\nThe Variance Inflation Factor (VIF) quantifies multicollinearity. VIF &gt; 10 suggests serious problems; VIF &gt; 5 warrants attention.\n\n\nCode\nvif(int_model)\n\n\n      x1       x2    x1:x2 \n1.006276 1.061022 1.066455",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Multiple Regression</span>"
    ]
  },
  {
    "objectID": "chapters/20-multiple-regression.html#model-selection",
    "href": "chapters/20-multiple-regression.html#model-selection",
    "title": "25  Multiple Regression",
    "section": "25.7 Model Selection",
    "text": "25.7 Model Selection\nWith many potential predictors, how do we choose which to include? Adding variables always improves fit to the training data but may hurt prediction on new data through overfitting.\nSeveral criteria balance fit and complexity:\nAdjusted R² penalizes for the number of predictors.\nAIC (Akaike Information Criterion) estimates prediction error, penalizing complexity. Lower is better.\nBIC (Bayesian Information Criterion) similar to AIC but penalizes complexity more heavily.\n\n\nCode\n# Compare models\nAIC(add_model, int_model)\n\n\n          df      AIC\nadd_model  4 406.1808\nint_model  5 290.7926\n\n\nCode\nBIC(add_model, int_model)\n\n\n          df      BIC\nadd_model  4 416.6015\nint_model  5 303.8185",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Multiple Regression</span>"
    ]
  },
  {
    "objectID": "chapters/20-multiple-regression.html#model-selection-strategies",
    "href": "chapters/20-multiple-regression.html#model-selection-strategies",
    "title": "25  Multiple Regression",
    "section": "25.8 Model Selection Strategies",
    "text": "25.8 Model Selection Strategies\nForward selection starts with no predictors and adds them one at a time based on statistical criteria.\nBackward elimination starts with all predictors and removes them one at a time.\nAll subsets examines all possible combinations and selects the best.\nNo strategy is universally best. Automated selection can lead to overfitting and unstable models. Theory-driven model building—starting with predictors you have scientific reasons to include—is often preferable.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Multiple Regression</span>"
    ]
  },
  {
    "objectID": "chapters/20-multiple-regression.html#polynomial-regression",
    "href": "chapters/20-multiple-regression.html#polynomial-regression",
    "title": "25  Multiple Regression",
    "section": "25.9 Polynomial Regression",
    "text": "25.9 Polynomial Regression\nPolynomial terms can capture non-linear relationships while still using the linear regression framework:\n\n\nCode\n# Non-linear relationship\nx &lt;- seq(0, 10, length.out = 100)\ny &lt;- 2 + 0.5*x - 0.1*x^2 + rnorm(100, sd = 0.5)\n\n# Fit polynomial models\nmodel1 &lt;- lm(y ~ poly(x, 1))  # Linear\nmodel2 &lt;- lm(y ~ poly(x, 2))  # Quadratic\nmodel3 &lt;- lm(y ~ poly(x, 5))  # Degree 5\n\n# Compare\nAIC(model1, model2, model3)\n\n\n       df      AIC\nmodel1  3 250.3022\nmodel2  4 121.8041\nmodel3  7 126.6399\n\n\nHigher-degree polynomials fit better but risk overfitting. The principle of parsimony suggests using the simplest adequate model.\n\n\n\n\n\n\nFigure 25.4: Illustration of polynomial regression showing increasing model complexity",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Multiple Regression</span>"
    ]
  },
  {
    "objectID": "chapters/20-multiple-regression.html#assumptions",
    "href": "chapters/20-multiple-regression.html#assumptions",
    "title": "25  Multiple Regression",
    "section": "25.10 Assumptions",
    "text": "25.10 Assumptions\nMultiple regression shares assumptions with simple regression: linearity (in each predictor), independence, normality of residuals, and constant variance. Additionally, predictors should not be perfectly correlated (no perfect multicollinearity).\nCheck assumptions with residual plots. Partial regression plots can help diagnose problems with individual predictors.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Multiple Regression</span>"
    ]
  },
  {
    "objectID": "chapters/20-multiple-regression.html#practical-guidelines",
    "href": "chapters/20-multiple-regression.html#practical-guidelines",
    "title": "25  Multiple Regression",
    "section": "25.11 Practical Guidelines",
    "text": "25.11 Practical Guidelines\nStart with a theoretically motivated model rather than throwing in all available predictors. Check for multicollinearity before interpreting coefficients. Use cross-validation to assess prediction performance. Report standardized coefficients when comparing the relative importance of predictors on different scales.\nBe humble about causation. Multiple regression describes associations; experimental manipulation is needed to establish causation.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Multiple Regression</span>"
    ]
  },
  {
    "objectID": "chapters/19-single-factor-anova.html",
    "href": "chapters/19-single-factor-anova.html",
    "title": "26  Single Factor ANOVA",
    "section": "",
    "text": "26.1 Beyond Two Groups\nThe t-test compares two groups, but many experiments involve more than two. We might compare three drug treatments, five temperature conditions, or four genetic strains. Running multiple t-tests creates problems: with many comparisons, false positives become likely even when no true differences exist.\nAnalysis of Variance (ANOVA) provides a solution. It tests whether any of the group means differ from the others in a single test, controlling the overall Type I error rate.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Single Factor ANOVA</span>"
    ]
  },
  {
    "objectID": "chapters/19-single-factor-anova.html#the-anova-framework",
    "href": "chapters/19-single-factor-anova.html#the-anova-framework",
    "title": "26  Single Factor ANOVA",
    "section": "26.2 The ANOVA Framework",
    "text": "26.2 The ANOVA Framework\nAnalysis of Variance (ANOVA), developed by Ronald A. Fisher (Fisher 1925), partitions the total variation in the data into components: variation between groups (due to treatment effects) and variation within groups (due to random error).\n\n\n\n\n\n\nFigure 26.1: ANOVA partitions total variation into between-group and within-group components\n\n\n\nThe key insight is that if groups have equal means, the between-group variation should be similar to the within-group variation. If the between-group variation is much larger, the group means probably differ.\n\n\n\n\n\n\nFigure 26.2: Comparison of between-group and within-group variation under different scenarios",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Single Factor ANOVA</span>"
    ]
  },
  {
    "objectID": "chapters/19-single-factor-anova.html#the-f-test",
    "href": "chapters/19-single-factor-anova.html#the-f-test",
    "title": "26  Single Factor ANOVA",
    "section": "26.3 The F-Test",
    "text": "26.3 The F-Test\nANOVA uses the F-statistic:\n\\[F = \\frac{MS_{between}}{MS_{within}} = \\frac{\\text{Variance between groups}}{\\text{Variance within groups}}\\]\nUnder the null hypothesis (all group means equal), F follows an F-distribution. Large F values indicate that group means differ more than expected by chance.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Single Factor ANOVA</span>"
    ]
  },
  {
    "objectID": "chapters/19-single-factor-anova.html#one-way-anova-in-r",
    "href": "chapters/19-single-factor-anova.html#one-way-anova-in-r",
    "title": "26  Single Factor ANOVA",
    "section": "26.4 One-Way ANOVA in R",
    "text": "26.4 One-Way ANOVA in R\n\n\nCode\n# Example using iris data\niris_aov &lt;- aov(Sepal.Length ~ Species, data = iris)\nsummary(iris_aov)\n\n\n             Df Sum Sq Mean Sq F value Pr(&gt;F)    \nSpecies       2  63.21  31.606   119.3 &lt;2e-16 ***\nResiduals   147  38.96   0.265                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe significant p-value tells us that sepal length differs among species, but not which species differ from which.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Single Factor ANOVA</span>"
    ]
  },
  {
    "objectID": "chapters/19-single-factor-anova.html#anova-assumptions",
    "href": "chapters/19-single-factor-anova.html#anova-assumptions",
    "title": "26  Single Factor ANOVA",
    "section": "26.5 ANOVA Assumptions",
    "text": "26.5 ANOVA Assumptions\nLike the t-test, ANOVA assumes:\n\nNormality: Observations within each group are normally distributed\nHomogeneity of variance: Groups have equal variances\nIndependence: Observations are independent\n\nANOVA is robust to mild violations of normality, especially with balanced designs and large samples. Serious violations of homogeneity of variance are more problematic but can be addressed with Welch’s ANOVA or transformations.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Single Factor ANOVA</span>"
    ]
  },
  {
    "objectID": "chapters/19-single-factor-anova.html#post-hoc-comparisons",
    "href": "chapters/19-single-factor-anova.html#post-hoc-comparisons",
    "title": "26  Single Factor ANOVA",
    "section": "26.6 Post-Hoc Comparisons",
    "text": "26.6 Post-Hoc Comparisons\nA significant ANOVA tells us groups differ but not how. Post-hoc tests compare specific pairs of groups while controlling for multiple comparisons.\nTukey’s HSD (Honestly Significant Difference) compares all pairs:\n\n\nCode\nTukeyHSD(iris_aov)\n\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = Sepal.Length ~ Species, data = iris)\n\n$Species\n                      diff       lwr       upr p adj\nversicolor-setosa    0.930 0.6862273 1.1737727     0\nvirginica-setosa     1.582 1.3382273 1.8257727     0\nvirginica-versicolor 0.652 0.4082273 0.8957727     0\n\n\nEach pairwise comparison includes the difference in means, confidence interval, and adjusted p-value.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Single Factor ANOVA</span>"
    ]
  },
  {
    "objectID": "chapters/19-single-factor-anova.html#planned-contrasts",
    "href": "chapters/19-single-factor-anova.html#planned-contrasts",
    "title": "26  Single Factor ANOVA",
    "section": "26.7 Planned Contrasts",
    "text": "26.7 Planned Contrasts\nIf you have specific hypotheses about which groups should differ (decided before seeing the data), planned contrasts are more powerful than post-hoc tests. They focus statistical power on the comparisons you care about.\n\n\nCode\n# Example: Compare setosa to the average of the other two species\ncontrasts(iris$Species) &lt;- cbind(\n  setosa_vs_others = c(2, -1, -1)\n)\nsummary.lm(aov(Sepal.Length ~ Species, data = iris))\n\n\n\nCall:\naov(formula = Sepal.Length ~ Species, data = iris)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.6880 -0.3285 -0.0060  0.3120  1.3120 \n\nCoefficients:\n                        Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)              5.84333    0.04203 139.020  &lt; 2e-16 ***\nSpeciessetosa_vs_others -0.41867    0.02972 -14.086  &lt; 2e-16 ***\nSpecies                  0.46103    0.07280   6.333 2.77e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5148 on 147 degrees of freedom\nMultiple R-squared:  0.6187,    Adjusted R-squared:  0.6135 \nF-statistic: 119.3 on 2 and 147 DF,  p-value: &lt; 2.2e-16",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Single Factor ANOVA</span>"
    ]
  },
  {
    "objectID": "chapters/19-single-factor-anova.html#fixed-vs.-random-effects",
    "href": "chapters/19-single-factor-anova.html#fixed-vs.-random-effects",
    "title": "26  Single Factor ANOVA",
    "section": "26.8 Fixed vs. Random Effects",
    "text": "26.8 Fixed vs. Random Effects\nFixed effects are specific treatments of interest that would be the same if the study were replicated—drug A, drug B, drug C. Conclusions apply only to these specific treatments.\nRandom effects are levels sampled from a larger population—particular subjects, batches, or locations. The goal is to generalize to the population of possible levels, not just those observed.\nThe distinction matters because it affects how F-ratios are calculated and what conclusions can be drawn.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Single Factor ANOVA</span>"
    ]
  },
  {
    "objectID": "chapters/19-single-factor-anova.html#effect-sizes-in-anova",
    "href": "chapters/19-single-factor-anova.html#effect-sizes-in-anova",
    "title": "26  Single Factor ANOVA",
    "section": "26.9 Effect Sizes in ANOVA",
    "text": "26.9 Effect Sizes in ANOVA\nBeyond statistical significance, report how much of the variance is explained by your factors.\nEta-squared (\\(\\eta^2\\)): Proportion of total variance explained by the factor\n\\[\\eta^2 = \\frac{SS_{between}}{SS_{total}}\\]\nPartial eta-squared (\\(\\eta^2_p\\)): Proportion of variance explained after accounting for other factors\nOmega-squared (\\(\\omega^2\\)): Less biased estimate of variance explained in the population\n\n\nCode\n# Calculate effect sizes\nss &lt;- summary(iris_aov)[[1]]\nss_between &lt;- ss[\"Species\", \"Sum Sq\"]\nss_within &lt;- ss[\"Residuals\", \"Sum Sq\"]\nss_total &lt;- ss_between + ss_within\n\neta_squared &lt;- ss_between / ss_total\ncat(\"Eta-squared:\", round(eta_squared, 3), \"\\n\")\n\n\nEta-squared: 0.619 \n\n\nCode\n# Omega-squared (less biased)\nms_within &lt;- ss[\"Residuals\", \"Mean Sq\"]\nn &lt;- nrow(iris)\nk &lt;- length(unique(iris$Species))\nomega_squared &lt;- (ss_between - (k-1) * ms_within) / (ss_total + ms_within)\ncat(\"Omega-squared:\", round(omega_squared, 3), \"\\n\")\n\n\nOmega-squared: 0.612",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Single Factor ANOVA</span>"
    ]
  },
  {
    "objectID": "chapters/19-single-factor-anova.html#pseudoreplication",
    "href": "chapters/19-single-factor-anova.html#pseudoreplication",
    "title": "26  Single Factor ANOVA",
    "section": "26.10 Pseudoreplication",
    "text": "26.10 Pseudoreplication\n\n\n\n\n\n\nA Common Design Flaw\n\n\n\nPseudoreplication occurs when non-independent observations are treated as independent replicates. This inflates the apparent sample size and leads to artificially small p-values.\nCommon examples: - Multiple measurements from the same individual treated as independent - Multiple cells from the same culture dish - Multiple fish from the same tank when treatment was applied to tanks - Technical replicates confused with biological replicates\n\n\nThe unit of replication must be the unit to which the treatment was independently applied. If you treat three tanks with drug A and three with drug B, you have n=3 per group regardless of how many fish are in each tank.\n\n\nCode\n# Wrong: treats individual fish as independent\n# If 10 fish per tank, and tanks are the true units:\nset.seed(42)\n# This overstates the evidence because fish within tanks are correlated\ntank_A &lt;- rep(c(10, 12, 11), each = 10) + rnorm(30, sd = 1)  # 3 tanks, 10 fish each\ntank_B &lt;- rep(c(8, 9, 8.5), each = 10) + rnorm(30, sd = 1)\n\n# Pseudoreplicated analysis (WRONG - n appears to be 30 per group)\ncat(\"Pseudoreplicated p-value:\", t.test(tank_A, tank_B)$p.value, \"\\n\")\n\n\nPseudoreplicated p-value: 2.315344e-11 \n\n\nCode\n# Correct analysis (using tank means, n = 3 per group)\nmeans_A &lt;- c(mean(tank_A[1:10]), mean(tank_A[11:20]), mean(tank_A[21:30]))\nmeans_B &lt;- c(mean(tank_B[1:10]), mean(tank_B[11:20]), mean(tank_B[21:30]))\ncat(\"Correct p-value:\", t.test(means_A, means_B)$p.value, \"\\n\")\n\n\nCorrect p-value: 0.008405113 \n\n\nThe correct analysis has less power (larger p-value) because it honestly reflects the true sample size.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Single Factor ANOVA</span>"
    ]
  },
  {
    "objectID": "chapters/19-single-factor-anova.html#anova-as-a-general-linear-model",
    "href": "chapters/19-single-factor-anova.html#anova-as-a-general-linear-model",
    "title": "26  Single Factor ANOVA",
    "section": "26.11 ANOVA as a General Linear Model",
    "text": "26.11 ANOVA as a General Linear Model\nANOVA is a special case of the general linear model (GLM). Both t-tests and ANOVA can be expressed as regression with indicator variables (dummy coding). This unified framework shows that these seemingly different methods are fundamentally the same.\n\n\nCode\n# ANOVA using lm() with dummy coding\n# Equivalent to aov()\niris_lm &lt;- lm(Sepal.Length ~ Species, data = iris)\nanova(iris_lm)\n\n\nAnalysis of Variance Table\n\nResponse: Sepal.Length\n           Df Sum Sq Mean Sq F value    Pr(&gt;F)    \nSpecies     2 63.212  31.606  119.26 &lt; 2.2e-16 ***\nResiduals 147 38.956   0.265                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe connection becomes clear when you realize: - A one-sample t-test is regression on an intercept - A two-sample t-test is regression with one binary predictor - One-way ANOVA is regression with multiple indicator variables\nThis unified view is powerful: once you understand regression, you understand the entire family of linear models.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Single Factor ANOVA</span>"
    ]
  },
  {
    "objectID": "chapters/19-single-factor-anova.html#practice-exercises",
    "href": "chapters/19-single-factor-anova.html#practice-exercises",
    "title": "26  Single Factor ANOVA",
    "section": "26.12 Practice Exercises",
    "text": "26.12 Practice Exercises\n\n\n\n\n\n\nExercise 1: Plant Growth Analysis\n\n\n\nThe PlantGrowth dataset in R contains weights of plants obtained under three different conditions: control, treatment 1, and treatment 2.\n\nPerform a one-way ANOVA to test whether the treatments affect plant weight\nCheck the assumptions using appropriate diagnostic plots\nIf the ANOVA is significant, perform Tukey’s HSD to identify which groups differ\nCalculate and interpret the eta-squared effect size\nVisualize the results with a boxplot or violin plot\n\n\n\n\n\n\n\n\n\nExercise 2: Diet and Weight Loss\n\n\n\nA researcher tests four different diets on 40 participants (10 per diet). After 8 weeks, weight loss (in kg) is recorded. Create a simulated dataset and:\n\nPerform a one-way ANOVA\nTest the homogeneity of variance assumption using Levene’s test\nPerform post-hoc comparisons using Tukey’s HSD\nCalculate omega-squared to estimate the population effect size\nWrite a brief interpretation of the results\n\n\n\n\n\n\n\n\n\nExercise 3: Planned Contrasts\n\n\n\nUsing the chickwts dataset, which contains chicken weights for different feed supplements:\n\nExamine the feed types and formulate two specific contrasts before analysis\nPerform a one-way ANOVA\nTest your planned contrasts\nCompare the p-values from planned contrasts to post-hoc tests\nDiscuss why planned contrasts might be preferable when you have specific hypotheses\n\n\n\n\n\n\n\n\n\nExercise 4: Pseudoreplication Detection\n\n\n\nA student measures enzyme activity in cells. They have 3 culture dishes per treatment (control and experimental), with 20 cells measured per dish.\n\nWhat is the true sample size for each treatment?\nWhat would happen to the p-value if cells were incorrectly treated as independent replicates?\nWrite R code to demonstrate the difference between the pseudoreplicated and correct analysis\nExplain how you would properly analyze this experiment\n\n\n\n\n\n\n\n\n\nExercise 5: ANOVA as Regression\n\n\n\nUsing the iris dataset:\n\nPerform a one-way ANOVA using aov() to test for differences in Petal.Length across species\nPerform the same analysis using lm() and compare the results\nExamine the coefficients from the lm() model and interpret what they represent\nUse anova() on the lm() object to get the ANOVA table\nExplain how the regression framework relates to the ANOVA framework",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Single Factor ANOVA</span>"
    ]
  },
  {
    "objectID": "chapters/19-single-factor-anova.html#summary",
    "href": "chapters/19-single-factor-anova.html#summary",
    "title": "26  Single Factor ANOVA",
    "section": "26.13 Summary",
    "text": "26.13 Summary\nSingle-factor ANOVA provides a framework for comparing means across multiple groups:\n\nOne-way ANOVA tests whether any group means differ in a single test\nThe F-test assesses whether between-group variance exceeds within-group variance\nPost-hoc tests identify which specific groups differ while controlling for multiple comparisons\nPlanned contrasts are more powerful when you have specific hypotheses\nFixed effects are specific treatments; random effects are sampled from populations\nEffect sizes (eta-squared, omega-squared) quantify the proportion of variance explained\nPseudoreplication is a critical design flaw that must be avoided\nANOVA is a special case of the general linear model\n\nAlways check assumptions, report effect sizes alongside p-values, and ensure your unit of analysis matches your unit of replication.\n\n\n\n\n\n\nFisher, Ronald A. 1925. Statistical Methods for Research Workers. Edinburgh: Oliver; Boyd.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Single Factor ANOVA</span>"
    ]
  },
  {
    "objectID": "chapters/20-multifactor-anova.html",
    "href": "chapters/20-multifactor-anova.html",
    "title": "27  Multi-Factor ANOVA",
    "section": "",
    "text": "27.1 Introduction to Factorial Designs\nWhile one-way ANOVA compares groups defined by a single factor, many biological experiments manipulate multiple factors simultaneously. A factorial design examines all combinations of factor levels, allowing us to study not only the independent effect of each factor but also how factors interact with each other.\nConsider an experiment testing the effects of both temperature and nutrient concentration on algal growth. A factorial design would include all combinations: high temperature with high nutrients, high temperature with low nutrients, low temperature with high nutrients, and low temperature with low nutrients. This approach is more efficient than conducting separate experiments for each factor, and it reveals interactions that single-factor designs would miss.\nFactorial designs are characterized by: - Multiple factors (independent variables) - All combinations of factor levels are tested - Ability to test for interactions between factors - Greater efficiency than separate one-factor experiments\nThe notation \\(a \\times b\\) factorial design means factor A has \\(a\\) levels and factor B has \\(b\\) levels, producing \\(a \\times b\\) treatment combinations. A \\(2 \\times 3\\) design has 6 treatment groups; a \\(2 \\times 2 \\times 2\\) design has 8 treatment groups.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Multi-Factor ANOVA</span>"
    ]
  },
  {
    "objectID": "chapters/20-multifactor-anova.html#two-way-anova",
    "href": "chapters/20-multifactor-anova.html#two-way-anova",
    "title": "27  Multi-Factor ANOVA",
    "section": "27.2 Two-Way ANOVA",
    "text": "27.2 Two-Way ANOVA\nTwo-way ANOVA analyzes data from a two-factor experimental design. The model partitions variance into:\n\nMain effect of Factor A: The average effect of Factor A across all levels of Factor B\nMain effect of Factor B: The average effect of Factor B across all levels of Factor A\nInteraction effect (A × B): The extent to which the effect of one factor depends on the level of the other factor\nResidual (error): Unexplained variation within treatment groups\n\nThe statistical model is:\n\\[y_{ijk} = \\mu + \\alpha_i + \\beta_j + (\\alpha\\beta)_{ij} + \\epsilon_{ijk}\\]\nwhere: - \\(\\mu\\) is the grand mean - \\(\\alpha_i\\) is the effect of level \\(i\\) of Factor A - \\(\\beta_j\\) is the effect of level \\(j\\) of Factor B - \\((\\alpha\\beta)_{ij}\\) is the interaction effect - \\(\\epsilon_{ijk}\\) is the random error for observation \\(k\\) in treatment combination \\(ij\\)\n\n\nCode\n# Simulated factorial experiment: Temperature × Nutrient effects on growth\nset.seed(123)\nn &lt;- 15  # replicates per treatment\n\n# Create factorial design data\ntemperature &lt;- rep(c(\"Low\", \"High\"), each = 2*n)\nnutrient &lt;- rep(rep(c(\"Low\", \"High\"), each = n), 2)\n\n# Simulate growth with main effects AND interaction\n# Low temp & low nutrient: baseline 10\n# High nutrient adds 5\n# High temp adds 3\n# BUT interaction: high temp × high nutrient gives extra boost of 4\ngrowth &lt;- numeric(4*n)\ngrowth[temperature == \"Low\" & nutrient == \"Low\"] &lt;- rnorm(n, 10, 1.5)\ngrowth[temperature == \"Low\" & nutrient == \"High\"] &lt;- rnorm(n, 15, 1.5)\ngrowth[temperature == \"High\" & nutrient == \"Low\"] &lt;- rnorm(n, 13, 1.5)\ngrowth[temperature == \"High\" & nutrient == \"High\"] &lt;- rnorm(n, 22, 1.5)  # Synergistic effect\n\nfactorial_data &lt;- data.frame(\n  temperature = factor(temperature, levels = c(\"Low\", \"High\")),\n  nutrient = factor(nutrient, levels = c(\"Low\", \"High\")),\n  growth = growth\n)\n\n# Visualize the design\nggplot(factorial_data, aes(x = temperature, y = growth, fill = nutrient)) +\n  geom_boxplot(alpha = 0.7) +\n  geom_jitter(width = 0.1, alpha = 0.3, size = 1) +\n  labs(title = \"Factorial Design: Temperature × Nutrient\",\n       subtitle = \"Each combination of factor levels is tested\",\n       x = \"Temperature\",\n       y = \"Growth Rate\",\n       fill = \"Nutrient\\nLevel\") +\n  scale_fill_manual(values = c(\"Low\" = \"#E69F00\", \"High\" = \"#56B4E9\")) +\n  theme_minimal(base_size = 12)\n\n\n\n\n\n\n\n\nFigure 27.1: Two-way ANOVA factorial design showing main effects and interaction\n\n\n\n\n\n\n\nCode\n# Fit the two-way ANOVA model\nfactorial_aov &lt;- aov(growth ~ temperature * nutrient, data = factorial_data)\nsummary(factorial_aov)\n\n\n                     Df Sum Sq Mean Sq F value   Pr(&gt;F)    \ntemperature           1  427.4   427.4  228.63  &lt; 2e-16 ***\nnutrient              1  638.7   638.7  341.63  &lt; 2e-16 ***\ntemperature:nutrient  1   67.7    67.7   36.19 1.43e-07 ***\nResiduals            56  104.7     1.9                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe ANOVA table shows three F-tests: - temperature: Tests whether the main effect of temperature is significant - nutrient: Tests whether the main effect of nutrient is significant - temperature:nutrient: Tests whether the interaction is significant",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Multi-Factor ANOVA</span>"
    ]
  },
  {
    "objectID": "chapters/20-multifactor-anova.html#understanding-interactions",
    "href": "chapters/20-multifactor-anova.html#understanding-interactions",
    "title": "27  Multi-Factor ANOVA",
    "section": "27.3 Understanding Interactions",
    "text": "27.3 Understanding Interactions\nAn interaction occurs when the effect of one factor depends on the level of another factor. Interactions are one of the most important features of factorial designs—they reveal that factors do not operate independently.\nTypes of effects:\n\nAdditive effects (no interaction): The effect of Factor A is the same regardless of Factor B’s level. Effects simply add together.\nSynergistic interaction: The combined effect is greater than the sum of individual effects. For example, two drugs together might be more effective than expected from their individual effects.\nAntagonistic interaction: The combined effect is less than expected. One factor might diminish or reverse the effect of another.\n\n\nInteraction Plots\nThe clearest way to understand interactions is through interaction plots, which show the mean response at each factor combination.\n\n\nCode\n# Calculate means for each combination\nmeans_data &lt;- factorial_data %&gt;%\n  group_by(temperature, nutrient) %&gt;%\n  summarize(mean_growth = mean(growth),\n            se = sd(growth) / sqrt(n()),\n            .groups = \"drop\")\n\n# Create interaction plot\nggplot(means_data, aes(x = temperature, y = mean_growth,\n                       color = nutrient, group = nutrient)) +\n  geom_line(linewidth = 1.2) +\n  geom_point(size = 4) +\n  geom_errorbar(aes(ymin = mean_growth - se, ymax = mean_growth + se),\n                width = 0.1, linewidth = 1) +\n  labs(title = \"Interaction Plot: Temperature × Nutrient\",\n       subtitle = \"Non-parallel lines indicate an interaction effect\",\n       x = \"Temperature\",\n       y = \"Mean Growth Rate\",\n       color = \"Nutrient\\nLevel\") +\n  scale_color_manual(values = c(\"Low\" = \"#E69F00\", \"High\" = \"#56B4E9\")) +\n  theme_minimal(base_size = 12) +\n  theme(legend.position = \"right\")\n\n# Alternative: base R interaction.plot\ninteraction.plot(\n  x.factor = factorial_data$temperature,\n  trace.factor = factorial_data$nutrient,\n  response = factorial_data$growth,\n  col = c(\"#E69F00\", \"#56B4E9\"),\n  lwd = 2,\n  xlab = \"Temperature\",\n  ylab = \"Mean Growth Rate\",\n  trace.label = \"Nutrient\"\n)\n\n\n\n\n\n\n\n\nFigure 27.2: Interaction plot showing non-parallel lines indicating interaction between factors\n\n\n\n\n\n\n\n\n\n\n\nFigure 27.3: Interaction plot showing non-parallel lines indicating interaction between factors\n\n\n\n\n\nInterpreting interaction plots:\n\nParallel lines suggest no interaction—the effect of one factor is constant across levels of the other\nNon-parallel lines suggest an interaction—the effect of one factor changes depending on the other factor\nCrossing lines indicate a strong interaction, potentially with a reversal of effects\n\n\n\n\n\n\n\nFigure 27.4: Illustration of interaction patterns showing parallel versus non-parallel lines\n\n\n\nThe figure above illustrates common interaction patterns. In panel (a), parallel lines indicate no interaction—the effect of Factor A is the same at both levels of Factor B. In panel (b), non-parallel lines reveal an interaction—the effect of Factor A differs depending on Factor B’s level.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Multi-Factor ANOVA</span>"
    ]
  },
  {
    "objectID": "chapters/20-multifactor-anova.html#interpreting-main-effects-vs-interactions",
    "href": "chapters/20-multifactor-anova.html#interpreting-main-effects-vs-interactions",
    "title": "27  Multi-Factor ANOVA",
    "section": "27.4 Interpreting Main Effects vs Interactions",
    "text": "27.4 Interpreting Main Effects vs Interactions\nWhen a significant interaction exists, interpreting main effects requires caution. The main effect represents an average across levels of the other factor, but if there’s an interaction, this average may not meaningfully represent what happens at any particular level.\n\n\n\n\n\n\nWhen Interactions are Present\n\n\n\nIf the interaction is significant, focus on simple effects—the effect of one factor at each level of the other factor—rather than main effects. The main effect is an average that may obscure important differences.\n\n\nSimple effects analysis examines the effect of one factor separately at each level of the other factor:\n\n\nCode\n# Simple effects: effect of nutrient at each temperature level\n# Low temperature\nlow_temp_data &lt;- factorial_data %&gt;% filter(temperature == \"Low\")\nt.test(growth ~ nutrient, data = low_temp_data)\n\n\n\n    Welch Two Sample t-test\n\ndata:  growth by nutrient\nt = -8.2269, df = 26.34, p-value = 9.446e-09\nalternative hypothesis: true difference in means between group Low and group High is not equal to 0\n95 percent confidence interval:\n -5.500596 -3.302476\nsample estimates:\n mean in group Low mean in group High \n          10.22858           14.63011 \n\n\nCode\n# High temperature\nhigh_temp_data &lt;- factorial_data %&gt;% filter(temperature == \"High\")\nt.test(growth ~ nutrient, data = high_temp_data)\n\n\n\n    Welch Two Sample t-test\n\ndata:  growth by nutrient\nt = -18.771, df = 27.893, p-value &lt; 2.2e-16\nalternative hypothesis: true difference in means between group Low and group High is not equal to 0\n95 percent confidence interval:\n -9.593152 -7.705141\nsample estimates:\n mean in group Low mean in group High \n          13.44293           22.09208 \n\n\nIn our example, the nutrient effect is significant at both temperatures, but the magnitude differs—the interaction shows that the nutrient boost is stronger at high temperature.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Multi-Factor ANOVA</span>"
    ]
  },
  {
    "objectID": "chapters/20-multifactor-anova.html#ancova-analysis-of-covariance",
    "href": "chapters/20-multifactor-anova.html#ancova-analysis-of-covariance",
    "title": "27  Multi-Factor ANOVA",
    "section": "27.5 ANCOVA: Analysis of Covariance",
    "text": "27.5 ANCOVA: Analysis of Covariance\nAnalysis of Covariance (ANCOVA) combines ANOVA with regression by including both categorical factors and continuous covariates. The covariate is a continuous variable that you want to control for—it’s not a treatment you manipulate, but a source of variation you want to account for.\nCommon uses of ANCOVA: - Controlling for pre-existing differences (baseline measurements) - Increasing precision by removing variance explained by the covariate - Adjusting for confounding variables - Testing whether regression slopes differ across groups\n\nThe ANCOVA Model\nThe ANCOVA model for one factor and one covariate is:\n\\[y_{ij} = \\mu + \\alpha_i + \\beta(x_{ij} - \\bar{x}) + \\epsilon_{ij}\\]\nwhere: - \\(\\mu\\) is the overall mean - \\(\\alpha_i\\) is the effect of group \\(i\\) - \\(\\beta\\) is the regression slope (common to all groups) - \\(x_{ij}\\) is the covariate value - \\(\\bar{x}\\) is the mean of the covariate - \\(\\epsilon_{ij}\\) is the random error\nThe key assumption is that the relationship between the covariate and response has the same slope in all groups (homogeneity of regression slopes).\n\n\nCode\n# Simulated ANCOVA example: Effect of diet on final weight, controlling for initial weight\nset.seed(567)\nn_per_group &lt;- 20\n\n# Three diet treatments\ndiet_data &lt;- data.frame(\n  diet = factor(rep(c(\"Control\", \"Low-fat\", \"High-protein\"), each = n_per_group)),\n  initial_weight = c(\n    rnorm(n_per_group, 75, 8),   # Control group\n    rnorm(n_per_group, 78, 8),   # Low-fat (slightly heavier initially)\n    rnorm(n_per_group, 73, 8)    # High-protein (slightly lighter initially)\n  )\n) %&gt;%\n  mutate(\n    # Final weight depends on diet AND initial weight\n    # Diet effects: Control = 0, Low-fat = -3, High-protein = -5\n    diet_effect = case_when(\n      diet == \"Control\" ~ 0,\n      diet == \"Low-fat\" ~ -3,\n      diet == \"High-protein\" ~ -5\n    ),\n    # Final weight = initial + diet effect + some regression to mean + noise\n    final_weight = initial_weight + diet_effect + 0.3 * (initial_weight - 75) + rnorm(n(), 0, 3)\n  )\n\n# Visualize the ANCOVA setup\nggplot(diet_data, aes(x = initial_weight, y = final_weight, color = diet)) +\n  geom_point(alpha = 0.7, size = 2) +\n  geom_smooth(method = \"lm\", se = FALSE, linewidth = 1) +\n  labs(title = \"ANCOVA: Diet Effect on Final Weight\",\n       subtitle = \"Parallel regression lines show diet effects while controlling for initial weight\",\n       x = \"Initial Weight (kg)\",\n       y = \"Final Weight (kg)\",\n       color = \"Diet\") +\n  scale_color_manual(values = c(\"Control\" = \"#E69F00\", \"Low-fat\" = \"#56B4E9\", \"High-protein\" = \"#009E73\")) +\n  theme_minimal(base_size = 12)\n\n\n\n\n\n\n\n\nFigure 27.5: ANCOVA example showing treatment effects on final weight while controlling for initial weight\n\n\n\n\n\n\n\nFitting ANCOVA in R\n\n\nCode\n# Fit ANCOVA model\nancova_model &lt;- aov(final_weight ~ initial_weight + diet, data = diet_data)\nsummary(ancova_model)\n\n\n               Df Sum Sq Mean Sq F value   Pr(&gt;F)    \ninitial_weight  1   7582    7582  607.91  &lt; 2e-16 ***\ndiet            2    270     135   10.83 0.000106 ***\nResiduals      56    698      12                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nNote the order of terms: the covariate (initial_weight) is entered first to remove its effect before testing the factor (diet).\n\n\nCode\n# Compare to ANOVA without covariate\nanova_only &lt;- aov(final_weight ~ diet, data = diet_data)\nsummary(anova_only)\n\n\n            Df Sum Sq Mean Sq F value   Pr(&gt;F)    \ndiet         2   1951   975.5   8.426 0.000623 ***\nResiduals   57   6600   115.8                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCode\n# The ANCOVA has smaller residual SS and more power\ncat(\"\\nResidual SS (ANOVA):\", round(sum(residuals(anova_only)^2), 1), \"\\n\")\n\n\n\nResidual SS (ANOVA): 6599.6 \n\n\nCode\ncat(\"Residual SS (ANCOVA):\", round(sum(residuals(ancova_model)^2), 1), \"\\n\")\n\n\nResidual SS (ANCOVA): 698.5 \n\n\n\n\nAdjusted Means\nANCOVA produces adjusted means—the group means estimated at the overall mean of the covariate. These are the means we would expect if all groups had started with the same covariate value.\n\n\nCode\n# Get adjusted means\nlibrary(emmeans)\nancova_emm &lt;- emmeans(ancova_model, \"diet\")\nancova_emm\n\n\n diet         emmean    SE df lower.CL upper.CL\n Control        73.7 0.802 56     72.1     75.3\n High-protein   68.6 0.818 56     66.9     70.2\n Low-fat        70.3 0.864 56     68.6     72.0\n\nConfidence level used: 0.95 \n\n\nCode\n# Pairwise comparisons of adjusted means\npairs(ancova_emm, adjust = \"tukey\")\n\n\n contrast                   estimate   SE df t.ratio p.value\n Control - (High-protein)       5.11 1.12 56   4.567 &lt;0.0001\n Control - (Low-fat)            3.38 1.22 56   2.774  0.0202\n (High-protein) - (Low-fat)    -1.73 1.25 56  -1.385  0.3554\n\nP value adjustment: tukey method for comparing a family of 3 estimates \n\n\n\n\nTesting Homogeneity of Slopes\nA critical assumption is that regression slopes are equal across groups. Test this by including the interaction term:\n\n\nCode\n# Test for unequal slopes (interaction between covariate and factor)\nslope_test &lt;- aov(final_weight ~ initial_weight * diet, data = diet_data)\nsummary(slope_test)\n\n\n                    Df Sum Sq Mean Sq F value   Pr(&gt;F)    \ninitial_weight       1   7582    7582  594.01  &lt; 2e-16 ***\ndiet                 2    270     135   10.58 0.000133 ***\ninitial_weight:diet  2      9       5    0.36 0.699390    \nResiduals           54    689      13                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nA significant interaction indicates slopes differ across groups, violating the ANCOVA assumption. In that case, you should: 1. Report separate regressions for each group 2. Use a more complex model (separate slopes ANCOVA) 3. Consider whether the interaction itself is scientifically meaningful\n\n\nCode\n# Visualize slopes by group\nggplot(diet_data, aes(x = initial_weight, y = final_weight, color = diet)) +\n  geom_point(alpha = 0.6) +\n  geom_smooth(method = \"lm\", se = TRUE, alpha = 0.2) +\n  labs(title = \"Testing Homogeneity of Regression Slopes\",\n       subtitle = \"Slopes should be approximately parallel for valid ANCOVA\",\n       x = \"Initial Weight (kg)\",\n       y = \"Final Weight (kg)\",\n       color = \"Diet\") +\n  scale_color_manual(values = c(\"Control\" = \"#E69F00\", \"Low-fat\" = \"#56B4E9\", \"High-protein\" = \"#009E73\")) +\n  theme_minimal(base_size = 12)\n\n\n\n\n\n\n\n\nFigure 27.6: Testing homogeneity of slopes: parallel lines indicate the assumption is met\n\n\n\n\n\n\n\n\n\n\n\nANCOVA Assumptions\n\n\n\n\nIndependence of observations\nNormality of residuals\nHomogeneity of variance across groups\nLinear relationship between covariate and response\nHomogeneity of regression slopes (equal slopes across groups)\nCovariate measured without error (or with negligible error)\nCovariate independent of treatment (especially important in observational studies)\n\nThe last assumption is critical: if the treatment affects the covariate, ANCOVA can remove real treatment effects. The covariate should ideally be measured before treatment is applied.\n\n\n\n\nWhen to Use ANCOVA\nUse ANCOVA when: - You have a continuous covariate measured before treatment - You want to increase precision by accounting for baseline variation - Groups differ on the covariate (but not due to treatment) - The covariate-response relationship is linear with equal slopes\nAvoid ANCOVA when: - The covariate is affected by treatment - Slopes clearly differ across groups - The covariate is categorical (use factorial ANOVA instead) - Sample sizes are very small",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Multi-Factor ANOVA</span>"
    ]
  },
  {
    "objectID": "chapters/20-multifactor-anova.html#nested-designs",
    "href": "chapters/20-multifactor-anova.html#nested-designs",
    "title": "27  Multi-Factor ANOVA",
    "section": "27.6 Nested Designs",
    "text": "27.6 Nested Designs\nIn nested designs, levels of one factor exist only within levels of another factor. This differs from a factorial design where all combinations of factors are crossed.\nCommon examples in biology: - Students nested within classrooms (each student is in only one classroom) - Samples nested within sites (each sample comes from one site) - Technicians nested within labs (each technician works in one lab) - Subsamples nested within experimental units\nIn a nested design, we cannot estimate an interaction because not all factor combinations exist. The nested factor is typically a random effect representing sampling variability.\nNotation: Factor B is nested within Factor A, written as B(A) or B/A.\n\n\nCode\n# Simulated nested design: Technicians (nested within Labs)\nset.seed(456)\nn_labs &lt;- 3\nn_techs_per_lab &lt;- 3\nn_measurements &lt;- 5\n\nlab &lt;- rep(paste0(\"Lab\", 1:n_labs), each = n_techs_per_lab * n_measurements)\ntech &lt;- rep(paste0(\"Tech\", 1:(n_labs * n_techs_per_lab)), each = n_measurements)\n\n# Lab effects (fixed)\nlab_effects &lt;- c(100, 105, 110)\n# Technician effects (random, nested within labs)\ntech_effects &lt;- rnorm(n_labs * n_techs_per_lab, 0, 3)\n\nmeasurement &lt;- numeric(length(lab))\nfor (i in 1:length(lab)) {\n  lab_idx &lt;- as.numeric(substr(lab[i], 4, 4))\n  tech_idx &lt;- as.numeric(substr(tech[i], 5, 5))\n  measurement[i] &lt;- lab_effects[lab_idx] + tech_effects[tech_idx] + rnorm(1, 0, 2)\n}\n\nnested_data &lt;- data.frame(\n  lab = factor(lab),\n  tech = factor(tech),\n  measurement = measurement\n)\n\n# Visualize nested structure\nggplot(nested_data, aes(x = tech, y = measurement, color = lab)) +\n  geom_point(alpha = 0.6, size = 2) +\n  stat_summary(fun = mean, geom = \"point\", size = 4, shape = 18) +\n  facet_wrap(~ lab, scales = \"free_x\") +\n  labs(title = \"Nested Design: Technicians within Labs\",\n       subtitle = \"Each technician works in only one lab\",\n       x = \"Technician\",\n       y = \"Measurement\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1),\n        legend.position = \"none\")\n\n\n\n\n\n\n\n\nFigure 27.7: Nested design example: technicians nested within labs\n\n\n\n\n\n\n\nCode\n# Analysis of nested design\n# Technician is nested within lab: tech %in% lab or tech/lab\nnested_aov &lt;- aov(measurement ~ lab + Error(tech %in% lab), data = nested_data)\nsummary(nested_aov)\n\n\n\nError: tech:lab\n          Df Sum Sq Mean Sq F value  Pr(&gt;F)   \nlab        2 1157.6   578.8   24.02 0.00137 **\nResiduals  6  144.6    24.1                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nError: Within\n          Df Sum Sq Mean Sq F value Pr(&gt;F)\nResiduals 36  150.8   4.189               \n\n\nCode\n# Alternative using lme4 for mixed effects\nlibrary(lme4)\nnested_lmer &lt;- lmer(measurement ~ lab + (1|lab:tech), data = nested_data)\nsummary(nested_lmer)\n\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: measurement ~ lab + (1 | lab:tech)\n   Data: nested_data\n\nREML criterion at convergence: 198\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-2.12446 -0.44723 -0.04123  0.60879  1.92716 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n lab:tech (Intercept) 3.982    1.996   \n Residual             4.189    2.047   \nNumber of obs: 45, groups:  lab:tech, 9\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept)  100.966      1.268  79.653\nlabLab2        1.425      1.793   0.795\nlabLab3       11.401      1.793   6.360\n\nCorrelation of Fixed Effects:\n        (Intr) labLb2\nlabLab2 -0.707       \nlabLab3 -0.707  0.500\n\n\nThe nested design partitions variance into: - Between labs (fixed effect) - Between technicians within labs (random effect) - Within technicians (residual error)",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Multi-Factor ANOVA</span>"
    ]
  },
  {
    "objectID": "chapters/20-multifactor-anova.html#repeated-measures-anova",
    "href": "chapters/20-multifactor-anova.html#repeated-measures-anova",
    "title": "27  Multi-Factor ANOVA",
    "section": "27.7 Repeated Measures ANOVA",
    "text": "27.7 Repeated Measures ANOVA\nWhen the same subjects are measured multiple times under different conditions, observations are not independent. Repeated measures ANOVA accounts for the correlation structure in the data by treating subjects as a blocking factor or random effect.\nAdvantages: - Increased statistical power (controls for individual differences) - Requires fewer subjects - Each subject serves as their own control\nAssumptions: - Sphericity: The variance of differences between all pairs of repeated measures should be equal - Violations of sphericity can be corrected using Greenhouse-Geisser or Huynh-Feldt adjustments\n\n\nCode\n# Simulated repeated measures: Drug effect over time\nset.seed(789)\nn_subjects &lt;- 12\ntimepoints &lt;- c(\"Baseline\", \"Week_1\", \"Week_2\", \"Week_4\")\n\n# Each subject has a baseline level\nbaseline_values &lt;- rnorm(n_subjects, 50, 10)\n\n# Treatment effect increases over time\ntime_effects &lt;- c(0, -3, -6, -8)\n\n# Create data\nrm_data &lt;- expand.grid(\n  subject = factor(1:n_subjects),\n  time = factor(timepoints, levels = timepoints)\n) %&gt;%\n  arrange(subject, time) %&gt;%\n  mutate(\n    baseline = rep(baseline_values, each = length(timepoints)),\n    time_num = as.numeric(time),\n    response = baseline + time_effects[time_num] + rnorm(n(), 0, 2)\n  )\n\n# Visualize repeated measures\nggplot(rm_data, aes(x = time, y = response, group = subject)) +\n  geom_line(alpha = 0.4, color = \"gray40\") +\n  geom_point(alpha = 0.4, size = 2) +\n  stat_summary(aes(group = 1), fun = mean, geom = \"line\",\n               color = \"red\", linewidth = 1.5) +\n  stat_summary(aes(group = 1), fun = mean, geom = \"point\",\n               color = \"red\", size = 4) +\n  labs(title = \"Repeated Measures: Drug Response Over Time\",\n       subtitle = \"Gray lines show individual subjects; red line shows mean response\",\n       x = \"Time Point\",\n       y = \"Response Variable\") +\n  theme_minimal(base_size = 12)\n\n\n\n\n\n\n\n\nFigure 27.8: Repeated measures design showing within-subject changes over time\n\n\n\n\n\n\n\nCode\n# Traditional repeated measures ANOVA\n# Error term specifies within-subject factor\nrm_aov &lt;- aov(response ~ time + Error(subject/time), data = rm_data)\nsummary(rm_aov)\n\n\n\nError: subject\n          Df Sum Sq Mean Sq F value Pr(&gt;F)\nResiduals 11   2802   254.7               \n\nError: subject:time\n          Df Sum Sq Mean Sq F value   Pr(&gt;F)    \ntime       3  525.4  175.13   45.13 8.86e-12 ***\nResiduals 33  128.0    3.88                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCode\n# Check sphericity assumption\nlibrary(car)\n# Reshape to wide format for Mauchly's test\nrm_wide &lt;- rm_data %&gt;%\n  select(subject, time, response) %&gt;%\n  pivot_wider(names_from = time, values_from = response)\n\n# Alternative: Use mixed effects model (handles violations better)\nlibrary(lme4)\nrm_lmer &lt;- lmer(response ~ time + (1|subject), data = rm_data)\nsummary(rm_lmer)\n\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: response ~ time + (1 | subject)\n   Data: rm_data\n\nREML criterion at convergence: 240.5\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-2.7487 -0.5256  0.1230  0.6037  1.3658 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n subject  (Intercept) 62.71    7.919   \n Residual              3.88    1.970   \nNumber of obs: 48, groups:  subject, 12\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept)  46.1978     2.3557  19.611\ntimeWeek_1   -3.0335     0.8042  -3.772\ntimeWeek_2   -6.7738     0.8042  -8.423\ntimeWeek_4   -8.5300     0.8042 -10.607\n\nCorrelation of Fixed Effects:\n           (Intr) tmWk_1 tmWk_2\ntimeWeek_1 -0.171              \ntimeWeek_2 -0.171  0.500       \ntimeWeek_4 -0.171  0.500  0.500\n\n\nCode\nanova(rm_lmer)\n\n\nAnalysis of Variance Table\n     npar Sum Sq Mean Sq F value\ntime    3  525.4  175.13  45.134\n\n\nPost-hoc tests for repeated measures:\n\n\nCode\n# Pairwise comparisons with correction for multiple testing\nlibrary(emmeans)\nrm_emm &lt;- emmeans(rm_lmer, \"time\")\npairs(rm_emm, adjust = \"bonferroni\")\n\n\n contrast          estimate    SE df t.ratio p.value\n Baseline - Week_1     3.03 0.804 33   3.772  0.0038\n Baseline - Week_2     6.77 0.804 33   8.423 &lt;0.0001\n Baseline - Week_4     8.53 0.804 33  10.607 &lt;0.0001\n Week_1 - Week_2       3.74 0.804 33   4.651  0.0003\n Week_1 - Week_4       5.50 0.804 33   6.835 &lt;0.0001\n Week_2 - Week_4       1.76 0.804 33   2.184  0.2171\n\nDegrees-of-freedom method: kenward-roger \nP value adjustment: bonferroni method for 6 tests",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Multi-Factor ANOVA</span>"
    ]
  },
  {
    "objectID": "chapters/20-multifactor-anova.html#mixed-effects-models",
    "href": "chapters/20-multifactor-anova.html#mixed-effects-models",
    "title": "27  Multi-Factor ANOVA",
    "section": "27.8 Mixed Effects Models",
    "text": "27.8 Mixed Effects Models\nMixed effects models (also called multilevel models or hierarchical models) include both fixed effects (factors of primary interest) and random effects (sources of random variation).\nWhen to use mixed effects models: - Repeated measures or longitudinal data - Nested or hierarchical data structures - Multiple sources of random variation - Unbalanced designs or missing data - Continuous covariates combined with grouping factors\nFixed vs Random Effects:\n\nFixed effects: Specific levels you chose (e.g., specific drug treatments, particular temperatures)\nRandom effects: Levels sampled from a larger population (e.g., subjects, batches, field sites)\n\nThe lme4 package in R provides powerful tools for mixed effects models:\n\n\nCode\n# Example: Growth curve with random intercepts and slopes\nset.seed(321)\nn_individuals &lt;- 15\nn_timepoints &lt;- 6\n\ngrowth_data &lt;- expand.grid(\n  individual = factor(1:n_individuals),\n  time = 0:(n_timepoints-1)\n) %&gt;%\n  mutate(\n    # Random intercept for each individual\n    intercept = rep(rnorm(n_individuals, 5, 1), each = n_timepoints),\n    # Random slope for each individual\n    slope = rep(rnorm(n_individuals, 0.8, 0.15), each = n_timepoints),\n    # Generate size with individual variation\n    size = intercept + slope * time + rnorm(n(), 0, 0.3)\n  )\n\n# Fit mixed effects model with random intercepts and slopes\nmixed_model &lt;- lmer(size ~ time + (time|individual), data = growth_data)\nsummary(mixed_model)\n\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: size ~ time + (time | individual)\n   Data: growth_data\n\nREML criterion at convergence: 233.6\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-1.8644 -0.6987 -0.1743  0.6597  2.0678 \n\nRandom effects:\n Groups     Name        Variance Std.Dev. Corr \n individual (Intercept) 0.10232  0.3199        \n            time        0.01272  0.1128   -0.13\n Residual               0.61532  0.7844        \nNumber of obs: 90, groups:  individual, 15\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept)   5.2214     0.1683   31.03\ntime          0.7760     0.0565   13.73\n\nCorrelation of Fixed Effects:\n     (Intr)\ntime -0.650\n\n\nCode\n# Compare to model with only random intercepts\nmixed_model_int &lt;- lmer(size ~ time + (1|individual), data = growth_data)\n\n# Likelihood ratio test\nanova(mixed_model_int, mixed_model)\n\n\nData: growth_data\nModels:\nmixed_model_int: size ~ time + (1 | individual)\nmixed_model: size ~ time + (time | individual)\n                npar    AIC    BIC  logLik -2*log(L)  Chisq Df Pr(&gt;Chisq)\nmixed_model_int    4 236.62 246.62 -114.31    228.62                     \nmixed_model        6 239.35 254.35 -113.68    227.35 1.2682  2     0.5304\n\n\n\n\nCode\n# Visualize random effects\nggplot(growth_data, aes(x = time, y = size, group = individual)) +\n  geom_line(alpha = 0.3, color = \"gray40\") +\n  geom_point(alpha = 0.3, size = 1.5) +\n  geom_abline(intercept = fixef(mixed_model)[1],\n              slope = fixef(mixed_model)[2],\n              color = \"red\", linewidth = 1.5) +\n  labs(title = \"Mixed Effects Model: Growth Curves\",\n       subtitle = \"Gray lines show individuals; red line shows population average (fixed effect)\",\n       x = \"Time\",\n       y = \"Size\") +\n  theme_minimal(base_size = 12)\n\n\n\n\n\n\n\n\nFigure 27.9: Mixed effects model showing fixed effect (population trend) and random effects (individual variation)",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Multi-Factor ANOVA</span>"
    ]
  },
  {
    "objectID": "chapters/20-multifactor-anova.html#split-plot-designs",
    "href": "chapters/20-multifactor-anova.html#split-plot-designs",
    "title": "27  Multi-Factor ANOVA",
    "section": "27.9 Split-Plot Designs",
    "text": "27.9 Split-Plot Designs\nSplit-plot designs arise when different factors are applied at different scales or when complete randomization is impractical. Originally developed for agricultural field trials, they’re common in biological experiments.\nStructure: - Whole plots: Larger experimental units receiving one factor (hard-to-change factor) - Split plots (or subplots): Smaller units within whole plots receiving another factor (easy-to-change factor)\nExample: Testing the effect of irrigation (whole plot factor) and fertilizer (split-plot factor) on crop yield. Irrigation is applied to large field plots, while different fertilizers can be applied to smaller areas within each irrigation plot.\nIn laboratory settings: - Whole plot: Temperature chambers (hard to randomize, limited number) - Split plot: Different nutrient treatments within each chamber (easy to randomize)\n\n\nCode\n# Simulated split-plot design\n# Whole plot: Temperature (2 levels, 4 replicates each = 8 chambers)\n# Split plot: Nutrient (3 levels per chamber)\nset.seed(654)\n\nn_chambers_per_temp &lt;- 4\nn_nutrients &lt;- 3\nn_reps &lt;- 2  # technical replicates within each split plot\n\nsplit_plot_data &lt;- expand.grid(\n  temp = factor(rep(c(\"Low\", \"High\"), each = n_chambers_per_temp)),\n  chamber = factor(1:(2 * n_chambers_per_temp)),\n  nutrient = factor(paste0(\"N\", 1:n_nutrients)),\n  rep = 1:n_reps\n) %&gt;%\n  mutate(\n    temp_effect = ifelse(temp == \"Low\", 0, 5),\n    # Chambers vary even at same temperature (whole plot error)\n    chamber_effect = rep(rnorm(2 * n_chambers_per_temp, 0, 1.5),\n                        each = n_nutrients * n_reps),\n    nutrient_effect = rep(c(0, 3, 6), times = 2 * n_chambers_per_temp * n_reps),\n    # Interaction between temp and nutrient\n    interaction = ifelse(temp == \"High\" & nutrient == \"N3\", 3, 0),\n    yield = 10 + temp_effect + chamber_effect + nutrient_effect +\n            interaction + rnorm(n(), 0, 1)\n  )\n\n\nError in `mutate()`:\nℹ In argument: `chamber_effect = rep(...)`.\nCaused by error:\n! `chamber_effect` must be size 384 or 1, not 48.\n\n\nCode\n# Visualize split-plot structure\nggplot(split_plot_data, aes(x = nutrient, y = yield, color = temp)) +\n  geom_point(alpha = 0.4, position = position_jitterdodge(jitter.width = 0.2, dodge.width = 0.5)) +\n  stat_summary(fun = mean, geom = \"point\", size = 4,\n               position = position_dodge(width = 0.5)) +\n  stat_summary(fun.data = mean_se, geom = \"errorbar\", width = 0.2,\n               position = position_dodge(width = 0.5)) +\n  facet_wrap(~ temp) +\n  labs(title = \"Split-Plot Design: Temperature (whole plot) × Nutrient (split plot)\",\n       subtitle = \"Temperature applied to whole chambers; nutrients varied within chambers\",\n       x = \"Nutrient Level (Split-Plot Factor)\",\n       y = \"Yield\",\n       color = \"Temperature\\n(Whole-Plot)\") +\n  theme_minimal(base_size = 11)\n\n\nError:\n! object 'split_plot_data' not found\n\n\n\n\nCode\n# Analysis of split-plot design\n# Whole plot factor: temperature (tested against chamber error)\n# Split plot factor: nutrient (tested against split-plot error)\n# Use mixed model with chamber as random effect\n\nlibrary(lme4)\nsplit_plot_model &lt;- lmer(yield ~ temp * nutrient + (1|chamber),\n                         data = split_plot_data)\n\n\nError:\n! bad 'data': object 'split_plot_data' not found\n\n\nCode\nsummary(split_plot_model)\n\n\nError in `h()`:\n! error in evaluating the argument 'object' in selecting a method for function 'summary': object 'split_plot_model' not found\n\n\nCode\nanova(split_plot_model)\n\n\nError:\n! object 'split_plot_model' not found\n\n\nCode\n# Note: The whole-plot factor (temp) has fewer degrees of freedom\n# because it varies only among chambers, not within them\n\n\nKey features of split-plot analysis: - Different error terms for whole-plot and split-plot factors - Whole-plot tests have fewer degrees of freedom (less powerful) - Split-plot tests are more powerful (more replication) - Interactions use split-plot error term",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Multi-Factor ANOVA</span>"
    ]
  },
  {
    "objectID": "chapters/20-multifactor-anova.html#practical-considerations-for-complex-designs",
    "href": "chapters/20-multifactor-anova.html#practical-considerations-for-complex-designs",
    "title": "27  Multi-Factor ANOVA",
    "section": "27.10 Practical Considerations for Complex Designs",
    "text": "27.10 Practical Considerations for Complex Designs\n\nSample Size and Power\nComplex designs require careful power analysis. The power for detecting: - Main effects depends on the number of replicates and effect size - Interactions is typically lower than for main effects—interactions require larger samples\nUse simulation-based power analysis for complex designs where analytical solutions are difficult.\n\n\nBalanced vs Unbalanced Designs\nBalanced designs (equal sample sizes in all cells) are preferable: - Simpler interpretation - Greater statistical power - Robust to assumption violations - Easier to test for interactions\nUnbalanced designs complicate analysis: - Type I, II, and III sums of squares give different results - Main effects and interactions are correlated - Reduced power for some effects\nWhen unavoidable, use Type III sums of squares with caution and consider mixed effects models.\n\n\nEffect Sizes in Multi-Factor ANOVA\nReport effect sizes for all significant effects:\nPartial eta-squared (\\(\\eta^2_p\\)): Proportion of variance explained by each effect, removing variance from other effects:\n\\[\\eta^2_p = \\frac{SS_{effect}}{SS_{effect} + SS_{error}}\\]\n\n\nCode\n# Calculate effect sizes from factorial ANOVA\nlibrary(effectsize)\neta_squared(factorial_aov, partial = TRUE)\n\n\n# Effect Size for ANOVA (Type I)\n\nParameter            | Eta2 (partial) |       95% CI\n----------------------------------------------------\ntemperature          |           0.80 | [0.73, 1.00]\nnutrient             |           0.86 | [0.80, 1.00]\ntemperature:nutrient |           0.39 | [0.23, 1.00]\n\n- One-sided CIs: upper bound fixed at [1.00].\n\n\n\n\nMultiple Comparisons\nWith many factor levels and combinations, multiple comparison corrections become crucial:\n\n\nCode\n# Post-hoc comparisons for factorial design\nlibrary(emmeans)\nfactorial_emm &lt;- emmeans(factorial_aov, ~ temperature * nutrient)\n\n# All pairwise comparisons\npairs(factorial_emm, adjust = \"tukey\")\n\n\n contrast             estimate    SE df t.ratio p.value\n Low Low - High Low      -3.21 0.499 56  -6.438 &lt;0.0001\n Low Low - Low High      -4.40 0.499 56  -8.816 &lt;0.0001\n Low Low - High High    -11.86 0.499 56 -23.761 &lt;0.0001\n High Low - Low High     -1.19 0.499 56  -2.378  0.0932\n High Low - High High    -8.65 0.499 56 -17.323 &lt;0.0001\n Low High - High High    -7.46 0.499 56 -14.946 &lt;0.0001\n\nP value adjustment: tukey method for comparing a family of 4 estimates \n\n\nCode\n# Simple effects: nutrient effect at each temperature\nnutrient_simple &lt;- emmeans(factorial_aov, ~ nutrient | temperature)\npairs(nutrient_simple, adjust = \"bonferroni\")\n\n\ntemperature = Low:\n contrast   estimate    SE df t.ratio p.value\n Low - High    -4.40 0.499 56  -8.816 &lt;0.0001\n\ntemperature = High:\n contrast   estimate    SE df t.ratio p.value\n Low - High    -8.65 0.499 56 -17.323 &lt;0.0001\n\n\n\n\nModel Assumptions\nCheck assumptions for all error terms in complex designs:\n\n\nCode\n# Diagnostic plots\npar(mfrow = c(2, 2))\nplot(factorial_aov)\npar(mfrow = c(1, 1))\n\n# Test homogeneity of variance\nlibrary(car)\nleveneTest(growth ~ temperature * nutrient, data = factorial_data)\n\n\nLevene's Test for Homogeneity of Variance (center = median)\n      Df F value Pr(&gt;F)\ngroup  3  0.8227 0.4869\n      56               \n\n\nCode\n# Test normality of residuals\nshapiro.test(residuals(factorial_aov))\n\n\n\n    Shapiro-Wilk normality test\n\ndata:  residuals(factorial_aov)\nW = 0.98365, p-value = 0.6001\n\n\n\n\n\n\n\n\nFigure 27.10: Diagnostic plots for checking ANOVA assumptions\n\n\n\n\n\n\n\nReporting Results\nWhen reporting multi-factor ANOVA results, include:\n\nDescriptive statistics for each cell (means, SDs, sample sizes)\nANOVA table with F-values, degrees of freedom, and p-values for all effects\nEffect sizes (partial eta-squared or omega-squared)\nPost-hoc tests or simple effects when interactions are significant\nDiagnostic plots confirming assumptions (in supplementary materials)\nGraphical display of means with error bars (interaction plots for two-way designs)\n\nExample reporting:\n\nWe conducted a 2 × 2 factorial ANOVA to examine the effects of temperature (low vs. high) and nutrient level (low vs. high) on algal growth rate. There was a significant main effect of temperature, F(1, 56) = 45.2, p &lt; .001, \\(\\eta^2_p\\) = 0.45, and a significant main effect of nutrient level, F(1, 56) = 89.7, p &lt; .001, \\(\\eta^2_p\\) = 0.62. Critically, there was a significant temperature × nutrient interaction, F(1, 56) = 12.3, p &lt; .001, \\(\\eta^2_p\\) = 0.18. Simple effects analysis revealed that the nutrient effect was significant at both temperatures (both p &lt; .001), but the magnitude was greater at high temperature (Δ = 9.1 units) than at low temperature (Δ = 5.2 units).",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Multi-Factor ANOVA</span>"
    ]
  },
  {
    "objectID": "chapters/20-multifactor-anova.html#summary",
    "href": "chapters/20-multifactor-anova.html#summary",
    "title": "27  Multi-Factor ANOVA",
    "section": "27.11 Summary",
    "text": "27.11 Summary\nMulti-factor ANOVA extends the basic ANOVA framework to designs with multiple factors:\n\nFactorial designs test all combinations of factor levels, allowing estimation of main effects and interactions\nInteractions reveal that factors do not operate independently—the effect of one factor depends on another\nInteraction plots with non-parallel lines indicate interactions; interpret simple effects rather than main effects when interactions are significant\nANCOVA combines categorical factors with continuous covariates, controlling for baseline variation and increasing statistical power\nNested designs have hierarchical structure where levels of one factor exist only within another; common in hierarchical sampling\nRepeated measures ANOVA accounts for correlation when the same subjects are measured multiple times\nMixed effects models include both fixed and random effects, providing flexibility for complex data structures\nSplit-plot designs arise when factors are applied at different scales or complete randomization is impractical\nAlways report effect sizes, check assumptions, and use appropriate corrections for multiple comparisons\nBalanced designs are preferable but mixed effects models can handle unbalanced data\n\nUnderstanding when and how to use multi-factor ANOVA is essential for designing efficient experiments and extracting maximum information from biological data. The ability to test for interactions is particularly valuable—discovering that factors interact often leads to deeper biological insights than finding main effects alone.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Multi-Factor ANOVA</span>"
    ]
  },
  {
    "objectID": "chapters/20-multifactor-anova.html#practice-exercises",
    "href": "chapters/20-multifactor-anova.html#practice-exercises",
    "title": "27  Multi-Factor ANOVA",
    "section": "27.12 Practice Exercises",
    "text": "27.12 Practice Exercises\n\nDesign Question: You want to test the effect of three fertilizer types (A, B, C) and two watering regimes (low, high) on plant growth.\n\nHow many treatment combinations are in this design?\nDraw a diagram showing all treatment groups\nIs this a balanced design if you have 10 plants per treatment?\n\nInteraction Interpretation: An experiment tests the effect of drug (present vs. absent) and exercise (yes vs. no) on weight loss. The means are:\n\nNo drug, no exercise: 2 kg lost\nNo drug, exercise: 5 kg lost\nDrug, no exercise: 6 kg lost\nDrug + exercise: 8 kg lost\nSketch an interaction plot\nIs there an interaction? How do you know?\nInterpret the results in practical terms\n\nData Analysis: Using the built-in ToothGrowth dataset in R, which examines the effect of Vitamin C dose (0.5, 1.0, 2.0 mg) and delivery method (orange juice vs. ascorbic acid) on tooth length in guinea pigs:\ndata(ToothGrowth)\n\nConduct a two-way ANOVA\nCreate an interaction plot\nIs there a significant interaction?\nCalculate effect sizes\nWrite a brief results paragraph\n\nNested vs. Crossed: For each scenario, identify whether the design is nested or crossed:\n\n\nStudents within schools, testing two teaching methods across all schools\n\n\nPatients within clinics, with each clinic using all three drug treatments\n\n\nLeaves within trees, where trees are sampled from three different forests\n\n\nTwo temperature conditions and three light conditions, all combinations tested\n\n\nRepeated Measures: Design a repeated measures experiment to test whether a new drug reduces anxiety over time.\n\nSpecify your factors and levels\nHow many participants would you need for 80% power?\nWhat would your repeated measure be?\nWhat assumptions must you check?\n\nSample Size: In a 3 × 2 factorial design:\n\nHow many treatment combinations exist?\nIf you want 20 observations per cell, how many total observations do you need?\nWhich effect (main or interaction) typically requires larger sample sizes to detect?\n\nMixed Effects Application: You measure bacterial growth in 20 Petri dishes, with 5 measurements per dish across four timepoints.\n\nWhat is the fixed effect?\nWhat is the random effect?\nWrite the lmer() model formula\nWhy is a mixed effects model appropriate here?\n\nPractical Design: You have access to 4 growth chambers and want to test 3 light conditions and 2 CO₂ levels.\n\nWhy might a split-plot design be necessary?\nWhich factor should be the whole-plot factor? Why?\nSketch the design layout\nWhat are the implications for statistical power?",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Multi-Factor ANOVA</span>"
    ]
  },
  {
    "objectID": "chapters/21-glm.html",
    "href": "chapters/21-glm.html",
    "title": "28  Generalized Linear Models",
    "section": "",
    "text": "28.1 Beyond Normal Distributions\nStandard linear regression assumes that the response variable is continuous and normally distributed. But many important response variables violate these assumptions. Binary outcomes (success/failure, alive/dead) follow binomial distributions. Count data (number of events, cells, species) often follow Poisson distributions.\nGeneralized Linear Models (GLMs) extend linear regression to handle these situations. They provide a unified framework for modeling responses that follow different distributions from the exponential family.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Generalized Linear Models</span>"
    ]
  },
  {
    "objectID": "chapters/21-glm.html#frequency-analysis-categorical-response-variables",
    "href": "chapters/21-glm.html#frequency-analysis-categorical-response-variables",
    "title": "28  Generalized Linear Models",
    "section": "28.2 Frequency Analysis: Categorical Response Variables",
    "text": "28.2 Frequency Analysis: Categorical Response Variables\nBefore diving into GLMs, it’s important to understand how we analyze categorical response variables. When observations fall into categories rather than being measured on a continuous scale, we count the frequency in each category and compare observed frequencies to expected values.\n\nChi-Square Goodness of Fit Test\nThe goodness of fit test asks whether observed frequencies match a hypothesized distribution. The classic example comes from Mendelian genetics.\n\n\nCode\n# Mendel's pea experiment - F2 phenotype ratios\n# Expected: 9:3:3:1 for Yellow-Smooth:Yellow-Wrinkled:Green-Smooth:Green-Wrinkled\nobserved &lt;- c(315, 101, 108, 32)  # Mendel's actual data\nexpected_ratios &lt;- c(9/16, 3/16, 3/16, 1/16)\n\n# Perform chi-square test\nchisq.test(observed, p = expected_ratios)\n\n\n\n    Chi-squared test for given probabilities\n\ndata:  observed\nX-squared = 0.47002, df = 3, p-value = 0.9254\n\n\nThe test statistic measures deviation from expected:\n\\[\\chi^2 = \\sum \\frac{(O_i - E_i)^2}{E_i}\\]\nwhere \\(O_i\\) are observed and \\(E_i\\) are expected counts. Under the null hypothesis (observed frequencies match expected), this follows a chi-square distribution with \\(df = k - 1\\) where \\(k\\) is the number of categories.\n\n\n\n\n\n\nChi-Square Assumptions\n\n\n\n\nIndependence: Observations must be independent\nExpected counts: No more than 20% of expected counts should be &lt; 5\nSample size: Total sample should be reasonably large\n\nCheck expected values before interpreting results:\n\n\nCode\nn &lt;- sum(observed)\nexpected &lt;- n * expected_ratios\ncat(\"Expected counts:\", round(expected, 1))\n\n\nExpected counts: 312.8 104.2 104.2 34.8\n\n\n\n\n\n\nContingency Table Analysis\nWhen we have two categorical variables, we use a contingency table to examine their association. The null hypothesis is that the variables are independent.\n\n\nCode\n# Example: Hair color and eye color association\n# Data from 1000 students\nhair_eye &lt;- matrix(c(347, 191,    # Blue eyes: blonde, brunette\n                     177, 329),   # Brown eyes: blonde, brunette\n                   nrow = 2, byrow = TRUE)\nrownames(hair_eye) &lt;- c(\"Blue_eyes\", \"Brown_eyes\")\ncolnames(hair_eye) &lt;- c(\"Blonde\", \"Brunette\")\n\n# View the contingency table\nhair_eye\n\n\n           Blonde Brunette\nBlue_eyes     347      191\nBrown_eyes    177      329\n\n\nCode\n# Chi-square test of independence\nchisq.test(hair_eye)\n\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  hair_eye\nX-squared = 89.703, df = 1, p-value &lt; 2.2e-16\n\n\nFor contingency tables, \\(df = (r-1)(c-1)\\) where \\(r\\) and \\(c\\) are the number of rows and columns.\n\n\nCode\n# Visualize with a mosaic plot\nmosaicplot(hair_eye, main = \"Hair and Eye Color Association\",\n           color = c(\"gold\", \"brown\"), shade = FALSE)\n\n\n\n\n\n\n\n\nFigure 28.1: Mosaic plot showing the association between hair color and eye color\n\n\n\n\n\n\n\nStandardized Residuals\nTo understand where associations are strongest, examine standardized residuals:\n\n\nCode\n# Which cells deviate most from independence?\ntest_result &lt;- chisq.test(hair_eye)\ntest_result$residuals\n\n\n              Blonde  Brunette\nBlue_eyes   4.683940 -4.701920\nBrown_eyes -4.829778  4.848318\n\n\nResiduals &gt; 2 or &lt; -2 indicate cells contributing substantially to the association. Positive residuals mean more observations than expected under independence; negative means fewer.\n\n\nG-Test (Log-Likelihood Ratio Test)\nThe G-test is an alternative to chi-square based on likelihood ratios:\n\\[G = 2 \\sum O_i \\ln\\left(\\frac{O_i}{E_i}\\right)\\]\nThe G-test and chi-square give similar results for large samples, but G-tests are preferred when: - Sample sizes are small - Differences between observed and expected are small - You want to decompose complex tables\n\n\nCode\n# Manual G-test calculation\nobserved_flat &lt;- as.vector(hair_eye)\nexpected_flat &lt;- as.vector(test_result$expected)\nG &lt;- 2 * sum(observed_flat * log(observed_flat / expected_flat))\np_value &lt;- 1 - pchisq(G, df = 1)\n\ncat(\"G statistic:\", round(G, 3), \"\\n\")\n\n\nG statistic: 92.248 \n\n\nCode\ncat(\"p-value:\", format(p_value, scientific = TRUE), \"\\n\")\n\n\np-value: 0e+00 \n\n\n\n\nOdds Ratios: Measuring Effect Size\nThe chi-square test tells us whether variables are associated, but not the strength of association. For 2×2 tables, the odds ratio quantifies effect size.\nThe odds of an event are \\(\\frac{p}{1-p}\\). The odds ratio compares odds between groups:\n\\[OR = \\frac{odds_1}{odds_2} = \\frac{a/b}{c/d} = \\frac{ad}{bc}\\]\n\n\nCode\n# Odds ratio for hair/eye color data\na &lt;- hair_eye[1,1]  # Blue eyes, Blonde\nb &lt;- hair_eye[1,2]  # Blue eyes, Brunette\nc &lt;- hair_eye[2,1]  # Brown eyes, Blonde\nd &lt;- hair_eye[2,2]  # Brown eyes, Brunette\n\nodds_ratio &lt;- (a * d) / (b * c)\ncat(\"Odds Ratio:\", round(odds_ratio, 2), \"\\n\")\n\n\nOdds Ratio: 3.38 \n\n\nAn OR of 3.38 means blue-eyed individuals have about 3.4 times the odds of being blonde compared to brown-eyed individuals.\n\nOR = 1: No association\nOR &gt; 1: Positive association\nOR &lt; 1: Negative association\n\n\n\nCode\n# Confidence interval for odds ratio (using log transform)\nlog_OR &lt;- log(odds_ratio)\nse_log_OR &lt;- sqrt(1/a + 1/b + 1/c + 1/d)\nci_log &lt;- log_OR + c(-1.96, 1.96) * se_log_OR\nci_OR &lt;- exp(ci_log)\n\ncat(\"95% CI for OR: [\", round(ci_OR[1], 2), \",\", round(ci_OR[2], 2), \"]\\n\")\n\n\n95% CI for OR: [ 2.62 , 4.35 ]\n\n\n\n\nFisher’s Exact Test\nFor small sample sizes (expected counts &lt; 5), Fisher’s exact test is more appropriate. It calculates exact probabilities rather than relying on the chi-square approximation.\n\n\nCode\n# Small sample example\nsmall_table &lt;- matrix(c(3, 1, 1, 3), nrow = 2)\nfisher.test(small_table)\n\n\n\n    Fisher's Exact Test for Count Data\n\ndata:  small_table\np-value = 0.4857\nalternative hypothesis: true odds ratio is not equal to 1\n95 percent confidence interval:\n   0.2117329 621.9337505\nsample estimates:\nodds ratio \n  6.408309 \n\n\nFisher’s test is preferred for 2×2 tables with small samples and provides a confidence interval for the odds ratio.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Generalized Linear Models</span>"
    ]
  },
  {
    "objectID": "chapters/21-glm.html#components-of-a-glm",
    "href": "chapters/21-glm.html#components-of-a-glm",
    "title": "28  Generalized Linear Models",
    "section": "28.3 Components of a GLM",
    "text": "28.3 Components of a GLM\nGLMs have three components:\nRandom component: Specifies the probability distribution of the response variable (e.g., binomial, Poisson, normal).\nSystematic component: The linear predictor, a linear combination of explanatory variables: \\[\\eta = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\ldots\\]\nLink function: Connects the random and systematic components, transforming the expected value of the response to the scale of the linear predictor.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Generalized Linear Models</span>"
    ]
  },
  {
    "objectID": "chapters/21-glm.html#the-link-function",
    "href": "chapters/21-glm.html#the-link-function",
    "title": "28  Generalized Linear Models",
    "section": "28.4 The Link Function",
    "text": "28.4 The Link Function\nDifferent distributions use different link functions:\n\n\n\nDistribution\nTypical Link\nLink Function\n\n\n\n\nNormal\nIdentity\n\\(\\eta = \\mu\\)\n\n\nBinomial\nLogit\n\\(\\eta = \\log(\\frac{\\mu}{1-\\mu})\\)\n\n\nPoisson\nLog\n\\(\\eta = \\log(\\mu)\\)",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Generalized Linear Models</span>"
    ]
  },
  {
    "objectID": "chapters/21-glm.html#logistic-regression",
    "href": "chapters/21-glm.html#logistic-regression",
    "title": "28  Generalized Linear Models",
    "section": "28.5 Logistic Regression",
    "text": "28.5 Logistic Regression\nLogistic regression models binary outcomes. The response is 0 or 1 (failure or success), and we model the probability of success as a function of predictors.\nThe logistic function maps the linear predictor to probabilities:\n\\[P(Y = 1 | X) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 X)}}\\]\nEquivalently, we model the log-odds:\n\\[\\log\\left(\\frac{P}{1-P}\\right) = \\beta_0 + \\beta_1 X\\]\n\n\nCode\n# The logistic function\ncurve(1 / (1 + exp(-x)), from = -6, to = 6, \n      xlab = \"Linear Predictor\", ylab = \"Probability\",\n      main = \"The Logistic Function\", lwd = 2, col = \"blue\")\n\n\n\n\n\n\n\n\nFigure 28.2: The logistic function mapping the linear predictor to probabilities between 0 and 1",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Generalized Linear Models</span>"
    ]
  },
  {
    "objectID": "chapters/21-glm.html#fitting-logistic-regression",
    "href": "chapters/21-glm.html#fitting-logistic-regression",
    "title": "28  Generalized Linear Models",
    "section": "28.6 Fitting Logistic Regression",
    "text": "28.6 Fitting Logistic Regression\n\n\nCode\n# Example: predicting transmission type from mpg\ndata(mtcars)\nmtcars$am &lt;- factor(mtcars$am, labels = c(\"automatic\", \"manual\"))\n\nlogit_model &lt;- glm(am ~ mpg, data = mtcars, family = binomial)\nsummary(logit_model)\n\n\n\nCall:\nglm(formula = am ~ mpg, family = binomial, data = mtcars)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)   \n(Intercept)  -6.6035     2.3514  -2.808  0.00498 **\nmpg           0.3070     0.1148   2.673  0.00751 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 43.230  on 31  degrees of freedom\nResidual deviance: 29.675  on 30  degrees of freedom\nAIC: 33.675\n\nNumber of Fisher Scoring iterations: 5",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Generalized Linear Models</span>"
    ]
  },
  {
    "objectID": "chapters/21-glm.html#interpreting-logistic-coefficients",
    "href": "chapters/21-glm.html#interpreting-logistic-coefficients",
    "title": "28  Generalized Linear Models",
    "section": "28.7 Interpreting Logistic Coefficients",
    "text": "28.7 Interpreting Logistic Coefficients\nCoefficients are on the log-odds scale. To interpret them:\nExponentiate to get odds ratios:\n\n\nCode\nexp(coef(logit_model))\n\n\n(Intercept)         mpg \n0.001355579 1.359379288 \n\n\nThe odds ratio for mpg (1.36) means that each additional mpg is associated with 36% higher odds of having a manual transmission.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Generalized Linear Models</span>"
    ]
  },
  {
    "objectID": "chapters/21-glm.html#making-predictions",
    "href": "chapters/21-glm.html#making-predictions",
    "title": "28  Generalized Linear Models",
    "section": "28.8 Making Predictions",
    "text": "28.8 Making Predictions\n\n\nCode\n# Predict probability for specific mpg values\nnew_data &lt;- data.frame(mpg = c(15, 20, 25, 30))\npredict(logit_model, newdata = new_data, type = \"response\")\n\n\n        1         2         3         4 \n0.1194021 0.3862832 0.7450109 0.9313311 \n\n\nThe type = \"response\" argument returns probabilities rather than log-odds.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Generalized Linear Models</span>"
    ]
  },
  {
    "objectID": "chapters/21-glm.html#multiple-logistic-regression",
    "href": "chapters/21-glm.html#multiple-logistic-regression",
    "title": "28  Generalized Linear Models",
    "section": "28.9 Multiple Logistic Regression",
    "text": "28.9 Multiple Logistic Regression\nLike linear regression, logistic regression can include multiple predictors. This allows us to:\n\nControl for confounding variables\nExamine how multiple factors together predict the outcome\nTest for interactions between predictors\n\n\n\nCode\n# Multiple logistic regression: am ~ mpg + wt + hp\nmulti_logit &lt;- glm(am ~ mpg + wt + hp, data = mtcars, family = binomial)\nsummary(multi_logit)\n\n\n\nCall:\nglm(formula = am ~ mpg + wt + hp, family = binomial, data = mtcars)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)  \n(Intercept) -15.72137   40.00281  -0.393   0.6943  \nmpg           1.22930    1.58109   0.778   0.4369  \nwt           -6.95492    3.35297  -2.074   0.0381 *\nhp            0.08389    0.08228   1.020   0.3079  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 43.2297  on 31  degrees of freedom\nResidual deviance:  8.7661  on 28  degrees of freedom\nAIC: 16.766\n\nNumber of Fisher Scoring iterations: 10\n\n\n\n\nCode\n# Odds ratios for all predictors\nexp(coef(multi_logit))\n\n\n (Intercept)          mpg           wt           hp \n1.486947e-07 3.418843e+00 9.539266e-04 1.087513e+00 \n\n\nNotice how coefficients change compared to the simple model—this is the effect of controlling for other variables.\n\n\nCode\n# Compare models with likelihood ratio test\nanova(logit_model, multi_logit, test = \"Chisq\")\n\n\nAnalysis of Deviance Table\n\nModel 1: am ~ mpg\nModel 2: am ~ mpg + wt + hp\n  Resid. Df Resid. Dev Df Deviance  Pr(&gt;Chi)    \n1        30    29.6752                          \n2        28     8.7661  2   20.909 2.882e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe likelihood ratio test compares nested models. A significant p-value indicates the fuller model fits significantly better.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Generalized Linear Models</span>"
    ]
  },
  {
    "objectID": "chapters/21-glm.html#poisson-regression",
    "href": "chapters/21-glm.html#poisson-regression",
    "title": "28  Generalized Linear Models",
    "section": "28.10 Poisson Regression",
    "text": "28.10 Poisson Regression\nPoisson regression models count data—the number of events in a fixed period or area. The response must be non-negative integers, and we assume events occur independently at a constant rate.\n\\[\\log(\\mu) = \\beta_0 + \\beta_1 X\\]\n\n\nCode\n# Example: modeling count data\nset.seed(42)\nexposure &lt;- runif(100, 1, 10)\ncounts &lt;- rpois(100, lambda = exp(0.5 + 0.3 * exposure))\n\npois_model &lt;- glm(counts ~ exposure, family = poisson)\nsummary(pois_model)\n\n\n\nCall:\nglm(formula = counts ~ exposure, family = poisson)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  0.42499    0.10624    4.00 6.32e-05 ***\nexposure     0.30714    0.01345   22.84  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 744.110  on 99  degrees of freedom\nResidual deviance:  97.826  on 98  degrees of freedom\nAIC: 498.52\n\nNumber of Fisher Scoring iterations: 4",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Generalized Linear Models</span>"
    ]
  },
  {
    "objectID": "chapters/21-glm.html#overdispersion",
    "href": "chapters/21-glm.html#overdispersion",
    "title": "28  Generalized Linear Models",
    "section": "28.11 Overdispersion",
    "text": "28.11 Overdispersion\nA key assumption of Poisson regression is that the mean equals the variance. When variance exceeds the mean (overdispersion), standard errors are underestimated and p-values become too small.\n\n\nCode\n# Check for overdispersion\n# Ratio of residual deviance to df should be near 1\ndispersion_ratio &lt;- pois_model$deviance / pois_model$df.residual\ncat(\"Dispersion ratio:\", round(dispersion_ratio, 3), \"\\n\")\n\n\nDispersion ratio: 0.998 \n\n\nCode\ncat(\"Values &gt; 1.5 suggest overdispersion\\n\")\n\n\nValues &gt; 1.5 suggest overdispersion\n\n\n\nHandling Overdispersion\nQuasi-Poisson estimates the dispersion parameter from the data rather than assuming it equals 1:\n\n\nCode\n# Create overdispersed data for demonstration\nset.seed(42)\nn &lt;- 100\nx &lt;- runif(n, 1, 10)\n# Generate overdispersed counts (negative binomial acts like overdispersed Poisson)\ny_overdispersed &lt;- rnbinom(n, size = 2, mu = exp(0.5 + 0.3 * x))\n\n# Standard Poisson (ignores overdispersion)\npois_fit &lt;- glm(y_overdispersed ~ x, family = poisson)\n\n# Quasi-Poisson (accounts for overdispersion)\nquasi_fit &lt;- glm(y_overdispersed ~ x, family = quasipoisson)\n\n# Compare standard errors\ncat(\"Poisson SE:\", round(summary(pois_fit)$coefficients[2, 2], 4), \"\\n\")\n\n\nPoisson SE: 0.0129 \n\n\nCode\ncat(\"Quasi-Poisson SE:\", round(summary(quasi_fit)$coefficients[2, 2], 4), \"\\n\")\n\n\nQuasi-Poisson SE: 0.0327 \n\n\nCode\ncat(\"SE inflation factor:\", round(summary(quasi_fit)$coefficients[2, 2] /\n                                   summary(pois_fit)$coefficients[2, 2], 2), \"\\n\")\n\n\nSE inflation factor: 2.53 \n\n\nSimilarly, quasibinomial handles overdispersion in binomial data:\n\n\nCode\n# Quasibinomial example\n# family = quasibinomial adjusts for extra-binomial variation\n\n\n\n\n\n\n\n\nWhen to Use Quasi-Likelihood\n\n\n\nUse quasipoisson or quasibinomial when:\n\nDispersion ratio is substantially &gt; 1 (overdispersion)\nYou don’t need AIC for model comparison (quasi-models don’t have AIC)\nThe basic model structure is correct but variance assumptions are violated\n\nFor severe overdispersion, consider negative binomial regression (package MASS) which models overdispersion explicitly.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Generalized Linear Models</span>"
    ]
  },
  {
    "objectID": "chapters/21-glm.html#model-assessment",
    "href": "chapters/21-glm.html#model-assessment",
    "title": "28  Generalized Linear Models",
    "section": "28.12 Model Assessment",
    "text": "28.12 Model Assessment\nGLMs use deviance rather than R² to assess fit. Deviance compares the fitted model to a saturated model (one parameter per observation).\nNull deviance: Deviance with only the intercept Residual deviance: Deviance of the fitted model\nA large drop from null to residual deviance indicates the predictors explain substantial variation.\n\n\nCode\n# Compare deviances\nwith(logit_model, null.deviance - deviance)\n\n\n[1] 13.55457\n\n\nCode\n# Chi-square test for improvement\nwith(logit_model, pchisq(null.deviance - deviance, \n                         df.null - df.residual, \n                         lower.tail = FALSE))\n\n\n[1] 0.0002317271",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Generalized Linear Models</span>"
    ]
  },
  {
    "objectID": "chapters/21-glm.html#model-comparison-with-aic",
    "href": "chapters/21-glm.html#model-comparison-with-aic",
    "title": "28  Generalized Linear Models",
    "section": "28.13 Model Comparison with AIC",
    "text": "28.13 Model Comparison with AIC\nAs with linear models, AIC helps compare GLMs:\n\n\nCode\n# Compare models\nmodel1 &lt;- glm(am ~ mpg, data = mtcars, family = binomial)\nmodel2 &lt;- glm(am ~ mpg + wt, data = mtcars, family = binomial)\nmodel3 &lt;- glm(am ~ mpg * wt, data = mtcars, family = binomial)\n\nAIC(model1, model2, model3)\n\n\n       df      AIC\nmodel1  2 33.67517\nmodel2  3 23.18426\nmodel3  4 24.49947",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Generalized Linear Models</span>"
    ]
  },
  {
    "objectID": "chapters/21-glm.html#assumptions-and-diagnostics",
    "href": "chapters/21-glm.html#assumptions-and-diagnostics",
    "title": "28  Generalized Linear Models",
    "section": "28.14 Assumptions and Diagnostics",
    "text": "28.14 Assumptions and Diagnostics\nGLM assumptions include: - Correct specification of the distribution - Correct link function - Independence of observations - No extreme multicollinearity\nDiagnostic tools include: - Residual plots (deviance or Pearson residuals) - Influence measures - Goodness-of-fit tests\n\n\nCode\npar(mfrow = c(1, 2))\nplot(logit_model, which = c(1, 2))\n\n\n\n\n\n\n\n\nFigure 28.3: Diagnostic plots for assessing logistic regression model assumptions",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Generalized Linear Models</span>"
    ]
  },
  {
    "objectID": "chapters/21-glm.html#summary",
    "href": "chapters/21-glm.html#summary",
    "title": "28  Generalized Linear Models",
    "section": "28.15 Summary",
    "text": "28.15 Summary\nGLMs provide a flexible framework for modeling non-normal response variables while maintaining the interpretability of linear models. Logistic regression for binary outcomes and Poisson regression for counts are the most common applications, but the framework extends to other distributions as needed.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Generalized Linear Models</span>"
    ]
  },
  {
    "objectID": "chapters/21-glm.html#exercises",
    "href": "chapters/21-glm.html#exercises",
    "title": "28  Generalized Linear Models",
    "section": "28.16 Exercises",
    "text": "28.16 Exercises\n\n\n\n\n\n\nExercise G.1: Logistic Regression for Binary Outcomes\n\n\n\nYou study the effect of study time on exam success. Students either pass (1) or fail (0), and you record their study hours:\nstudy_hours &lt;- c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12)\npass &lt;- c(0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1)\n\nFit a logistic regression model predicting pass/fail from study hours\nInterpret the coefficient for study hours on the log-odds scale\nCalculate and interpret the odds ratio for a one-hour increase in study time\nPredict the probability of passing for a student who studies 5 hours\nAt what study time does the model predict a 50% probability of passing?\nCreate a visualization showing the fitted logistic curve with the observed data points\n\n\n\nCode\n# Your code here\n\n\n\n\n\n\n\n\n\n\nExercise G.2: Poisson Regression for Count Data\n\n\n\nYou count the number of bacterial colonies on petri dishes as a function of antibiotic concentration:\nconcentration &lt;- c(0, 0.5, 1.0, 1.5, 2.0, 2.5, 3.0)\ncolonies &lt;- c(142, 118, 89, 67, 45, 32, 21)\n\nFit a Poisson regression model\nInterpret the coefficient for concentration\nCalculate the expected number of colonies at concentration = 1.5\nCheck for overdispersion by comparing residual deviance to degrees of freedom\nIf overdispersed, refit using quasipoisson and compare standard errors\nCreate a plot showing observed counts and model predictions\n\n\n\nCode\n# Your code here\n\n\n\n\n\n\n\n\n\n\nExercise G.3: Model Comparison and Selection\n\n\n\nUsing the mtcars dataset, predict whether a car has an automatic transmission (am: 0 = automatic, 1 = manual) from various predictors:\n\nFit three logistic regression models:\n\nModel 1: am ~ mpg\nModel 2: am ~ mpg + wt\nModel 3: am ~ mpg + wt + hp\n\nCompare models using AIC\nPerform likelihood ratio tests to compare nested models\nWhich model would you choose and why?\nFor your chosen model, interpret the coefficients in terms of odds ratios\nUse the model to predict transmission type for a car with mpg=20, wt=3.0, hp=150\n\n\n\nCode\n# Your code here\n\n\n\n\n\n\n\n\n\n\nExercise G.4: Chi-Square and Contingency Tables\n\n\n\nA genetics study crosses two heterozygous plants and observes offspring phenotypes. Expected ratio is 9:3:3:1.\nobserved &lt;- c(293, 105, 98, 32)  # Four phenotype categories\n\nPerform a chi-square goodness-of-fit test\nCalculate the expected counts and verify the chi-square assumption is met\nCalculate the contribution of each category to the total chi-square statistic\nVisualize observed vs. expected with a barplot\nDo the data support the Mendelian hypothesis?\n\nNow consider a 2×2 contingency table for treatment outcome by gender:\n\n\n\n\nSuccess\nFailure\n\n\n\n\nMale\n45\n25\n\n\nFemale\n60\n20\n\n\n\n\nTest for independence using chi-square\nCalculate and interpret the odds ratio\nUse Fisher’s exact test and compare to the chi-square result\n\n\n\nCode\n# Your code here\n\n\n\n\n\n\n\n\n\n\nExercise G.5: Handling Overdispersion\n\n\n\nSimulate overdispersed count data and explore the consequences:\nset.seed(123)\nx &lt;- seq(1, 10, length.out = 50)\n# Negative binomial creates overdispersion\ny &lt;- rnbinom(50, size = 2, mu = exp(1 + 0.3 * x))\n\nFit both a Poisson and quasi-Poisson model\nCalculate the dispersion parameter\nCompare the standard errors between the two models\nHow does ignoring overdispersion affect inference?\nFit a negative binomial model using MASS::glm.nb() and compare to the quasi-Poisson approach\nWhich model would you prefer for these data and why?\n\n\n\nCode\n# Your code here\nlibrary(MASS)  # for glm.nb()",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Generalized Linear Models</span>"
    ]
  },
  {
    "objectID": "chapters/22-statistical-learning.html",
    "href": "chapters/22-statistical-learning.html",
    "title": "29  Core Concepts in Statistical Learning",
    "section": "",
    "text": "29.1 From Inference to Prediction\nTraditional statistics emphasizes inference—understanding relationships, testing hypotheses, and quantifying uncertainty. Statistical learning (or machine learning) shifts focus toward prediction—building models that accurately predict outcomes for new data (Hastie, Tibshirani, and Friedman 2009).\nBoth approaches use similar mathematical tools, but the goals differ. In inference, we want to understand the true relationship between variables. In prediction, we want accurate predictions, even if the model does not perfectly capture the underlying mechanism.",
    "crumbs": [
      "Statistical Learning",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Core Concepts in Statistical Learning</span>"
    ]
  },
  {
    "objectID": "chapters/22-statistical-learning.html#the-overfitting-problem",
    "href": "chapters/22-statistical-learning.html#the-overfitting-problem",
    "title": "29  Core Concepts in Statistical Learning",
    "section": "29.2 The Overfitting Problem",
    "text": "29.2 The Overfitting Problem\nModels are built to fit training data as closely as possible. A linear regression minimizes squared errors; a logistic regression maximizes likelihood. But models that fit training data too well often predict poorly on new data.\nOverfitting occurs when a model captures noise specific to the training data rather than the true underlying pattern. Complex models with many parameters are especially susceptible.\nThe solution is to evaluate models on data they have not seen—held-out test data or through cross-validation.",
    "crumbs": [
      "Statistical Learning",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Core Concepts in Statistical Learning</span>"
    ]
  },
  {
    "objectID": "chapters/22-statistical-learning.html#loss-functions-quantifying-prediction-error",
    "href": "chapters/22-statistical-learning.html#loss-functions-quantifying-prediction-error",
    "title": "29  Core Concepts in Statistical Learning",
    "section": "29.3 Loss Functions: Quantifying Prediction Error",
    "text": "29.3 Loss Functions: Quantifying Prediction Error\nA loss function (or cost function) measures how wrong a prediction is. It quantifies the penalty for predicting \\(\\hat{y}\\) when the true value is \\(y\\).\n\nCommon Loss Functions for Regression\nSquared Error Loss (L2): The most common loss for continuous outcomes: \\[L(y, \\hat{y}) = (y - \\hat{y})^2\\]\nSquaring penalizes large errors more heavily than small ones. Linear regression minimizes the sum of squared errors (SSE or RSS).\nAbsolute Error Loss (L1): Less sensitive to outliers: \\[L(y, \\hat{y}) = |y - \\hat{y}|\\]\nMean Squared Error (MSE) and Root Mean Squared Error (RMSE) are averages across all predictions: \\[\\text{MSE} = \\frac{1}{n}\\sum_{i=1}^n (y_i - \\hat{y}_i)^2, \\quad \\text{RMSE} = \\sqrt{\\text{MSE}}\\]\n\n\nCommon Loss Functions for Classification\n0-1 Loss: The simplest classification loss—1 if wrong, 0 if correct: \\[L(y, \\hat{y}) = \\mathbb{I}(y \\neq \\hat{y})\\]\nThe average 0-1 loss is the error rate; one minus the error rate is accuracy.\nLog Loss (Cross-Entropy): Used when we have predicted probabilities \\(\\hat{p}\\): \\[L(y, \\hat{p}) = -[y \\log(\\hat{p}) + (1-y) \\log(1-\\hat{p})]\\]\nLog loss penalizes confident wrong predictions severely—predicting probability 0.99 for the wrong class incurs much larger loss than predicting 0.6.\n\n\nCode\npar(mfrow = c(1, 2))\n\n# Regression loss functions\nerrors &lt;- seq(-3, 3, length.out = 100)\nplot(errors, errors^2, type = \"l\", col = \"blue\", lwd = 2,\n     xlab = \"Prediction Error (y - ŷ)\", ylab = \"Loss\",\n     main = \"Regression Loss Functions\")\nlines(errors, abs(errors), col = \"red\", lwd = 2)\nlegend(\"top\", c(\"Squared (L2)\", \"Absolute (L1)\"),\n       col = c(\"blue\", \"red\"), lwd = 2)\n\n# Classification log loss\np &lt;- seq(0.01, 0.99, length.out = 100)\nplot(p, -log(p), type = \"l\", col = \"blue\", lwd = 2,\n     xlab = \"Predicted Probability for True Class\", ylab = \"Log Loss\",\n     main = \"Classification Log Loss\")\nabline(v = 0.5, lty = 2, col = \"gray\")\n\n\n\n\n\n\n\n\nFigure 29.1: Comparison of squared loss (penalizes large errors heavily) versus absolute loss (more robust to outliers)\n\n\n\n\n\n\n\nWhy Loss Functions Matter\nDifferent loss functions lead to different optimal predictions:\n\nSquared loss → optimal prediction is the mean\nAbsolute loss → optimal prediction is the median\n0-1 loss → optimal prediction is the mode (most frequent class)\n\nThe choice of loss function should reflect how errors affect your application. Medical diagnosis may weight false negatives (missed disease) more heavily than false positives.",
    "crumbs": [
      "Statistical Learning",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Core Concepts in Statistical Learning</span>"
    ]
  },
  {
    "objectID": "chapters/22-statistical-learning.html#cross-validation",
    "href": "chapters/22-statistical-learning.html#cross-validation",
    "title": "29  Core Concepts in Statistical Learning",
    "section": "29.4 Cross-Validation",
    "text": "29.4 Cross-Validation\n\nThe Problem: Training Error vs. Test Error\nA fundamental insight of statistical learning is that training error (how well we fit the data used to build the model) is an overly optimistic estimate of test error (how well we predict new data).\n\n\nCode\n# Demonstrate training vs test error\nset.seed(42)\nn &lt;- 100\nx &lt;- sort(runif(n, 0, 10))\ny_true &lt;- sin(x) + 0.5 * cos(0.5 * x)\ny &lt;- y_true + rnorm(n, sd = 0.3)\n\n# Split into training and test\ntrain_idx &lt;- sample(1:n, 70)\ntrain_data &lt;- data.frame(x = x[train_idx], y = y[train_idx])\ntest_data &lt;- data.frame(x = x[-train_idx], y = y[-train_idx])\n\n# Fit polynomials of increasing degree\nlibrary(splines)\ndegrees &lt;- 1:15\ntrain_error &lt;- test_error &lt;- numeric(length(degrees))\n\nfor (i in seq_along(degrees)) {\n  d &lt;- degrees[i]\n  fit &lt;- lm(y ~ poly(x, d), data = train_data)\n  train_error[i] &lt;- mean((train_data$y - predict(fit, train_data))^2)\n  test_error[i] &lt;- mean((test_data$y - predict(fit, test_data))^2)\n}\n\n# Plot\nplot(degrees, train_error, type = \"b\", pch = 19, col = \"blue\",\n     xlab = \"Model Complexity (Polynomial Degree)\",\n     ylab = \"Mean Squared Error\",\n     main = \"Training vs Test Error\", ylim = c(0, max(test_error)))\nlines(degrees, test_error, type = \"b\", pch = 19, col = \"red\")\nlegend(\"topright\", c(\"Training Error\", \"Test Error\"),\n       col = c(\"blue\", \"red\"), pch = 19, lty = 1)\nabline(v = degrees[which.min(test_error)], lty = 2, col = \"gray\")\n\n\n\n\n\n\n\n\nFigure 29.2: Training error always decreases with model complexity, but test error eventually increases due to overfitting. The optimal model minimizes test error.\n\n\n\n\n\nNotice that training error keeps decreasing as complexity increases, eventually reaching near zero. But test error follows a U-shape—it decreases initially as the model captures true patterns, then increases as the model starts fitting noise.\n\n\nK-Fold Cross-Validation\nCross-validation estimates how well a model will generalize to new data without requiring a separate test set.\nK-fold cross-validation: 1. Split data into k roughly equal parts (folds) 2. For each fold: train on k-1 folds, test on the held-out fold 3. Average performance across all folds\n\n\nCode\n# Simple CV example with linear regression\nlibrary(boot)\n\n# Generate data\nset.seed(42)\nx &lt;- rnorm(100)\ny &lt;- 2 + 3*x + rnorm(100)\ndata &lt;- data.frame(x, y)\n\n# Fit model and perform CV\nmodel &lt;- glm(y ~ x, data = data)\n\n# 10-fold cross-validation\ncv_result &lt;- cv.glm(data, model, K = 10)\ncat(\"CV estimate of prediction error:\", round(cv_result$delta[1], 3), \"\\n\")\n\n\nCV estimate of prediction error: 0.846 \n\n\nLeave-one-out cross-validation (LOOCV) is k-fold with k = n: each observation is held out once. More computationally expensive but lower variance.\n\n\nBootstrap for Error Estimation\nThe bootstrap can also estimate prediction error. The approach:\n\nDraw a bootstrap sample (n observations with replacement)\nFit the model on the bootstrap sample\nEvaluate on observations NOT selected (the “out-of-bag” observations)\nRepeat and average\n\nThis is similar to cross-validation but uses the natural ~37% of observations left out of each bootstrap sample.\n\n\nCode\n# Bootstrap estimate of prediction error\nset.seed(123)\nn_boot &lt;- 100\nboot_errors &lt;- numeric(n_boot)\n\nfor (b in 1:n_boot) {\n  # Bootstrap sample\n  boot_idx &lt;- sample(1:nrow(data), replace = TRUE)\n  oob_idx &lt;- setdiff(1:nrow(data), unique(boot_idx))\n\n  if (length(oob_idx) &gt; 0) {\n    fit &lt;- lm(y ~ x, data = data[boot_idx, ])\n    boot_errors[b] &lt;- mean((data$y[oob_idx] - predict(fit, data[oob_idx, ]))^2)\n  }\n}\n\ncat(\"Bootstrap estimate of prediction error:\", round(mean(boot_errors), 3), \"\\n\")\n\n\nBootstrap estimate of prediction error: 0.867 \n\n\n\n\n\n\n\n\nChoosing a CV Strategy\n\n\n\n\nk = 5 or k = 10: Standard choices that balance bias and variance\nLOOCV (k = n): Low bias but high variance; expensive for large n\nBootstrap: Useful when you also want confidence intervals\nRepeated CV: Run k-fold multiple times with different splits for more stable estimates",
    "crumbs": [
      "Statistical Learning",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Core Concepts in Statistical Learning</span>"
    ]
  },
  {
    "objectID": "chapters/22-statistical-learning.html#bias-variance-tradeoff",
    "href": "chapters/22-statistical-learning.html#bias-variance-tradeoff",
    "title": "29  Core Concepts in Statistical Learning",
    "section": "29.5 Bias-Variance Tradeoff",
    "text": "29.5 Bias-Variance Tradeoff\nPrediction error has two components:\nBias: Error from approximating a complex reality with a simpler model. Simple models have high bias—they may miss important patterns.\nVariance: Error from sensitivity to training data. Complex models have high variance—they change substantially with different training samples.\nThe best predictions come from models that balance bias and variance. As model complexity increases, bias decreases but variance increases. The optimal complexity minimizes total prediction error.",
    "crumbs": [
      "Statistical Learning",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Core Concepts in Statistical Learning</span>"
    ]
  },
  {
    "objectID": "chapters/22-statistical-learning.html#regularization-controlling-model-complexity",
    "href": "chapters/22-statistical-learning.html#regularization-controlling-model-complexity",
    "title": "29  Core Concepts in Statistical Learning",
    "section": "29.6 Regularization: Controlling Model Complexity",
    "text": "29.6 Regularization: Controlling Model Complexity\nRegularization addresses overfitting by adding a penalty term that discourages complex models. This is particularly important when you have many predictors relative to observations, or when predictors are correlated.\n\nThe Regularization Idea\nStandard linear regression minimizes the sum of squared residuals (RSS):\n\\[\\text{RSS} = \\sum_{i=1}^n (y_i - \\beta_0 - \\sum_{j=1}^p \\beta_j x_{ij})^2\\]\nRegularized regression adds a penalty term \\(\\lambda P(\\beta)\\) that shrinks coefficients toward zero:\n\\[\\text{Minimize: } \\text{RSS} + \\lambda P(\\beta)\\]\nThe regularization parameter \\(\\lambda\\) controls the strength of the penalty: - \\(\\lambda = 0\\): No penalty, equivalent to ordinary least squares - \\(\\lambda \\to \\infty\\): Very strong penalty, coefficients shrink toward zero\n\n\nRidge Regression (L2 Penalty)\nRidge regression (Hoerl and Kennard 1970) uses the sum of squared coefficients as the penalty:\n\\[P(\\beta) = \\sum_{j=1}^p \\beta_j^2\\]\nThis shrinks all coefficients toward zero but never exactly to zero. Ridge is particularly effective when predictors are correlated (multicollinearity).\n\n\nCode\nlibrary(glmnet)\n\n# Generate sample data with correlated predictors\nset.seed(42)\nn &lt;- 100\np &lt;- 10\nX &lt;- matrix(rnorm(n * p), n, p)\n# Create correlated predictors\nX[, 2] &lt;- X[, 1] + rnorm(n, sd = 0.5)\nX[, 3] &lt;- X[, 1] + rnorm(n, sd = 0.5)\ntrue_beta &lt;- c(3, -2, 1.5, rep(0, p - 3))\ny &lt;- X %*% true_beta + rnorm(n)\n\n# Fit ridge regression across lambda values\nridge_fit &lt;- glmnet(X, y, alpha = 0)  # alpha = 0 for ridge\n\n# Plot coefficient paths\nplot(ridge_fit, xvar = \"lambda\", main = \"Ridge Regression Coefficients\")\n\n\n\n\n\n\n\n\nFigure 29.3: Ridge regression coefficient paths: as lambda increases, coefficients shrink toward zero but never reach exactly zero\n\n\n\n\n\n\n\nLasso Regression (L1 Penalty)\nLasso (Least Absolute Shrinkage and Selection Operator) (Tibshirani 1996) uses the sum of absolute values as the penalty:\n\\[P(\\beta) = \\sum_{j=1}^p |\\beta_j|\\]\nUnlike ridge, lasso can shrink coefficients exactly to zero, effectively performing variable selection. This produces sparse models that are easier to interpret.\n\n\nCode\n# Fit lasso regression\nlasso_fit &lt;- glmnet(X, y, alpha = 1)  # alpha = 1 for lasso\n\nplot(lasso_fit, xvar = \"lambda\", main = \"Lasso Regression Coefficients\")\n\n\n\n\n\n\n\n\nFigure 29.4: Lasso regression coefficient paths: as lambda increases, coefficients shrink and some become exactly zero (variable selection)\n\n\n\n\n\n\n\nElastic Net: Combining Ridge and Lasso\nElastic net combines both penalties:\n\\[P(\\beta) = \\alpha \\sum_{j=1}^p |\\beta_j| + (1-\\alpha) \\sum_{j=1}^p \\beta_j^2\\]\nThe mixing parameter \\(\\alpha\\) controls the balance: - \\(\\alpha = 0\\): Pure ridge - \\(\\alpha = 1\\): Pure lasso - \\(0 &lt; \\alpha &lt; 1\\): Combination\nElastic net is often preferred when predictors are correlated—it tends to select groups of correlated variables together.\n\n\nChoosing Lambda with Cross-Validation\nThe regularization parameter \\(\\lambda\\) is typically chosen by cross-validation:\n\n\nCode\n# Cross-validation for lasso\nset.seed(123)\ncv_lasso &lt;- cv.glmnet(X, y, alpha = 1)\n\n# Plot cross-validation results\nplot(cv_lasso)\n\n# Optimal lambda values\ncat(\"Lambda with minimum CV error:\", round(cv_lasso$lambda.min, 4), \"\\n\")\n\n\nLambda with minimum CV error: 0.0378 \n\n\nCode\ncat(\"Lambda within 1 SE of minimum:\", round(cv_lasso$lambda.1se, 4), \"\\n\")\n\n\nLambda within 1 SE of minimum: 0.0957 \n\n\n\n\n\n\n\n\nFigure 29.5: Cross-validation to select optimal lambda: the left dashed line marks the minimum error, the right marks the most regularized model within one standard error\n\n\n\n\n\nThe lambda.1se (one standard error rule) often provides a more parsimonious model with nearly as good performance as the minimum.\n\n\nComparing Regularization Methods\n\n\nCode\n# Fit models with optimal lambda\nridge_cv &lt;- cv.glmnet(X, y, alpha = 0)\nlasso_cv &lt;- cv.glmnet(X, y, alpha = 1)\n\n# Extract coefficients\ncoef_ols &lt;- coef(lm(y ~ X))\ncoef_ridge &lt;- coef(ridge_cv, s = \"lambda.1se\")\ncoef_lasso &lt;- coef(lasso_cv, s = \"lambda.1se\")\n\n# Compare (excluding intercept)\ncomparison &lt;- data.frame(\n  True = c(NA, true_beta),\n  OLS = as.vector(coef_ols),\n  Ridge = as.vector(coef_ridge),\n  Lasso = as.vector(coef_lasso)\n)\nrownames(comparison) &lt;- c(\"Intercept\", paste0(\"X\", 1:p))\nround(comparison, 3)\n\n\n          True    OLS  Ridge  Lasso\nIntercept   NA  0.069  0.175  0.121\nX1         3.0  3.214  1.587  2.248\nX2        -2.0 -2.217 -0.764 -1.423\nX3         1.5  1.559  1.499  1.618\nX4         0.0 -0.126 -0.157 -0.053\nX5         0.0 -0.061 -0.009  0.000\nX6         0.0  0.103 -0.009  0.000\nX7         0.0 -0.023 -0.008  0.000\nX8         0.0  0.036  0.009  0.000\nX9         0.0  0.051  0.081  0.000\nX10        0.0 -0.113 -0.127 -0.046\n\n\nNotice that lasso correctly identifies the zero coefficients (variables 4-10), while ridge shrinks them but doesn’t eliminate them.\n\n\n\n\n\n\nWhen to Use Each Method\n\n\n\n\nRidge: When you believe all predictors are relevant and want to handle multicollinearity\nLasso: When you want automatic variable selection and a sparse model\nElastic Net: When predictors are correlated and you want both selection and grouping\n\nImportant: Always standardize predictors before applying regularization, as the penalty treats all coefficients equally. The glmnet function does this automatically by default.",
    "crumbs": [
      "Statistical Learning",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Core Concepts in Statistical Learning</span>"
    ]
  },
  {
    "objectID": "chapters/22-statistical-learning.html#smoothing-from-simple-averages-to-flexible-curves",
    "href": "chapters/22-statistical-learning.html#smoothing-from-simple-averages-to-flexible-curves",
    "title": "29  Core Concepts in Statistical Learning",
    "section": "29.7 Smoothing: From Simple Averages to Flexible Curves",
    "text": "29.7 Smoothing: From Simple Averages to Flexible Curves\nWhen the relationship between a predictor and outcome is non-linear, we need methods more flexible than linear regression. Smoothing methods estimate curves by averaging nearby observations, allowing the data to reveal its own pattern.\n\nBin Smoothing\nThe simplest smoothing approach is bin smoothing (also called binning): divide the predictor into intervals (bins) and estimate the outcome as the average within each bin.\n\n\nCode\n# Generate non-linear data\nset.seed(42)\nn &lt;- 200\nx &lt;- runif(n, 0, 10)\ny &lt;- sin(x) + rnorm(n, sd = 0.3)\n\n# Bin smoothing with different bin widths\npar(mfrow = c(1, 2))\n\n# Narrow bins\nn_bins &lt;- 20\nbreaks &lt;- seq(min(x), max(x), length.out = n_bins + 1)\nbin_means &lt;- sapply(1:n_bins, function(i) {\n  in_bin &lt;- x &gt;= breaks[i] & x &lt; breaks[i + 1]\n  if (sum(in_bin) &gt; 0) mean(y[in_bin]) else NA\n})\nbin_centers &lt;- (breaks[-1] + breaks[-(n_bins + 1)]) / 2\n\nplot(x, y, pch = 16, col = \"gray60\", main = \"Narrow bins (20)\")\npoints(bin_centers, bin_means, col = \"blue\", pch = 19, cex = 1.5)\nlines(bin_centers, bin_means, col = \"blue\", lwd = 2)\n\n# Wide bins\nn_bins &lt;- 5\nbreaks &lt;- seq(min(x), max(x), length.out = n_bins + 1)\nbin_means &lt;- sapply(1:n_bins, function(i) {\n  in_bin &lt;- x &gt;= breaks[i] & x &lt; breaks[i + 1]\n  if (sum(in_bin) &gt; 0) mean(y[in_bin]) else NA\n})\nbin_centers &lt;- (breaks[-1] + breaks[-(n_bins + 1)]) / 2\n\nplot(x, y, pch = 16, col = \"gray60\", main = \"Wide bins (5)\")\npoints(bin_centers, bin_means, col = \"red\", pch = 19, cex = 1.5)\nlines(bin_centers, bin_means, col = \"red\", lwd = 2)\n\n\n\n\n\n\n\n\nFigure 29.6: Bin smoothing divides data into intervals and estimates each segment as the mean of points in that bin\n\n\n\n\n\nBin smoothing illustrates the bias-variance tradeoff in smoothing:\n\nNarrow bins: Capture local variation (low bias) but are noisy (high variance)\nWide bins: Smooth over noise (low variance) but may miss true curvature (high bias)\n\nThe main limitation of bin smoothing is the discontinuity at bin boundaries—the estimate jumps from one bin to the next.\n\n\nKernel Smoothing\nKernel smoothing improves on binning by using weighted averages, where closer points receive more weight. This creates smooth, continuous estimates.\nThe estimate at any point \\(x_0\\) is:\n\\[\\hat{f}(x_0) = \\frac{\\sum_{i=1}^n K\\left(\\frac{x_i - x_0}{h}\\right) y_i}{\\sum_{i=1}^n K\\left(\\frac{x_i - x_0}{h}\\right)}\\]\nwhere \\(K\\) is a kernel function (typically Gaussian or Epanechnikov) and \\(h\\) is the bandwidth controlling smoothness.\n\n\nCode\n# Kernel smoothing function\nkernel_smooth &lt;- function(x0, x, y, bandwidth) {\n  weights &lt;- dnorm(x, mean = x0, sd = bandwidth)\n  sum(weights * y) / sum(weights)\n}\n\n# Apply to grid of points\nx_grid &lt;- seq(0, 10, length.out = 200)\n\npar(mfrow = c(1, 3))\n\nfor (bw in c(0.2, 0.5, 1.0)) {\n  y_smooth &lt;- sapply(x_grid, function(x0) kernel_smooth(x0, x, y, bw))\n\n  plot(x, y, pch = 16, col = \"gray60\", main = paste(\"Bandwidth =\", bw))\n  lines(x_grid, y_smooth, col = \"blue\", lwd = 2)\n  lines(x_grid, sin(x_grid), col = \"red\", lwd = 2, lty = 2)\n}\n\n\n\n\n\n\n\n\nFigure 29.7: Kernel smoothing uses weighted averages with Gaussian weights, creating smooth estimates\n\n\n\n\n\nThe bandwidth parameter plays the same role as the number of bins:\n\nSmall bandwidth: More local, follows the data closely (risk of overfitting)\nLarge bandwidth: More global, smoother curve (risk of over-smoothing)\n\nKernel smoothing eliminates the discontinuity problem of bin smoothing while retaining its intuitive local-averaging interpretation.",
    "crumbs": [
      "Statistical Learning",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Core Concepts in Statistical Learning</span>"
    ]
  },
  {
    "objectID": "chapters/22-statistical-learning.html#splines-flexible-curve-fitting",
    "href": "chapters/22-statistical-learning.html#splines-flexible-curve-fitting",
    "title": "29  Core Concepts in Statistical Learning",
    "section": "29.8 Splines: Flexible Curve Fitting",
    "text": "29.8 Splines: Flexible Curve Fitting\nWhile LOESS provides local smoothing, splines offer a more structured approach to fitting flexible curves. A spline is a piecewise polynomial function that joins smoothly at points called knots.\n\nWhy Splines?\nLinear regression assumes a straight-line relationship, which is often too restrictive. We could fit polynomial regression (e.g., \\(y = \\beta_0 + \\beta_1 x + \\beta_2 x^2 + \\beta_3 x^3\\)), but polynomials can behave erratically, especially at the edges of the data.\nSplines provide flexibility while maintaining smooth, well-behaved curves.\n\n\nRegression Splines\nRegression splines fit piecewise polynomials at fixed knot locations. The splines package provides basis functions for incorporating splines into linear models:\n\n\nCode\nlibrary(splines)\n\n# Generate non-linear data\nset.seed(42)\nx &lt;- seq(0, 10, length.out = 100)\ny &lt;- sin(x) + 0.5 * cos(2*x) + rnorm(100, sd = 0.3)\ndata &lt;- data.frame(x, y)\n\n# Fit different models\nfit_linear &lt;- lm(y ~ x, data = data)\nfit_poly &lt;- lm(y ~ poly(x, 5), data = data)\nfit_spline &lt;- lm(y ~ bs(x, df = 6), data = data)  # B-spline with 6 df\n\n# Predictions\ndata$pred_linear &lt;- predict(fit_linear)\ndata$pred_poly &lt;- predict(fit_poly)\ndata$pred_spline &lt;- predict(fit_spline)\n\npar(mfrow = c(1, 3))\n\nplot(x, y, pch = 16, col = \"gray60\", main = \"Linear\")\nlines(x, data$pred_linear, col = \"red\", lwd = 2)\n\nplot(x, y, pch = 16, col = \"gray60\", main = \"Polynomial (degree 5)\")\nlines(x, data$pred_poly, col = \"blue\", lwd = 2)\n\nplot(x, y, pch = 16, col = \"gray60\", main = \"Spline (6 df)\")\nlines(x, data$pred_spline, col = \"darkgreen\", lwd = 2)\n\n\n\n\n\n\n\n\nFigure 29.8: Comparison of linear, polynomial, and spline fits for non-linear data\n\n\n\n\n\n\n\nNatural Splines\nNatural splines add the constraint that the function is linear beyond the boundary knots. This prevents the wild behavior that polynomials often exhibit at the edges:\n\n\nCode\n# Compare B-spline and natural spline\nfit_bs &lt;- lm(y ~ bs(x, df = 6), data = data)\nfit_ns &lt;- lm(y ~ ns(x, df = 6), data = data)\n\n# Extend prediction range to see edge behavior\nx_ext &lt;- seq(-2, 12, length.out = 200)\npred_bs &lt;- predict(fit_bs, newdata = data.frame(x = x_ext))\npred_ns &lt;- predict(fit_ns, newdata = data.frame(x = x_ext))\n\nplot(x, y, pch = 16, col = \"gray60\", xlim = c(-2, 12), ylim = c(-3, 3),\n     main = \"B-spline vs Natural Spline at Boundaries\")\nlines(x_ext, pred_bs, col = \"blue\", lwd = 2)\nlines(x_ext, pred_ns, col = \"darkgreen\", lwd = 2)\nabline(v = range(x), lty = 2, col = \"gray\")\nlegend(\"topright\", c(\"B-spline\", \"Natural spline\", \"Data range\"),\n       col = c(\"blue\", \"darkgreen\", \"gray\"), lty = c(1, 1, 2), lwd = c(2, 2, 1))\n\n\n\n\n\n\n\n\nFigure 29.9: Natural splines constrain the fit to be linear beyond the data boundaries, reducing edge effects\n\n\n\n\n\n\n\nSmoothing Splines\nSmoothing splines take a different approach: instead of pre-specifying knots, they place a knot at every data point and control smoothness through a penalty on the second derivative:\n\\[\\text{Minimize: } \\sum_{i=1}^n (y_i - f(x_i))^2 + \\lambda \\int f''(x)^2 dx\\]\nThe smoothing parameter \\(\\lambda\\) is typically chosen by cross-validation:\n\n\nCode\n# Fit smoothing spline with cross-validation\nsmooth_fit &lt;- smooth.spline(x, y, cv = TRUE)\n\nplot(x, y, pch = 16, col = \"gray60\", main = \"Smoothing Spline\")\nlines(smooth_fit, col = \"purple\", lwd = 2)\ncat(\"Optimal degrees of freedom:\", round(smooth_fit$df, 2), \"\\n\")\n\n\nOptimal degrees of freedom: 14.81 \n\n\n\n\n\n\n\n\nFigure 29.10: Smoothing spline with automatic cross-validation selection of the smoothing parameter\n\n\n\n\n\n\n\n\n\n\n\nChoosing the Right Approach\n\n\n\n\nRegression splines (bs, ns): When you want to include splines in a regression model with other predictors\nNatural splines: When extrapolation behavior matters\nSmoothing splines: For exploratory smoothing with automatic tuning\nLOESS: For local, non-parametric smoothing (especially useful for visualization)",
    "crumbs": [
      "Statistical Learning",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Core Concepts in Statistical Learning</span>"
    ]
  },
  {
    "objectID": "chapters/22-statistical-learning.html#loess-flexible-non-parametric-smoothing",
    "href": "chapters/22-statistical-learning.html#loess-flexible-non-parametric-smoothing",
    "title": "29  Core Concepts in Statistical Learning",
    "section": "29.9 LOESS: Flexible Non-Parametric Smoothing",
    "text": "29.9 LOESS: Flexible Non-Parametric Smoothing\nLOESS (Locally Estimated Scatterplot Smoothing) (Cleveland 1979) fits local regressions to subsets of data, weighted by distance from each point.\n\n\nCode\n# Compare linear regression and LOESS\nset.seed(123)\nx &lt;- seq(0, 4*pi, length.out = 100)\ny &lt;- sin(x) + rnorm(100, sd = 0.3)\n\nplot(x, y, pch = 16, col = \"gray60\", main = \"Linear vs LOESS\")\nabline(lm(y ~ x), col = \"red\", lwd = 2)\nlines(x, predict(loess(y ~ x, span = 0.3)), col = \"blue\", lwd = 2)\nlegend(\"topright\", c(\"Linear\", \"LOESS\"), col = c(\"red\", \"blue\"), lwd = 2)\n\n\n\n\n\n\n\n\nFigure 29.11: Comparison of linear regression and LOESS smoothing for non-linear data\n\n\n\n\n\nThe span parameter controls smoothness: smaller values fit more locally (more flexible), larger values average more broadly (smoother).",
    "crumbs": [
      "Statistical Learning",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Core Concepts in Statistical Learning</span>"
    ]
  },
  {
    "objectID": "chapters/22-statistical-learning.html#classification",
    "href": "chapters/22-statistical-learning.html#classification",
    "title": "29  Core Concepts in Statistical Learning",
    "section": "29.10 Classification",
    "text": "29.10 Classification\nWhen the response is categorical, we have a classification problem rather than regression. The goal is to predict which category an observation belongs to.\nLogistic regression produces probabilities that can be converted to class predictions.\nDecision trees recursively partition the feature space based on simple rules.\nRandom forests combine many decision trees for more robust predictions.",
    "crumbs": [
      "Statistical Learning",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Core Concepts in Statistical Learning</span>"
    ]
  },
  {
    "objectID": "chapters/22-statistical-learning.html#k-nearest-neighbors",
    "href": "chapters/22-statistical-learning.html#k-nearest-neighbors",
    "title": "29  Core Concepts in Statistical Learning",
    "section": "29.11 K-Nearest Neighbors",
    "text": "29.11 K-Nearest Neighbors\nK-nearest neighbors (kNN) is one of the simplest and most intuitive classification algorithms. To classify a new observation, kNN finds the k closest observations in the training data and assigns the most common class among those neighbors.\n\n\nCode\n# Simulate two-class data\nset.seed(42)\nn &lt;- 100\nclass1 &lt;- data.frame(\n  x1 = rnorm(n/2, mean = 2, sd = 1),\n  x2 = rnorm(n/2, mean = 2, sd = 1),\n  class = \"A\"\n)\nclass2 &lt;- data.frame(\n  x1 = rnorm(n/2, mean = 4, sd = 1),\n  x2 = rnorm(n/2, mean = 4, sd = 1),\n  class = \"B\"\n)\ntrain_data &lt;- rbind(class1, class2)\n\n# New point to classify\nnew_point &lt;- data.frame(x1 = 3.2, x2 = 3.5)\n\n# Plot\nplot(train_data$x1, train_data$x2,\n     col = ifelse(train_data$class == \"A\", \"blue\", \"red\"),\n     pch = 16, xlab = \"Feature 1\", ylab = \"Feature 2\",\n     main = \"K-Nearest Neighbors (k=5)\")\npoints(new_point$x1, new_point$x2, pch = 8, cex = 2, lwd = 2)\n\n# Find 5 nearest neighbors\ndistances &lt;- sqrt((train_data$x1 - new_point$x1)^2 +\n                  (train_data$x2 - new_point$x2)^2)\nnearest &lt;- order(distances)[1:5]\n\n# Draw circles around nearest neighbors\npoints(train_data$x1[nearest], train_data$x2[nearest],\n       cex = 2, col = ifelse(train_data$class[nearest] == \"A\", \"blue\", \"red\"))\nlegend(\"topleft\", c(\"Class A\", \"Class B\", \"New point\"),\n       col = c(\"blue\", \"red\", \"black\"), pch = c(16, 16, 8))\n\n\n\n\n\n\n\n\nFigure 29.12: K-nearest neighbors classification: the new point (star) is classified based on its nearest neighbors\n\n\n\n\n\nThe choice of k is critical and illustrates the bias-variance tradeoff:\n\nSmall k (e.g., k=1): Very flexible, low bias but high variance. The decision boundary is jagged and sensitive to individual training points—prone to overfitting.\nLarge k: Smoother decision boundary, higher bias but lower variance. May miss local patterns—prone to underfitting.\n\n\n\nCode\nlibrary(class)\n\n# Create a grid for visualization\nx1_grid &lt;- seq(0, 6, length.out = 100)\nx2_grid &lt;- seq(0, 6, length.out = 100)\ngrid &lt;- expand.grid(x1 = x1_grid, x2 = x2_grid)\n\npar(mfrow = c(1, 3))\n\nfor (k_val in c(1, 15, 50)) {\n  # Predict on grid\n  pred &lt;- knn(train = train_data[, 1:2],\n              test = grid,\n              cl = train_data$class,\n              k = k_val)\n\n  # Plot decision regions\n  plot(grid$x1, grid$x2, col = ifelse(pred == \"A\",\n       rgb(0, 0, 1, 0.1), rgb(1, 0, 0, 0.1)),\n       pch = 15, cex = 0.5, xlab = \"Feature 1\", ylab = \"Feature 2\",\n       main = paste(\"k =\", k_val))\n  points(train_data$x1, train_data$x2,\n         col = ifelse(train_data$class == \"A\", \"blue\", \"red\"), pch = 16)\n}\n\n\n\n\n\n\n\n\nFigure 29.13: Effect of k on kNN classification: small k creates complex boundaries (potential overfitting), large k creates smooth boundaries (potential underfitting)\n\n\n\n\n\n\nSelecting k with Cross-Validation\nWe choose k by evaluating classification accuracy across different values using cross-validation:\n\n\nCode\n# Evaluate different k values\nset.seed(123)\nk_values &lt;- seq(1, 50, by = 2)\n\n# Simple holdout validation\ntest_idx &lt;- sample(1:nrow(train_data), 30)\ntrain_subset &lt;- train_data[-test_idx, ]\ntest_subset &lt;- train_data[test_idx, ]\n\naccuracy &lt;- sapply(k_values, function(k) {\n  pred &lt;- knn(train = train_subset[, 1:2],\n              test = test_subset[, 1:2],\n              cl = train_subset$class,\n              k = k)\n  mean(pred == test_subset$class)\n})\n\ntrain_accuracy &lt;- sapply(k_values, function(k) {\n  pred &lt;- knn(train = train_subset[, 1:2],\n              test = train_subset[, 1:2],\n              cl = train_subset$class,\n              k = k)\n  mean(pred == train_subset$class)\n})\n\nplot(k_values, train_accuracy, type = \"l\", col = \"blue\", lwd = 2,\n     xlab = \"k (number of neighbors)\", ylab = \"Accuracy\",\n     main = \"Training vs Test Accuracy\", ylim = c(0.5, 1))\nlines(k_values, accuracy, col = \"red\", lwd = 2)\nlegend(\"bottomright\", c(\"Training\", \"Test\"), col = c(\"blue\", \"red\"), lwd = 2)\n\n\n\n\n\n\n\n\nFigure 29.14: Cross-validation accuracy for different values of k: accuracy on training data decreases with k, but test accuracy peaks at intermediate values\n\n\n\n\n\nNotice that training accuracy is perfect (1.0) when k=1—each point is its own nearest neighbor. But test accuracy tells the true story of generalization performance.",
    "crumbs": [
      "Statistical Learning",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Core Concepts in Statistical Learning</span>"
    ]
  },
  {
    "objectID": "chapters/22-statistical-learning.html#confusion-matrices",
    "href": "chapters/22-statistical-learning.html#confusion-matrices",
    "title": "29  Core Concepts in Statistical Learning",
    "section": "29.12 Confusion Matrices",
    "text": "29.12 Confusion Matrices\nClassification performance is evaluated with a confusion matrix:\n\n\n\n\nPredicted Positive\nPredicted Negative\n\n\n\n\nActual Positive\nTrue Positive (TP)\nFalse Negative (FN)\n\n\nActual Negative\nFalse Positive (FP)\nTrue Negative (TN)\n\n\n\nKey metrics: - Accuracy: (TP + TN) / Total - Sensitivity (Recall): TP / (TP + FN) — how many positives were caught - Specificity: TN / (TN + FP) — how many negatives were correctly identified - Precision: TP / (TP + FP) — among positive predictions, how many were correct\n\nThe Problem with Accuracy\nAccuracy can be misleading with imbalanced classes. If 95% of emails are legitimate, a classifier that labels everything as “not spam” achieves 95% accuracy while being completely useless for its intended purpose.\n\n\nCode\n# Imbalanced class example\nset.seed(42)\n# 95% negative, 5% positive (e.g., rare disease screening)\nn &lt;- 1000\nactual &lt;- factor(c(rep(\"Negative\", 950), rep(\"Positive\", 50)))\n\n# Naive classifier: always predict negative\nnaive_pred &lt;- factor(rep(\"Negative\", n), levels = c(\"Negative\", \"Positive\"))\n\n# Calculate metrics\nTP &lt;- sum(naive_pred == \"Positive\" & actual == \"Positive\")\nTN &lt;- sum(naive_pred == \"Negative\" & actual == \"Negative\")\nFP &lt;- sum(naive_pred == \"Positive\" & actual == \"Negative\")\nFN &lt;- sum(naive_pred == \"Negative\" & actual == \"Positive\")\n\ncat(\"Accuracy:\", (TP + TN) / n, \"\\n\")\n\n\nAccuracy: 0.95 \n\n\nCode\ncat(\"Sensitivity (Recall):\", TP / (TP + FN), \"\\n\")\n\n\nSensitivity (Recall): 0 \n\n\nCode\ncat(\"The classifier catches 0% of positive cases!\\n\")\n\n\nThe classifier catches 0% of positive cases!\n\n\n\n\nF1 Score and Balanced Accuracy\nFor imbalanced data, better metrics include:\nF1 Score: The harmonic mean of precision and recall, balancing both concerns:\n\\[F_1 = 2 \\cdot \\frac{\\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}} = \\frac{2 \\cdot TP}{2 \\cdot TP + FP + FN}\\]\nBalanced Accuracy: The average of sensitivity and specificity:\n\\[\\text{Balanced Accuracy} = \\frac{\\text{Sensitivity} + \\text{Specificity}}{2}\\]\n\n\nCode\n# Better classifier for the imbalanced data\nset.seed(123)\n# Suppose we have a model that catches 80% of positives but has some false positives\nbetter_pred &lt;- actual  # Start with actual\n# Correctly identify 80% of positives\npos_idx &lt;- which(actual == \"Positive\")\nneg_idx &lt;- which(actual == \"Negative\")\nbetter_pred[sample(pos_idx, 10)] &lt;- \"Negative\"  # Miss 10 of 50 positives (20%)\nbetter_pred[sample(neg_idx, 50)] &lt;- \"Positive\"  # 50 false positives\n\n# Confusion matrix\nTP &lt;- sum(better_pred == \"Positive\" & actual == \"Positive\")\nTN &lt;- sum(better_pred == \"Negative\" & actual == \"Negative\")\nFP &lt;- sum(better_pred == \"Positive\" & actual == \"Negative\")\nFN &lt;- sum(better_pred == \"Negative\" & actual == \"Positive\")\n\nprecision &lt;- TP / (TP + FP)\nrecall &lt;- TP / (TP + FN)  # Sensitivity\nspecificity &lt;- TN / (TN + FP)\n\n# Calculate metrics\naccuracy &lt;- (TP + TN) / n\nf1 &lt;- 2 * precision * recall / (precision + recall)\nbalanced_acc &lt;- (recall + specificity) / 2\n\ncat(\"Accuracy:\", round(accuracy, 3), \"\\n\")\n\n\nAccuracy: 0.94 \n\n\nCode\ncat(\"Precision:\", round(precision, 3), \"\\n\")\n\n\nPrecision: 0.444 \n\n\nCode\ncat(\"Recall (Sensitivity):\", round(recall, 3), \"\\n\")\n\n\nRecall (Sensitivity): 0.8 \n\n\nCode\ncat(\"F1 Score:\", round(f1, 3), \"\\n\")\n\n\nF1 Score: 0.571 \n\n\nCode\ncat(\"Balanced Accuracy:\", round(balanced_acc, 3), \"\\n\")\n\n\nBalanced Accuracy: 0.874 \n\n\n\n\n\n\n\n\nWhich Metric to Use?\n\n\n\n\nAccuracy: Only when classes are balanced\nF1 Score: When you care about both precision and recall equally\nSensitivity/Recall: When missing positives is costly (disease screening)\nPrecision: When false positives are costly (spam filtering)\nBalanced Accuracy: Quick summary for imbalanced data",
    "crumbs": [
      "Statistical Learning",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Core Concepts in Statistical Learning</span>"
    ]
  },
  {
    "objectID": "chapters/22-statistical-learning.html#roc-curves-and-auc",
    "href": "chapters/22-statistical-learning.html#roc-curves-and-auc",
    "title": "29  Core Concepts in Statistical Learning",
    "section": "29.13 ROC Curves and AUC",
    "text": "29.13 ROC Curves and AUC\nMany classifiers output probabilities rather than hard class labels. By varying the threshold for classifying as positive, we trade off sensitivity against specificity.\nThe Receiver Operating Characteristic (ROC) curve plots sensitivity (true positive rate) against 1 - specificity (false positive rate) at all possible thresholds.\n\n\nCode\n# Simulate a classifier with probabilities\nset.seed(42)\nn &lt;- 500\nactual &lt;- factor(c(rep(1, 100), rep(0, 400)))  # 20% positive\n\n# Generate predicted probabilities (imperfect classifier)\nprobs &lt;- c(rbeta(100, 3, 2),   # Positives: higher probs\n           rbeta(400, 2, 3))   # Negatives: lower probs\n\n# Calculate ROC curve manually\nthresholds &lt;- seq(0, 1, by = 0.01)\nroc_data &lt;- data.frame(\n  threshold = thresholds,\n  TPR = sapply(thresholds, function(t) {\n    pred &lt;- ifelse(probs &gt;= t, 1, 0)\n    sum(pred == 1 & actual == 1) / sum(actual == 1)\n  }),\n  FPR = sapply(thresholds, function(t) {\n    pred &lt;- ifelse(probs &gt;= t, 1, 0)\n    sum(pred == 1 & actual == 0) / sum(actual == 0)\n  })\n)\n\n# Plot ROC curve\nplot(roc_data$FPR, roc_data$TPR, type = \"l\", lwd = 2, col = \"blue\",\n     xlab = \"False Positive Rate (1 - Specificity)\",\n     ylab = \"True Positive Rate (Sensitivity)\",\n     main = \"ROC Curve\")\nabline(0, 1, lty = 2, col = \"gray\")  # Random classifier line\n\n# Add points for specific thresholds\nhighlight &lt;- c(0.3, 0.5, 0.7)\nfor (t in highlight) {\n  idx &lt;- which.min(abs(roc_data$threshold - t))\n  points(roc_data$FPR[idx], roc_data$TPR[idx], pch = 19, cex = 1.5)\n  text(roc_data$FPR[idx] + 0.05, roc_data$TPR[idx],\n       paste(\"t =\", t), cex = 0.8)\n}\nlegend(\"bottomright\", c(\"ROC Curve\", \"Random Classifier\"),\n       col = c(\"blue\", \"gray\"), lty = c(1, 2), lwd = c(2, 1))\n\n\n\n\n\n\n\n\nFigure 29.15: ROC curve showing the tradeoff between sensitivity and false positive rate; the dashed diagonal represents random guessing\n\n\n\n\n\n\nArea Under the Curve (AUC)\nThe AUC (Area Under the ROC Curve) summarizes classifier performance in a single number:\n\nAUC = 0.5: No better than random guessing\nAUC = 1.0: Perfect classification\nAUC &gt; 0.8: Generally considered good\n\n\n\nCode\n# Calculate AUC using trapezoidal rule\nauc &lt;- sum(diff(roc_data$FPR[order(roc_data$FPR)]) *\n           (head(roc_data$TPR[order(roc_data$FPR)], -1) +\n            tail(roc_data$TPR[order(roc_data$FPR)], -1)) / 2)\ncat(\"AUC:\", round(abs(auc), 3), \"\\n\")\n\n\nAUC: 0.765 \n\n\n\n\nComparing Classifiers with ROC\nROC curves allow direct comparison of classifiers:\n\n\nCode\n# Simulate three classifiers of varying quality\nset.seed(42)\n\n# Good classifier\nprobs_good &lt;- c(rbeta(100, 4, 1.5), rbeta(400, 1.5, 4))\n\n# Medium classifier (our original)\nprobs_medium &lt;- probs\n\n# Poor classifier\nprobs_poor &lt;- c(rbeta(100, 2, 2), rbeta(400, 2, 2))\n\n# Function to calculate ROC data\ncalc_roc &lt;- function(probs, actual) {\n  thresholds &lt;- seq(0, 1, by = 0.01)\n  data.frame(\n    TPR = sapply(thresholds, function(t) {\n      pred &lt;- ifelse(probs &gt;= t, 1, 0)\n      sum(pred == 1 & actual == 1) / sum(actual == 1)\n    }),\n    FPR = sapply(thresholds, function(t) {\n      pred &lt;- ifelse(probs &gt;= t, 1, 0)\n      sum(pred == 1 & actual == 0) / sum(actual == 0)\n    })\n  )\n}\n\nroc_good &lt;- calc_roc(probs_good, actual)\nroc_medium &lt;- calc_roc(probs_medium, actual)\nroc_poor &lt;- calc_roc(probs_poor, actual)\n\n# Plot comparison\nplot(roc_good$FPR, roc_good$TPR, type = \"l\", lwd = 2, col = \"darkgreen\",\n     xlab = \"False Positive Rate\", ylab = \"True Positive Rate\",\n     main = \"ROC Curve Comparison\")\nlines(roc_medium$FPR, roc_medium$TPR, lwd = 2, col = \"blue\")\nlines(roc_poor$FPR, roc_poor$TPR, lwd = 2, col = \"red\")\nabline(0, 1, lty = 2, col = \"gray\")\n\nlegend(\"bottomright\",\n       c(\"Good (AUC ≈ 0.90)\", \"Medium (AUC ≈ 0.75)\", \"Poor (AUC ≈ 0.50)\"),\n       col = c(\"darkgreen\", \"blue\", \"red\"), lwd = 2)\n\n\n\n\n\n\n\n\nFigure 29.16: Comparing classifiers using ROC curves: higher curves (larger AUC) indicate better performance\n\n\n\n\n\n\n\nPrecision-Recall Curves\nFor highly imbalanced data, precision-recall curves can be more informative than ROC curves because they focus on the minority (positive) class:\n\n\nCode\n# Calculate precision-recall curve\npr_data &lt;- data.frame(\n  threshold = thresholds,\n  precision = sapply(thresholds, function(t) {\n    pred &lt;- ifelse(probs &gt;= t, 1, 0)\n    tp &lt;- sum(pred == 1 & actual == 1)\n    fp &lt;- sum(pred == 1 & actual == 0)\n    if (tp + fp == 0) return(NA)\n    tp / (tp + fp)\n  }),\n  recall = sapply(thresholds, function(t) {\n    pred &lt;- ifelse(probs &gt;= t, 1, 0)\n    sum(pred == 1 & actual == 1) / sum(actual == 1)\n  })\n)\n\n# Remove NA values\npr_data &lt;- pr_data[!is.na(pr_data$precision), ]\n\nplot(pr_data$recall, pr_data$precision, type = \"l\", lwd = 2, col = \"purple\",\n     xlab = \"Recall (Sensitivity)\", ylab = \"Precision\",\n     main = \"Precision-Recall Curve\", ylim = c(0, 1))\nabline(h = mean(actual == 1), lty = 2, col = \"gray\")  # Baseline\nlegend(\"topright\", c(\"PR Curve\", \"Baseline (random)\"),\n       col = c(\"purple\", \"gray\"), lty = c(1, 2), lwd = c(2, 1))\n\n\n\n\n\n\n\n\nFigure 29.17: Precision-recall curve for imbalanced classification; the horizontal dashed line shows baseline precision (proportion of positives)",
    "crumbs": [
      "Statistical Learning",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Core Concepts in Statistical Learning</span>"
    ]
  },
  {
    "objectID": "chapters/22-statistical-learning.html#the-curse-of-dimensionality",
    "href": "chapters/22-statistical-learning.html#the-curse-of-dimensionality",
    "title": "29  Core Concepts in Statistical Learning",
    "section": "29.14 The Curse of Dimensionality",
    "text": "29.14 The Curse of Dimensionality\nWe described how methods such as LDA and QDA are not meant to be used with many predictors \\(p\\) because the number of parameters that we need to estimate becomes too large. Kernel methods such as kNN or local regression do not have model parameters to estimate. However, they also face a challenge when multiple predictors are used due to what is referred to as the curse of dimensionality. The dimension here refers to the fact that when we have \\(p\\) predictors, the distance between two observations is computed in \\(p\\)-dimensional space.\nA useful way of understanding the curse of dimensionality is by considering how large we have to make a span/neighborhood/window to include a given percentage of the data. Remember that with larger neighborhoods, our methods lose flexibility.\nFor example, suppose we have one continuous predictor with equally spaced points in the [0,1] interval and we want to create windows that include 1/10th of data. Then it’s easy to see that our windows have to be of size 0.1.\nNow, for two predictors, if we decide to keep the neighborhood just as small (10% for each dimension), we include only 1 point. If we want to include 10% of the data, then we need to increase the size of each side of the square to \\(\\sqrt{.10} \\approx .316\\).\nUsing the same logic, if we want to include 10% of the data in a three-dimensional space, then the side of each cube is \\(\\sqrt[3]{.10} \\approx 0.464\\). In general, to include 10% of the data in a case with \\(p\\) dimensions, we need an interval with each side of size \\(\\sqrt[p]{.10}\\) of the total. This proportion gets close to 1 quickly, and if the proportion is 1 it means we include all the data and are no longer smoothing.\n\n\nCode\np &lt;- 1:100\nplot(p, .1^(1/p), type = \"l\", lwd = 2, col = \"steelblue\",\n     xlab = \"Number of Dimensions (p)\",\n     ylab = \"Side Length to Include 10% of Data\",\n     main = \"The Curse of Dimensionality\",\n     ylim = c(0, 1))\nabline(h = 1, lty = 2, col = \"gray50\")\n\n\n\n\n\n\n\n\nFigure 29.18: As dimensions increase, the neighborhood size needed to include a fixed proportion of data grows rapidly. By 100 dimensions, any ‘local’ neighborhood must span nearly the entire data range.\n\n\n\n\n\nBy the time we reach 100 predictors, the neighborhood is no longer very local, as each side covers almost the entire dataset.\nThis motivates the use of methods that adapt to higher dimensions while still producing interpretable models. Decision trees and random forests are examples of such methods.",
    "crumbs": [
      "Statistical Learning",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Core Concepts in Statistical Learning</span>"
    ]
  },
  {
    "objectID": "chapters/22-statistical-learning.html#decision-trees-cart",
    "href": "chapters/22-statistical-learning.html#decision-trees-cart",
    "title": "29  Core Concepts in Statistical Learning",
    "section": "29.15 Decision Trees (CART)",
    "text": "29.15 Decision Trees (CART)\nClassification and Regression Trees (CART) make predictions by recursively partitioning the feature space into regions. At each node, the algorithm asks a yes/no question about a single feature, splitting observations into two groups. The process continues until a stopping criterion is met.\n\nMotivating Example: Olive Oil Classification\nTo motivate decision trees, consider a dataset that includes the breakdown of olive oil composition into 8 fatty acids:\n\n\nCode\nlibrary(dslabs)\ndata(\"olive\")\nnames(olive)\n\n\n [1] \"region\"      \"area\"        \"palmitic\"    \"palmitoleic\" \"stearic\"    \n [6] \"oleic\"       \"linoleic\"    \"linolenic\"   \"arachidic\"   \"eicosenoic\" \n\n\nWe will try to predict the region of origin using the fatty acid composition values as predictors.\n\n\nCode\ntable(olive$region)\n\n\n\nNorthern Italy       Sardinia Southern Italy \n           151             98            323 \n\n\nWe remove the area column because we won’t use it as a predictor.\n\n\nCode\nolive &lt;- select(olive, -area)\n\n\nIf we examine the distribution of each predictor stratified by region, we see that eicosenoic is only present in Southern Italy and that linoleic separates Northern Italy from Sardinia:\n\n\nCode\nolive %&gt;%\n  pivot_longer(-region, names_to = \"fatty_acid\", values_to = \"percentage\") %&gt;%\n  ggplot(aes(region, percentage, fill = region)) +\n  geom_boxplot() +\n  facet_wrap(~fatty_acid, scales = \"free\", ncol = 4) +\n  theme(axis.text.x = element_blank(), legend.position = \"bottom\") +\n  labs(title = \"Fatty Acid Composition by Region\")\n\n\n\n\n\n\n\n\nFigure 29.19: Distribution of fatty acid composition by region. Some predictors like eicosenoic and linoleic clearly separate regions.\n\n\n\n\n\nThis implies that we should be able to build an algorithm that predicts perfectly. We can see this clearly by plotting the values for eicosenoic and linoleic:\n\n\nCode\nolive %&gt;%\n  ggplot(aes(eicosenoic, linoleic, color = region)) +\n  geom_point() +\n  geom_vline(xintercept = 0.065, lty = 2) +\n  geom_segment(x = -0.2, y = 10.54, xend = 0.065, yend = 10.54,\n               color = \"black\", lty = 2) +\n  labs(title = \"Perfect Classification with Simple Rules\")\n\n\n\n\n\n\n\n\nFigure 29.20: With just two predictors, we can draw decision boundaries that perfectly separate the regions\n\n\n\n\n\nBy eye, we can construct a prediction rule that partitions the predictor space so that each partition contains outcomes of only one category:\n\nIf eicosenoic &gt; 0.065, predict Southern Italy\nIf not, then if linoleic &gt; 10.535, predict Sardinia\nOtherwise, predict Northern Italy\n\nThis is exactly what a decision tree does—it learns these rules from data. A tree is basically a flow chart of yes/no questions. The general idea is to use data to create these trees with predictions at the ends, referred to as nodes.\n\n\nHow Trees Work\nThe key idea: find the split that best separates the data at each step.\nRegression and decision trees operate by predicting an outcome variable \\(Y\\) by partitioning the predictors. We partition the predictor space into \\(J\\) non-overlapping regions, \\(R_1, R_2, \\ldots, R_J\\), and then for any predictor \\(x\\) that falls within region \\(R_j\\), we estimate \\(f(x)\\) with the average (for regression) or majority class (for classification) of the training observations in that region.\nTrees create partitions recursively. We start with one partition, the entire predictor space. After the first step we have two partitions. After the second step we split one of these partitions into two and have three partitions, then four, and so on.\nOnce we select a partition to split, we find a predictor \\(j\\) and value \\(s\\) that define two new partitions:\n\\[\nR_1(j,s) = \\{\\mathbf{x} \\mid x_j &lt; s\\} \\mbox{  and  } R_2(j,s) = \\{\\mathbf{x} \\mid x_j \\geq s\\}\n\\]\nBut how do we pick \\(j\\) and \\(s\\)? We find the pair that minimizes our loss function.\nFor regression trees, we minimize the residual sum of squares (RSS):\n\\[\n\\sum_{i:\\, x_i \\in R_1(j,s)} (y_i - \\hat{y}_{R_1})^2 +\n\\sum_{i:\\, x_i \\in R_2(j,s)} (y_i - \\hat{y}_{R_2})^2\n\\]\nwhere \\(\\hat{y}_{R_1}\\) and \\(\\hat{y}_{R_2}\\) are the average outcomes in each region.\nFor classification trees, we use the Gini impurity:\n\\[\n\\mbox{Gini}(j) = \\sum_{k=1}^K \\hat{p}_{j,k}(1-\\hat{p}_{j,k})\n\\]\nwhere \\(\\hat{p}_{j,k}\\) is the proportion of observations in partition \\(j\\) that are of class \\(k\\). A pure node (all one class) has Gini = 0.\nEntropy is a very similar quantity:\n\\[\n\\mbox{Entropy}(j) = -\\sum_{k=1}^K \\hat{p}_{j,k}\\log(\\hat{p}_{j,k})\n\\]\n(with \\(0 \\times \\log(0)\\) defined as 0). Both Gini and entropy are 0 for perfectly pure nodes and increase as the class distribution becomes more mixed.\n\n\nCode\nlibrary(rpart)\nlibrary(rpart.plot)\n\n# Build a classification tree\ndata(iris)\ntree_class &lt;- rpart(Species ~ ., data = iris, method = \"class\")\n\n# Visualize with rpart.plot\nrpart.plot(tree_class, extra = 104, box.palette = \"RdYlGn\",\n           main = \"Classification Tree for Iris Species\")\n\n\n\n\n\n\n\n\nFigure 29.21: A CART decision tree for classifying iris species. Each node shows the predicted class, proportion of observations, and the splitting rule.\n\n\n\n\n\n\n\nInterpreting Tree Output\nThe tree visualization shows:\n\nNode prediction: The predicted class (or value for regression)\nSplit rule: The feature and threshold used to split\nProportions: Distribution of classes at each node\nSample size: Number of observations reaching each node\n\n\n\nCode\n# Detailed tree summary\nsummary(tree_class, cp = 0.1)\n\n\nCall:\nrpart(formula = Species ~ ., data = iris, method = \"class\")\n  n= 150 \n\n    CP nsplit rel error xerror       xstd\n1 0.50      0      1.00   1.18 0.05017303\n2 0.44      1      0.50   0.71 0.06115009\n3 0.01      2      0.06   0.12 0.03322650\n\nVariable importance\n Petal.Width Petal.Length Sepal.Length  Sepal.Width \n          34           31           21           14 \n\nNode number 1: 150 observations,    complexity param=0.5\n  predicted class=setosa      expected loss=0.6666667  P(node) =1\n    class counts:    50    50    50\n   probabilities: 0.333 0.333 0.333 \n  left son=2 (50 obs) right son=3 (100 obs)\n  Primary splits:\n      Petal.Length &lt; 2.45 to the left,  improve=50.00000, (0 missing)\n      Petal.Width  &lt; 0.8  to the left,  improve=50.00000, (0 missing)\n      Sepal.Length &lt; 5.45 to the left,  improve=34.16405, (0 missing)\n      Sepal.Width  &lt; 3.35 to the right, improve=19.03851, (0 missing)\n  Surrogate splits:\n      Petal.Width  &lt; 0.8  to the left,  agree=1.000, adj=1.00, (0 split)\n      Sepal.Length &lt; 5.45 to the left,  agree=0.920, adj=0.76, (0 split)\n      Sepal.Width  &lt; 3.35 to the right, agree=0.833, adj=0.50, (0 split)\n\nNode number 2: 50 observations\n  predicted class=setosa      expected loss=0  P(node) =0.3333333\n    class counts:    50     0     0\n   probabilities: 1.000 0.000 0.000 \n\nNode number 3: 100 observations,    complexity param=0.44\n  predicted class=versicolor  expected loss=0.5  P(node) =0.6666667\n    class counts:     0    50    50\n   probabilities: 0.000 0.500 0.500 \n  left son=6 (54 obs) right son=7 (46 obs)\n  Primary splits:\n      Petal.Width  &lt; 1.75 to the left,  improve=38.969400, (0 missing)\n      Petal.Length &lt; 4.75 to the left,  improve=37.353540, (0 missing)\n      Sepal.Length &lt; 6.15 to the left,  improve=10.686870, (0 missing)\n      Sepal.Width  &lt; 2.45 to the left,  improve= 3.555556, (0 missing)\n  Surrogate splits:\n      Petal.Length &lt; 4.75 to the left,  agree=0.91, adj=0.804, (0 split)\n      Sepal.Length &lt; 6.15 to the left,  agree=0.73, adj=0.413, (0 split)\n      Sepal.Width  &lt; 2.95 to the left,  agree=0.67, adj=0.283, (0 split)\n\nNode number 6: 54 observations\n  predicted class=versicolor  expected loss=0.09259259  P(node) =0.36\n    class counts:     0    49     5\n   probabilities: 0.000 0.907 0.093 \n\nNode number 7: 46 observations\n  predicted class=virginica   expected loss=0.02173913  P(node) =0.3066667\n    class counts:     0     1    45\n   probabilities: 0.000 0.022 0.978 \n\n\n\n\nRegression Trees\nWhen the outcome is continuous, we call the method a regression tree. To illustrate, we will use poll data from the 2008 presidential election where we try to estimate the conditional expectation of poll margin \\(Y\\) given day \\(x\\).\n\n\nCode\ndata(\"polls_2008\")\nqplot(day, margin, data = polls_2008) +\n  labs(title = \"2008 Presidential Poll Data\", x = \"Days before election\", y = \"Poll margin\")\n\n\n\n\n\n\n\n\nFigure 29.22: 2008 presidential poll data: margin (Obama - McCain) over time\n\n\n\n\n\nLet’s fit a regression tree using the rpart function:\n\n\nCode\nfit &lt;- rpart(margin ~ ., data = polls_2008)\nrafalib::mypar()\nplot(fit, margin = 0.1)\ntext(fit, cex = 0.75)\n\n\n\n\n\n\n\n\nFigure 29.23: Regression tree for poll data showing where the algorithm decided to split\n\n\n\n\n\nThe tree shows that the first split is made at day 39.5, then further splits occur at days 86.5, 49.5, 117.5, and so on. The final estimate \\(\\hat{f}(x)\\) is a step function:\n\n\nCode\npolls_2008 %&gt;%\n  mutate(y_hat = predict(fit)) %&gt;%\n  ggplot() +\n  geom_point(aes(day, margin)) +\n  geom_step(aes(day, y_hat), col = \"red\", linewidth = 1) +\n  labs(title = \"Regression Tree Fit\", x = \"Day\", y = \"Poll margin\")\n\n\n\n\n\n\n\n\nFigure 29.24: Regression tree predictions for poll data create a step function\n\n\n\n\n\nTrees can also be applied to multiple predictors:\n\n\nCode\n# Build a regression tree\ntree_reg &lt;- rpart(mpg ~ wt + hp + cyl, data = mtcars, method = \"anova\")\n\nrpart.plot(tree_reg, extra = 101, box.palette = \"Blues\",\n           main = \"Regression Tree for MPG\")\n\n\n\n\n\n\n\n\nFigure 29.25: A regression tree predicting car fuel efficiency (mpg) from weight and horsepower\n\n\n\n\n\n\n\nThe Decision Boundary\nTrees partition the feature space into rectangular regions:\n\n\nCode\n# Visualize decision boundary for 2D case\ntree_2d &lt;- rpart(Species ~ Petal.Length + Petal.Width, data = iris, method = \"class\")\n\n# Create grid for prediction\npetal_length_seq &lt;- seq(0, 7, length.out = 200)\npetal_width_seq &lt;- seq(0, 3, length.out = 200)\ngrid &lt;- expand.grid(Petal.Length = petal_length_seq,\n                    Petal.Width = petal_width_seq)\ngrid$pred &lt;- predict(tree_2d, grid, type = \"class\")\n\n# Plot\nplot(grid$Petal.Length, grid$Petal.Width,\n     col = c(rgb(1,0,0,0.1), rgb(0,1,0,0.1), rgb(0,0,1,0.1))[grid$pred],\n     pch = 15, cex = 0.3,\n     xlab = \"Petal Length\", ylab = \"Petal Width\",\n     main = \"Decision Tree Boundaries\")\npoints(iris$Petal.Length, iris$Petal.Width,\n       col = c(\"red\", \"green\", \"blue\")[iris$Species],\n       pch = 19, cex = 0.8)\nlegend(\"topleft\", levels(iris$Species),\n       col = c(\"red\", \"green\", \"blue\"), pch = 19)\n\n\n\n\n\n\n\n\nFigure 29.26: Decision tree partition of the feature space. Each rectangular region is assigned to a class based on the majority vote of training points in that region.\n\n\n\n\n\n\n\nControlling Tree Complexity\nTrees easily overfit—they can keep splitting until each leaf contains a single observation. Several parameters control complexity:\n\ncp (complexity parameter): Every time we split and define two new partitions, our training set RSS decreases. The RSS must improve by a factor of cp for the new partition to be added. Large values of cp force the algorithm to stop earlier, resulting in fewer nodes.\nminsplit: Minimum observations required in a partition before attempting to split further. The default in rpart is 20.\nminbucket: Minimum number of observations in each terminal node (leaf). Defaults to round(minsplit/3).\nmaxdepth: Maximum depth of the tree\n\nIf we set cp = 0 and minsplit = 2, our prediction becomes as flexible as possible and simply memorizes the training data—a clear case of overfitting:\n\n\nCode\nfit_overfit &lt;- rpart(margin ~ ., data = polls_2008,\n                      control = rpart.control(cp = 0, minsplit = 2))\npolls_2008 %&gt;%\n  mutate(y_hat = predict(fit_overfit)) %&gt;%\n  ggplot() +\n  geom_point(aes(day, margin)) +\n  geom_step(aes(day, y_hat), col = \"red\") +\n  labs(title = \"Overfitting with cp = 0\", x = \"Day\", y = \"Poll margin\")\n\n\n\n\n\n\n\n\nFigure 29.27: With cp=0 and minsplit=2, the tree overfits by memorizing every point in the training data\n\n\n\n\n\nThe larger these values are, the more data is averaged to compute a predictor, reducing variability but restricting flexibility. We use cross-validation to select the optimal balance.\n\n\nCode\npar(mfrow = c(1, 3))\n\n# Simple tree (high cp)\ntree_simple &lt;- rpart(Species ~ ., data = iris, cp = 0.1)\nrpart.plot(tree_simple, main = \"cp = 0.1 (simple)\")\n\n# Medium tree\ntree_medium &lt;- rpart(Species ~ ., data = iris, cp = 0.02)\nrpart.plot(tree_medium, main = \"cp = 0.02 (medium)\")\n\n# Complex tree (low cp)\ntree_complex &lt;- rpart(Species ~ ., data = iris, cp = 0.001)\nrpart.plot(tree_complex, main = \"cp = 0.001 (complex)\")\n\n\n\n\n\n\n\n\nFigure 29.28: Effect of the complexity parameter on tree structure: smaller cp allows more splits and greater complexity\n\n\n\n\n\n\n\nPruning with Cross-Validation\nThe optimal complexity is typically chosen by cross-validation:\n\n\nCode\n# Fit full tree\nfull_tree &lt;- rpart(Species ~ ., data = iris, cp = 0.001)\n\n# Plot CV error vs complexity\nplotcp(full_tree)\n\n# Print CP table\nprintcp(full_tree)\n\n\n\nClassification tree:\nrpart(formula = Species ~ ., data = iris, cp = 0.001)\n\nVariables actually used in tree construction:\n[1] Petal.Length Petal.Width \n\nRoot node error: 100/150 = 0.66667\n\nn= 150 \n\n     CP nsplit rel error xerror     xstd\n1 0.500      0      1.00   1.17 0.050735\n2 0.440      1      0.50   0.60 0.060000\n3 0.001      2      0.06   0.08 0.027520\n\n\nCode\n# Prune to optimal cp\nbest_cp &lt;- full_tree$cptable[which.min(full_tree$cptable[, \"xerror\"]), \"CP\"]\npruned_tree &lt;- prune(full_tree, cp = best_cp)\n\n\n\n\n\n\n\n\nFigure 29.29: Cross-validation error as a function of tree complexity. The dashed line shows one standard error above the minimum, often used to select a simpler tree.\n\n\n\n\n\n\n\n\n\n\n\nAdvantages and Disadvantages of Trees\n\n\n\nAdvantages: - Highly interpretable—easy to explain to non-statisticians - Handle both numeric and categorical predictors - Capture non-linear relationships and interactions automatically - Robust to outliers and don’t require feature scaling\nDisadvantages: - High variance—small changes in data can produce very different trees - Prone to overfitting without careful tuning - Axis-aligned splits can’t capture diagonal relationships efficiently - Generally lower predictive accuracy than ensemble methods",
    "crumbs": [
      "Statistical Learning",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Core Concepts in Statistical Learning</span>"
    ]
  },
  {
    "objectID": "chapters/22-statistical-learning.html#random-forests",
    "href": "chapters/22-statistical-learning.html#random-forests",
    "title": "29  Core Concepts in Statistical Learning",
    "section": "29.16 Random Forests",
    "text": "29.16 Random Forests\nRandom forests (Breiman 2001) are a very popular machine learning approach that addresses the shortcomings of decision trees using a clever idea. The goal is to improve prediction performance and reduce instability by averaging multiple decision trees (a forest of trees constructed with randomness). It has two features that help accomplish this.\nThe first step is bootstrap aggregation or bagging. The general idea is to generate many predictors, each using regression or classification trees, and then form a final prediction based on the average prediction of all these trees. To assure that the individual trees are not the same, we use the bootstrap to induce randomness. These two features combined explain the name: the bootstrap makes the individual trees randomly different, and the combination of trees is the forest.\n\nThe Random Forest Algorithm\nThe specific steps are:\n\nBuild \\(B\\) decision trees using the training set. We refer to the fitted models as \\(T_1, T_2, \\dots, T_B\\).\nFor every observation in the test set, form a prediction \\(\\hat{y}_j\\) using tree \\(T_j\\).\nFor continuous outcomes, form a final prediction with the average \\(\\hat{y} = \\frac{1}{B} \\sum_{j=1}^B \\hat{y}_j\\). For categorical data classification, predict \\(\\hat{y}\\) with majority vote (most frequent class among \\(\\hat{y}_1, \\dots, \\hat{y}_B\\)).\n\nTo create \\(T_j, \\, j=1,\\ldots,B\\) from the training set:\n\nCreate a bootstrap training set by sampling \\(N\\) observations from the training set with replacement. This is the first way to induce randomness.\nAt each split, consider only a random subset of \\(m\\) features (typically \\(m = \\sqrt{p}\\) for classification, \\(m = p/3\\) for regression). This reduces correlation between trees in the forest, thereby improving prediction accuracy.\n\nThe randomness serves two purposes: - Bagging reduces variance by averaging many noisy but unbiased trees - Random feature selection decorrelates the trees, making the average more effective\n\n\nWhy Averaging Produces Smooth Estimates\nA key insight is that the average of many step functions can be smooth. Let’s illustrate with the polls data:\n\n\nCode\npar(mfrow = c(1, 2))\n\n# Single tree (from before)\nfit_tree &lt;- rpart(margin ~ ., data = polls_2008)\npolls_2008 %&gt;%\n  mutate(y_hat = predict(fit_tree)) %&gt;%\n  with(plot(day, margin, pch = 16, cex = 0.5,\n            main = \"Single Regression Tree\"))\npolls_2008 %&gt;%\n  mutate(y_hat = predict(fit_tree)) %&gt;%\n  with(lines(day, y_hat, col = \"red\", type = \"s\", lwd = 2))\n\n# Random forest\nfit_rf &lt;- randomForest(margin ~ ., data = polls_2008)\n\n\nError in `randomForest()`:\n! could not find function \"randomForest\"\n\n\nCode\npolls_2008 %&gt;%\n  mutate(y_hat = predict(fit_rf, newdata = polls_2008)) %&gt;%\n  with(plot(day, margin, pch = 16, cex = 0.5,\n            main = \"Random Forest\"))\n\n\nError in `mutate()`:\nℹ In argument: `y_hat = predict(fit_rf, newdata = polls_2008)`.\nCaused by error:\n! object 'fit_rf' not found\n\n\nCode\npolls_2008 %&gt;%\n  mutate(y_hat = predict(fit_rf, newdata = polls_2008)) %&gt;%\n  with(lines(day, y_hat, col = \"blue\", lwd = 2))\n\n\nError in `mutate()`:\nℹ In argument: `y_hat = predict(fit_rf, newdata = polls_2008)`.\nCaused by error:\n! object 'fit_rf' not found\n\n\n\n\n\n\n\n\nFigure 29.30: Random forest predictions are much smoother than single trees because averaging many step functions produces a smooth curve\n\n\n\n\n\nNotice that the random forest estimate is much smoother than what we achieved with the single regression tree. This is possible because the average of many step functions can be smooth—each bootstrap sample produces a slightly different tree, and their average traces out a smooth curve.\n\n\nRandom Forests in R\n\n\nCode\nlibrary(randomForest)\nset.seed(42)\n\n# Fit random forest\nrf_model &lt;- randomForest(Species ~ ., data = iris,\n                          ntree = 500,       # Number of trees\n                          mtry = 2,          # Features tried at each split\n                          importance = TRUE)  # Calculate variable importance\n\n# Model summary\nprint(rf_model)\n\n\n\nCall:\n randomForest(formula = Species ~ ., data = iris, ntree = 500,      mtry = 2, importance = TRUE) \n               Type of random forest: classification\n                     Number of trees: 500\nNo. of variables tried at each split: 2\n\n        OOB estimate of  error rate: 4%\nConfusion matrix:\n           setosa versicolor virginica class.error\nsetosa         50          0         0        0.00\nversicolor      0         47         3        0.06\nvirginica       0          3        47        0.06\n\n\nCode\n# Plot error vs number of trees\nplot(rf_model, main = \"Random Forest: Error vs. Number of Trees\")\nlegend(\"topright\", colnames(rf_model$err.rate), col = 1:4, lty = 1:4)\n\n\n\n\n\n\n\n\nFigure 29.31: Random forest OOB error rate decreasing as more trees are added\n\n\n\n\n\n\n\nOut-of-Bag (OOB) Error\nEach bootstrap sample uses about 63% of observations. The remaining 37% (out-of-bag samples) provide a built-in test set:\n\n\nCode\n# OOB confusion matrix\nrf_model$confusion\n\n\n           setosa versicolor virginica class.error\nsetosa         50          0         0        0.00\nversicolor      0         47         3        0.06\nvirginica       0          3        47        0.06\n\n\nCode\n# OOB error rate\ncat(\"OOB Error Rate:\", round(1 - sum(diag(rf_model$confusion[,1:3])) /\n                              sum(rf_model$confusion[,1:3]), 3), \"\\n\")\n\n\nOOB Error Rate: 0.04 \n\n\nOOB error is nearly as accurate as cross-validation but comes “for free” during training.\n\n\nVariable Importance\nRandom forests provide measures of how important each predictor is:\n\n\nCode\n# Variable importance plot\nvarImpPlot(rf_model, main = \"Variable Importance\")\n\n# Numeric importance values\nimportance(rf_model)\n\n\n                setosa versicolor virginica MeanDecreaseAccuracy\nSepal.Length  6.202069  8.1714623  7.112845            10.837857\nSepal.Width   4.389926 -0.0394507  4.433121             4.505871\nPetal.Length 22.142310 32.6681049 28.411695            33.420235\nPetal.Width  22.452771 32.9325603 30.673079            33.808242\n             MeanDecreaseGini\nSepal.Length         9.273382\nSepal.Width          2.178884\nPetal.Length        43.873860\nPetal.Width         43.879867\n\n\n\n\n\n\n\n\nFigure 29.32: Variable importance from random forest: Mean Decrease Accuracy measures how much removing a variable hurts prediction; Mean Decrease Gini measures the total reduction in node impurity\n\n\n\n\n\nMean Decrease Accuracy: For each tree, predictions are made on OOB samples. Then the values of variable \\(j\\) are randomly permuted, and predictions are made again. The decrease in accuracy from permutation measures importance.\nMean Decrease Gini: Total decrease in Gini impurity from splits on variable \\(j\\), averaged over all trees.\n\n\nTuning Random Forests\nKey parameters to tune:\n\nntree: Number of trees (more is generally better, but with diminishing returns)\nmtry: Number of features considered at each split\nnodesize: Minimum size of terminal nodes\n\n\n\nCode\n# Tune mtry\noob_error &lt;- sapply(1:4, function(m) {\n  rf &lt;- randomForest(Species ~ ., data = iris, mtry = m, ntree = 200)\n  rf$err.rate[200, \"OOB\"]\n})\n\nplot(1:4, oob_error, type = \"b\", pch = 19,\n     xlab = \"mtry (features at each split)\",\n     ylab = \"OOB Error Rate\",\n     main = \"Tuning mtry Parameter\")\n\n\n\n\n\n\n\n\nFigure 29.33: Random forest OOB error as a function of mtry (number of features considered at each split)\n\n\n\n\n\n\n\nRandom Forest for Regression\n\n\nCode\n# Regression random forest\nset.seed(42)\nrf_reg &lt;- randomForest(mpg ~ ., data = mtcars, ntree = 500, importance = TRUE)\n\n# Performance\ncat(\"Variance explained:\", round(rf_reg$rsq[500] * 100, 1), \"%\\n\")\n\n\nVariance explained: 83.8 %\n\n\nCode\ncat(\"MSE:\", round(rf_reg$mse[500], 2), \"\\n\")\n\n\nMSE: 5.71 \n\n\nCode\n# Variable importance for regression\nvarImpPlot(rf_reg, main = \"Variable Importance for MPG Prediction\")",
    "crumbs": [
      "Statistical Learning",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Core Concepts in Statistical Learning</span>"
    ]
  },
  {
    "objectID": "chapters/22-statistical-learning.html#support-vector-machines-svm",
    "href": "chapters/22-statistical-learning.html#support-vector-machines-svm",
    "title": "29  Core Concepts in Statistical Learning",
    "section": "29.17 Support Vector Machines (SVM)",
    "text": "29.17 Support Vector Machines (SVM)\nSupport Vector Machines (Cortes and Vapnik 1995) find the hyperplane that best separates classes by maximizing the margin—the distance between the boundary and the nearest points from each class.\n\nThe Maximum Margin Classifier\nFor linearly separable data, infinitely many lines could separate the classes. SVM chooses the line with the largest margin:\n\n\nCode\nlibrary(e1071)\n\n# Create simple 2D data\nset.seed(42)\nn &lt;- 40\nx1 &lt;- c(rnorm(n/2, mean = 0), rnorm(n/2, mean = 3))\nx2 &lt;- c(rnorm(n/2, mean = 0), rnorm(n/2, mean = 3))\ny &lt;- factor(c(rep(-1, n/2), rep(1, n/2)))\nsvm_data &lt;- data.frame(x1, x2, y)\n\n# Fit linear SVM\nsvm_linear &lt;- svm(y ~ x1 + x2, data = svm_data, kernel = \"linear\",\n                   cost = 10, scale = FALSE)\n\n# Plot\nplot(svm_linear, svm_data, x1 ~ x2,\n     col = c(\"lightblue\", \"lightpink\"),\n     symbolPalette = c(\"blue\", \"red\"),\n     svSymbol = \"x\", dataSymbol = \"o\")\n\n\n\n\n\n\n\n\nFigure 29.34: Support Vector Machine concept: the decision boundary (solid line) maximizes the margin (distance to nearest points). Support vectors are the points on the margin boundaries.\n\n\n\n\n\n\n\nSoft Margin and the Cost Parameter\nReal data is rarely perfectly separable. Soft margin SVM allows some points to violate the margin, controlled by the cost parameter \\(C\\):\n\nHigh C: Small margin, few violations (may overfit)\nLow C: Large margin, more violations (may underfit)\n\n\n\nCode\npar(mfrow = c(1, 3))\n\nfor (cost_val in c(0.1, 1, 100)) {\n  svm_fit &lt;- svm(y ~ x1 + x2, data = svm_data, kernel = \"linear\",\n                  cost = cost_val, scale = FALSE)\n  plot(svm_fit, svm_data, x1 ~ x2,\n       col = c(\"lightblue\", \"lightpink\"),\n       main = paste(\"Cost =\", cost_val))\n}\n\n\nError in `plot.new()`:\n! figure margins too large\n\n\n\n\nThe Kernel Trick\nFor non-linear boundaries, SVM uses kernels to implicitly map data to higher dimensions where classes become linearly separable:\nCommon kernels:\n\nLinear: \\(K(x, x') = x \\cdot x'\\) (no transformation)\nPolynomial: \\(K(x, x') = (1 + x \\cdot x')^d\\)\nRadial Basis Function (RBF): \\(K(x, x') = \\exp(-\\gamma \\|x - x'\\|^2)\\)\n\n\n\nCode\n# Create non-linear data\nset.seed(123)\nn &lt;- 200\nr1 &lt;- runif(n/2, 0, 2)\ntheta1 &lt;- runif(n/2, 0, 2*pi)\nr2 &lt;- runif(n/2, 3, 5)\ntheta2 &lt;- runif(n/2, 0, 2*pi)\n\nx1 &lt;- c(r1 * cos(theta1), r2 * cos(theta2))\nx2 &lt;- c(r1 * sin(theta1), r2 * sin(theta2))\ny &lt;- factor(c(rep(\"inner\", n/2), rep(\"outer\", n/2)))\ncircle_data &lt;- data.frame(x1, x2, y)\n\npar(mfrow = c(1, 3))\n\n# Linear (fails)\nsvm_lin &lt;- svm(y ~ ., data = circle_data, kernel = \"linear\")\nplot(svm_lin, circle_data, col = c(\"lightblue\", \"lightpink\"),\n     main = \"Linear Kernel\")\n\n\nError in `plot.new()`:\n! figure margins too large\n\n\nCode\n# Polynomial\nsvm_poly &lt;- svm(y ~ ., data = circle_data, kernel = \"polynomial\", degree = 2)\nplot(svm_poly, circle_data, col = c(\"lightblue\", \"lightpink\"),\n     main = \"Polynomial Kernel (d=2)\")\n\n\nError in `plot.new()`:\n! figure margins too large\n\n\nCode\n# RBF\nsvm_rbf &lt;- svm(y ~ ., data = circle_data, kernel = \"radial\", gamma = 0.5)\nplot(svm_rbf, circle_data, col = c(\"lightblue\", \"lightpink\"),\n     main = \"RBF Kernel\")\n\n\nError in `plot.new()`:\n! figure margins too large\n\n\n\n\nTuning SVM with Cross-Validation\nThe key parameters to tune are: - cost: Penalty for margin violations - gamma: For RBF kernel, controls the “reach” of each training example\n\n\nCode\n# Tune SVM using cross-validation\nset.seed(42)\ntune_result &lt;- tune(svm, Species ~ ., data = iris,\n                     kernel = \"radial\",\n                     ranges = list(\n                       cost = c(0.1, 1, 10, 100),\n                       gamma = c(0.01, 0.1, 0.5, 1)\n                     ))\n\n# Best parameters\ncat(\"Best parameters:\\n\")\n\n\nBest parameters:\n\n\nCode\nprint(tune_result$best.parameters)\n\n\n  cost gamma\n4  100  0.01\n\n\nCode\n# Best model performance\ncat(\"\\nBest model error:\", round(tune_result$best.performance, 3), \"\\n\")\n\n\n\nBest model error: 0.027 \n\n\nCode\n# Use best model\nbest_svm &lt;- tune_result$best.model\ntable(Predicted = predict(best_svm), Actual = iris$Species)\n\n\n            Actual\nPredicted    setosa versicolor virginica\n  setosa         50          0         0\n  versicolor      0         45         0\n  virginica       0          5        50\n\n\n\n\nMulticlass SVM\nSVM is inherently binary, but extends to multiple classes via:\n\nOne-vs-One: Fit \\(\\binom{K}{2}\\) classifiers for all pairs of classes; classify by voting\nOne-vs-All: Fit \\(K\\) classifiers (each class vs. rest); classify to highest-scoring class\n\nR’s svm() uses one-vs-one by default.\n\n\nSupport Vector Regression (SVR)\nSVMs can also be used for regression problems. Support Vector Regression works by fitting a tube of width \\(\\epsilon\\) around the data—points inside the tube contribute no loss, while points outside are penalized.\nThe key idea is that instead of minimizing squared errors (like in linear regression), SVR minimizes how much predictions deviate beyond a tolerance margin \\(\\epsilon\\):\n\\[\nL_\\epsilon(y, \\hat{y}) = \\begin{cases} 0 & \\text{if } |y - \\hat{y}| \\leq \\epsilon \\\\ |y - \\hat{y}| - \\epsilon & \\text{otherwise} \\end{cases}\n\\]\nThis is called the \\(\\epsilon\\)-insensitive loss function.\n\n\nCode\n# Generate non-linear data\nset.seed(42)\nn &lt;- 100\nx &lt;- seq(0, 4*pi, length.out = n)\ny &lt;- sin(x) + rnorm(n, sd = 0.3)\nsvr_data &lt;- data.frame(x = x, y = y)\n\n# Fit SVR with RBF kernel\nsvr_model &lt;- svm(y ~ x, data = svr_data, kernel = \"radial\",\n                  epsilon = 0.3, cost = 10)\n\n# Predictions\nsvr_data$pred &lt;- predict(svr_model, svr_data)\n\n# Plot\nplot(x, y, pch = 16, col = \"gray60\", main = \"Support Vector Regression\")\nlines(x, svr_data$pred, col = \"blue\", lwd = 2)\nlines(x, sin(x), col = \"red\", lwd = 2, lty = 2)\nlegend(\"topright\", c(\"SVR fit\", \"True function\"),\n       col = c(\"blue\", \"red\"), lty = c(1, 2), lwd = 2)\n\n\n\n\n\n\n\n\nFigure 29.35: Support Vector Regression fits a tube around the data. Points within the tube (width epsilon) have zero loss.\n\n\n\n\n\nKey SVR parameters:\n\nepsilon: Width of the insensitive tube. Larger values give smoother fits.\ncost (C): Penalty for points outside the tube. Higher C fits the data more closely.\nkernel: As with classification, RBF kernels can capture non-linear patterns.\n\n\n\nCode\npar(mfrow = c(1, 3))\n\nfor (eps in c(0.1, 0.3, 0.5)) {\n  svr_fit &lt;- svm(y ~ x, data = svr_data, kernel = \"radial\",\n                  epsilon = eps, cost = 10)\n  plot(x, y, pch = 16, col = \"gray60\", main = paste(\"epsilon =\", eps))\n  lines(x, predict(svr_fit), col = \"blue\", lwd = 2)\n}\n\n\n\n\n\n\n\n\nFigure 29.36: Effect of epsilon on SVR: larger epsilon creates wider tubes and smoother fits\n\n\n\n\n\n\n\n\n\n\n\nSVM vs. Other Methods\n\n\n\nAdvantages of SVM: - Effective in high-dimensional spaces (even when dimensions &gt; samples) - Memory efficient (uses only support vectors) - Versatile through different kernels\nDisadvantages: - Doesn’t provide probability estimates directly (though they can be computed) - Sensitive to feature scaling—always standardize! - Can be slow on very large datasets - Kernel and parameter selection can be tricky",
    "crumbs": [
      "Statistical Learning",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Core Concepts in Statistical Learning</span>"
    ]
  },
  {
    "objectID": "chapters/22-statistical-learning.html#comparing-classification-methods",
    "href": "chapters/22-statistical-learning.html#comparing-classification-methods",
    "title": "29  Core Concepts in Statistical Learning",
    "section": "29.18 Comparing Classification Methods",
    "text": "29.18 Comparing Classification Methods\nDifferent methods have different strengths:\n\n\n\n\n\n\n\n\n\n\nMethod\nInterpretability\nHandles Non-linearity\nSpeed\nBest For\n\n\n\n\nkNN\nLow\nYes (inherently)\nSlow for large data\nSimple problems, few features\n\n\nDecision Tree\nHigh\nYes\nFast\nInterpretability needed\n\n\nRandom Forest\nMedium\nYes\nModerate\nGeneral purpose, variable importance\n\n\nSVM\nLow\nYes (with kernels)\nModerate\nHigh-dimensional data\n\n\nLogistic Regression\nHigh\nNo (needs feature engineering)\nFast\nProbability estimates, inference",
    "crumbs": [
      "Statistical Learning",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Core Concepts in Statistical Learning</span>"
    ]
  },
  {
    "objectID": "chapters/22-statistical-learning.html#practical-workflow",
    "href": "chapters/22-statistical-learning.html#practical-workflow",
    "title": "29  Core Concepts in Statistical Learning",
    "section": "29.19 Practical Workflow",
    "text": "29.19 Practical Workflow\nA typical statistical learning workflow:\n\nSplit data into training and test sets\nExplore the training data\nBuild candidate models with different algorithms or parameters\nEvaluate using cross-validation on training data\nSelect the best model\nFinal evaluation on held-out test data\nReport honest estimates of performance\n\nNever use test data for model building or selection—that defeats the purpose of holding it out.",
    "crumbs": [
      "Statistical Learning",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Core Concepts in Statistical Learning</span>"
    ]
  },
  {
    "objectID": "chapters/22-statistical-learning.html#when-to-use-statistical-learning",
    "href": "chapters/22-statistical-learning.html#when-to-use-statistical-learning",
    "title": "29  Core Concepts in Statistical Learning",
    "section": "29.20 When to Use Statistical Learning",
    "text": "29.20 When to Use Statistical Learning\nStatistical learning excels when: - Prediction is the primary goal - Relationships are complex or non-linear - You have substantial data - Interpretability is less critical\nTraditional statistical methods may be preferable when: - Understanding relationships matters more than prediction - Sample sizes are small - You need confidence intervals and hypothesis tests - Interpretability is essential",
    "crumbs": [
      "Statistical Learning",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Core Concepts in Statistical Learning</span>"
    ]
  },
  {
    "objectID": "chapters/22-statistical-learning.html#connection-to-dimensionality-reduction",
    "href": "chapters/22-statistical-learning.html#connection-to-dimensionality-reduction",
    "title": "29  Core Concepts in Statistical Learning",
    "section": "29.21 Connection to Dimensionality Reduction",
    "text": "29.21 Connection to Dimensionality Reduction\nHigh-dimensional data often benefit from dimensionality reduction before applying statistical learning methods. Techniques like PCA, clustering, and discriminant analysis are covered in detail in Chapter 33.",
    "crumbs": [
      "Statistical Learning",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Core Concepts in Statistical Learning</span>"
    ]
  },
  {
    "objectID": "chapters/22-statistical-learning.html#exercises",
    "href": "chapters/22-statistical-learning.html#exercises",
    "title": "29  Core Concepts in Statistical Learning",
    "section": "29.22 Exercises",
    "text": "29.22 Exercises\n\n\n\n\n\n\nExercise SL.1: Decision Trees and Random Forests\n\n\n\n\nCreate a simple dataset where the outcome grows 0.75 units on average for every increase in a predictor:\n\n\n\nCode\nn &lt;- 1000\nsigma &lt;- 0.25\nx &lt;- rnorm(n, 0, 1)\ny &lt;- 0.75 * x + rnorm(n, 0, sigma)\ndat &lt;- data.frame(x = x, y = y)\n\n\nUse rpart to fit a regression tree and save the result to fit.\n\nPlot the final tree so that you can see where the partitions occurred.\nMake a scatterplot of y versus x along with the predicted values based on the fit.\nNow model with a random forest instead of a regression tree using randomForest from the randomForest package, and remake the scatterplot with the prediction line.\nUse the function plot to see if the random forest has converged or if we need more trees.\nIt seems that the default values for the random forest result in an estimate that is too flexible (not smooth). Re-run the random forest but this time with nodesize set at 50 and maxnodes set at 25. Remake the plot.\nWe see that this yields smoother results. Let’s use the train function to help us pick these values. From the caret manual we see that we can’t tune the maxnodes parameter or the nodesize argument with randomForest, so we will use the Rborist package and tune the minNode argument. Use the train function to try values minNode &lt;- seq(5, 250, 25). See which value minimizes the estimated RMSE.\nMake a scatterplot along with the prediction from the best fitted model.\n\n\n\n\n\n\n\n\n\nExercise SL.2: Classification Trees\n\n\n\n\nUse the rpart function to fit a classification tree to the tissue_gene_expression dataset. Use the train function to estimate the accuracy. Try out cp values of seq(0, 0.05, 0.01). Plot the accuracy to report the results of the best model.\n\n\n\nCode\nlibrary(dslabs)\ndata(\"tissue_gene_expression\")\n\n\n\nStudy the confusion matrix for the best fitting classification tree. What do you observe happening for placenta?\nNotice that placentas are called endometrium more often than placenta. Note also that the number of placentas is just six, and that, by default, rpart requires 20 observations before splitting a node. Thus it is not possible with these parameters to have a node in which placentas are the majority. Rerun the above analysis but this time permit rpart to split any node by using the argument control = rpart.control(minsplit = 0). Does the accuracy increase? Look at the confusion matrix again.\nPlot the tree from the best fitting model obtained in exercise 11.\nWe can see that with just six genes, we are able to predict the tissue type. Now let’s see if we can do even better with a random forest. Use the train function and the rf method to train a random forest. Try out values of mtry ranging from, at least, seq(50, 200, 25). What mtry value maximizes accuracy? To permit small nodesize to grow as we did with the classification trees, use the following argument: nodesize = 1. This will take several seconds to run. If you want to test it out, try using smaller values with ntree. Set the seed to 1990.\nUse the function varImp on the output of train and save it to an object called imp.\nThe rpart model we ran above produced a tree that used just six predictors. Extracting the predictor names is not straightforward, but can be done. If the output of the call to train was fit_rpart, we can extract the names like this:\n\n\n\nCode\nind &lt;- !(fit_rpart$finalModel$frame$var == \"&lt;leaf&gt;\")\ntree_terms &lt;-\n  fit_rpart$finalModel$frame$var[ind] %&gt;%\n  unique() %&gt;%\n  as.character()\ntree_terms\n\n\nWhat is the variable importance in the random forest call for these predictors? Where do they rank?\n\nAdvanced: Extract the top 50 predictors based on importance, take a subset of x with just these predictors and apply the function heatmap to see how these genes behave across the tissues. We will introduce the heatmap function in Chapter 32.",
    "crumbs": [
      "Statistical Learning",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Core Concepts in Statistical Learning</span>"
    ]
  },
  {
    "objectID": "chapters/22-statistical-learning.html#summary",
    "href": "chapters/22-statistical-learning.html#summary",
    "title": "29  Core Concepts in Statistical Learning",
    "section": "29.23 Summary",
    "text": "29.23 Summary\nStatistical learning provides powerful tools for prediction and pattern discovery:\n\nOverfitting is the central challenge—models that fit training data too well predict poorly\nLoss functions quantify prediction error (squared loss for regression, log loss for classification)\nCross-validation provides honest estimates of predictive performance\n\nTraining error is always optimistic; test error reveals true performance\nK-fold CV and bootstrap estimate generalization error\n\nThe bias-variance tradeoff governs model complexity choices\nRegularization (ridge, lasso, elastic net) controls overfitting by penalizing model complexity\n\nRidge shrinks coefficients but keeps all predictors\nLasso performs variable selection by shrinking some coefficients to zero\nCross-validation selects the optimal regularization strength\n\nSmoothing methods estimate flexible curves from data\n\nBin smoothing divides data into intervals\nKernel smoothing uses weighted averages for continuous estimates\nSplines fit piecewise polynomials with controlled smoothness\nLOESS fits local regressions weighted by distance\n\nK-nearest neighbors illustrates how hyperparameters control model complexity\nDecision trees (CART) recursively partition data using simple rules\n\nHighly interpretable but prone to overfitting\nControlled via complexity parameters and pruning\n\nRandom forests combine many trees for robust predictions\n\nBagging and random feature selection reduce variance\nOut-of-bag error provides built-in validation\nVariable importance measures identify key predictors\n\nSupport vector machines find maximum-margin decision boundaries\n\nKernel trick enables non-linear classification\nSupport vector regression (SVR) extends to continuous outcomes\nEffective in high-dimensional spaces\n\nConfusion matrices summarize classification performance with metrics like accuracy, sensitivity, and precision\nF1 score and balanced accuracy are better metrics for imbalanced data\nROC curves and AUC allow comparison of classifiers across all thresholds\nPrecision-recall curves are preferred for highly imbalanced problems\nThe choice between traditional statistics and machine learning depends on goals",
    "crumbs": [
      "Statistical Learning",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Core Concepts in Statistical Learning</span>"
    ]
  },
  {
    "objectID": "chapters/22-statistical-learning.html#additional-resources",
    "href": "chapters/22-statistical-learning.html#additional-resources",
    "title": "29  Core Concepts in Statistical Learning",
    "section": "29.24 Additional Resources",
    "text": "29.24 Additional Resources\n\nJames et al. (2023) - The standard introduction to statistical learning\nThulin (2025) - Modern perspectives on statistics with R\nCrawley (2007) - Practical statistical methods in R\n\n\n\n\n\n\n\nBreiman, Leo. 2001. “Random Forests.” Machine Learning 45 (1): 5–32.\n\n\nCleveland, William S. 1979. “Robust Locally Weighted Regression and Smoothing Scatterplots.” Journal of the American Statistical Association 74 (368): 829–36.\n\n\nCortes, Corinna, and Vladimir Vapnik. 1995. “Support-Vector Networks.” Machine Learning 20 (3): 273–97.\n\n\nCrawley, Michael J. 2007. The r Book. John Wiley & Sons.\n\n\nHastie, Trevor, Robert Tibshirani, and Jerome Friedman. 2009. The Elements of Statistical Learning: Data Mining, Inference, and Prediction. 2nd ed. New York: Springer.\n\n\nHoerl, Arthur E., and Robert W. Kennard. 1970. “Ridge Regression: Biased Estimation for Nonorthogonal Problems.” Technometrics 12 (1): 55–67.\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2023. An Introduction to Statistical Learning with Applications in r. 2nd ed. Springer. https://www.statlearning.com.\n\n\nThulin, Måns. 2025. Modern Statistics with r. CRC Press. https://www.modernstatisticswithr.com.\n\n\nTibshirani, Robert. 1996. “Regression Shrinkage and Selection via the Lasso.” Journal of the Royal Statistical Society: Series B (Methodological) 58 (1): 267–88.",
    "crumbs": [
      "Statistical Learning",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Core Concepts in Statistical Learning</span>"
    ]
  },
  {
    "objectID": "chapters/31-bayesian-statistics.html",
    "href": "chapters/31-bayesian-statistics.html",
    "title": "30  Bayesian Statistics",
    "section": "",
    "text": "30.1 A Different Approach to Probability\nWhat does it mean when an election forecaster tells us that a candidate has a 90% chance of winning? In classical (frequentist) statistics, probability refers to long-run frequencies of events. A parameter like the true proportion of voters supporting a candidate is a fixed but unknown number—it doesn’t make sense to assign it a probability.\nBayesian statistics takes a different view: probability represents our degree of belief about uncertain quantities. Under this framework, we can legitimately say “there is a 90% probability that candidate A will win” because we’re expressing our uncertainty about the outcome.\nThis chapter introduces Bayesian thinking and shows how it provides a principled framework for combining prior knowledge with observed data.",
    "crumbs": [
      "Statistical Learning",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Bayesian Statistics</span>"
    ]
  },
  {
    "objectID": "chapters/31-bayesian-statistics.html#bayes-theorem-the-foundation",
    "href": "chapters/31-bayesian-statistics.html#bayes-theorem-the-foundation",
    "title": "30  Bayesian Statistics",
    "section": "30.2 Bayes’ Theorem: The Foundation",
    "text": "30.2 Bayes’ Theorem: The Foundation\nBayes’ theorem relates conditional probabilities:\n\\[\nP(A \\mid B) = \\frac{P(B \\mid A) \\cdot P(A)}{P(B)}\n\\]\nThis simple equation has profound implications. It tells us how to update our beliefs about \\(A\\) after observing evidence \\(B\\).\n\nA Diagnostic Test Example\nConsider a test for cystic fibrosis with 99% accuracy:\n\\[\nP(+ \\mid D=1) = 0.99 \\quad \\text{and} \\quad P(- \\mid D=0) = 0.99\n\\]\nwhere \\(+\\) denotes a positive test result and \\(D\\) indicates disease status (1 = has disease, 0 = doesn’t).\nIf a randomly selected person tests positive, what is the probability they actually have the disease?\nThe cystic fibrosis rate is approximately 1 in 3,900, so \\(P(D=1) = 0.00025\\).\nApplying Bayes’ theorem:\n\\[\n\\begin{aligned}\nP(D=1 \\mid +) &= \\frac{P(+ \\mid D=1) \\cdot P(D=1)}{P(+)} \\\\[1em]\n&= \\frac{P(+ \\mid D=1) \\cdot P(D=1)}{P(+ \\mid D=1) \\cdot P(D=1) + P(+ \\mid D=0) \\cdot P(D=0)}\n\\end{aligned}\n\\]\nPlugging in the numbers:\n\\[\nP(D=1 \\mid +) = \\frac{0.99 \\times 0.00025}{0.99 \\times 0.00025 + 0.01 \\times 0.99975} = 0.024\n\\]\nDespite the test having 99% accuracy, the probability of having the disease given a positive test is only about 2.4%. This counterintuitive result arises because the disease is so rare—most positive tests are false positives from the large healthy population.\n\n\nSimulation to Verify\n\n\nCode\nset.seed(42)\nprev &lt;- 0.00025\nN &lt;- 100000\naccuracy &lt;- 0.99\n\n# Randomly assign disease status\noutcome &lt;- sample(c(\"Disease\", \"Healthy\"), N, replace = TRUE,\n                  prob = c(prev, 1 - prev))\n\nN_D &lt;- sum(outcome == \"Disease\")\nN_H &lt;- sum(outcome == \"Healthy\")\n\ncat(\"People with disease:\", N_D, \"\\n\")\n\n\nPeople with disease: 28 \n\n\nCode\ncat(\"Healthy people:\", N_H, \"\\n\")\n\n\nHealthy people: 99972 \n\n\nCode\n# Administer test\ntest &lt;- vector(\"character\", N)\ntest[outcome == \"Disease\"] &lt;- sample(c(\"+\", \"-\"), N_D, replace = TRUE,\n                                      prob = c(accuracy, 1 - accuracy))\ntest[outcome == \"Healthy\"] &lt;- sample(c(\"-\", \"+\"), N_H, replace = TRUE,\n                                      prob = c(accuracy, 1 - accuracy))\n\n# Results\nconfusion &lt;- table(outcome, test)\nprint(confusion)\n\n\n         test\noutcome       -     +\n  Disease     0    28\n  Healthy 98948  1024\n\n\nCode\n# Probability of disease given positive test\ncat(\"\\nP(Disease | Positive test):\",\n    round(sum(test == \"+\" & outcome == \"Disease\") / sum(test == \"+\"), 3), \"\\n\")\n\n\n\nP(Disease | Positive test): 0.027 \n\n\nThe simulation confirms our calculation: despite the high test accuracy, most people who test positive are actually healthy because the disease is so rare.\n\n\n\n\n\n\nThe Base Rate Fallacy\n\n\n\nIgnoring the base rate (prior probability) of a condition leads to dramatically wrong conclusions. This is critical in medical testing, forensic evidence, and any diagnostic situation. Always consider:\n\nHow accurate is the test? (sensitivity, specificity)\nHow common is the condition? (base rate/prevalence)\nWhat population is being tested? (targeted vs. general screening)",
    "crumbs": [
      "Statistical Learning",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Bayesian Statistics</span>"
    ]
  },
  {
    "objectID": "chapters/31-bayesian-statistics.html#from-bayes-theorem-to-bayesian-inference",
    "href": "chapters/31-bayesian-statistics.html#from-bayes-theorem-to-bayesian-inference",
    "title": "30  Bayesian Statistics",
    "section": "30.3 From Bayes’ Theorem to Bayesian Inference",
    "text": "30.3 From Bayes’ Theorem to Bayesian Inference\nBayes’ theorem becomes a framework for statistical inference when we apply it to unknown parameters:\n\\[\nP(\\theta \\mid \\text{data}) = \\frac{P(\\text{data} \\mid \\theta) \\cdot P(\\theta)}{P(\\text{data})}\n\\]\nThe components have specific names:\n\n\n\n\n\n\n\n\nTerm\nName\nMeaning\n\n\n\n\n\\(P(\\theta \\mid \\text{data})\\)\nPosterior\nUpdated belief after seeing data\n\n\n\\(P(\\text{data} \\mid \\theta)\\)\nLikelihood\nProbability of data given parameter\n\n\n\\(P(\\theta)\\)\nPrior\nInitial belief before seeing data\n\n\n\\(P(\\text{data})\\)\nEvidence\nNormalizing constant\n\n\n\nThe posterior combines what we knew before (prior) with what the data tell us (likelihood).",
    "crumbs": [
      "Statistical Learning",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Bayesian Statistics</span>"
    ]
  },
  {
    "objectID": "chapters/31-bayesian-statistics.html#hierarchical-models-shrinkage-and-borrowing-strength",
    "href": "chapters/31-bayesian-statistics.html#hierarchical-models-shrinkage-and-borrowing-strength",
    "title": "30  Bayesian Statistics",
    "section": "30.4 Hierarchical Models: Shrinkage and Borrowing Strength",
    "text": "30.4 Hierarchical Models: Shrinkage and Borrowing Strength\nOne of Bayesian statistics’ most powerful applications is hierarchical modeling, where we model variability at multiple levels. This allows us to “borrow strength” across related observations.\n\nA Baseball Example\nConsider José Iglesias, a baseball player who in April 2013 had the following statistics:\n\n\n\nMonth\nAt Bats\nHits\nAVG\n\n\n\n\nApril\n20\n9\n.450\n\n\n\nA batting average (AVG) of .450 is extraordinarily high—no player has finished a season above .400 since Ted Williams in 1941. How should we predict José’s season-ending average?\nThe frequentist approach uses only José’s data. With \\(p = 0.450\\) as our estimate and \\(n = 20\\) at bats:\n\\[\n\\text{SE} = \\sqrt{\\frac{0.450 \\times 0.550}{20}} = 0.111\n\\]\nA 95% confidence interval is \\(.450 \\pm 0.222\\), or roughly \\(.228\\) to \\(.672\\). This interval is wide and centered at .450, implying our best guess is that José will break Ted Williams’ record.\nThe Bayesian approach incorporates what we know about baseball players in general.\n\n\nThe Prior: What Do We Know About Batting Averages?\nLet’s examine batting averages for players with substantial at-bats over recent seasons:\n\n\nCode\nlibrary(Lahman)\n\n\nError in `library()`:\n! there is no package called 'Lahman'\n\n\nCode\nbatting_data &lt;- Batting |&gt;\n  filter(yearID %in% 2010:2012) |&gt;\n  mutate(AVG = H / AB) |&gt;\n  filter(AB &gt; 500)\n\n\nError:\n! object 'Batting' not found\n\n\nCode\nbatting_data |&gt;\n  ggplot(aes(AVG)) +\n  geom_histogram(bins = 30, fill = \"steelblue\", color = \"white\") +\n  facet_wrap(~yearID) +\n  labs(x = \"Batting Average\", y = \"Count\",\n       title = \"MLB Batting Averages by Year\")\n\n\nError:\n! object 'batting_data' not found\n\n\nCode\n# Summary statistics\ncat(\"Mean batting average:\", round(mean(batting_data$AVG), 3), \"\\n\")\n\n\nError:\n! object 'batting_data' not found\n\n\nCode\ncat(\"SD of batting averages:\", round(sd(batting_data$AVG), 3), \"\\n\")\n\n\nError:\n! object 'batting_data' not found\n\n\nThe average player has an AVG around .275 with a standard deviation of about 0.027. José’s .450 is over six standard deviations above the mean—an extreme anomaly if taken at face value.\n\n\nThe Hierarchical Model\nWe model the data at two levels:\nLevel 1 (Prior): Each player has a “true” batting ability \\(p\\) drawn from a population distribution: \\[\np \\sim N(\\mu, \\tau^2)\n\\]\nLevel 2 (Likelihood): Given ability \\(p\\), observed performance \\(Y\\) varies due to luck: \\[\nY \\mid p \\sim N(p, \\sigma^2)\n\\]\nFrom the data: - Population mean: \\(\\mu = 0.275\\) - Population SD: \\(\\tau = 0.027\\) - Individual sampling error: \\(\\sigma = \\sqrt{p(1-p)/n} \\approx 0.111\\)\n\n\nThe Posterior: Combining Prior and Data\nFor this model, the posterior distribution has a beautiful closed form. The posterior mean is:\n\\[\nE(p \\mid Y = y) = B \\cdot \\mu + (1-B) \\cdot y\n\\]\nwhere the shrinkage factor \\(B\\) is:\n\\[\nB = \\frac{\\sigma^2}{\\sigma^2 + \\tau^2}\n\\]\nThis is a weighted average of the population mean \\(\\mu\\) and observed data \\(y\\). The weights depend on: - How much individual variation there is (\\(\\sigma\\)) - How much population variation there is (\\(\\tau\\))\nWhen individual uncertainty is high relative to population variation, we “shrink” more toward the population mean.\n\n\nApplying to José Iglesias\n\n\nCode\n# Parameters\nmu &lt;- 0.275      # Population mean\ntau &lt;- 0.027     # Population SD\nsigma &lt;- 0.111   # José's sampling error (from 20 at-bats)\ny &lt;- 0.450       # José's observed average\n\n# Shrinkage factor\nB &lt;- sigma^2 / (sigma^2 + tau^2)\ncat(\"Shrinkage factor B:\", round(B, 3), \"\\n\")\n\n\nShrinkage factor B: 0.944 \n\n\nCode\n# Posterior mean\nposterior_mean &lt;- B * mu + (1 - B) * y\ncat(\"Posterior mean (predicted true ability):\", round(posterior_mean, 3), \"\\n\")\n\n\nPosterior mean (predicted true ability): 0.285 \n\n\nCode\n# Posterior standard error\nposterior_se &lt;- sqrt(1 / (1/sigma^2 + 1/tau^2))\ncat(\"Posterior SE:\", round(posterior_se, 3), \"\\n\")\n\n\nPosterior SE: 0.026 \n\n\nCode\n# 95% credible interval\nci_lower &lt;- posterior_mean - 1.96 * posterior_se\nci_upper &lt;- posterior_mean + 1.96 * posterior_se\ncat(\"95% credible interval: [\", round(ci_lower, 3), \",\", round(ci_upper, 3), \"]\\n\")\n\n\n95% credible interval: [ 0.233 , 0.336 ]\n\n\nThe Bayesian estimate “shrinks” José’s .450 toward the population average, giving a predicted true ability of about .285. The 95% credible interval is much narrower than the frequentist interval.\n\n\nThe Outcome\nHere’s how José actually performed through the season:\n\n\n\nMonth\nAt Bats\nHits\nAVG\n\n\n\n\nApril\n20\n9\n.450\n\n\nMay\n26\n11\n.423\n\n\nJune\n86\n34\n.395\n\n\nJuly\n83\n17\n.205\n\n\nAugust\n85\n25\n.294\n\n\nSeptember\n50\n10\n.200\n\n\nTotal (w/o April)\n330\n97\n.294\n\n\n\nHis final average (excluding April) was .294—almost exactly what the Bayesian model predicted! The hierarchical approach correctly recognized that his early performance was partially luck and regressed his prediction toward the population mean.\n\n\nCode\n# Visualize shrinkage\nobservations &lt;- c(0.450, 0.320, 0.260, 0.200)\nlabels &lt;- c(\"José (Apr)\", \"Good hitter\", \"Average\", \"Struggling\")\n\n# Calculate posteriors for each\nposteriors &lt;- B * mu + (1 - B) * observations\n\n# Plot\nplot(1:4, observations, pch = 19, cex = 2, col = \"blue\",\n     ylim = c(0.15, 0.50), xlim = c(0.5, 4.5),\n     xaxt = \"n\", xlab = \"\", ylab = \"Batting Average\",\n     main = \"Bayesian Shrinkage: Observations → Posteriors\")\naxis(1, at = 1:4, labels = labels, las = 2)\n\npoints(1:4, posteriors, pch = 17, cex = 2, col = \"red\")\narrows(1:4, observations, 1:4, posteriors,\n       length = 0.1, col = \"gray50\", lwd = 2)\nabline(h = mu, lty = 2, col = \"darkgreen\", lwd = 2)\n\nlegend(\"topright\",\n       c(\"Observed\", \"Posterior estimate\", \"Population mean\"),\n       pch = c(19, 17, NA), lty = c(NA, NA, 2),\n       col = c(\"blue\", \"red\", \"darkgreen\"), pt.cex = 1.5)\n\n\n\n\n\n\n\n\nFigure 30.1: Bayesian shrinkage pulls extreme observations toward the population mean\n\n\n\n\n\nNotice how the shrinkage is proportional to how extreme the observation is. José’s .450 shrinks dramatically, while the average player’s .260 barely moves.",
    "crumbs": [
      "Statistical Learning",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Bayesian Statistics</span>"
    ]
  },
  {
    "objectID": "chapters/31-bayesian-statistics.html#credible-intervals-vs.-confidence-intervals",
    "href": "chapters/31-bayesian-statistics.html#credible-intervals-vs.-confidence-intervals",
    "title": "30  Bayesian Statistics",
    "section": "30.5 Credible Intervals vs. Confidence Intervals",
    "text": "30.5 Credible Intervals vs. Confidence Intervals\nBayesian inference produces credible intervals rather than confidence intervals. The interpretation differs:\n\n\n\n\n\n\n\n\n\nFrequentist Confidence Interval\nBayesian Credible Interval\n\n\n\n\nStatement\n“95% of intervals from repeated sampling would contain the true value”\n“There is a 95% probability the true value lies in this interval”\n\n\nParameter\nFixed but unknown\nRandom variable with a distribution\n\n\nInterpretation\nAbout the procedure\nAbout the parameter\n\n\n\nThe Bayesian interpretation is often more intuitive: we can directly say “there’s a 95% probability the true batting average is between .233 and .337.”",
    "crumbs": [
      "Statistical Learning",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Bayesian Statistics</span>"
    ]
  },
  {
    "objectID": "chapters/31-bayesian-statistics.html#choosing-priors",
    "href": "chapters/31-bayesian-statistics.html#choosing-priors",
    "title": "30  Bayesian Statistics",
    "section": "30.6 Choosing Priors",
    "text": "30.6 Choosing Priors\nThe prior distribution encodes what we believe before seeing data. Prior choice is both a strength (incorporating domain knowledge) and a criticism (subjectivity) of Bayesian methods.\n\nTypes of Priors\nPriors fall along a spectrum of informativeness. Informative priors incorporate substantial prior knowledge—for example, a prior for batting average based on historical data might be N(0.275, 0.027²), reflecting that most players hit between .220 and .330. Weakly informative priors provide gentle regularization without imposing strong beliefs; a prior of N(0, 10²) for a regression coefficient allows a wide range of values while preferring smaller effects over extremely large ones. Finally, non-informative or diffuse priors attempt to “let the data speak” by imposing minimal prior constraints—for instance, a uniform prior on [0, 1] for a probability parameter, or an improper prior where P(θ) ∝ 1 across the entire real line.\n\n\n\n\n\n\nEmpirical Bayes\n\n\n\nWhen we estimate prior parameters from data (as we did with batting averages), this is called empirical Bayes. It’s a practical compromise: we use the data twice—once to set the prior, once for inference—but it often works well in practice.\n\n\n\n\nPrior Sensitivity\nIt’s good practice to check whether conclusions depend strongly on prior choice:\n\n\nCode\n# How does the posterior change with different priors?\ntau_values &lt;- seq(0.01, 0.15, length.out = 100)\n\nposterior_means &lt;- sapply(tau_values, function(tau) {\n  B &lt;- sigma^2 / (sigma^2 + tau^2)\n  B * mu + (1 - B) * y\n})\n\nplot(tau_values, posterior_means, type = \"l\", lwd = 2, col = \"blue\",\n     xlab = expression(paste(\"Prior SD (\", tau, \")\")),\n     ylab = \"Posterior Mean\",\n     main = \"Prior Sensitivity Analysis\")\nabline(h = y, lty = 2, col = \"red\")      # Observed value\nabline(h = mu, lty = 2, col = \"green\")   # Prior mean\nabline(v = 0.027, lty = 3, col = \"gray\") # Our chosen tau\n\nlegend(\"right\", c(\"Posterior mean\", \"Observed (.450)\", \"Prior mean (.275)\", \"τ = 0.027\"),\n       col = c(\"blue\", \"red\", \"green\", \"gray\"), lty = c(1, 2, 2, 3), lwd = c(2, 1, 1, 1))\n\n\n\n\n\n\n\n\nFigure 30.2: Posterior mean as a function of prior standard deviation: with strong priors (small τ), we trust the population average; with weak priors (large τ), we trust José’s data",
    "crumbs": [
      "Statistical Learning",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Bayesian Statistics</span>"
    ]
  },
  {
    "objectID": "chapters/31-bayesian-statistics.html#when-bayesian-methods-excel",
    "href": "chapters/31-bayesian-statistics.html#when-bayesian-methods-excel",
    "title": "30  Bayesian Statistics",
    "section": "30.7 When Bayesian Methods Excel",
    "text": "30.7 When Bayesian Methods Excel\nBayesian approaches are particularly valuable in several contexts. They excel at combining multiple sources of information, allowing you to incorporate prior knowledge, expert opinion, and data from related studies into a single coherent analysis. When working with hierarchical or multilevel data—such as patients grouped within hospitals or students nested within schools—Bayesian hierarchical models naturally capture this structure and allow appropriate borrowing of strength across groups. Bayesian methods are especially useful when working with small samples, where prior information can stabilize estimates that would otherwise be unreliable. The framework also supports sequential updating, where yesterday’s posterior becomes today’s prior as new data arrive, making it natural for adaptive designs and ongoing monitoring studies. Finally, modern MCMC methods enable fitting complex models that would be analytically or computationally intractable using traditional frequentist approaches, opening doors to realistic models of complicated phenomena.",
    "crumbs": [
      "Statistical Learning",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Bayesian Statistics</span>"
    ]
  },
  {
    "objectID": "chapters/31-bayesian-statistics.html#computational-bayesian-statistics",
    "href": "chapters/31-bayesian-statistics.html#computational-bayesian-statistics",
    "title": "30  Bayesian Statistics",
    "section": "30.8 Computational Bayesian Statistics",
    "text": "30.8 Computational Bayesian Statistics\nFor complex models, posteriors don’t have closed-form solutions. Modern Bayesian statistics relies on Markov Chain Monte Carlo (MCMC) methods to sample from posterior distributions.\nPopular tools include: - Stan (via rstan or brms packages in R) - JAGS (via rjags package) - PyMC (in Python)\n\n\nCode\n# Example using brms (Bayesian Regression Models using Stan)\nlibrary(brms)\n\n# Fit a Bayesian linear regression\nmodel &lt;- brm(\n  formula = weight ~ height,\n  data = my_data,\n  prior = c(\n    prior(normal(0, 10), class = \"b\"),      # Prior on slope\n    prior(normal(100, 50), class = \"Intercept\")  # Prior on intercept\n  )\n)\n\n# Summarize posterior distributions\nsummary(model)",
    "crumbs": [
      "Statistical Learning",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Bayesian Statistics</span>"
    ]
  },
  {
    "objectID": "chapters/31-bayesian-statistics.html#exercises",
    "href": "chapters/31-bayesian-statistics.html#exercises",
    "title": "30  Bayesian Statistics",
    "section": "30.9 Exercises",
    "text": "30.9 Exercises\n\n\n\n\n\n\nExercise Bay.1: Sally Clark Case - Base Rate Fallacy\n\n\n\n\nIn 1999 in England, Sally Clark was convicted of murdering her two infant sons after both were found dead, apparently from Sudden Infant Death Syndrome (SIDS). Expert testimony stated that the chance of two SIDS deaths was 1 in 73 million, calculated as \\((1/8500)^2\\). What error did this calculation make?\nSuppose the probability of a second SIDS case given a first is actually 1/100 (due to genetic factors). What is \\(P(\\text{two SIDS deaths})\\)?\nAccording to Bayes’ theorem, the probability the prosecution wanted was \\(P(\\text{guilty} \\mid \\text{two deaths})\\), not \\(P(\\text{two deaths} \\mid \\text{innocent})\\). Assume:\n\n\\(P(\\text{two deaths} \\mid \\text{guilty}) = 0.50\\)\n\\(P(\\text{guilty}) = 1/1,000,000\\) (rate of child-murdering parents)\n\\(P(\\text{two natural deaths}) = 1/8500 \\times 1/100 = 1.2 \\times 10^{-5}\\)\n\nCalculate \\(P(\\text{guilty} \\mid \\text{two deaths})\\).\n\n\n\n\n\n\n\n\n\nExercise Bay.2: Bayesian Election Polling\n\n\n\n\nConsider a Bayesian model for Florida election polling:\n\n\n\nCode\nlibrary(dslabs)\ndata(polls_us_election_2016)\n\npolls &lt;- polls_us_election_2016 |&gt;\n  filter(state == \"Florida\" & enddate &gt;= \"2016-11-04\") |&gt;\n  mutate(spread = rawpoll_clinton/100 - rawpoll_trump/100)\n\n# Calculate average spread and SE\nresults &lt;- polls |&gt;\n  summarize(\n    avg = mean(spread),\n    se = sd(spread) / sqrt(n())\n  )\n\n\nUsing a prior of \\(N(0, 0.01^2)\\) (based on Florida being historically close), calculate the posterior mean and 95% credible interval for the true spread.\n\nHow does the posterior change if we use a wider prior (\\(\\tau = 0.05\\))? What about a narrower prior (\\(\\tau = 0.005\\))?",
    "crumbs": [
      "Statistical Learning",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Bayesian Statistics</span>"
    ]
  },
  {
    "objectID": "chapters/31-bayesian-statistics.html#summary",
    "href": "chapters/31-bayesian-statistics.html#summary",
    "title": "30  Bayesian Statistics",
    "section": "30.10 Summary",
    "text": "30.10 Summary\n\nBayes’ theorem provides a principled way to update beliefs based on evidence\nThe base rate fallacy occurs when prior probabilities are ignored\nBayesian inference treats parameters as random variables with distributions\nThe posterior combines prior beliefs with observed data\nHierarchical models borrow strength across related observations through shrinkage\nCredible intervals have a more intuitive interpretation than confidence intervals\nPrior sensitivity analysis checks whether conclusions depend on prior assumptions\nModern Bayesian computation uses MCMC methods implemented in tools like Stan",
    "crumbs": [
      "Statistical Learning",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Bayesian Statistics</span>"
    ]
  },
  {
    "objectID": "chapters/31-bayesian-statistics.html#additional-resources",
    "href": "chapters/31-bayesian-statistics.html#additional-resources",
    "title": "30  Bayesian Statistics",
    "section": "30.11 Additional Resources",
    "text": "30.11 Additional Resources\n\nGelman et al. (2013) - The comprehensive reference for Bayesian data analysis\nKruschke (2014) - Accessible introduction with R examples\nMcElreath (2020) - Excellent modern treatment with Stan\nJames et al. (2023) - Connects Bayesian ideas to statistical learning\n\n\n\n\n\n\n\nGelman, Andrew, John B. Carlin, Hal S. Stern, David B. Dunson, Aki Vehtari, and Donald B. Rubin. 2013. Bayesian Data Analysis. 3rd ed. Boca Raton, FL: CRC Press.\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2023. An Introduction to Statistical Learning with Applications in r. 2nd ed. Springer. https://www.statlearning.com.\n\n\nKruschke, John K. 2014. Doing Bayesian Data Analysis: A Tutorial with r, JAGS, and Stan. 2nd ed. Boston: Academic Press.\n\n\nMcElreath, Richard. 2020. Statistical Rethinking: A Bayesian Course with Examples in r and Stan. 2nd ed. Boca Raton, FL: CRC Press.",
    "crumbs": [
      "Statistical Learning",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Bayesian Statistics</span>"
    ]
  },
  {
    "objectID": "chapters/32-classification.html",
    "href": "chapters/32-classification.html",
    "title": "31  Classification Methods",
    "section": "",
    "text": "31.1 Introduction to Classification\nClassification is a supervised learning task where the goal is to predict categorical outcomes. Given a set of features (predictors), we want to assign observations to one of several predefined classes.\nClassification problems are ubiquitous in biology and bioengineering:\nIn this chapter, we focus on K-Nearest Neighbors (KNN), a fundamental classification algorithm that illustrates key concepts applicable to all classification methods. Other classification methods like decision trees, random forests, and support vector machines are covered in Chapter 29.",
    "crumbs": [
      "Statistical Learning",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Classification Methods</span>"
    ]
  },
  {
    "objectID": "chapters/32-classification.html#introduction-to-classification",
    "href": "chapters/32-classification.html#introduction-to-classification",
    "title": "31  Classification Methods",
    "section": "",
    "text": "Diagnosing disease states from biomarkers\nClassifying cell types from gene expression\nPredicting protein function from sequence\nIdentifying species from morphological measurements",
    "crumbs": [
      "Statistical Learning",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Classification Methods</span>"
    ]
  },
  {
    "objectID": "chapters/32-classification.html#distance-the-foundation-of-classification",
    "href": "chapters/32-classification.html#distance-the-foundation-of-classification",
    "title": "31  Classification Methods",
    "section": "31.2 Distance: The Foundation of Classification",
    "text": "31.2 Distance: The Foundation of Classification\nMany classification algorithms rely on measuring how similar or different observations are. Distance quantifies this similarity—observations that are close together are considered similar.\n\nEuclidean Distance\nThe most common distance measure is Euclidean distance, the straight-line distance between two points:\nFor two points in two dimensions: \\[\nd(A, B) = \\sqrt{(A_x - B_x)^2 + (A_y - B_y)^2}\n\\]\n\n\nCode\nrafalib::mypar()\nplot(c(0, 1, 1), c(0, 0, 1), pch = 16, cex = 2,\n     xaxt = \"n\", yaxt = \"n\", xlab = \"\", ylab = \"\", bty = \"n\",\n     xlim = c(-0.25, 1.25), ylim = c(-0.25, 1.25))\nlines(c(0, 1, 1, 0), c(0, 0, 1, 0))\ntext(0, .2, expression(paste('(A'[x]*',A'[y]*')')), cex = 1.5)\ntext(1, 1.2, expression(paste('(B'[x]*',B'[y]*')')), cex = 1.5)\ntext(-0.1, 0, \"A\", cex = 2)\ntext(1.1, 1, \"B\", cex = 2)\n\n\n\n\n\n\n\n\nFigure 31.1: Euclidean distance between two points in a Cartesian plane\n\n\n\n\n\nIn one dimension, distance is simply the absolute difference: \\[\nd(A, B) = |A - B|\n\\]\n\n\nDistance in Higher Dimensions\nWith high-dimensional data (many features), the concept extends naturally. For observations with \\(p\\) features: \\[\nd(i, j) = \\sqrt{\\sum_{k=1}^{p} (x_{ik} - x_{jk})^2}\n\\]\nThis is the generalized Euclidean distance—we sum the squared differences across all features.\n\n\nExample: Distance Between Digits\nLet’s compute distances between handwritten digit images using the MNIST dataset:\n\n\nCode\nlibrary(dslabs)\nif (!exists(\"mnist\")) mnist &lt;- read_mnist()\n\nset.seed(1995)\nind &lt;- which(mnist$train$labels %in% c(2, 7)) %&gt;% sample(500)\nx &lt;- mnist$train$images[ind, ]\ny &lt;- mnist$train$labels[ind]\n\n\nEach digit image has 784 pixels (features). The labels for the first three observations are:\n\n\nCode\ny[1:3]\n\n\n[1] 7 2 7\n\n\nWe expect digits of the same type to be closer to each other:\n\n\nCode\nx_1 &lt;- x[1, ]\nx_2 &lt;- x[2, ]\nx_3 &lt;- x[3, ]\n\n# Distance between two 7s\ndist_same &lt;- sqrt(sum((x_1 - x_2)^2))\n\n# Distance between a 7 and a 2\ndist_diff_1 &lt;- sqrt(sum((x_1 - x_3)^2))\ndist_diff_2 &lt;- sqrt(sum((x_2 - x_3)^2))\n\ncat(\"Distance between two 7s:\", round(dist_same, 1), \"\\n\")\n\n\nDistance between two 7s: 3273.4 \n\n\nCode\ncat(\"Distance between 7 and 2:\", round(dist_diff_1, 1), \"and\", round(dist_diff_2, 1), \"\\n\")\n\n\nDistance between 7 and 2: 2311 and 2635.9 \n\n\nAs expected, the two 7s are closer to each other than to the 2.\n\n\nComputing Distance Matrices\nThe dist() function efficiently computes pairwise distances:\n\n\nCode\nd &lt;- dist(x)\nclass(d)\n\n\n[1] \"dist\"\n\n\nWe can visualize the distance matrix to see clustering patterns:\n\n\nCode\nrafalib::mypar()\nimage(as.matrix(d)[order(y), order(y)],\n      col = rev(RColorBrewer::brewer.pal(9, \"RdBu\")),\n      main = \"Distance Matrix (ordered by digit)\")\n\n\n\n\n\n\n\n\nFigure 31.2: Distance matrix for digit images, ordered by label. Similar digits (same class) cluster together with smaller distances (darker colors).\n\n\n\n\n\nThe block structure reveals that 2s are more similar to other 2s, and 7s are more similar to other 7s.\n\n\nOther Distance Metrics\nBeyond Euclidean distance, other metrics are sometimes appropriate:\nManhattan Distance (L1 norm): Sum of absolute differences \\[\nd(x_i, x_j) = \\sum_{k=1}^{p} |x_{ik} - x_{jk}|\n\\]\nMinkowski Distance (generalized): \\[\nd(x_i, x_j) = \\left(\\sum_{k=1}^{p} |x_{ik} - x_{jk}|^q\\right)^{1/q}\n\\]\nWhen \\(q = 2\\), this is Euclidean; when \\(q = 1\\), this is Manhattan.",
    "crumbs": [
      "Statistical Learning",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Classification Methods</span>"
    ]
  },
  {
    "objectID": "chapters/32-classification.html#k-nearest-neighbors-knn",
    "href": "chapters/32-classification.html#k-nearest-neighbors-knn",
    "title": "31  Classification Methods",
    "section": "31.3 K-Nearest Neighbors (KNN)",
    "text": "31.3 K-Nearest Neighbors (KNN)\n\nThe KNN Algorithm\nK-Nearest Neighbors is one of the simplest and most intuitive classification algorithms. The idea is straightforward: classify a new observation based on the classes of its nearest neighbors in the training data.\nThe algorithm:\n\nChoose the number of neighbors \\(k\\)\nFor a new observation, find the \\(k\\) closest training points (using a distance metric)\nAssign the class that is most common among these neighbors (majority vote)\n\nFor regression problems, predict the average of the neighbors’ values.\n\n\n\n\n\n\nIntuition\n\n\n\n“Tell me who your neighbors are, and I’ll tell you who you are.”\nKNN makes no assumptions about the underlying data distribution—it simply uses proximity to make predictions.\n\n\n\n\nKNN for Classification\n\n\nCode\n# Simulate two-class data\nset.seed(42)\nn &lt;- 100\nclass1 &lt;- data.frame(\n  x1 = rnorm(n/2, mean = 2, sd = 1),\n  x2 = rnorm(n/2, mean = 2, sd = 1),\n  class = \"A\"\n)\nclass2 &lt;- data.frame(\n  x1 = rnorm(n/2, mean = 4, sd = 1),\n  x2 = rnorm(n/2, mean = 4, sd = 1),\n  class = \"B\"\n)\ntrain_data &lt;- rbind(class1, class2)\n\n# New point to classify\nnew_point &lt;- data.frame(x1 = 3.2, x2 = 3.5)\n\n# Find 5 nearest neighbors\ndistances &lt;- sqrt((train_data$x1 - new_point$x1)^2 +\n                  (train_data$x2 - new_point$x2)^2)\nnearest &lt;- order(distances)[1:5]\n\n# Plot\nplot(train_data$x1, train_data$x2,\n     col = ifelse(train_data$class == \"A\", \"blue\", \"red\"),\n     pch = 16, xlab = \"Feature 1\", ylab = \"Feature 2\",\n     main = \"K-Nearest Neighbors (k=5)\")\npoints(new_point$x1, new_point$x2, pch = 8, cex = 2, lwd = 2)\n\n# Highlight nearest neighbors\npoints(train_data$x1[nearest], train_data$x2[nearest],\n       cex = 2, col = ifelse(train_data$class[nearest] == \"A\", \"blue\", \"red\"))\nlegend(\"topleft\", c(\"Class A\", \"Class B\", \"New point\"),\n       col = c(\"blue\", \"red\", \"black\"), pch = c(16, 16, 8))\n\n\n\n\n\n\n\n\nFigure 31.3: K-nearest neighbors classification: the new point (star) is classified based on its k nearest training points\n\n\n\n\n\n\n\nThe Effect of K: Bias-Variance Tradeoff\nThe choice of \\(k\\) is critical and illustrates the bias-variance tradeoff:\n\nSmall k (e.g., k=1): Very flexible decision boundary, low bias but high variance. Prone to overfitting—the boundary follows noise in the training data.\nLarge k: Smoother decision boundary, higher bias but lower variance. May miss local patterns—prone to underfitting.\n\n\n\nCode\n# Create a grid for visualization\nx1_grid &lt;- seq(0, 6, length.out = 100)\nx2_grid &lt;- seq(0, 6, length.out = 100)\ngrid &lt;- expand.grid(x1 = x1_grid, x2 = x2_grid)\n\npar(mfrow = c(1, 3))\n\nfor (k_val in c(1, 15, 50)) {\n  # Predict on grid\n  pred &lt;- knn(train = train_data[, 1:2],\n              test = grid,\n              cl = train_data$class,\n              k = k_val)\n\n  # Plot decision regions\n  plot(grid$x1, grid$x2,\n       col = ifelse(pred == \"A\", rgb(0, 0, 1, 0.1), rgb(1, 0, 0, 0.1)),\n       pch = 15, cex = 0.5, xlab = \"Feature 1\", ylab = \"Feature 2\",\n       main = paste(\"k =\", k_val))\n  points(train_data$x1, train_data$x2,\n         col = ifelse(train_data$class == \"A\", \"blue\", \"red\"), pch = 16)\n}\n\n\n\n\n\n\n\n\nFigure 31.4: Effect of k on KNN decision boundaries: small k creates complex boundaries (overfitting risk), large k creates smooth boundaries (underfitting risk)\n\n\n\n\n\nNotice how k=1 creates a very jagged boundary that follows individual training points, while k=50 creates a smooth boundary that may miss local structure.\n\n\nSelecting K with Cross-Validation\nWe choose the optimal \\(k\\) using cross-validation to estimate generalization performance:\n\n\nCode\n# Split data for validation\nset.seed(123)\ntest_idx &lt;- sample(1:nrow(train_data), 30)\ntrain_subset &lt;- train_data[-test_idx, ]\ntest_subset &lt;- train_data[test_idx, ]\n\nk_values &lt;- seq(1, 50, by = 2)\n\n# Calculate accuracies\naccuracy &lt;- sapply(k_values, function(k) {\n  pred &lt;- knn(train = train_subset[, 1:2],\n              test = test_subset[, 1:2],\n              cl = train_subset$class,\n              k = k)\n  mean(pred == test_subset$class)\n})\n\ntrain_accuracy &lt;- sapply(k_values, function(k) {\n  pred &lt;- knn(train = train_subset[, 1:2],\n              test = train_subset[, 1:2],\n              cl = train_subset$class,\n              k = k)\n  mean(pred == train_subset$class)\n})\n\nplot(k_values, train_accuracy, type = \"l\", col = \"blue\", lwd = 2,\n     xlab = \"k (number of neighbors)\", ylab = \"Accuracy\",\n     main = \"Training vs Test Accuracy\", ylim = c(0.5, 1))\nlines(k_values, accuracy, col = \"red\", lwd = 2)\nlegend(\"bottomright\", c(\"Training\", \"Test\"), col = c(\"blue\", \"red\"), lwd = 2)\n\n\n\n\n\n\n\n\nFigure 31.5: Training vs test accuracy as a function of k: training accuracy is perfect at k=1, but test accuracy reveals the true generalization performance\n\n\n\n\n\nWhen k=1, training accuracy is perfect—each point is its own nearest neighbor. But test accuracy reveals how well the model actually generalizes.\n\n\nKNN with the caret Package\nThe caret package provides a convenient interface for KNN with built-in cross-validation:\n\n\nCode\nlibrary(dslabs)\ndata(\"mnist_27\")\n\n# Train KNN with cross-validation to select k\ntrain_knn &lt;- train(y ~ ., method = \"knn\",\n                   data = mnist_27$train,\n                   tuneGrid = data.frame(k = seq(9, 71, 2)))\n\n# Best k value\ntrain_knn$bestTune\n\n\n    k\n30 67\n\n\nCode\n# Test accuracy\nconfusionMatrix(predict(train_knn, mnist_27$test, type = \"raw\"),\n                mnist_27$test$y)$overall[\"Accuracy\"]\n\n\nAccuracy \n   0.825 \n\n\n\n\nVisualizing KNN Decision Boundaries\nKNN can capture complex, non-linear decision boundaries:\n\n\nCode\n# Helper function for plotting conditional probabilities\nplot_cond_prob &lt;- function(p_hat = NULL) {\n  tmp &lt;- mnist_27$true_p\n  if (!is.null(p_hat)) {\n    tmp &lt;- mutate(tmp, p = p_hat)\n  }\n  tmp %&gt;% ggplot(aes(x_1, x_2, z = p, fill = p)) +\n    geom_raster(show.legend = FALSE) +\n    scale_fill_gradientn(colors = c(\"#F8766D\", \"white\", \"#00BFC4\")) +\n    stat_contour(breaks = c(0.5), color = \"black\")\n}\n\np1 &lt;- plot_cond_prob() + ggtitle(\"True conditional probability\")\np2 &lt;- plot_cond_prob(predict(train_knn, newdata = mnist_27$true_p, type = \"prob\")[, 2]) +\n  ggtitle(\"KNN estimate\")\n\ngridExtra::grid.arrange(p2, p1, nrow = 1)\n\n\n\n\n\n\n\n\nFigure 31.6: KNN decision boundary compared to the true conditional probability. KNN is flexible enough to capture the non-linear pattern.",
    "crumbs": [
      "Statistical Learning",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Classification Methods</span>"
    ]
  },
  {
    "objectID": "chapters/32-classification.html#predictor-space",
    "href": "chapters/32-classification.html#predictor-space",
    "title": "31  Classification Methods",
    "section": "31.4 Predictor Space",
    "text": "31.4 Predictor Space\nThe predictor space is a conceptual framework for understanding classification algorithms. It consists of all possible values the predictor variables can take.\nFor the digit classification problem, each observation is a point in 784-dimensional space. Classification algorithms partition this space into regions, each assigned to a class.\nKey concepts:\n\nDecision boundary: The surface separating regions assigned to different classes\nNeighborhood: The region around a point within a specified distance\nFeature scaling: Important because distance calculations are sensitive to scale\n\n\n\n\n\n\n\nFeature Scaling\n\n\n\nKNN and other distance-based methods are sensitive to feature scales. A variable ranging from 0-1000 will dominate distance calculations compared to a variable ranging from 0-1.\nAlways standardize features before applying KNN:\n\n\nCode\n# Standardize features\nx_scaled &lt;- scale(x)\n\n# Or use caret's preProcess\ntrain_knn &lt;- train(y ~ ., method = \"knn\",\n                   data = training_data,\n                   preProcess = c(\"center\", \"scale\"))",
    "crumbs": [
      "Statistical Learning",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Classification Methods</span>"
    ]
  },
  {
    "objectID": "chapters/32-classification.html#knn-for-regression",
    "href": "chapters/32-classification.html#knn-for-regression",
    "title": "31  Classification Methods",
    "section": "31.5 KNN for Regression",
    "text": "31.5 KNN for Regression\nKNN can also predict continuous outcomes. Instead of majority voting, we average the neighbors’ values:\n\\[\n\\hat{y} = \\frac{1}{k} \\sum_{x_i \\in N_k(x)} y_i\n\\]\nwhere \\(N_k(x)\\) is the set of k nearest neighbors to x.\n\n\nCode\n# Generate non-linear data\nset.seed(42)\nn &lt;- 100\nx &lt;- seq(0, 2*pi, length.out = n)\ny &lt;- sin(x) + rnorm(n, sd = 0.3)\nreg_data &lt;- data.frame(x = x, y = y)\n\npar(mfrow = c(1, 3))\n\nfor (k_val in c(3, 10, 30)) {\n  # Predict using KNN regression\n  x_grid &lt;- seq(0, 2*pi, length.out = 200)\n\n  # Manual KNN regression\n  pred &lt;- sapply(x_grid, function(xnew) {\n    dists &lt;- abs(reg_data$x - xnew)\n    neighbors &lt;- order(dists)[1:k_val]\n    mean(reg_data$y[neighbors])\n  })\n\n  plot(x, y, pch = 16, col = \"gray60\", main = paste(\"k =\", k_val))\n  lines(x_grid, pred, col = \"blue\", lwd = 2)\n  lines(x_grid, sin(x_grid), col = \"red\", lwd = 2, lty = 2)\n  legend(\"topright\", c(\"KNN fit\", \"True function\"),\n         col = c(\"blue\", \"red\"), lty = c(1, 2), lwd = 2, cex = 0.7)\n}\n\n\n\n\n\n\n\n\nFigure 31.7: KNN regression with different values of k: smaller k gives more flexible (wiggly) fits, larger k gives smoother fits",
    "crumbs": [
      "Statistical Learning",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Classification Methods</span>"
    ]
  },
  {
    "objectID": "chapters/32-classification.html#advantages-and-limitations-of-knn",
    "href": "chapters/32-classification.html#advantages-and-limitations-of-knn",
    "title": "31  Classification Methods",
    "section": "31.6 Advantages and Limitations of KNN",
    "text": "31.6 Advantages and Limitations of KNN\n\nAdvantages\nKNN offers several appealing properties that make it a valuable tool in the classification toolkit. The algorithm is simple and intuitive—it’s easy to understand the logic and explain predictions to non-technical audiences. Unlike many machine learning methods, KNN has no training phase; all computation happens at prediction time, a property sometimes called “lazy learning.” This makes it trivial to update the model with new training data. The method is non-parametric, making no assumptions about the underlying data distribution, which allows it to adapt to complex patterns in the data. KNN naturally handles multiclass problems without any modification to the basic algorithm—simply take the majority vote among k neighbors regardless of how many classes exist. Finally, KNN can capture complex, non-linear patterns and decision boundaries that linear methods would miss entirely.\n\n\nLimitations\nDespite these strengths, KNN has important limitations that restrict its applicability. The method is computationally expensive for large datasets because each prediction requires computing distances to all training points, which becomes prohibitive as datasets grow. KNN is sensitive to irrelevant features—since all features contribute equally to distance calculations, adding noise variables degrades performance. This sensitivity also means the method requires careful feature scaling; variables on different scales will dominate distance calculations and bias results. The algorithm suffers from the curse of dimensionality: in high-dimensional spaces, all points become approximately equidistant, making the notion of “nearest” neighbors less meaningful. Finally, KNN provides no interpretable model—unlike decision trees or linear models, you cannot easily identify which features are most important for making predictions, limiting its use when interpretability matters.",
    "crumbs": [
      "Statistical Learning",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Classification Methods</span>"
    ]
  },
  {
    "objectID": "chapters/32-classification.html#exercises",
    "href": "chapters/32-classification.html#exercises",
    "title": "31  Classification Methods",
    "section": "31.7 Exercises",
    "text": "31.7 Exercises\n\n\n\n\n\n\nExercise Cl.1: Tissue Classification with KNN\n\n\n\n\nLoad the tissue_gene_expression dataset. Split the data into training and test sets, then use kNN to predict tissue type. Try values of \\(k = 1, 3, 5, 7, 9, 11\\) and report which gives the best test accuracy.\n\n\n\nCode\ndata(\"tissue_gene_expression\")\ndim(tissue_gene_expression$x)\ntable(tissue_gene_expression$y)\n\n\n\n\n\n\n\n\n\n\nExercise Cl.2: Sex Prediction from Height\n\n\n\n\nWe previously used logistic regression to predict sex from height. Use kNN to do the same. Use cross-validation to select the optimal \\(k\\) and compare the F1 score to the logistic regression result of approximately 0.6.\n\n\n\n\n\n\n\n\n\nExercise Cl.3: Distance Metrics\n\n\n\n\nFor the digit classification problem with 2s and 7s, experiment with different distance metrics (Euclidean vs Manhattan). Does the choice of metric affect classification accuracy?\n\n\n\n\n\n\n\n\n\nExercise Cl.4: Weighted KNN\n\n\n\n\nImplement weighted KNN, where closer neighbors contribute more to the prediction. Compare performance to standard KNN.",
    "crumbs": [
      "Statistical Learning",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Classification Methods</span>"
    ]
  },
  {
    "objectID": "chapters/32-classification.html#summary",
    "href": "chapters/32-classification.html#summary",
    "title": "31  Classification Methods",
    "section": "31.8 Summary",
    "text": "31.8 Summary\n\nDistance measures similarity between observations and is fundamental to many classification methods\nK-Nearest Neighbors (KNN) classifies observations based on the majority class of nearby training points\nThe choice of k represents a bias-variance tradeoff: small k is flexible but noisy, large k is smooth but may miss local patterns\nCross-validation should be used to select the optimal k\nFeature scaling is critical for distance-based methods\nKNN is simple and effective but can be slow for large datasets and suffers from the curse of dimensionality\nThe predictor space framework helps visualize how classification algorithms partition the feature space",
    "crumbs": [
      "Statistical Learning",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Classification Methods</span>"
    ]
  },
  {
    "objectID": "chapters/32-classification.html#additional-resources",
    "href": "chapters/32-classification.html#additional-resources",
    "title": "31  Classification Methods",
    "section": "31.9 Additional Resources",
    "text": "31.9 Additional Resources\n\nJames et al. (2023) - Comprehensive treatment of KNN and classification\nThulin (2025) - Modern statistics with R including machine learning\nThe caret package vignette for practical classification workflows\n\n\n\n\n\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2023. An Introduction to Statistical Learning with Applications in r. 2nd ed. Springer. https://www.statlearning.com.\n\n\nThulin, Måns. 2025. Modern Statistics with r. CRC Press. https://www.modernstatisticswithr.com.",
    "crumbs": [
      "Statistical Learning",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Classification Methods</span>"
    ]
  },
  {
    "objectID": "chapters/33-clustering.html",
    "href": "chapters/33-clustering.html",
    "title": "32  Clustering",
    "section": "",
    "text": "32.1 Introduction to Unsupervised Learning\nThe machine learning algorithms we’ve discussed so far—regression, classification—are examples of supervised learning. The name comes from the fact that we use known outcomes (labels) in a training set to “supervise” the creation of our prediction algorithm.\nUnsupervised learning is fundamentally different: we do not necessarily know the outcomes and instead are interested in discovering hidden structure in the data. Clustering algorithms are the most common unsupervised methods—they use features to group observations into clusters without predefined labels.",
    "crumbs": [
      "Statistical Learning",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Clustering</span>"
    ]
  },
  {
    "objectID": "chapters/33-clustering.html#introduction-to-unsupervised-learning",
    "href": "chapters/33-clustering.html#introduction-to-unsupervised-learning",
    "title": "32  Clustering",
    "section": "",
    "text": "When Clustering is Useful\nClustering excels when:\n\nYou want to discover natural groupings in your data\nLabels are unavailable or expensive to obtain\nYou’re performing exploratory data analysis\nYou need to segment populations for further study\n\nExamples in biology:\n\nIdentifying cell types from single-cell RNA-seq data\nGrouping patients by disease subtype\nFinding functional modules in protein interaction networks\nClassifying microbiome communities\n\n\n\nClustering Requires Distance\nA first step in any clustering algorithm is defining a distance between observations. How we measure similarity fundamentally affects what clusters we find. We typically use Euclidean distance, but other metrics (Manhattan, correlation-based) are sometimes more appropriate.",
    "crumbs": [
      "Statistical Learning",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Clustering</span>"
    ]
  },
  {
    "objectID": "chapters/33-clustering.html#a-motivating-example-movie-ratings",
    "href": "chapters/33-clustering.html#a-motivating-example-movie-ratings",
    "title": "32  Clustering",
    "section": "32.2 A Motivating Example: Movie Ratings",
    "text": "32.2 A Motivating Example: Movie Ratings\nLet’s construct a clustering example using movie ratings. We’ll identify whether there are natural groupings among popular movies based on how users rate them.\n\n\nCode\nlibrary(dslabs)\ndata(\"movielens\")\n\n# Select 50 most-rated movies\ntop &lt;- movielens %&gt;%\n  group_by(movieId) %&gt;%\n  summarize(n = n(), title = first(title)) %&gt;%\n  top_n(50, n) %&gt;%\n  pull(movieId)\n\n# Create rating matrix for users with at least 25 ratings\nx &lt;- movielens %&gt;%\n  filter(movieId %in% top) %&gt;%\n  group_by(userId) %&gt;%\n  filter(n() &gt;= 25) %&gt;%\n  ungroup() %&gt;%\n  select(title, userId, rating) %&gt;%\n  pivot_wider(names_from = userId, values_from = rating)\n\nrow_names &lt;- str_remove(x$title, \": Episode\") %&gt;% str_trunc(20)\nx &lt;- x[, -1] %&gt;% as.matrix()\n\n# Center by user and movie means\nx &lt;- sweep(x, 2, colMeans(x, na.rm = TRUE))\nx &lt;- sweep(x, 1, rowMeans(x, na.rm = TRUE))\nrownames(x) &lt;- row_names\n\n\nWe now have a matrix of centered ratings. Can we find groups of similar movies based on how users rate them?\n\n\nCode\nd &lt;- dist(x)",
    "crumbs": [
      "Statistical Learning",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Clustering</span>"
    ]
  },
  {
    "objectID": "chapters/33-clustering.html#hierarchical-clustering",
    "href": "chapters/33-clustering.html#hierarchical-clustering",
    "title": "32  Clustering",
    "section": "32.3 Hierarchical Clustering",
    "text": "32.3 Hierarchical Clustering\nHierarchical clustering builds a tree-like structure (dendrogram) showing nested relationships between observations or clusters.\n\nThe Agglomerative Algorithm\nThe most common approach is agglomerative (bottom-up) clustering:\n\nStart with each observation as its own cluster\nFind the two closest clusters and merge them\nRepeat until all observations are in a single cluster\nThe result is a tree showing the sequence of merges\n\n\n\nCode\nh &lt;- hclust(d)\n\n\n\n\nVisualizing with Dendrograms\nA dendrogram displays the hierarchical structure:\n\n\nCode\nrafalib::mypar()\nplot(h, cex = 0.65, main = \"Hierarchical Clustering of Movies\", xlab = \"\")\n\n\n\n\n\n\n\n\nFigure 32.1: Hierarchical clustering dendrogram of movies. Height indicates distance at which clusters merge. Movies that split at lower heights are more similar.\n\n\n\n\n\n\n\nInterpreting Dendrograms\nTo find the distance between any two movies:\n\nStart at the movies’ leaves\nMove up until you reach the node where they first join\nThe height of that node is the distance between them\n\nFor example, the Star Wars movies merge at low heights (they’re similar), while the distance between Raiders of the Lost Ark and Silence of the Lambs is much greater.\n\n\nCutting the Tree to Form Clusters\nTo create discrete clusters, we “cut” the dendrogram either:\n\nAt a specified height (minimum distance for cluster membership)\nTo obtain a specified number of clusters\n\n\n\nCode\ngroups &lt;- cutree(h, k = 10)\n\n\nLet’s examine some of the resulting clusters:\n\n\nCode\n# Blockbusters cluster\ncat(\"Cluster 4 (Blockbusters):\\n\")\n\n\nCluster 4 (Blockbusters):\n\n\nCode\nnames(groups)[groups == 4]\n\n\n[1] \"Fugitive, The\"        \"Speed\"                \"Mission: Impossible\" \n[4] \"Independence Day ...\"\n\n\n\n\nCode\n# Another cluster\ncat(\"\\nCluster 9:\\n\")\n\n\n\nCluster 9:\n\n\nCode\nnames(groups)[groups == 9]\n\n\n[1] \"Dumb & Dumber (Du...\" \"Ace Ventura: Pet ...\" \"Mask, The\"           \n\n\nThe clustering reveals meaningful groupings—similar genres and franchises cluster together.\n\n\nLinkage Methods\nDifferent linkage methods define how cluster-to-cluster distance is computed:\n\n\n\n\n\n\n\n\nMethod\nDescription\nCharacteristics\n\n\n\n\nComplete\nMaximum distance between any two points\nProduces compact, spherical clusters\n\n\nSingle\nMinimum distance between any two points\nTends to create elongated chains\n\n\nAverage\nMean distance between all pairs\nBalanced approach\n\n\nWard’s\nMinimizes within-cluster variance\nOften produces most interpretable clusters\n\n\n\n\n\nCode\n# Create sample data for comparison\nset.seed(123)\nsample_data &lt;- data.frame(\n  x = c(rnorm(20, 0, 1), rnorm(20, 4, 1), rnorm(20, 2, 3)),\n  y = c(rnorm(20, 0, 1), rnorm(20, 4, 1), rnorm(20, 8, 1))\n)\ndist_matrix &lt;- dist(sample_data)\n\npar(mfrow = c(2, 2))\nplot(hclust(dist_matrix, method = \"single\"), main = \"Single Linkage\", xlab = \"\")\nplot(hclust(dist_matrix, method = \"complete\"), main = \"Complete Linkage\", xlab = \"\")\nplot(hclust(dist_matrix, method = \"average\"), main = \"Average Linkage\", xlab = \"\")\nplot(hclust(dist_matrix, method = \"ward.D2\"), main = \"Ward's Method\", xlab = \"\")\n\n\n\n\n\n\n\n\nFigure 32.2: Comparison of different linkage methods on the same data. Each method produces different cluster structures.\n\n\n\n\n\n\n\n\n\n\n\nChoosing a Linkage Method\n\n\n\n\nWard’s method often produces the most interpretable clusters and is a good default\nComplete linkage is useful when you want compact clusters\nSingle linkage can detect elongated structures but is sensitive to noise\nAverage linkage is a compromise between single and complete\n\n\n\n\n\nMathematical Properties\nCophenetic correlation measures how well the dendrogram preserves original distances:\n\n\nCode\n# Cophenetic correlation for Ward's method\nhc_ward &lt;- hclust(dist_matrix, method = \"ward.D2\")\ncophenetic_dist &lt;- cophenetic(hc_ward)\ncor(dist_matrix, cophenetic_dist)\n\n\n[1] 0.814684\n\n\nHigher values (closer to 1) indicate better preservation of the original distance structure.",
    "crumbs": [
      "Statistical Learning",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Clustering</span>"
    ]
  },
  {
    "objectID": "chapters/33-clustering.html#k-means-clustering",
    "href": "chapters/33-clustering.html#k-means-clustering",
    "title": "32  Clustering",
    "section": "32.4 K-Means Clustering",
    "text": "32.4 K-Means Clustering\nK-means clustering partitions observations into exactly \\(K\\) groups by minimizing within-cluster variance. Unlike hierarchical clustering, you must specify \\(K\\) in advance.\n\nThe K-Means Algorithm\n\nInitialize: Randomly select \\(K\\) cluster centers\nAssign: Assign each observation to the nearest center\nUpdate: Recompute centers as the mean of assigned observations\nRepeat: Continue until assignments no longer change\n\nThe algorithm minimizes the within-cluster sum of squares (WSS): \\[\n\\text{WSS} = \\sum_{k=1}^{K} \\sum_{x_i \\in C_k} ||x_i - \\mu_k||^2\n\\]\nwhere \\(\\mu_k\\) is the centroid of cluster \\(k\\).\n\n\nK-Means in R\n\n\nCode\n# K-means requires no missing values\nx_0 &lt;- x\nx_0[is.na(x_0)] &lt;- 0\n\n# Run k-means with 10 clusters\nset.seed(42)\nk &lt;- kmeans(x_0, centers = 10, nstart = 25)\n\n# Cluster assignments\ngroups_km &lt;- k$cluster\ntable(groups_km)\n\n\ngroups_km\n 1  2  3  4  5  6  7  8  9 10 \n 9  6  3  6  8  1  3  3  3  8 \n\n\nThe nstart parameter runs the algorithm multiple times with different random starts, selecting the best result. This is important because k-means can converge to local optima.\n\n\nComparing to Hierarchical Clustering\n\n\nCode\n# Compare k-means to hierarchical clustering\ntable(KMeans = groups_km, Hierarchical = groups)\n\n\n      Hierarchical\nKMeans 1 2 3 4 5 6 7 8 9 10\n    1  1 2 0 1 2 1 0 2 0  0\n    2  0 0 0 2 3 1 0 0 0  0\n    3  0 0 0 0 0 0 0 0 3  0\n    4  1 5 0 0 0 0 0 0 0  0\n    5  0 1 0 0 0 2 3 2 0  0\n    6  0 0 0 0 0 0 0 0 0  1\n    7  0 0 3 0 0 0 0 0 0  0\n    8  0 0 3 0 0 0 0 0 0  0\n    9  0 0 0 1 0 0 0 2 0  0\n    10 8 0 0 0 0 0 0 0 0  0\n\n\n\n\nVisualizing K-Means Results\n\n\nCode\n# Demonstrate k-means on 2D data\nset.seed(42)\nkm_example &lt;- kmeans(sample_data, centers = 3, nstart = 25)\n\nplot(sample_data$x, sample_data$y,\n     col = km_example$cluster,\n     pch = 19, cex = 1.5,\n     xlab = \"X\", ylab = \"Y\",\n     main = \"K-Means Clustering (K=3)\")\npoints(km_example$centers, pch = 4, cex = 3, lwd = 3)\nlegend(\"topleft\", c(\"Cluster 1\", \"Cluster 2\", \"Cluster 3\", \"Centers\"),\n       col = c(1, 2, 3, 1), pch = c(19, 19, 19, 4))\n\n\n\n\n\n\n\n\nFigure 32.3: K-means clustering on simulated data with three natural clusters",
    "crumbs": [
      "Statistical Learning",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Clustering</span>"
    ]
  },
  {
    "objectID": "chapters/33-clustering.html#choosing-the-number-of-clusters",
    "href": "chapters/33-clustering.html#choosing-the-number-of-clusters",
    "title": "32  Clustering",
    "section": "32.5 Choosing the Number of Clusters",
    "text": "32.5 Choosing the Number of Clusters\nA fundamental challenge in clustering is determining the appropriate number of clusters. Several methods can help:\n\nThe Elbow Method\nPlot within-cluster sum of squares against \\(K\\) and look for an “elbow” where the rate of decrease slows:\n\n\nCode\nwss &lt;- sapply(1:10, function(k) {\n  kmeans(sample_data, centers = k, nstart = 20)$tot.withinss\n})\n\nplot(1:10, wss, type = \"b\", pch = 19,\n     xlab = \"Number of Clusters (K)\",\n     ylab = \"Total Within-Cluster Sum of Squares\",\n     main = \"Elbow Method\")\n\n\n\n\n\n\n\n\nFigure 32.4: Elbow method for selecting number of clusters. Look for the ‘bend’ in the curve where adding more clusters provides diminishing returns.\n\n\n\n\n\n\n\nSilhouette Analysis\nThe silhouette score measures how similar an observation is to its own cluster compared to other clusters:\n\\[\ns(i) = \\frac{b(i) - a(i)}{\\max(a(i), b(i))}\n\\]\nwhere: - \\(a(i)\\) = average distance to other points in the same cluster - \\(b(i)\\) = average distance to points in the nearest other cluster\nValues range from -1 to 1: - Close to 1: well-clustered - Close to 0: on the boundary - Negative: possibly misclassified\n\n\nCode\nlibrary(cluster)\n\n# Calculate silhouette scores for different K\nsil_width &lt;- sapply(2:10, function(k) {\n  km &lt;- kmeans(sample_data, centers = k, nstart = 20)\n  ss &lt;- silhouette(km$cluster, dist(sample_data))\n  mean(ss[, \"sil_width\"])\n})\n\npar(mfrow = c(1, 2))\nplot(2:10, sil_width, type = \"b\", pch = 19,\n     xlab = \"Number of Clusters\",\n     ylab = \"Average Silhouette Width\",\n     main = \"Silhouette Method\")\n\n# Detailed silhouette plot for K=3\nkm_3 &lt;- kmeans(sample_data, centers = 3, nstart = 20)\nsil_3 &lt;- silhouette(km_3$cluster, dist(sample_data))\nplot(sil_3, col = 1:3, main = \"Silhouette Plot (K=3)\")\n\n\n\n\n\n\n\n\nFigure 32.5: Silhouette analysis for different numbers of clusters. Higher average silhouette width indicates better-defined clusters.\n\n\n\n\n\n\n\nGap Statistic\nThe gap statistic compares the within-cluster dispersion to that expected under a null reference distribution:\n\n\nCode\nlibrary(cluster)\ngap_stat &lt;- clusGap(sample_data, FUN = kmeans, nstart = 25, K.max = 10, B = 50)\nplot(gap_stat, main = \"Gap Statistic\")",
    "crumbs": [
      "Statistical Learning",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Clustering</span>"
    ]
  },
  {
    "objectID": "chapters/33-clustering.html#heatmaps",
    "href": "chapters/33-clustering.html#heatmaps",
    "title": "32  Clustering",
    "section": "32.6 Heatmaps",
    "text": "32.6 Heatmaps\nHeatmaps are powerful visualization tools that combine clustering with color-coded data display. They’re particularly useful for high-dimensional data like gene expression matrices.\n\nBasic Heatmap Construction\nThe idea:\n\nPlot the data matrix as an image (colors represent values)\nCluster rows (observations) and reorder by similarity\nCluster columns (features) and reorder by similarity\nDisplay dendrograms showing the hierarchical structure\n\n\n\nHeatmaps in R\n\n\nCode\ndata(\"tissue_gene_expression\")\n\n# Center the data\nx_tissue &lt;- sweep(tissue_gene_expression$x, 2,\n                   colMeans(tissue_gene_expression$x))\n\n# Basic heatmap (subsample for visibility)\nset.seed(42)\ngene_sample &lt;- sample(1:ncol(x_tissue), 50)\nheatmap(x_tissue[, gene_sample],\n        col = brewer.pal(11, \"RdBu\"),\n        margins = c(8, 8),\n        cexRow = 0.7,\n        main = \"Tissue Gene Expression\")\n\n\n\n\n\n\n\n\nFigure 32.6: Heatmap of tissue gene expression data with hierarchical clustering on both rows and columns\n\n\n\n\n\n\n\nCustomizing Heatmaps with Color Bars\nAdding color bars shows group membership:\n\n\nCode\n# Color palette for tissues\ntissue_colors &lt;- brewer.pal(7, \"Set2\")[as.numeric(tissue_gene_expression$y)]\n\n# Heatmap with row colors\nheatmap(x_tissue[, gene_sample],\n        col = brewer.pal(11, \"RdBu\"),\n        RowSideColors = tissue_colors,\n        margins = c(8, 10),\n        cexRow = 0.7,\n        main = \"Tissue Gene Expression\")\n\n# Add legend\nlegend(\"topright\", legend = levels(tissue_gene_expression$y),\n       fill = brewer.pal(7, \"Set2\"), cex = 0.7, bty = \"n\")\n\n\n\n\n\n\n\n\nFigure 32.7: Heatmap with color sidebar indicating tissue type. Hierarchical clustering reveals that samples group by tissue.\n\n\n\n\n\n\n\nFiltering Features for Better Visualization\nNot all features contribute meaningful information to clustering. Filtering to high-variance features often improves visualization:\n\n\nCode\nlibrary(matrixStats)\n\n# Select 50 most variable genes\nsds &lt;- colSds(x_tissue, na.rm = TRUE)\ntop_genes &lt;- order(sds, decreasing = TRUE)[1:50]\n\nheatmap(x_tissue[, top_genes],\n        col = brewer.pal(11, \"RdBu\"),\n        RowSideColors = tissue_colors,\n        margins = c(10, 10),\n        cexRow = 0.7, cexCol = 0.6,\n        main = \"Top 50 Variable Genes\")\n\n\n\n\n\n\n\n\nFigure 32.8: Heatmap of the 50 most variable genes. High-variance features often reveal the most interesting biological patterns.\n\n\n\n\n\n\n\nAdvanced Heatmaps with pheatmap\nThe pheatmap package provides more control and better aesthetics:\n\n\nCode\nlibrary(pheatmap)\n\n# Prepare annotation\nannotation_row &lt;- data.frame(\n  Tissue = tissue_gene_expression$y,\n  row.names = rownames(x_tissue)\n)\n\npheatmap(x_tissue[, top_genes],\n         color = colorRampPalette(brewer.pal(11, \"RdBu\"))(100),\n         annotation_row = annotation_row,\n         show_rownames = FALSE,\n         show_colnames = FALSE,\n         main = \"Tissue Gene Expression Heatmap\",\n         fontsize = 8)\n\n\n\n\n\n\n\n\nFigure 32.9: Enhanced heatmap using pheatmap with annotation tracks and customized appearance",
    "crumbs": [
      "Statistical Learning",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Clustering</span>"
    ]
  },
  {
    "objectID": "chapters/33-clustering.html#practical-considerations",
    "href": "chapters/33-clustering.html#practical-considerations",
    "title": "32  Clustering",
    "section": "32.7 Practical Considerations",
    "text": "32.7 Practical Considerations\n\nStandardization\nFeatures on different scales should be standardized before clustering:\n\n\nCode\n# Standardize features\nx_scaled &lt;- scale(x_tissue)\n\n# This ensures all features contribute equally to distance calculations\n\n\n\n\nHandling Missing Data\nK-means cannot handle missing values. Options include:\n\nRemove observations or features with missing data\nImpute missing values\nUse algorithms that handle missingness (some hierarchical methods)\n\n\n\nCluster Validation\nHow do we know if clusters are “real”? Several approaches:\n\nStability: Do clusters persist across different subsamples?\nExternal validation: Do clusters correspond to known groups?\nInternal metrics: Silhouette scores, within-cluster variance\n\n\n\nCode\n# Compare clusters to known tissue labels\ntable(Cluster = km_3$cluster, True = sample_data$x &gt; 2)\n\n\n       True\nCluster FALSE TRUE\n      1    20    0\n      2     0   26\n      3    11    3\n\n\n\n\nReproducibility\nK-means depends on random initialization. Always:\n\nSet a seed for reproducibility\nUse nstart to run multiple initializations\nReport the random seed used",
    "crumbs": [
      "Statistical Learning",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Clustering</span>"
    ]
  },
  {
    "objectID": "chapters/33-clustering.html#exercises",
    "href": "chapters/33-clustering.html#exercises",
    "title": "32  Clustering",
    "section": "32.8 Exercises",
    "text": "32.8 Exercises\n\n\n\n\n\n\nExercise Clu.1: Distance Computation\n\n\n\n\nLoad the tissue_gene_expression dataset. Remove the row means and compute the distance between each observation. Store the result in d.\n\n\n\n\n\n\n\n\n\nExercise Clu.2: Hierarchical Clustering\n\n\n\n\nMake a hierarchical clustering plot and add the tissue types as labels. Which tissues cluster together?\n\n\n\n\n\n\n\n\n\nExercise Clu.3: K-Means Clustering\n\n\n\n\nRun k-means clustering on the data with \\(K = 7\\). Make a table comparing the identified clusters to the actual tissue types. Run the algorithm several times to see how the answer changes.\n\n\n\n\n\n\n\n\n\nExercise Clu.4: Heatmap Visualization\n\n\n\n\nSelect the 50 most variable genes. Create a heatmap with:\n\nObservations in columns\nCentered predictors\nA color bar showing tissue types\nUse ColSideColors argument and col = brewer.pal(11, \"RdBu\")\n\n\n\n\n\n\n\n\n\n\nExercise Clu.5: Comparing Linkage Methods\n\n\n\n\nCompare different linkage methods on the movie data. Which produces the most interpretable clusters?",
    "crumbs": [
      "Statistical Learning",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Clustering</span>"
    ]
  },
  {
    "objectID": "chapters/33-clustering.html#k-means-vs-hierarchical-clustering",
    "href": "chapters/33-clustering.html#k-means-vs-hierarchical-clustering",
    "title": "32  Clustering",
    "section": "32.9 K-Means vs Hierarchical Clustering",
    "text": "32.9 K-Means vs Hierarchical Clustering\n\n\n\nAspect\nK-Means\nHierarchical\n\n\n\n\nClusters\nMust specify K\nCan cut at any level\n\n\nStructure\nFlat partition\nNested hierarchy\n\n\nScalability\nO(nKt) - efficient\nO(n³) - slower\n\n\nShape\nSpherical clusters\nArbitrary shapes\n\n\nMissing data\nCannot handle\nSome methods can\n\n\nReproducibility\nDepends on initialization\nDeterministic",
    "crumbs": [
      "Statistical Learning",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Clustering</span>"
    ]
  },
  {
    "objectID": "chapters/33-clustering.html#summary",
    "href": "chapters/33-clustering.html#summary",
    "title": "32  Clustering",
    "section": "32.10 Summary",
    "text": "32.10 Summary\n\nUnsupervised learning discovers structure without predefined labels\nHierarchical clustering builds a tree showing nested relationships\n\nDendrograms visualize the hierarchy\nLinkage methods define cluster-to-cluster distance\nCut the tree to form discrete clusters\n\nK-means clustering partitions data into K groups\n\nMinimizes within-cluster variance\nRequires specifying K in advance\nUse multiple starts to avoid local optima\n\nHeatmaps combine clustering with data visualization\n\nColor encodes values\nRows and columns reordered by similarity\nFilter to high-variance features for clarity\n\nChoosing K requires judgment and multiple criteria\n\nElbow method: look for bend in WSS curve\nSilhouette analysis: measure cluster coherence\nDomain knowledge should guide interpretation",
    "crumbs": [
      "Statistical Learning",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Clustering</span>"
    ]
  },
  {
    "objectID": "chapters/33-clustering.html#additional-resources",
    "href": "chapters/33-clustering.html#additional-resources",
    "title": "32  Clustering",
    "section": "32.11 Additional Resources",
    "text": "32.11 Additional Resources\n\nJames et al. (2023) - Comprehensive treatment of clustering methods\nLogan (2010) - Applications in biological research\nHahsler, M., Piekenbrock, M., & Doran, D. (2019). dbscan: Fast Density-Based Clustering with R\npheatmap and ComplexHeatmap package documentation for advanced heatmaps\n\n\n\n\n\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2023. An Introduction to Statistical Learning with Applications in r. 2nd ed. Springer. https://www.statlearning.com.\n\n\nLogan, Murray. 2010. Biostatistical Design and Analysis Using r. Wiley-Blackwell.",
    "crumbs": [
      "Statistical Learning",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Clustering</span>"
    ]
  },
  {
    "objectID": "chapters/23-dimensionality-reduction.html",
    "href": "chapters/23-dimensionality-reduction.html",
    "title": "33  Dimensionality Reduction and Multivariate Methods",
    "section": "",
    "text": "33.1 The Challenge of High-Dimensional Data\nModern biology generates datasets with many variables: gene expression across thousands of genes, metabolomic profiles with hundreds of compounds, morphological measurements on many traits. When datasets have many variables, visualization becomes challenging and statistical analysis becomes complicated.\nA typical machine learning challenge might include hundreds or thousands of predictors. For example, to compare each of the 784 features in a digit recognition problem, we would need to create 306,936 scatterplots! Creating a single scatter-plot of all the data is impossible due to the high dimensionality.\nDimensionality reduction creates a smaller set of new variables that capture most of the information in the original data. The general idea is to reduce the dimension of the dataset while preserving important characteristics, such as the distance between features or observations. These techniques help us:",
    "crumbs": [
      "Statistical Learning",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Dimensionality Reduction and Multivariate Methods</span>"
    ]
  },
  {
    "objectID": "chapters/23-dimensionality-reduction.html#the-challenge-of-high-dimensional-data",
    "href": "chapters/23-dimensionality-reduction.html#the-challenge-of-high-dimensional-data",
    "title": "33  Dimensionality Reduction and Multivariate Methods",
    "section": "",
    "text": "Visualize high-dimensional data in 2D or 3D\nIdentify patterns and clusters\nRemove noise and redundancy\nReduce complexity of downstream models\nCreate composite variables for analysis",
    "crumbs": [
      "Statistical Learning",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Dimensionality Reduction and Multivariate Methods</span>"
    ]
  },
  {
    "objectID": "chapters/23-dimensionality-reduction.html#principal-component-analysis-pca",
    "href": "chapters/23-dimensionality-reduction.html#principal-component-analysis-pca",
    "title": "33  Dimensionality Reduction and Multivariate Methods",
    "section": "33.2 Principal Component Analysis (PCA)",
    "text": "33.2 Principal Component Analysis (PCA)\nPrincipal Component Analysis (PCA) (Pearson 1901; Hotelling 1933) is the most widely used dimensionality reduction technique. It finds new variables (principal components) that are linear combinations of the originals, chosen to capture maximum variance. The technique behind it—the singular value decomposition—is also useful in many other contexts.\n\nIntuition: Preserving Distance\nBefore diving into the mathematics, let’s build intuition with a simple example. Consider twin heights data where we have height measurements for 100 pairs of twins—some adults, some children.\n\n\nCode\nset.seed(1988)\nn &lt;- 100\nSigma &lt;- matrix(c(9, 9 * 0.9, 9 * 0.92, 9 * 1), 2, 2)\nx &lt;- rbind(mvrnorm(n / 2, c(69, 69), Sigma),\n           mvrnorm(n / 2, c(55, 55), Sigma))\n\nlim &lt;- c(48, 78)\nrafalib::mypar()\nplot(x, xlim = lim, ylim = lim,\n     xlab = \"Twin 1 Height\", ylab = \"Twin 2 Height\",\n     main = \"Twin Heights: Adults and Children\")\nlines(x[c(1, 2), ], col = \"blue\", lwd = 2)\nlines(x[c(2, 51), ], col = \"red\", lwd = 2)\npoints(x[c(1, 2, 51), ], pch = 16)\n\n\n\n\n\n\n\n\nFigure 33.1: Simulated twin heights showing two groups (adults and children) with high correlation between twins\n\n\n\n\n\nThe scatterplot reveals high correlation and two clear groups: adults (upper right) and children (lower left). We can compute distances between observations:\n\n\nCode\nd &lt;- dist(x)\ncat(\"Distance between observations 1 and 2:\", round(as.matrix(d)[1, 2], 2), \"\\n\")\n\n\nDistance between observations 1 and 2: 1.98 \n\n\nCode\ncat(\"Distance between observations 2 and 51:\", round(as.matrix(d)[2, 51], 2), \"\\n\")\n\n\nDistance between observations 2 and 51: 18.74 \n\n\nNow suppose we want to reduce from two dimensions to one while preserving these distances. A naive approach is to simply use one of the original variables:\n\n\nCode\nz &lt;- x[, 1]\nrafalib::mypar()\nplot(dist(x) / sqrt(2), dist(z),\n     xlab = \"Original distance (scaled)\", ylab = \"One-dimension distance\",\n     main = \"Distance Approximation Using One Variable\")\nabline(0, 1, col = \"red\")\n\n\n\n\n\n\n\n\nFigure 33.2: Using only one dimension (Twin 1 height) underestimates the true distances\n\n\n\n\n\nThis works reasonably well, but we can do better. Notice that the variation in the data lies mostly along the diagonal. If we transform to the average and difference:\n\n\nCode\nz &lt;- cbind((x[, 2] + x[, 1]) / 2, x[, 2] - x[, 1])\nrafalib::mypar()\nplot(z, xlim = lim, ylim = lim - mean(lim),\n     xlab = \"Average Height\", ylab = \"Difference\",\n     main = \"Transformed Coordinates\")\nlines(z[c(1, 2), ], col = \"blue\", lwd = 2)\nlines(z[c(2, 51), ], col = \"red\", lwd = 2)\npoints(z[c(1, 2, 51), ], pch = 16)\n\n\n\n\n\n\n\n\nFigure 33.3: After rotating to average and difference coordinates, distances are mostly explained by the first dimension\n\n\n\n\n\nNow the distances are mostly explained by the first dimension (the average). Using just this one dimension gives better distance approximation:\n\n\nCode\ncat(\"Typical error with original variable:\",\n    round(sd(dist(x) - dist(x[, 1]) * sqrt(2)), 2), \"\\n\")\n\n\nTypical error with original variable: 1.21 \n\n\nCode\ncat(\"Typical error with average:\",\n    round(sd(dist(x) - dist(z[, 1]) * sqrt(2)), 2), \"\\n\")\n\n\nTypical error with average: 0.32 \n\n\nThe average of the twin heights is essentially the first principal component! PCA finds these optimal linear combinations automatically.\n\n\nLinear Transformations\nEach row of the original matrix \\(X\\) was transformed using a linear transformation to create \\(Z\\):\n\\[Z_{i,1} = a_{1,1} X_{i,1} + a_{2,1} X_{i,2}\\]\nwith \\(a_{1,1} = 0.5\\) and \\(a_{2,1} = 0.5\\) (the average).\nIn matrix notation: \\[\nZ = X A\n\\mbox{ with }\nA = \\begin{pmatrix}\n1/2 & 1 \\\\\n1/2 & -1\n\\end{pmatrix}\n\\]\nDimension reduction can be described as applying a transformation \\(A\\) to a matrix \\(X\\) that moves the information to the first few columns of \\(Z = XA\\), then keeping just these informative columns.\n\n\nOrthogonal Transformations\nTo preserve distances exactly, we need an orthogonal transformation—one where the columns of \\(A\\) have unit length and are perpendicular:\n\n\nCode\n# Orthogonal transformation\nz[, 1] &lt;- (x[, 1] + x[, 2]) / sqrt(2)\nz[, 2] &lt;- (x[, 2] - x[, 1]) / sqrt(2)\n\n# This preserves the original distances exactly\ncat(\"Maximum distance difference:\", max(abs(dist(z) - dist(x))), \"\\n\")\n\n\nMaximum distance difference: 3.241851e-14 \n\n\nAn orthogonal rotation preserves all distances while reorganizing the variance. Now most variance is in the first dimension:\n\n\nCode\nqplot(z[, 1], bins = 20, color = I(\"black\"), fill = I(\"steelblue\")) +\n  labs(x = \"First Principal Component\", y = \"Count\",\n       title = \"PC1 Separates Adults from Children\")\n\n\n\n\n\n\n\n\nFigure 33.4: After orthogonal rotation, the first dimension clearly separates adults from children\n\n\n\n\n\nThe first PC clearly shows the two groups. We’ve reduced two dimensions to one with minimal information loss because the original variables were highly correlated:\n\n\nCode\ncat(\"Correlation between twin heights:\", round(cor(x[, 1], x[, 2]), 3), \"\\n\")\n\n\nCorrelation between twin heights: 0.988 \n\n\nCode\ncat(\"Correlation between PCs:\", round(cor(z[, 1], z[, 2]), 3), \"\\n\")\n\n\nCorrelation between PCs: 0.088 \n\n\n\n\nThe Eigenanalysis Foundation\nThe mathematical foundation involves eigenanalysis: decomposing the covariance (or correlation) matrix to find directions of maximum variation. For a detailed treatment of eigenvalues, eigenvectors, and their interpretation, see Section 46.9.\nGiven a covariance matrix \\(\\Sigma\\), eigenanalysis finds:\n\nEigenvectors: The directions of the principal components (loadings)\nEigenvalues: The variance explained by each component\n\nThe first principal component points in the direction of maximum variance. Each subsequent component is orthogonal (uncorrelated) and captures remaining variance in decreasing order.\nThe total variability can be defined as the sum of variances: \\[\nv_1 + v_2 + \\cdots + v_p\n\\]\nAn orthogonal transformation preserves total variability but redistributes it. PCA finds the transformation that concentrates variance in the first few components.\n\n\n\n\n\n\nInterpreting Eigenvalues and Eigenvectors\n\n\n\n\nEigenvalues tell you how much variance each PC captures—larger means more important\nEigenvectors (loadings) tell you how original variables combine to form each PC\nVariables with large absolute loadings contribute strongly to that PC\nThe sum of all eigenvalues equals the total variance in the data\n\nSee Section 46.9.5 for a complete explanation with examples.\n\n\n\n\n\n\n\n\nFigure 33.5: Geometric interpretation of principal components as directions of maximum variance\n\n\n\n\n\nPCA in R\n\n\nCode\n# PCA on iris data\niris_pca &lt;- prcomp(iris[, 1:4], scale. = TRUE)\n\n# Variance explained\nsummary(iris_pca)\n\n\nImportance of components:\n                          PC1    PC2     PC3     PC4\nStandard deviation     1.7084 0.9560 0.38309 0.14393\nProportion of Variance 0.7296 0.2285 0.03669 0.00518\nCumulative Proportion  0.7296 0.9581 0.99482 1.00000\n\n\nCode\n# Scree plot\npar(mfrow = c(1, 2))\nplot(iris_pca, type = \"l\", main = \"Scree Plot\")\n\n# PC scores colored by species\nplot(iris_pca$x[, 1:2],\n     col = c(\"red\", \"green\", \"blue\")[iris$Species],\n     pch = 19, xlab = \"PC1\", ylab = \"PC2\",\n     main = \"PCA of Iris Data\")\nlegend(\"topright\", levels(iris$Species),\n       col = c(\"red\", \"green\", \"blue\"), pch = 19)\n\n\n\n\n\n\n\n\nFigure 33.6: PCA scree plot and principal component scores for iris data colored by species\n\n\n\n\n\n\n\nLoadings: What Variables Drive Each PC?\nEach principal component is defined by its loadings—the coefficients showing how much each original variable contributes:\n\n\nCode\n# Loadings (rotation matrix)\niris_pca$rotation\n\n\n                    PC1         PC2        PC3        PC4\nSepal.Length  0.5210659 -0.37741762  0.7195664  0.2612863\nSepal.Width  -0.2693474 -0.92329566 -0.2443818 -0.1235096\nPetal.Length  0.5804131 -0.02449161 -0.1421264 -0.8014492\nPetal.Width   0.5648565 -0.06694199 -0.6342727  0.5235971\n\n\nLarge absolute loadings indicate that a variable strongly influences that component. The sign indicates the direction of the relationship.\n\n\nInterpreting PCA Results\nKey elements of PCA output:\n\nEigenvalues: Variance explained by each component (shown in scree plot)\nProportion of variance: How much of total variance each PC captures\nLoadings: Coefficients relating original variables to PCs\nScores: Values of the new variables for each observation\n\nThe first few PCs often capture most of the meaningful variation, allowing you to reduce many variables to just 2-3 for visualization and analysis.\n\n\n\n\n\n\nHow Many Components to Keep?\n\n\n\nCommon approaches:\n\nKeep components with eigenvalues &gt; 1 (Kaiser criterion)\nKeep enough to explain 80-90% of variance\nLook for an “elbow” in the scree plot\nUse cross-validation if using PCs for prediction\n\n\n\n\n\nBiplot Visualization\nA biplot shows both observations (scores) and variables (loadings) on the same plot:\n\n\nCode\nbiplot(iris_pca, col = c(\"gray50\", \"red\"), cex = 0.7,\n       main = \"PCA Biplot of Iris Data\")\n\n\n\n\n\n\n\n\nFigure 33.7: PCA biplot showing both observations and variable loadings simultaneously\n\n\n\n\n\nArrows show variable loadings—their direction and length indicate how each variable relates to the principal components.\n\n\nMNIST Example: High-Dimensional Image Data\nThe real power of PCA becomes apparent with truly high-dimensional data. The MNIST dataset of handwritten digits has 784 features (28×28 pixels). Can we reduce this dimensionality while preserving useful information?\n\n\nCode\nlibrary(dslabs)\nif (!exists(\"mnist\")) mnist &lt;- read_mnist()\n\n\nBecause pixels near each other on the image grid are correlated, we expect dimension reduction to work well:\n\n\nCode\ncol_means &lt;- colMeans(mnist$test$images)\npca &lt;- prcomp(mnist$train$images)\n\n# Scree plot\npc &lt;- 1:ncol(mnist$test$images)\nqplot(pc[1:50], pca$sdev[1:50], geom = \"line\") +\n  labs(x = \"Principal Component\", y = \"Standard Deviation\",\n       title = \"MNIST Scree Plot (first 50 PCs)\")\n\n\n\n\n\n\n\n\nFigure 33.8: Variance explained by principal components of MNIST digit images. The first few PCs capture substantial variance.\n\n\n\n\n\nThe first few PCs capture substantial variance:\n\n\nCode\nsummary(pca)$importance[, 1:5]\n\n\n                             PC1       PC2       PC3       PC4       PC5\nStandard deviation     576.82291 493.23822 459.89930 429.85624 408.56680\nProportion of Variance   0.09705   0.07096   0.06169   0.05389   0.04869\nCumulative Proportion    0.09705   0.16801   0.22970   0.28359   0.33228\n\n\nEven with just two dimensions, we can see structure related to digit class:\n\n\nCode\ndata.frame(PC1 = pca$x[, 1], PC2 = pca$x[, 2],\n           label = factor(mnist$train$label)) %&gt;%\n  sample_n(2000) %&gt;%\n  ggplot(aes(PC1, PC2, fill = label)) +\n  geom_point(cex = 3, pch = 21, alpha = 0.7) +\n  labs(title = \"MNIST Digits in PC Space\")\n\n\n\n\n\n\n\n\nFigure 33.9: First two principal components of MNIST data colored by digit label. Different digits occupy different regions of PC space.\n\n\n\n\n\nWe can visualize what each PC “looks for” by reshaping the loadings back to the 28×28 grid:\n\n\nCode\nlibrary(RColorBrewer)\ntmp &lt;- lapply(1:4, function(i) {\n  expand.grid(Row = 1:28, Column = 1:28) %&gt;%\n    mutate(id = i, label = paste0(\"PC\", i),\n           value = pca$rotation[, i])\n})\ntmp &lt;- Reduce(rbind, tmp)\n\ntmp %&gt;%\n  ggplot(aes(Row, Column, fill = value)) +\n  geom_raster() +\n  scale_y_reverse() +\n  scale_fill_gradientn(colors = brewer.pal(9, \"RdBu\")) +\n  facet_wrap(~label, nrow = 1) +\n  theme_minimal() +\n  labs(title = \"PCA Loadings as Images\")\n\n\n\n\n\n\n\n\nFigure 33.10: First four principal component loadings visualized as images. Each PC captures different aspects of digit structure.\n\n\n\n\n\n\n\nApplying PCA to Improve Classification\nPCA can reduce model complexity while maintaining predictive performance. Let’s use 36 PCs (explaining ~80% of variance) for kNN classification:\n\n\nCode\nlibrary(caret)\nk &lt;- 36\nx_train &lt;- pca$x[, 1:k]\ny &lt;- factor(mnist$train$labels)\nfit &lt;- knn3(x_train, y)\n\n# Transform test set using training PCA\nx_test &lt;- sweep(mnist$test$images, 2, col_means) %*% pca$rotation\nx_test &lt;- x_test[, 1:k]\n\n# Predict and evaluate\ny_hat &lt;- predict(fit, x_test, type = \"class\")\nconfusionMatrix(y_hat, factor(mnist$test$labels))$overall[\"Accuracy\"]\n\n\nAccuracy \n  0.9751 \n\n\nWith just 36 dimensions (instead of 784), we achieve excellent accuracy. This demonstrates the power of PCA for both visualization and model simplification.",
    "crumbs": [
      "Statistical Learning",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Dimensionality Reduction and Multivariate Methods</span>"
    ]
  },
  {
    "objectID": "chapters/23-dimensionality-reduction.html#principal-coordinate-analysis-pcoa",
    "href": "chapters/23-dimensionality-reduction.html#principal-coordinate-analysis-pcoa",
    "title": "33  Dimensionality Reduction and Multivariate Methods",
    "section": "33.3 Principal Coordinate Analysis (PCoA)",
    "text": "33.3 Principal Coordinate Analysis (PCoA)\nWhile PCA uses correlations among variables, Principal Coordinate Analysis (PCoA) (also called Metric Multidimensional Scaling) starts with a dissimilarity matrix among observations. This is valuable when:\n\nYou have a meaningful distance metric (e.g., genetic distances)\nVariables are mixed types or non-numeric\nThe data are counts (e.g., microbiome data)\n\n\n\nCode\n# PCoA example using Euclidean distances\ndist_matrix &lt;- dist(iris[, 1:4])\npcoa_result &lt;- cmdscale(dist_matrix, k = 2, eig = TRUE)\n\n# Proportion of variance explained\neig_vals &lt;- pcoa_result$eig[pcoa_result$eig &gt; 0]\nvar_explained &lt;- eig_vals / sum(eig_vals)\ncat(\"Variance explained by first two axes:\",\n    round(sum(var_explained[1:2]) * 100, 1), \"%\\n\")\n\n\nVariance explained by first two axes: 97.8 %\n\n\nCode\n# Plot\nplot(pcoa_result$points,\n     col = c(\"red\", \"green\", \"blue\")[iris$Species],\n     pch = 19,\n     xlab = paste0(\"PCoA1 (\", round(var_explained[1]*100, 1), \"%)\"),\n     ylab = paste0(\"PCoA2 (\", round(var_explained[2]*100, 1), \"%)\"),\n     main = \"PCoA of Iris Data\")\nlegend(\"topright\", levels(iris$Species),\n       col = c(\"red\", \"green\", \"blue\"), pch = 19)\n\n\n\n\n\n\n\n\nFigure 33.11: Principal Coordinate Analysis (PCoA) ordination of iris data using Euclidean distances\n\n\n\n\n\n\nWhen to Use PCoA vs. PCA\n\nPCA: Variables are measured on a common scale; interested in variable contributions\nPCoA: Have a distance matrix; want to preserve distances among samples\nFor Euclidean distances, PCA and PCoA give equivalent results",
    "crumbs": [
      "Statistical Learning",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Dimensionality Reduction and Multivariate Methods</span>"
    ]
  },
  {
    "objectID": "chapters/23-dimensionality-reduction.html#non-metric-multidimensional-scaling-nmds",
    "href": "chapters/23-dimensionality-reduction.html#non-metric-multidimensional-scaling-nmds",
    "title": "33  Dimensionality Reduction and Multivariate Methods",
    "section": "33.4 Non-Metric Multidimensional Scaling (NMDS)",
    "text": "33.4 Non-Metric Multidimensional Scaling (NMDS)\nNMDS is an ordination technique that preserves rank-order of distances rather than exact distances. It’s widely used in ecology because it makes no assumptions about the data distribution.\n\n\nCode\n# NMDS example\nlibrary(vegan)\nnmds_result &lt;- metaMDS(iris[, 1:4], k = 2, trymax = 100, trace = FALSE)\n\n# Stress value indicates fit (&lt; 0.1 is good, &lt; 0.2 is acceptable)\ncat(\"Stress:\", round(nmds_result$stress, 3), \"\\n\")\n\n\nStress: 0.038 \n\n\nCode\nplot(nmds_result$points,\n     col = c(\"red\", \"green\", \"blue\")[iris$Species],\n     pch = 19, xlab = \"NMDS1\", ylab = \"NMDS2\",\n     main = \"NMDS of Iris Data\")\nlegend(\"topright\", levels(iris$Species),\n       col = c(\"red\", \"green\", \"blue\"), pch = 19)\n\n\n\n\n\n\n\n\nFigure 33.12: Non-metric multidimensional scaling (NMDS) ordination of iris data\n\n\n\n\n\n\n\n\n\n\n\nMetric vs. Non-Metric Methods\n\n\n\nPCA and metric PCoA produce scores on a ratio scale—differences between scores are meaningful. These can be used directly in linear models.\nNon-metric multidimensional scaling (NMDS) produces ordinal rankings only. NMDS scores should not be used in parametric analyses like ANOVA or regression.",
    "crumbs": [
      "Statistical Learning",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Dimensionality Reduction and Multivariate Methods</span>"
    ]
  },
  {
    "objectID": "chapters/23-dimensionality-reduction.html#manova-multivariate-analysis-of-variance",
    "href": "chapters/23-dimensionality-reduction.html#manova-multivariate-analysis-of-variance",
    "title": "33  Dimensionality Reduction and Multivariate Methods",
    "section": "33.5 MANOVA: Multivariate Analysis of Variance",
    "text": "33.5 MANOVA: Multivariate Analysis of Variance\nWhen you have multiple response variables and want to test for group differences, MANOVA (Multivariate Analysis of Variance) is the appropriate technique. It extends ANOVA to multiple dependent variables simultaneously.\n\nWhy Not Multiple ANOVAs?\nRunning separate ANOVAs on each variable:\n\nIgnores correlations among response variables\nInflates Type I error rate with multiple tests\nMay miss differences only apparent when variables are considered together\n\nMANOVA tests whether group centroids differ in multivariate space.\n\n\nThe MANOVA Framework\nMANOVA decomposes the total multivariate variation:\n\\[\\mathbf{T} = \\mathbf{H} + \\mathbf{E}\\]\nwhere:\n\nT: Total sum of squares and cross-products matrix\nH: Hypothesis (between-groups) matrix\nE: Error (within-groups) matrix\n\nThese are matrices because we have multiple response variables.\n\n\n\n\n\n\nFigure 33.13: MANOVA decomposes multivariate variation into between-group and within-group components\n\n\n\n\n\nTest Statistics\nSeveral test statistics exist for MANOVA, each a function of the eigenvalues of \\(\\mathbf{HE}^{-1}\\):\n\n\n\n\n\n\n\nStatistic\nDescription\n\n\n\n\nWilks’ Lambda (Λ)\nProduct of 1/(1+λᵢ); most commonly used\n\n\nHotelling-Lawley Trace\nSum of eigenvalues\n\n\nPillai’s Trace\nSum of λᵢ/(1+λᵢ); most robust\n\n\nRoy’s Largest Root\nMaximum eigenvalue; most powerful but sensitive\n\n\n\nPillai’s Trace is generally recommended because it’s most robust to violations of assumptions.\n\n\nMANOVA in R\n\n\nCode\n# MANOVA on iris data\nmanova_model &lt;- manova(cbind(Sepal.Length, Sepal.Width,\n                              Petal.Length, Petal.Width) ~ Species,\n                       data = iris)\n\n# Summary with different test statistics\nsummary(manova_model, test = \"Pillai\")\n\n\n           Df Pillai approx F num Df den Df    Pr(&gt;F)    \nSpecies     2 1.1919   53.466      8    290 &lt; 2.2e-16 ***\nResiduals 147                                            \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCode\nsummary(manova_model, test = \"Wilks\")\n\n\n           Df    Wilks approx F num Df den Df    Pr(&gt;F)    \nSpecies     2 0.023439   199.15      8    288 &lt; 2.2e-16 ***\nResiduals 147                                              \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe significant result tells us that species differ in their multivariate centroid—the combination of all four measurements.\n\n\nFollow-Up Analyses\nA significant MANOVA should be followed by:\n\nUnivariate ANOVAs to see which variables differ\nDiscriminant Function Analysis to understand how groups differ\n\n\n\nCode\n# Univariate follow-ups\nsummary.aov(manova_model)\n\n\n Response Sepal.Length :\n             Df Sum Sq Mean Sq F value    Pr(&gt;F)    \nSpecies       2 63.212  31.606  119.26 &lt; 2.2e-16 ***\nResiduals   147 38.956   0.265                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n Response Sepal.Width :\n             Df Sum Sq Mean Sq F value    Pr(&gt;F)    \nSpecies       2 11.345  5.6725   49.16 &lt; 2.2e-16 ***\nResiduals   147 16.962  0.1154                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n Response Petal.Length :\n             Df Sum Sq Mean Sq F value    Pr(&gt;F)    \nSpecies       2 437.10 218.551  1180.2 &lt; 2.2e-16 ***\nResiduals   147  27.22   0.185                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n Response Petal.Width :\n             Df Sum Sq Mean Sq F value    Pr(&gt;F)    \nSpecies       2 80.413  40.207  960.01 &lt; 2.2e-16 ***\nResiduals   147  6.157   0.042                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\nMANOVA Assumptions\nMANOVA assumes:\n\nMultivariate normality within groups\nHomogeneity of covariance matrices across groups\nIndependence of observations\nNo multicollinearity among response variables\n\nTest homogeneity of covariance matrices with Box’s M test (though it’s sensitive to non-normality):\n\n\nCode\n# Box's M test (requires biotools package)\n# library(biotools)\n# boxM(iris[, 1:4], iris$Species)",
    "crumbs": [
      "Statistical Learning",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Dimensionality Reduction and Multivariate Methods</span>"
    ]
  },
  {
    "objectID": "chapters/23-dimensionality-reduction.html#discriminant-function-analysis-dfa",
    "href": "chapters/23-dimensionality-reduction.html#discriminant-function-analysis-dfa",
    "title": "33  Dimensionality Reduction and Multivariate Methods",
    "section": "33.6 Discriminant Function Analysis (DFA)",
    "text": "33.6 Discriminant Function Analysis (DFA)\nDiscriminant Function Analysis (DFA, also called Linear Discriminant Analysis or LDA) finds linear combinations of variables that best separate groups. It complements MANOVA by showing how groups differ.\n\nThe Goal of DFA\nDFA finds discriminant functions—weighted combinations of original variables—that maximize separation between groups while minimizing variation within groups.\nThe first discriminant function captures the most separation, the second captures remaining separation orthogonal to the first, and so on.\n\n\n\n\n\n\nFigure 33.14: Discriminant function analysis finds linear combinations that maximize group separation\n\n\n\n\n\nDFA in R\n\n\nCode\n# Linear Discriminant Analysis\nlda_model &lt;- lda(Species ~ ., data = iris)\n\n# View the model\nlda_model\n\n\nCall:\nlda(Species ~ ., data = iris)\n\nPrior probabilities of groups:\n    setosa versicolor  virginica \n 0.3333333  0.3333333  0.3333333 \n\nGroup means:\n           Sepal.Length Sepal.Width Petal.Length Petal.Width\nsetosa            5.006       3.428        1.462       0.246\nversicolor        5.936       2.770        4.260       1.326\nvirginica         6.588       2.974        5.552       2.026\n\nCoefficients of linear discriminants:\n                    LD1         LD2\nSepal.Length  0.8293776 -0.02410215\nSepal.Width   1.5344731 -2.16452123\nPetal.Length -2.2012117  0.93192121\nPetal.Width  -2.8104603 -2.83918785\n\nProportion of trace:\n   LD1    LD2 \n0.9912 0.0088 \n\n\nCode\n# Discriminant scores\nlda_scores &lt;- predict(lda_model)$x\n\n# Plot\nplot(lda_scores,\n     col = c(\"red\", \"green\", \"blue\")[iris$Species],\n     pch = 19,\n     main = \"Discriminant Function Scores\",\n     xlab = \"LD1\", ylab = \"LD2\")\nlegend(\"topright\", levels(iris$Species),\n       col = c(\"red\", \"green\", \"blue\"), pch = 19)\n\n\n\n\n\n\n\n\nFigure 33.15: Linear discriminant analysis showing group separation along discriminant functions\n\n\n\n\n\n\n\nInterpreting DFA Output\nKey components:\n\nCoefficients of linear discriminants: Weights for creating discriminant scores\nProportion of trace: Variance explained by each discriminant function\nGroup means: Average score on each discriminant function for each group\n\n\n\nCode\n# Coefficients (loadings)\nlda_model$scaling\n\n\n                    LD1         LD2\nSepal.Length  0.8293776 -0.02410215\nSepal.Width   1.5344731 -2.16452123\nPetal.Length -2.2012117  0.93192121\nPetal.Width  -2.8104603 -2.83918785\n\n\nCode\n# Proportion of separation explained\nlda_model$svd^2 / sum(lda_model$svd^2)\n\n\n[1] 0.991212605 0.008787395\n\n\n\n\nUsing DFA for Prediction\nDFA can classify new observations into groups based on their discriminant scores:\n\n\nCode\n# Classification accuracy\npredictions &lt;- predict(lda_model)$class\ntable(Predicted = predictions, Actual = iris$Species)\n\n\n            Actual\nPredicted    setosa versicolor virginica\n  setosa         50          0         0\n  versicolor      0         48         1\n  virginica       0          2        49\n\n\nCode\n# Classification accuracy\nmean(predictions == iris$Species)\n\n\n[1] 0.98\n\n\n\n\nCross-Validated Classification\nFor honest estimates of classification accuracy, use leave-one-out cross-validation:\n\n\nCode\n# Cross-validated LDA\nlda_cv &lt;- lda(Species ~ ., data = iris, CV = TRUE)\n\n# Cross-validated classification table\ntable(Predicted = lda_cv$class, Actual = iris$Species)\n\n\n            Actual\nPredicted    setosa versicolor virginica\n  setosa         50          0         0\n  versicolor      0         48         1\n  virginica       0          2        49\n\n\nCode\n# Cross-validated accuracy\nmean(lda_cv$class == iris$Species)\n\n\n[1] 0.98\n\n\n\n\nDFA for Biomarker Discovery\nDFA is valuable for identifying which variables best distinguish groups—useful in biomarker discovery:\n\n\nCode\n# Which variables contribute most to separation?\nscaling_df &lt;- data.frame(\n  Variable = rownames(lda_model$scaling),\n  LD1 = abs(lda_model$scaling[, 1]),\n  LD2 = abs(lda_model$scaling[, 2])\n)\n\nbarplot(scaling_df$LD1, names.arg = scaling_df$Variable,\n        main = \"Variable Contributions to LD1\",\n        ylab = \"Absolute Coefficient\",\n        col = \"steelblue\")\n\n\n\n\n\n\n\n\nFigure 33.16: Variable contributions to the first linear discriminant function",
    "crumbs": [
      "Statistical Learning",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Dimensionality Reduction and Multivariate Methods</span>"
    ]
  },
  {
    "objectID": "chapters/23-dimensionality-reduction.html#comparing-methods",
    "href": "chapters/23-dimensionality-reduction.html#comparing-methods",
    "title": "33  Dimensionality Reduction and Multivariate Methods",
    "section": "33.7 Comparing Methods",
    "text": "33.7 Comparing Methods\n\n\n\n\n\n\n\n\n\n\nMethod\nInput\nOutput\nSupervision\nBest For\n\n\n\n\nPCA\nVariables\nContinuous scores\nNone\nReducing correlated variables\n\n\nPCoA\nDistance matrix\nContinuous scores\nNone\nPreserving sample distances\n\n\nNMDS\nDistance matrix\nOrdinal scores\nNone\nEcological community data\n\n\nMANOVA\nVariables + groups\nTest statistics\nGroups known\nTesting group differences\n\n\nDFA\nVariables + groups\nDiscriminant scores\nGroups known\nClassifying observations\n\n\n\nFor clustering methods (hierarchical, k-means) that group observations based on similarity, see Chapter 32.",
    "crumbs": [
      "Statistical Learning",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Dimensionality Reduction and Multivariate Methods</span>"
    ]
  },
  {
    "objectID": "chapters/23-dimensionality-reduction.html#using-ordination-scores-in-further-analyses",
    "href": "chapters/23-dimensionality-reduction.html#using-ordination-scores-in-further-analyses",
    "title": "33  Dimensionality Reduction and Multivariate Methods",
    "section": "33.8 Using Ordination Scores in Further Analyses",
    "text": "33.8 Using Ordination Scores in Further Analyses\nPC scores and discriminant scores are legitimate new variables that can be used in downstream analysis:\n\nRegression of scores on other continuous variables\nANOVA comparing groups on ordination scores\nCorrelation of scores with environmental gradients\n\nThis is valuable when you have many correlated variables and want to reduce dimensionality before hypothesis testing.\n\n\nCode\n# Use PC scores in ANOVA\npc_scores &lt;- data.frame(\n  PC1 = iris_pca$x[, 1],\n  PC2 = iris_pca$x[, 2],\n  Species = iris$Species\n)\n\nsummary(aov(PC1 ~ Species, data = pc_scores))\n\n\n             Df Sum Sq Mean Sq F value Pr(&gt;F)    \nSpecies       2  406.4  203.21    1051 &lt;2e-16 ***\nResiduals   147   28.4    0.19                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1",
    "crumbs": [
      "Statistical Learning",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Dimensionality Reduction and Multivariate Methods</span>"
    ]
  },
  {
    "objectID": "chapters/23-dimensionality-reduction.html#practical-workflow",
    "href": "chapters/23-dimensionality-reduction.html#practical-workflow",
    "title": "33  Dimensionality Reduction and Multivariate Methods",
    "section": "33.9 Practical Workflow",
    "text": "33.9 Practical Workflow\n\nExplore data: Check for outliers, missing values, scaling issues\nStandardize if needed: Especially important when variables are on different scales\nChoose appropriate method: Based on your data type and question\nExamine output: Scree plots, loadings, clustering diagnostics\nValidate: Cross-validation for classification; permutation tests for significance\nInterpret biologically: What do the patterns mean in your system?",
    "crumbs": [
      "Statistical Learning",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Dimensionality Reduction and Multivariate Methods</span>"
    ]
  },
  {
    "objectID": "chapters/23-dimensionality-reduction.html#exercises",
    "href": "chapters/23-dimensionality-reduction.html#exercises",
    "title": "33  Dimensionality Reduction and Multivariate Methods",
    "section": "33.10 Exercises",
    "text": "33.10 Exercises\n\n\n\n\n\n\nExercise DR.1: PCA Exploration\n\n\n\n\nWe want to explore the tissue_gene_expression predictors by plotting them.\n\n\n\nCode\ndata(\"tissue_gene_expression\")\ndim(tissue_gene_expression$x)\n\n\nWe want to get an idea of which observations are close to each other, but the predictors are 500-dimensional so plotting is difficult. Plot the first two principal components with color representing tissue type.\n\nThe predictors for each observation are measured on the same device and experimental procedure. This introduces biases that can affect all the predictors from one observation. For each observation, compute the average across all predictors and then plot this against the first PC with color representing tissue. Report the correlation.\nWe see an association with the first PC and the observation averages. Redo the PCA but only after removing the center (row means).\nFor the first 10 PCs, make a boxplot showing the values for each tissue.\nPlot the percent variance explained by PC number. Hint: use the summary function.\n\n\n\n\n\n\n\n\n\nExercise DR.2: Distance Matrices\n\n\n\n\nLoad the following dataset and compute distances:\n\n\n\nCode\ndata(\"tissue_gene_expression\")\n\n\nCompare the distance between the first two observations (both cerebellums), the 39th and 40th (both colons), and the 73rd and 74th (both endometriums). Are observations of the same tissue type closer to each other?\n\nMake an image plot of all the distances using the image function. Hint: convert the distance object to a matrix first. Does the pattern suggest that observations of the same tissue type are generally closer?",
    "crumbs": [
      "Statistical Learning",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Dimensionality Reduction and Multivariate Methods</span>"
    ]
  },
  {
    "objectID": "chapters/23-dimensionality-reduction.html#non-linear-methods",
    "href": "chapters/23-dimensionality-reduction.html#non-linear-methods",
    "title": "33  Dimensionality Reduction and Multivariate Methods",
    "section": "33.11 Non-Linear Methods",
    "text": "33.11 Non-Linear Methods\nThe methods covered in this chapter are primarily linear dimensionality reduction techniques. PCA finds linear combinations of variables, and PCoA preserves Euclidean distances. However, biological data often lies on complex, non-linear manifolds.\nFor visualization of complex, non-linear structure, see Chapter 34 which covers:\n\nt-SNE (t-distributed Stochastic Neighbor Embedding): Excellent for visualizing local cluster structure\nUMAP (Uniform Manifold Approximation and Projection): Faster than t-SNE and may better preserve global structure\n\nThese non-linear methods are particularly valuable for single-cell RNA-seq data and other high-dimensional biological datasets with complex structure.",
    "crumbs": [
      "Statistical Learning",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Dimensionality Reduction and Multivariate Methods</span>"
    ]
  },
  {
    "objectID": "chapters/23-dimensionality-reduction.html#summary",
    "href": "chapters/23-dimensionality-reduction.html#summary",
    "title": "33  Dimensionality Reduction and Multivariate Methods",
    "section": "33.12 Summary",
    "text": "33.12 Summary\n\nDimensionality reduction creates fewer variables that capture most information\nPCA finds linear combinations that maximize variance\n\nOrthogonal transformations preserve distances while concentrating variance\nHighly correlated variables can be effectively reduced to fewer dimensions\nLoadings show which original variables contribute to each PC\nThe twin heights example shows how correlated variables compress to one dimension\n\nPCoA works from distance matrices; useful for ecological and genetic data\nNMDS preserves rank-order of distances; robust for non-normal data\nMANOVA tests whether groups differ on multiple response variables simultaneously\nDFA/LDA finds combinations that best discriminate known groups\nThese methods can be combined: use PCA to reduce dimensions, then classify\nPCA can substantially reduce model complexity while maintaining predictive performance\nFor non-linear structure, consider t-SNE or UMAP (see Chapter 34)",
    "crumbs": [
      "Statistical Learning",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Dimensionality Reduction and Multivariate Methods</span>"
    ]
  },
  {
    "objectID": "chapters/23-dimensionality-reduction.html#additional-resources",
    "href": "chapters/23-dimensionality-reduction.html#additional-resources",
    "title": "33  Dimensionality Reduction and Multivariate Methods",
    "section": "33.13 Additional Resources",
    "text": "33.13 Additional Resources\n\nChapter 46 - Matrix algebra fundamentals and eigenanalysis for PCA\nChapter 34 - Non-linear dimensionality reduction with t-SNE and UMAP\nJames et al. (2023) - Modern treatment of dimensionality reduction and clustering\nLogan (2010) - MANOVA and DFA in biological research contexts\nBorcard, D., Gillet, F., & Legendre, P. (2018). Numerical Ecology with R - Comprehensive ordination methods\n\n\n\n\n\n\n\nHotelling, Harold. 1933. “Analysis of a Complex of Statistical Variables into Principal Components.” Journal of Educational Psychology 24 (6): 417–41.\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2023. An Introduction to Statistical Learning with Applications in r. 2nd ed. Springer. https://www.statlearning.com.\n\n\nLogan, Murray. 2010. Biostatistical Design and Analysis Using r. Wiley-Blackwell.\n\n\nPearson, Karl. 1901. “On Lines and Planes of Closest Fit to Systems of Points in Space.” The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science 2 (11): 559–72.",
    "crumbs": [
      "Statistical Learning",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Dimensionality Reduction and Multivariate Methods</span>"
    ]
  },
  {
    "objectID": "chapters/34-tsne-umap.html",
    "href": "chapters/34-tsne-umap.html",
    "title": "34  Non-Linear Dimensionality Reduction: t-SNE and UMAP",
    "section": "",
    "text": "34.1 Beyond Linear Methods\nIn Chapter 33, we introduced Principal Component Analysis (PCA) as a powerful tool for dimensionality reduction. PCA works by finding linear combinations of variables that maximize variance. However, PCA has a fundamental limitation: it can only capture linear relationships.\nReal biological data often contains complex, non-linear structure:\nWhen data lies on a curved manifold in high-dimensional space, linear methods like PCA may fail to reveal the true structure. This chapter introduces two powerful non-linear methods: t-SNE and UMAP.",
    "crumbs": [
      "Statistical Learning",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Non-Linear Dimensionality Reduction: t-SNE and UMAP</span>"
    ]
  },
  {
    "objectID": "chapters/34-tsne-umap.html#beyond-linear-methods",
    "href": "chapters/34-tsne-umap.html#beyond-linear-methods",
    "title": "34  Non-Linear Dimensionality Reduction: t-SNE and UMAP",
    "section": "",
    "text": "Cell populations in single-cell RNA-seq form distinct clusters with non-linear boundaries\nDevelopmental trajectories follow curved paths through gene expression space\nProtein structures exhibit complex geometric relationships",
    "crumbs": [
      "Statistical Learning",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Non-Linear Dimensionality Reduction: t-SNE and UMAP</span>"
    ]
  },
  {
    "objectID": "chapters/34-tsne-umap.html#limitations-of-pca",
    "href": "chapters/34-tsne-umap.html#limitations-of-pca",
    "title": "34  Non-Linear Dimensionality Reduction: t-SNE and UMAP",
    "section": "34.2 Limitations of PCA",
    "text": "34.2 Limitations of PCA\nTo see where PCA falls short, consider a simple example: data arranged in a curved pattern.\n\n\nCode\n# Generate data on a curved manifold (Swiss roll-like)\nset.seed(42)\nn &lt;- 500\nt &lt;- runif(n, 0, 4*pi)\nx &lt;- t * cos(t)\ny &lt;- t * sin(t)\nz &lt;- runif(n, 0, 10)\ncolor &lt;- t  # Color by position along curve\n\ncurved_data &lt;- data.frame(x = x, y = y, z = z, t = t)\n\n# PCA\npca_result &lt;- prcomp(curved_data[, 1:3], scale. = TRUE)\n\npar(mfrow = c(1, 3))\n\n# Original 3D projection\nplot(curved_data$x, curved_data$y, col = heat.colors(100)[cut(color, 100)],\n     pch = 19, cex = 0.5, xlab = \"X\", ylab = \"Y\",\n     main = \"Original Data (X-Y projection)\")\n\n# PCA result\nplot(pca_result$x[, 1], pca_result$x[, 2],\n     col = heat.colors(100)[cut(color, 100)],\n     pch = 19, cex = 0.5, xlab = \"PC1\", ylab = \"PC2\",\n     main = \"PCA\")\n\n# What we want: unfolded\nplot(t, z, col = heat.colors(100)[cut(color, 100)],\n     pch = 19, cex = 0.5, xlab = \"Position along curve\", ylab = \"Z\",\n     main = \"Ideal: Unfolded Manifold\")\n\n\n\n\n\n\n\n\nFigure 34.1: PCA captures the direction of maximum variance but fails to ‘unfold’ the curved structure. The first PC spreads points along the curve but mixes points from different parts.\n\n\n\n\n\nPCA projects the data onto the directions of maximum variance, but it cannot “unfold” the curved structure. Points that are far apart along the manifold (different colors) may end up close together in the PCA projection.",
    "crumbs": [
      "Statistical Learning",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Non-Linear Dimensionality Reduction: t-SNE and UMAP</span>"
    ]
  },
  {
    "objectID": "chapters/34-tsne-umap.html#t-sne-t-distributed-stochastic-neighbor-embedding",
    "href": "chapters/34-tsne-umap.html#t-sne-t-distributed-stochastic-neighbor-embedding",
    "title": "34  Non-Linear Dimensionality Reduction: t-SNE and UMAP",
    "section": "34.3 t-SNE: t-distributed Stochastic Neighbor Embedding",
    "text": "34.3 t-SNE: t-distributed Stochastic Neighbor Embedding\nt-SNE (t-distributed Stochastic Neighbor Embedding) (Maaten and Hinton 2008) is a non-linear dimensionality reduction technique designed specifically for visualization. It was introduced by van der Maaten and Hinton in 2008 and has become one of the most popular methods for visualizing high-dimensional data.\n\nHow t-SNE Works\nThe core idea of t-SNE is to preserve local neighborhood structure. Points that are close together in high-dimensional space should remain close together in the low-dimensional embedding.\nThe algorithm works in two steps:\nStep 1: Define similarities in high-dimensional space\nFor each pair of points \\(i\\) and \\(j\\), t-SNE computes a probability \\(p_{ij}\\) that represents how likely point \\(j\\) would be picked as a neighbor of point \\(i\\) if neighbors were chosen in proportion to their probability density under a Gaussian centered at \\(i\\):\n\\[\np_{j|i} = \\frac{\\exp(-\\|x_i - x_j\\|^2 / 2\\sigma_i^2)}{\\sum_{k \\neq i} \\exp(-\\|x_i - x_k\\|^2 / 2\\sigma_i^2)}\n\\]\nThe variance \\(\\sigma_i\\) is chosen so that each point has a fixed number of effective neighbors (controlled by the perplexity parameter).\nStep 2: Define similarities in low-dimensional space\nIn the low-dimensional embedding, t-SNE uses a t-distribution (with one degree of freedom, i.e., a Cauchy distribution) to compute similarities:\n\\[\nq_{ij} = \\frac{(1 + \\|y_i - y_j\\|^2)^{-1}}{\\sum_{k \\neq l} (1 + \\|y_k - y_l\\|^2)^{-1}}\n\\]\nThe t-distribution has heavier tails than a Gaussian, which allows dissimilar points to be placed far apart without incurring a large penalty.\nStep 3: Minimize the divergence\nt-SNE then minimizes the Kullback-Leibler divergence between the two distributions using gradient descent:\n\\[\nKL(P \\| Q) = \\sum_{i \\neq j} p_{ij} \\log \\frac{p_{ij}}{q_{ij}}\n\\]\nThis pushes the low-dimensional embedding to match the neighborhood structure of the high-dimensional data.\n\n\nThe Perplexity Parameter\nPerplexity is the most important parameter in t-SNE. It loosely corresponds to the number of effective nearest neighbors considered for each point. Typical values range from 5 to 50.\n\nLow perplexity: Focuses on very local structure; may create many small, disconnected clusters\nHigh perplexity: Considers more global structure; clusters may merge together\n\n\n\nCode\n# Use MNIST digits for demonstration\nif (!exists(\"mnist\")) mnist &lt;- read_mnist()\n\n# Sample for speed\nset.seed(42)\nidx &lt;- sample(1:nrow(mnist$train$images), 2000)\nx_sample &lt;- mnist$train$images[idx, ]\nlabels &lt;- mnist$train$labels[idx]\n\n# t-SNE with different perplexities\ntsne_5 &lt;- Rtsne(x_sample, perplexity = 5, verbose = FALSE, max_iter = 500)\ntsne_30 &lt;- Rtsne(x_sample, perplexity = 30, verbose = FALSE, max_iter = 500)\ntsne_100 &lt;- Rtsne(x_sample, perplexity = 100, verbose = FALSE, max_iter = 500)\n\npar(mfrow = c(1, 3))\nplot(tsne_5$Y, col = rainbow(10)[labels + 1], pch = 19, cex = 0.5,\n     xlab = \"t-SNE 1\", ylab = \"t-SNE 2\", main = \"Perplexity = 5\")\nplot(tsne_30$Y, col = rainbow(10)[labels + 1], pch = 19, cex = 0.5,\n     xlab = \"t-SNE 1\", ylab = \"t-SNE 2\", main = \"Perplexity = 30\")\nplot(tsne_100$Y, col = rainbow(10)[labels + 1], pch = 19, cex = 0.5,\n     xlab = \"t-SNE 1\", ylab = \"t-SNE 2\", main = \"Perplexity = 100\")\n\n\n\n\n\n\n\n\nFigure 34.2: Effect of perplexity on t-SNE embeddings. Low perplexity emphasizes local structure (may fragment clusters), high perplexity emphasizes global structure (may merge clusters).\n\n\n\n\n\n\n\nt-SNE in R\nThe Rtsne package provides an efficient implementation:\n\n\nCode\n# Full t-SNE with good parameters\nset.seed(42)\ntsne_result &lt;- Rtsne(x_sample, perplexity = 30, verbose = FALSE, max_iter = 1000)\n\n# Plot with digit labels\ndf_tsne &lt;- data.frame(\n  tSNE1 = tsne_result$Y[, 1],\n  tSNE2 = tsne_result$Y[, 2],\n  digit = factor(labels)\n)\n\nggplot(df_tsne, aes(tSNE1, tSNE2, color = digit)) +\n  geom_point(alpha = 0.6, size = 1) +\n  scale_color_brewer(palette = \"Spectral\") +\n  labs(title = \"t-SNE of MNIST Digits\",\n       x = \"t-SNE Dimension 1\", y = \"t-SNE Dimension 2\") +\n  theme(legend.position = \"right\")\n\n\n\n\n\n\n\n\nFigure 34.3: t-SNE visualization of MNIST digits reveals clear cluster structure corresponding to digit classes\n\n\n\n\n\n\n\nImportant Considerations for t-SNE\n\n\n\n\n\n\nt-SNE Caveats\n\n\n\n\nCluster sizes are meaningless: t-SNE does not preserve density. A large cluster in t-SNE does not mean more points.\nDistances between clusters are meaningless: Only local structure is preserved. The distance between two clusters tells you nothing about their true similarity.\nResults depend on random initialization: Always set a seed for reproducibility. Run multiple times to check stability.\nPerplexity must be tuned: There’s no universally best value. Try multiple perplexities to understand your data.\nComputationally expensive: O(n²) complexity; can be slow for large datasets. Use PCA preprocessing to speed up.",
    "crumbs": [
      "Statistical Learning",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Non-Linear Dimensionality Reduction: t-SNE and UMAP</span>"
    ]
  },
  {
    "objectID": "chapters/34-tsne-umap.html#umap-uniform-manifold-approximation-and-projection",
    "href": "chapters/34-tsne-umap.html#umap-uniform-manifold-approximation-and-projection",
    "title": "34  Non-Linear Dimensionality Reduction: t-SNE and UMAP",
    "section": "34.4 UMAP: Uniform Manifold Approximation and Projection",
    "text": "34.4 UMAP: Uniform Manifold Approximation and Projection\nUMAP (Uniform Manifold Approximation and Projection) (McInnes, Healy, and Melville 2018) is a more recent algorithm that has become increasingly popular, especially in single-cell genomics. UMAP is often faster than t-SNE and may better preserve global structure.\n\nHow UMAP Works\nUMAP is grounded in Riemannian geometry and algebraic topology, but the practical intuition is similar to t-SNE: preserve neighborhood relationships.\nKey differences from t-SNE:\n\nMathematical foundation: UMAP assumes the data lies on a uniformly distributed manifold and uses concepts from topological data analysis.\nGraph-based approach: UMAP constructs a weighted graph representing the high-dimensional data, then optimizes a low-dimensional graph to match it.\nDifferent cost function: UMAP uses cross-entropy rather than KL divergence, which may help preserve more global structure.\nFaster: UMAP typically runs faster than t-SNE, especially on large datasets.\n\n\n\nKey UMAP Parameters\n\nn_neighbors: Similar to perplexity in t-SNE. Controls the balance between local and global structure. Typical values: 5-50.\nmin_dist: Controls how tightly UMAP packs points together. Small values create tighter clusters. Typical values: 0.0-0.5.\nmetric: Distance metric used. Default is Euclidean, but many options available.\n\n\n\nUMAP in R\nThe umap package provides a convenient interface:\n\n\nCode\n# UMAP\nset.seed(42)\numap_result &lt;- umap(x_sample, n_neighbors = 15, min_dist = 0.1)\n\n\nError in `umap()`:\n! could not find function \"umap\"\n\n\nCode\n# Plot\ndf_umap &lt;- data.frame(\n  UMAP1 = umap_result$layout[, 1],\n  UMAP2 = umap_result$layout[, 2],\n  digit = factor(labels)\n)\n\n\nError:\n! object 'umap_result' not found\n\n\nCode\nggplot(df_umap, aes(UMAP1, UMAP2, color = digit)) +\n  geom_point(alpha = 0.6, size = 1) +\n  scale_color_brewer(palette = \"Spectral\") +\n  labs(title = \"UMAP of MNIST Digits\",\n       x = \"UMAP Dimension 1\", y = \"UMAP Dimension 2\") +\n  theme(legend.position = \"right\")\n\n\nError:\n! object 'df_umap' not found\n\n\n\n\nEffect of UMAP Parameters\n\n\nCode\n# Different parameter combinations\npar(mfrow = c(2, 3))\n\n# Vary n_neighbors\nfor (nn in c(5, 15, 50)) {\n  set.seed(42)\n  result &lt;- umap(x_sample, n_neighbors = nn, min_dist = 0.1)\n  plot(result$layout, col = rainbow(10)[labels + 1], pch = 19, cex = 0.3,\n       xlab = \"UMAP 1\", ylab = \"UMAP 2\",\n       main = paste(\"n_neighbors =\", nn, \", min_dist = 0.1\"))\n}\n\n\nError in `umap()`:\n! could not find function \"umap\"\n\n\nCode\n# Vary min_dist\nfor (md in c(0.0, 0.1, 0.5)) {\n  set.seed(42)\n  result &lt;- umap(x_sample, n_neighbors = 15, min_dist = md)\n  plot(result$layout, col = rainbow(10)[labels + 1], pch = 19, cex = 0.3,\n       xlab = \"UMAP 1\", ylab = \"UMAP 2\",\n       main = paste(\"n_neighbors = 15, min_dist =\", md))\n}\n\n\nError in `umap()`:\n! could not find function \"umap\"",
    "crumbs": [
      "Statistical Learning",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Non-Linear Dimensionality Reduction: t-SNE and UMAP</span>"
    ]
  },
  {
    "objectID": "chapters/34-tsne-umap.html#comparing-pca-t-sne-and-umap",
    "href": "chapters/34-tsne-umap.html#comparing-pca-t-sne-and-umap",
    "title": "34  Non-Linear Dimensionality Reduction: t-SNE and UMAP",
    "section": "34.5 Comparing PCA, t-SNE, and UMAP",
    "text": "34.5 Comparing PCA, t-SNE, and UMAP\nLet’s directly compare these three methods on the same dataset:\n\n\nCode\n# PCA\npca_result &lt;- prcomp(x_sample)\n\npar(mfrow = c(1, 3))\n\n# PCA\nplot(pca_result$x[, 1], pca_result$x[, 2],\n     col = rainbow(10)[labels + 1], pch = 19, cex = 0.3,\n     xlab = \"PC1\", ylab = \"PC2\", main = \"PCA\")\nlegend(\"topright\", legend = 0:9, col = rainbow(10), pch = 19, cex = 0.6, ncol = 2)\n\n# t-SNE\nplot(tsne_result$Y[, 1], tsne_result$Y[, 2],\n     col = rainbow(10)[labels + 1], pch = 19, cex = 0.3,\n     xlab = \"t-SNE 1\", ylab = \"t-SNE 2\", main = \"t-SNE\")\n\n# UMAP\nplot(umap_result$layout[, 1], umap_result$layout[, 2],\n     col = rainbow(10)[labels + 1], pch = 19, cex = 0.3,\n     xlab = \"UMAP 1\", ylab = \"UMAP 2\", main = \"UMAP\")\n\n\nError:\n! object 'umap_result' not found\n\n\n\n\n\n\n\n\nFigure 34.4: Comparison of PCA, t-SNE, and UMAP on MNIST digits. PCA captures global variance but mixes clusters; t-SNE and UMAP reveal clear cluster structure.\n\n\n\n\n\n\nKey Differences\n\n\n\n\n\n\n\n\n\nAspect\nPCA\nt-SNE\nUMAP\n\n\n\n\nType\nLinear\nNon-linear\nNon-linear\n\n\nPreserves\nGlobal variance\nLocal structure\nLocal + some global\n\n\nSpeed\nVery fast\nSlow (O(n²))\nFast\n\n\nReproducibility\nDeterministic\nStochastic\nStochastic\n\n\nNew data\nCan project\nMust rerun\nCan project (with care)\n\n\nInterpretable axes\nYes (loadings)\nNo\nNo\n\n\nCluster distances\nMeaningful\nNot meaningful\nSomewhat meaningful\n\n\nBest for\nPreprocessing, linear patterns\nVisualization\nVisualization, larger data\n\n\n\n\n\nWhen to Use Each Method\nUse PCA when:\n\nYou need interpretable dimensions (loadings)\nYou want to preprocess data before other analyses\nThe data structure is approximately linear\nYou need to project new data points\nSpeed is critical\n\nUse t-SNE when:\n\nVisualization is the primary goal\nYou want to explore local cluster structure\nDataset size is moderate (&lt; 50,000 points)\nYou’ll examine multiple perplexity values\n\nUse UMAP when:\n\nYou have large datasets\nYou want faster computation than t-SNE\nYou care about preserving some global structure\nYou need to embed new points (though with caveats)",
    "crumbs": [
      "Statistical Learning",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Non-Linear Dimensionality Reduction: t-SNE and UMAP</span>"
    ]
  },
  {
    "objectID": "chapters/34-tsne-umap.html#biological-applications",
    "href": "chapters/34-tsne-umap.html#biological-applications",
    "title": "34  Non-Linear Dimensionality Reduction: t-SNE and UMAP",
    "section": "34.6 Biological Applications",
    "text": "34.6 Biological Applications\n\nSingle-Cell RNA-seq\nOne of the most common applications of t-SNE and UMAP is visualizing single-cell transcriptomic data:\n\n\nCode\n# Simulated single-cell-like data with distinct populations\nset.seed(123)\nn_cells &lt;- 1000\nn_genes &lt;- 50\n\n# Simulate 4 cell types\ncell_types &lt;- sample(1:4, n_cells, replace = TRUE, prob = c(0.4, 0.3, 0.2, 0.1))\n\n# Generate expression data with cell-type-specific patterns\nexpression &lt;- matrix(rnorm(n_cells * n_genes), nrow = n_cells)\nfor (i in 1:4) {\n  idx &lt;- cell_types == i\n  # Add cell-type specific signal to first few genes\n  expression[idx, 1:10] &lt;- expression[idx, 1:10] + i * 2\n  expression[idx, (i*10):(i*10+5)] &lt;- expression[idx, (i*10):(i*10+5)] + 3\n}\n\n# Apply methods\npca_cells &lt;- prcomp(expression, scale. = TRUE)\ntsne_cells &lt;- Rtsne(expression, perplexity = 30, verbose = FALSE)\numap_cells &lt;- umap(expression, n_neighbors = 15)\n\n\nError in `umap()`:\n! could not find function \"umap\"\n\n\nCode\npar(mfrow = c(1, 3))\nplot(pca_cells$x[, 1:2], col = cell_types, pch = 19, cex = 0.5,\n     xlab = \"PC1\", ylab = \"PC2\", main = \"PCA\")\nlegend(\"topright\", legend = paste(\"Type\", 1:4), col = 1:4, pch = 19, cex = 0.7)\n\nplot(tsne_cells$Y, col = cell_types, pch = 19, cex = 0.5,\n     xlab = \"t-SNE 1\", ylab = \"t-SNE 2\", main = \"t-SNE\")\n\nplot(umap_cells$layout, col = cell_types, pch = 19, cex = 0.5,\n     xlab = \"UMAP 1\", ylab = \"UMAP 2\", main = \"UMAP\")\n\n\nError:\n! object 'umap_cells' not found\n\n\n\n\n\n\n\n\nFigure 34.5: t-SNE and UMAP are commonly used to visualize cell populations in single-cell genomics data",
    "crumbs": [
      "Statistical Learning",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Non-Linear Dimensionality Reduction: t-SNE and UMAP</span>"
    ]
  },
  {
    "objectID": "chapters/34-tsne-umap.html#practical-workflow",
    "href": "chapters/34-tsne-umap.html#practical-workflow",
    "title": "34  Non-Linear Dimensionality Reduction: t-SNE and UMAP",
    "section": "34.7 Practical Workflow",
    "text": "34.7 Practical Workflow\nA typical workflow for non-linear dimensionality reduction:\n\nPreprocess data: Normalize, scale, and filter features\nApply PCA first: Reduce to 50-100 dimensions to speed up t-SNE/UMAP\nRun t-SNE or UMAP: With appropriate parameters\nExplore parameters: Try multiple perplexity/n_neighbors values\nValidate: Compare with known labels or biological markers\nIterate: Adjust preprocessing and parameters as needed\n\n\n\nCode\n# PCA preprocessing example\n# First reduce to 50 PCs, then run t-SNE\npca_50 &lt;- prcomp(x_sample)$x[, 1:50]\n\nset.seed(42)\ntsne_after_pca &lt;- Rtsne(pca_50, perplexity = 30, verbose = FALSE,\n                         pca = FALSE)  # Already did PCA\n\npar(mfrow = c(1, 2))\nplot(tsne_result$Y, col = rainbow(10)[labels + 1], pch = 19, cex = 0.3,\n     xlab = \"t-SNE 1\", ylab = \"t-SNE 2\", main = \"t-SNE (direct)\")\nplot(tsne_after_pca$Y, col = rainbow(10)[labels + 1], pch = 19, cex = 0.3,\n     xlab = \"t-SNE 1\", ylab = \"t-SNE 2\", main = \"t-SNE (after PCA to 50D)\")\n\n\n\n\n\n\n\n\nFigure 34.6: Using PCA as a preprocessing step before t-SNE speeds computation and can improve results by removing noise\n\n\n\n\n\n\n\n\n\n\n\nBest Practices\n\n\n\n\nAlways set a random seed for reproducibility\nTry multiple parameter values and compare results\nUse PCA preprocessing for large, high-dimensional datasets\nDon’t over-interpret cluster sizes or inter-cluster distances\nRun multiple times to assess stability of results\nValidate findings with independent methods or known biology",
    "crumbs": [
      "Statistical Learning",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Non-Linear Dimensionality Reduction: t-SNE and UMAP</span>"
    ]
  },
  {
    "objectID": "chapters/34-tsne-umap.html#exercises",
    "href": "chapters/34-tsne-umap.html#exercises",
    "title": "34  Non-Linear Dimensionality Reduction: t-SNE and UMAP",
    "section": "34.8 Exercises",
    "text": "34.8 Exercises\n\n\n\n\n\n\nExercise TU.1: Comparing Methods\n\n\n\n\nLoad the tissue_gene_expression dataset and apply PCA, t-SNE, and UMAP. Compare how well each method separates the different tissue types.\n\n\n\n\n\n\n\n\n\nExercise TU.2: t-SNE Perplexity\n\n\n\n\nFor the t-SNE analysis, try perplexity values of 5, 15, 30, and 50. Which value produces the clearest separation of tissue types?\n\n\n\n\n\n\n\n\n\nExercise TU.3: UMAP Parameters\n\n\n\n\nFor UMAP, experiment with n_neighbors values of 5, 15, and 50, and min_dist values of 0.0, 0.1, and 0.5. How do these parameters affect the visualization?\n\n\n\n\n\n\n\n\n\nExercise TU.4: Stability Analysis\n\n\n\n\nRun t-SNE on the MNIST digit dataset multiple times with different random seeds. How stable are the cluster positions? Do the same digits always cluster together?\n\n\n\n\n\n\n\n\n\nExercise TU.5: Computation Time\n\n\n\n\nTime how long PCA, t-SNE, and UMAP take to run on increasing subsets of data (500, 1000, 2000, 5000 points). Plot computation time versus sample size.\n\n\n\n\n\n\n\n\n\nExercise TU.6: PCA Preprocessing\n\n\n\n\nApply PCA preprocessing (reducing to 50 dimensions) before running t-SNE. Compare the results and computation time to running t-SNE directly on the full data.",
    "crumbs": [
      "Statistical Learning",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Non-Linear Dimensionality Reduction: t-SNE and UMAP</span>"
    ]
  },
  {
    "objectID": "chapters/34-tsne-umap.html#summary",
    "href": "chapters/34-tsne-umap.html#summary",
    "title": "34  Non-Linear Dimensionality Reduction: t-SNE and UMAP",
    "section": "34.9 Summary",
    "text": "34.9 Summary\n\nNon-linear dimensionality reduction methods can reveal structure that linear methods like PCA miss\nt-SNE is designed for visualization and excels at preserving local neighborhood structure\n\nThe perplexity parameter controls the balance between local and global structure\nCluster sizes and inter-cluster distances are not meaningful\nResults depend on random initialization—set a seed for reproducibility\n\nUMAP is a newer alternative that is often faster and may preserve more global structure\n\nKey parameters are n_neighbors and min_dist\nGenerally faster than t-SNE, especially on large datasets\n\nPCA vs t-SNE vs UMAP:\n\nPCA: Fast, interpretable, preserves global variance, works well for linear structure\nt-SNE: Best for visualizing local cluster structure, computationally expensive\nUMAP: Good balance of speed and quality, preserves some global structure\n\nBest practices: Use PCA preprocessing, try multiple parameter values, validate results, don’t over-interpret",
    "crumbs": [
      "Statistical Learning",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Non-Linear Dimensionality Reduction: t-SNE and UMAP</span>"
    ]
  },
  {
    "objectID": "chapters/34-tsne-umap.html#additional-resources",
    "href": "chapters/34-tsne-umap.html#additional-resources",
    "title": "34  Non-Linear Dimensionality Reduction: t-SNE and UMAP",
    "section": "34.10 Additional Resources",
    "text": "34.10 Additional Resources\n\nvan der Maaten & Hinton (2008). Visualizing Data using t-SNE. JMLR\nMcInnes et al. (2018). UMAP: Uniform Manifold Approximation and Projection. arXiv\nKobak & Berens (2019). The art of using t-SNE for single-cell transcriptomics. Nature Communications\nJames et al. (2023) - Additional perspectives on dimensionality reduction\n\n\n\n\n\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2023. An Introduction to Statistical Learning with Applications in r. 2nd ed. Springer. https://www.statlearning.com.\n\n\nMaaten, Laurens van der, and Geoffrey Hinton. 2008. “Visualizing Data Using t-SNE.” Journal of Machine Learning Research 9: 2579–2605.\n\n\nMcInnes, Leland, John Healy, and James Melville. 2018. “UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction.” arXiv Preprint arXiv:1802.03426.",
    "crumbs": [
      "Statistical Learning",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Non-Linear Dimensionality Reduction: t-SNE and UMAP</span>"
    ]
  },
  {
    "objectID": "chapters/24-high-performance-computing.html",
    "href": "chapters/24-high-performance-computing.html",
    "title": "35  High Performance Computing",
    "section": "",
    "text": "35.1 Why High Performance Computing?\nAs datasets grow and analyses become more complex, your laptop may not be enough. Genomic datasets can be terabytes in size. Simulations might require millions of iterations. Machine learning models may need to be trained on billions of data points. High Performance Computing (HPC) provides the resources to tackle problems that exceed what personal computers can handle.\nHPC systems come in different forms. Computing clusters—collections of interconnected computers working together—are common at universities and research institutions. Cloud computing services from Amazon (AWS), Google, and Microsoft (Azure) provide on-demand access to computing resources. GPUs (Graphics Processing Units) accelerate certain types of parallel computations.",
    "crumbs": [
      "Statistical Learning",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>High Performance Computing</span>"
    ]
  },
  {
    "objectID": "chapters/24-high-performance-computing.html#computing-clusters",
    "href": "chapters/24-high-performance-computing.html#computing-clusters",
    "title": "35  High Performance Computing",
    "section": "35.2 Computing Clusters",
    "text": "35.2 Computing Clusters\nA typical university computing cluster consists of a head node (login node) where you submit jobs, and many compute nodes where jobs actually run. The head node manages the queue of waiting jobs and allocates resources.\nAt the University of Oregon, the Talapas cluster provides researchers with access to thousands of CPU cores and specialized hardware including GPUs. Access requires an account, which graduate students can request through their research groups.",
    "crumbs": [
      "Statistical Learning",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>High Performance Computing</span>"
    ]
  },
  {
    "objectID": "chapters/24-high-performance-computing.html#connecting-to-remote-systems",
    "href": "chapters/24-high-performance-computing.html#connecting-to-remote-systems",
    "title": "35  High Performance Computing",
    "section": "35.3 Connecting to Remote Systems",
    "text": "35.3 Connecting to Remote Systems\nYou access remote systems through SSH (Secure Shell):\nssh username@talapas-login.uoregon.edu\nAfter authenticating, you are in a terminal on the remote system, working in a Unix environment just as you would locally. File transfer between your computer and the cluster uses scp or rsync:\n# Copy file to cluster\nscp data.csv username@talapas-login.uoregon.edu:~/project/\n\n# Copy file from cluster\nscp username@talapas-login.uoregon.edu:~/project/results.csv ./",
    "crumbs": [
      "Statistical Learning",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>High Performance Computing</span>"
    ]
  },
  {
    "objectID": "chapters/24-high-performance-computing.html#job-schedulers",
    "href": "chapters/24-high-performance-computing.html#job-schedulers",
    "title": "35  High Performance Computing",
    "section": "35.4 Job Schedulers",
    "text": "35.4 Job Schedulers\nYou do not run computationally intensive jobs directly on the login node. Instead, you submit them to a job scheduler (like SLURM on Talapas) that queues jobs and runs them when resources become available.\nA basic SLURM submission script:\n#!/bin/bash\n#SBATCH --job-name=my_analysis\n#SBATCH --account=your_account\n#SBATCH --partition=short\n#SBATCH --time=2:00:00\n#SBATCH --nodes=1\n#SBATCH --ntasks=1\n#SBATCH --cpus-per-task=4\n#SBATCH --mem=16G\n\n# Load required software\nmodule load R/4.2.1\n\n# Run your script\nRscript my_analysis.R\nSubmit with sbatch script.sh. Check job status with squeue -u username. Cancel jobs with scancel job_id.",
    "crumbs": [
      "Statistical Learning",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>High Performance Computing</span>"
    ]
  },
  {
    "objectID": "chapters/24-high-performance-computing.html#resource-requests",
    "href": "chapters/24-high-performance-computing.html#resource-requests",
    "title": "35  High Performance Computing",
    "section": "35.5 Resource Requests",
    "text": "35.5 Resource Requests\nJobs must request resources: time, memory, and CPUs. Request enough to complete your job but not so much that it waits unnecessarily in the queue. Start with conservative estimates and adjust based on actual usage.\nCommon SLURM directives: - --time: Maximum runtime (job is killed if exceeded) - --mem: Memory per node - --cpus-per-task: Number of CPU cores - --array: For running many similar jobs",
    "crumbs": [
      "Statistical Learning",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>High Performance Computing</span>"
    ]
  },
  {
    "objectID": "chapters/24-high-performance-computing.html#environment-modules",
    "href": "chapters/24-high-performance-computing.html#environment-modules",
    "title": "35  High Performance Computing",
    "section": "35.6 Environment Modules",
    "text": "35.6 Environment Modules\nHPC systems use environment modules to manage software. Instead of installing software yourself, you load pre-installed modules:\nmodule avail              # List available software\nmodule load R/4.2.1       # Load R\nmodule load python/3.10   # Load Python\nmodule list               # Show loaded modules\nmodule purge              # Unload all modules",
    "crumbs": [
      "Statistical Learning",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>High Performance Computing</span>"
    ]
  },
  {
    "objectID": "chapters/24-high-performance-computing.html#running-r-on-a-cluster",
    "href": "chapters/24-high-performance-computing.html#running-r-on-a-cluster",
    "title": "35  High Performance Computing",
    "section": "35.7 Running R on a Cluster",
    "text": "35.7 Running R on a Cluster\nR scripts run non-interactively on clusters. Instead of using RStudio, you write your analysis as a script and run it with Rscript:\n# my_analysis.R\nlibrary(tidyverse)\n\n# Read data\ndata &lt;- read.csv(\"large_dataset.csv\")\n\n# Perform analysis\nresults &lt;- data |&gt;\n  group_by(category) |&gt;\n  summarize(mean_value = mean(value))\n\n# Save results\nwrite.csv(results, \"output.csv\")",
    "crumbs": [
      "Statistical Learning",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>High Performance Computing</span>"
    ]
  },
  {
    "objectID": "chapters/24-high-performance-computing.html#parallelization-in-r",
    "href": "chapters/24-high-performance-computing.html#parallelization-in-r",
    "title": "35  High Performance Computing",
    "section": "35.8 Parallelization in R",
    "text": "35.8 Parallelization in R\nR can use multiple CPU cores to speed up computations. The parallel package provides tools for parallel processing:\nlibrary(parallel)\n\n# Detect number of cores\nn_cores &lt;- detectCores()\n\n# Create a cluster\ncl &lt;- makeCluster(n_cores - 1)\n\n# Parallel apply\nresults &lt;- parLapply(cl, data_list, analysis_function)\n\n# Stop the cluster\nstopCluster(cl)\nThe future and furrr packages provide more user-friendly parallelization.",
    "crumbs": [
      "Statistical Learning",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>High Performance Computing</span>"
    ]
  },
  {
    "objectID": "chapters/24-high-performance-computing.html#cloud-computing",
    "href": "chapters/24-high-performance-computing.html#cloud-computing",
    "title": "35  High Performance Computing",
    "section": "35.9 Cloud Computing",
    "text": "35.9 Cloud Computing\nCloud platforms (AWS, Google Cloud, Azure) offer computing resources on demand. You pay for what you use rather than having fixed resources.\nAdvantages: - Scale up quickly when needed - No hardware maintenance - Access to specialized hardware (GPUs, large memory instances)\nDisadvantages: - Costs can accumulate quickly - Requires learning platform-specific tools - Data transfer can be slow and expensive",
    "crumbs": [
      "Statistical Learning",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>High Performance Computing</span>"
    ]
  },
  {
    "objectID": "chapters/24-high-performance-computing.html#best-practices",
    "href": "chapters/24-high-performance-computing.html#best-practices",
    "title": "35  High Performance Computing",
    "section": "35.10 Best Practices",
    "text": "35.10 Best Practices\nStart small: Test your code on a small subset before running on full data.\nUse version control: Keep your scripts in Git for reproducibility.\nDocument everything: Future you (and others) need to understand what you did.\nSave intermediate results: If a job fails, you do not want to start from scratch.\nMonitor resource usage: Check how much time and memory your jobs actually use.\nClean up: Delete unnecessary files; storage is shared.",
    "crumbs": [
      "Statistical Learning",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>High Performance Computing</span>"
    ]
  },
  {
    "objectID": "chapters/24-high-performance-computing.html#getting-help",
    "href": "chapters/24-high-performance-computing.html#getting-help",
    "title": "35  High Performance Computing",
    "section": "35.11 Getting Help",
    "text": "35.11 Getting Help\nMost HPC systems have documentation and support staff. At UO, Research Advanced Computing Services (RACS) provides Talapas documentation and consultations. Reading the documentation before asking questions will make your interactions more productive.\nLearning to use HPC effectively takes time, but the ability to run large-scale analyses is essential for modern bioengineering research.",
    "crumbs": [
      "Statistical Learning",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>High Performance Computing</span>"
    ]
  },
  {
    "objectID": "chapters/25-presenting-results.html",
    "href": "chapters/25-presenting-results.html",
    "title": "36  Presenting Statistical Results",
    "section": "",
    "text": "36.1 The Art of Scientific Communication\nYou have collected your data, performed your statistical analyses, and obtained results. Now comes a critical challenge that many students underestimate: communicating those results clearly and professionally. The results section of a scientific paper serves as the bridge between your methods and your conclusions, and writing it well requires understanding both statistical concepts and scientific writing conventions.\nThis chapter covers the practical aspects of presenting statistical results—the style conventions, formatting standards, and communication strategies that transform raw statistical output into compelling scientific narrative.",
    "crumbs": [
      "Scientific Communication",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Presenting Statistical Results</span>"
    ]
  },
  {
    "objectID": "chapters/25-presenting-results.html#style-and-function-of-the-results-section",
    "href": "chapters/25-presenting-results.html#style-and-function-of-the-results-section",
    "title": "36  Presenting Statistical Results",
    "section": "36.2 Style and Function of the Results Section",
    "text": "36.2 Style and Function of the Results Section\nThe results section has a specific purpose: to present your findings objectively and systematically, without interpretation or speculation. This section answers the question “What did you find?” while leaving “What does it mean?” for the discussion.\n\nWriting Style Conventions\nScientific results are traditionally written in past tense and passive voice:\n\n“The mean expression level was significantly higher in the treatment group compared to the control group (t = 3.45, df = 28, p = 0.002).”\n\nRather than:\n\n“We find that the mean expression level is significantly higher…”\n\nThe past tense reflects that the research has been completed. The passive voice keeps the focus on the findings rather than the researchers. While some journals now accept active voice, the past tense remains standard for results sections.\n\n\nObjectivity and Precision\nResults sections should be factual and precise: - Report exact values, not vague descriptions - Avoid emotional or evaluative language - Let the numbers speak for themselves\nAvoid: “The results were very impressive and showed a dramatic improvement.”\nBetter: “Cell viability increased from 45.2% (±3.1) to 78.6% (±4.2) following treatment.”",
    "crumbs": [
      "Scientific Communication",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Presenting Statistical Results</span>"
    ]
  },
  {
    "objectID": "chapters/25-presenting-results.html#summarizing-statistical-analyses",
    "href": "chapters/25-presenting-results.html#summarizing-statistical-analyses",
    "title": "36  Presenting Statistical Results",
    "section": "36.3 Summarizing Statistical Analyses",
    "text": "36.3 Summarizing Statistical Analyses\nWhen reporting results, you need to convey what test you performed and what it revealed. The key elements include:\n\nThe statistical test used\nThe test statistic value\nDegrees of freedom (when applicable)\nThe p-value or confidence interval\nEffect size (increasingly expected in modern publications)\n\n\nCommon Reporting Formats\nDifferent statistical tests have standard reporting formats:\nt-tests: &gt; “Mean protein concentration was significantly higher in treated cells (M = 45.3 ng/mL, SD = 8.2) than in control cells (M = 31.7 ng/mL, SD = 7.4), t(48) = 6.12, p &lt; 0.001, d = 1.74.”\nANOVA: &gt; “Gene expression differed significantly among the three treatment groups, F(2, 87) = 15.34, p &lt; 0.001, η² = 0.26.”\nChi-square test: &gt; “There was a significant association between genotype and disease status, χ²(2) = 12.45, p = 0.002, V = 0.31.”\nCorrelation: &gt; “Body mass was positively correlated with metabolic rate, r(58) = 0.67, p &lt; 0.001.”\nRegression: &gt; “Temperature significantly predicted reaction rate, β = 0.82, t(43) = 7.89, p &lt; 0.001, R² = 0.59.”\n\n\nFormatting Statistical Symbols\nStatistical symbols follow specific conventions:\n\nUse italics for statistical symbols: p, t, F, r, n, M, SD\nDo not italicize Greek letters: α, β, χ², η²\nReport exact p-values to 2-3 decimal places (p = 0.034) rather than inequalities (p &lt; 0.05), except when p &lt; 0.001\nRound most statistics to 2 decimal places\nReport means and standard deviations to one more decimal place than the original measurements",
    "crumbs": [
      "Scientific Communication",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Presenting Statistical Results</span>"
    ]
  },
  {
    "objectID": "chapters/25-presenting-results.html#reporting-differences-and-directionality",
    "href": "chapters/25-presenting-results.html#reporting-differences-and-directionality",
    "title": "36  Presenting Statistical Results",
    "section": "36.4 Reporting Differences and Directionality",
    "text": "36.4 Reporting Differences and Directionality\nStatistical tests tell you whether a difference exists, but your results section must also communicate the direction and magnitude of that difference.\n\nStating Directionality Clearly\nAlways specify which group was higher, lower, faster, or slower:\nUnclear: “There was a significant difference between groups (p = 0.003).”\nClear: “The treatment group showed significantly higher expression levels than the control group (p = 0.003).”\n\n\nReporting Magnitude\nInclude the actual values so readers can judge biological significance:\nIncomplete: “Treatment significantly increased survival (p &lt; 0.01).”\nComplete: “Treatment increased 30-day survival from 34% to 67% (p &lt; 0.01).”\n\n\nEffect Sizes\nP-values tell you whether an effect is statistically distinguishable from zero; effect sizes tell you how large the effect is. Common effect size measures include:\n\n\n\nStatistic\nEffect Size Measure\nSmall\nMedium\nLarge\n\n\n\n\nt-test\nCohen’s d\n0.2\n0.5\n0.8\n\n\nANOVA\nη² (eta squared)\n0.01\n0.06\n0.14\n\n\nCorrelation\nr\n0.1\n0.3\n0.5\n\n\nChi-square\nCramér’s V\n0.1\n0.3\n0.5\n\n\n\nReport effect sizes alongside p-values:\n\n“The drug treatment significantly improved memory scores, t(46) = 3.21, p = 0.002, d = 0.94, indicating a large effect.”",
    "crumbs": [
      "Scientific Communication",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Presenting Statistical Results</span>"
    ]
  },
  {
    "objectID": "chapters/25-presenting-results.html#units-and-measurement",
    "href": "chapters/25-presenting-results.html#units-and-measurement",
    "title": "36  Presenting Statistical Results",
    "section": "36.5 Units and Measurement",
    "text": "36.5 Units and Measurement\nAlways include units for measured quantities. Readers need units to interpret your findings.\nWithout units: “Mean concentration was 45.3 (SD = 8.2).”\nWith units: “Mean concentration was 45.3 ng/mL (SD = 8.2 ng/mL).”\n\nSI Units\nUse International System of Units (SI) consistently: - Mass: kg, g, mg, μg - Length: m, cm, mm, μm, nm - Time: s, min, h - Concentration: M, mM, μM, mol/L - Temperature: °C or K\n\n\nFormatting Numbers\n\nUse consistent decimal places within a comparison\nUse scientific notation for very large or very small numbers: 3.2 × 10⁶ cells/mL\nInclude leading zeros: 0.05, not .05\nUse spaces or commas for large numbers: 10,000 or 10 000",
    "crumbs": [
      "Scientific Communication",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Presenting Statistical Results</span>"
    ]
  },
  {
    "objectID": "chapters/25-presenting-results.html#integrating-text-tables-and-figures",
    "href": "chapters/25-presenting-results.html#integrating-text-tables-and-figures",
    "title": "36  Presenting Statistical Results",
    "section": "36.6 Integrating Text, Tables, and Figures",
    "text": "36.6 Integrating Text, Tables, and Figures\nEffective results sections coordinate text, tables, and figures. Each serves a different purpose:\n\nText: Highlights key findings and guides interpretation\nTables: Present precise numerical values for comparison\nFigures: Show patterns, trends, and relationships visually\n\n\nReferring to Figures and Tables\nEvery figure and table must be referenced in the text:\n\n“Expression levels varied significantly among tissue types (Figure 3.2). The highest expression was observed in liver tissue, while muscle showed minimal expression (Table 3.1).”\n\n\n\nAvoiding Redundancy\nDon’t repeat the same information in text, table, and figure. Instead, present detailed data in tables or figures and summarize the key findings in text:\nRedundant: “As shown in Table 1, Group A had a mean of 45.3, Group B had a mean of 52.1, and Group C had a mean of 48.7.”\nBetter: “Mean values differed significantly among groups (Table 1), with Group B showing the highest response.”",
    "crumbs": [
      "Scientific Communication",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Presenting Statistical Results</span>"
    ]
  },
  {
    "objectID": "chapters/25-presenting-results.html#non-significant-results",
    "href": "chapters/25-presenting-results.html#non-significant-results",
    "title": "36  Presenting Statistical Results",
    "section": "36.7 Non-Significant Results",
    "text": "36.7 Non-Significant Results\nNon-significant results are still results and should be reported clearly:\n\n“There was no significant difference in survival rate between treatment and control groups (78% vs. 72%, χ²(1) = 1.24, p = 0.27).”\n\nReport exact p-values for non-significant results rather than simply stating “p &gt; 0.05” or “ns.” This gives readers information about how close the result was to significance and allows for meta-analysis.\n\n\n\n\n\n\nAbsence of Evidence vs. Evidence of Absence\n\n\n\nA non-significant result means you failed to detect an effect—it does not prove no effect exists. Avoid language like “the treatment had no effect.” Instead use “we did not detect a significant effect” or “there was no significant difference.”",
    "crumbs": [
      "Scientific Communication",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Presenting Statistical Results</span>"
    ]
  },
  {
    "objectID": "chapters/25-presenting-results.html#handling-multiple-comparisons",
    "href": "chapters/25-presenting-results.html#handling-multiple-comparisons",
    "title": "36  Presenting Statistical Results",
    "section": "36.8 Handling Multiple Comparisons",
    "text": "36.8 Handling Multiple Comparisons\nWhen conducting multiple statistical tests, address the issue of multiple comparisons:\n\n“To control for multiple comparisons, we applied Bonferroni correction, setting the significance threshold at α = 0.008 (0.05/6). After correction, three of the six comparisons remained significant.”\n\nOr:\n\n“We report uncorrected p-values but note that with 12 tests, approximately one significant result would be expected by chance at α = 0.05.”",
    "crumbs": [
      "Scientific Communication",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Presenting Statistical Results</span>"
    ]
  },
  {
    "objectID": "chapters/25-presenting-results.html#confidence-intervals",
    "href": "chapters/25-presenting-results.html#confidence-intervals",
    "title": "36  Presenting Statistical Results",
    "section": "36.9 Confidence Intervals",
    "text": "36.9 Confidence Intervals\nConfidence intervals often communicate more information than p-values alone. They show both statistical significance and precision of estimation:\n\n“Mean improvement in the treatment group was 12.3 points (95% CI: 8.1–16.5), significantly greater than zero.”\n\nWhen confidence intervals don’t cross zero (for differences) or one (for ratios), the result is statistically significant at the corresponding α level.\n\n\n\n\n\n\n\n\nFigure 36.1: Confidence intervals showing treatment effects on response variable across dose groups",
    "crumbs": [
      "Scientific Communication",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Presenting Statistical Results</span>"
    ]
  },
  {
    "objectID": "chapters/25-presenting-results.html#practical-example-writing-a-results-paragraph",
    "href": "chapters/25-presenting-results.html#practical-example-writing-a-results-paragraph",
    "title": "36  Presenting Statistical Results",
    "section": "36.10 Practical Example: Writing a Results Paragraph",
    "text": "36.10 Practical Example: Writing a Results Paragraph\nConsider an experiment testing whether a new fertilizer affects plant growth. Here’s how to construct a results paragraph:\nData summary: - Control group: n = 30, mean height = 24.3 cm, SD = 4.2 cm - Treatment group: n = 30, mean height = 31.7 cm, SD = 5.1 cm - t-test: t(58) = 6.17, p &lt; 0.001 - Cohen’s d = 1.59\nWell-written results paragraph:\n\n“Plants receiving the experimental fertilizer grew significantly taller than control plants. Mean height in the treatment group (M = 31.7 cm, SD = 5.1) was 7.4 cm greater than in the control group (M = 24.3 cm, SD = 4.2), representing a 30% increase in growth. This difference was statistically significant, t(58) = 6.17, p &lt; 0.001, with a large effect size (d = 1.59). Figure 1 shows the distribution of heights in both groups.”\n\nNotice how this paragraph: - States the main finding clearly - Provides specific numerical values - Reports both percentage and absolute differences - Includes all statistical details - References a figure - Uses past tense and maintains objectivity",
    "crumbs": [
      "Scientific Communication",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Presenting Statistical Results</span>"
    ]
  },
  {
    "objectID": "chapters/25-presenting-results.html#common-mistakes-to-avoid",
    "href": "chapters/25-presenting-results.html#common-mistakes-to-avoid",
    "title": "36  Presenting Statistical Results",
    "section": "36.11 Common Mistakes to Avoid",
    "text": "36.11 Common Mistakes to Avoid\n\nStatistical Reporting Errors\n\nMisreporting degrees of freedom: For a t-test comparing two groups of 25 each, df = 48, not 50\nInconsistent decimal places: Report “M = 45.32, SD = 8.10” not “M = 45.32, SD = 8.1”\nMissing test statistics: “p &lt; 0.05” without the test statistic is incomplete\nConfusing SD and SE: Standard deviation describes variability; standard error describes precision of the mean\n\n\n\nInterpretation Errors\n\nClaiming causation from correlation: Correlation does not imply causation\nOver-interpreting non-significance: Failure to reject H₀ is not acceptance of H₀\nEquating statistical and practical significance: A tiny effect can be statistically significant with large n\n\n\n\nPresentation Errors\n\nBurying important results: Lead with key findings\nMixing results and interpretation: Save interpretation for the discussion\nIncomplete reporting: Include all tests performed, including non-significant results",
    "crumbs": [
      "Scientific Communication",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Presenting Statistical Results</span>"
    ]
  },
  {
    "objectID": "chapters/25-presenting-results.html#reporting-guidelines-and-standards",
    "href": "chapters/25-presenting-results.html#reporting-guidelines-and-standards",
    "title": "36  Presenting Statistical Results",
    "section": "36.12 Reporting Guidelines and Standards",
    "text": "36.12 Reporting Guidelines and Standards\nMany fields have developed reporting guidelines for specific types of studies:\n\nCONSORT: Randomized controlled trials\nSTROBE: Observational studies\nPRISMA: Systematic reviews and meta-analyses\nARRIVE: Animal research\n\nThese guidelines specify what information to report and how to structure your presentation. Following them improves transparency and reproducibility.",
    "crumbs": [
      "Scientific Communication",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Presenting Statistical Results</span>"
    ]
  },
  {
    "objectID": "chapters/25-presenting-results.html#creating-reproducible-results",
    "href": "chapters/25-presenting-results.html#creating-reproducible-results",
    "title": "36  Presenting Statistical Results",
    "section": "36.13 Creating Reproducible Results",
    "text": "36.13 Creating Reproducible Results\nModern scientific practice increasingly emphasizes reproducibility. Consider:\n\nReport exact values from statistical software rather than rounded approximations\nInclude sample sizes for all analyses\nDescribe any data exclusions and the rationale\nNote software and versions used for analysis\nShare data and code when possible\n\n\n\nCode\n# Example: Extracting exact values for reporting\n# Instead of rounding manually, extract from model objects\n\nmodel_data &lt;- data.frame(\n  treatment = rep(c(\"Control\", \"Drug\"), each = 20),\n  response = c(rnorm(20, 50, 10), rnorm(20, 58, 12))\n)\n\nt_result &lt;- t.test(response ~ treatment, data = model_data)\n\n# Extract values programmatically\ncat(\"t =\", round(t_result$statistic, 2), \"\\n\")\n\n\nt = -0.68 \n\n\nCode\ncat(\"df =\", round(t_result$parameter, 1), \"\\n\")\n\n\ndf = 38 \n\n\nCode\ncat(\"p =\", round(t_result$p.value, 4), \"\\n\")\n\n\np = 0.5028 \n\n\nCode\ncat(\"95% CI: [\", round(t_result$conf.int[1], 2), \",\",\n    round(t_result$conf.int[2], 2), \"]\\n\")\n\n\n95% CI: [ -11.29 , 5.64 ]",
    "crumbs": [
      "Scientific Communication",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Presenting Statistical Results</span>"
    ]
  },
  {
    "objectID": "chapters/25-presenting-results.html#summary",
    "href": "chapters/25-presenting-results.html#summary",
    "title": "36  Presenting Statistical Results",
    "section": "36.14 Summary",
    "text": "36.14 Summary\nPresenting statistical results effectively requires:\n\nProper style: Past tense, passive voice, objective tone\nComplete reporting: Test statistic, degrees of freedom, p-value, effect size\nClear directionality: State which group was higher/lower/faster\nAppropriate precision: Consistent decimal places and units\nIntegration: Coordinate text, tables, and figures without redundancy\nTransparency: Report all results, including non-significant findings\n\nThe goal is to present your findings so clearly that readers can evaluate the evidence themselves. Statistical results should inform, not obscure. Master these conventions, and your scientific writing will communicate with precision and authority.",
    "crumbs": [
      "Scientific Communication",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Presenting Statistical Results</span>"
    ]
  },
  {
    "objectID": "chapters/25-presenting-results.html#additional-resources",
    "href": "chapters/25-presenting-results.html#additional-resources",
    "title": "36  Presenting Statistical Results",
    "section": "36.15 Additional Resources",
    "text": "36.15 Additional Resources\n\nAmerican Psychological Association. (2020). Publication Manual of the American Psychological Association (7th ed.) - The standard reference for statistical reporting format\nLang, T. A., & Secic, M. (2006). How to Report Statistics in Medicine - Medical statistics reporting guidelines\nThulin (2025) - Modern approaches to presenting statistical analyses\n\n\n\n\n\n\n\nThulin, Måns. 2025. Modern Statistics with r. CRC Press. https://www.modernstatisticswithr.com.",
    "crumbs": [
      "Scientific Communication",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Presenting Statistical Results</span>"
    ]
  },
  {
    "objectID": "chapters/A1-eugenics-history.html",
    "href": "chapters/A1-eugenics-history.html",
    "title": "37  The Eugenics History of Statistics",
    "section": "",
    "text": "37.1 Why This History Matters\nMany of the statistical methods we use today—correlation, regression, ANOVA, and others—were developed by scientists whose primary motivation was eugenics. Understanding this history is important not to discredit the methods themselves, which remain mathematically valid and useful, but to understand the context in which science develops and to remain vigilant about how scientific tools can be misused.\nScience does not exist in a vacuum. The questions scientists ask, the data they collect, and the interpretations they favor are shaped by the social contexts in which they work. The history of statistics and eugenics offers a stark example of this principle.",
    "crumbs": [
      "Historical Context",
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>The Eugenics History of Statistics</span>"
    ]
  },
  {
    "objectID": "chapters/A1-eugenics-history.html#francis-galton-founder-of-eugenics",
    "href": "chapters/A1-eugenics-history.html#francis-galton-founder-of-eugenics",
    "title": "37  The Eugenics History of Statistics",
    "section": "37.2 Francis Galton: Founder of Eugenics",
    "text": "37.2 Francis Galton: Founder of Eugenics\n\n\n\n\n\n\nFigure 37.1: Francis Galton (1822-1911), founder of eugenics and pioneer of statistical correlation\n\n\n\nFrancis Galton (1822–1911) was Charles Darwin’s half-cousin and a polymath who made genuine contributions to statistics, meteorology, and other fields. He invented correlation, pioneered the use of questionnaires, and conducted early twin studies. He is also the person who coined the term “eugenics” and devoted much of his career to promoting it.\nGalton believed that human intelligence was hereditary and that the “improvement” of the human race could be achieved through selective breeding. He studied prominent academics and concluded that talent ran in families, attributing this entirely to heredity while ignoring the advantages of wealth, education, and social connections.\nHis statistical innovations—correlation, regression to the mean—were developed specifically to analyze human inheritance and support eugenic arguments. The concept of “regression to the mean” came from his observation that tall parents had children who were tall but not as extremely tall as the parents, which he interpreted through a lens of hereditary “quality.”",
    "crumbs": [
      "Historical Context",
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>The Eugenics History of Statistics</span>"
    ]
  },
  {
    "objectID": "chapters/A1-eugenics-history.html#eugenics-in-america",
    "href": "chapters/A1-eugenics-history.html#eugenics-in-america",
    "title": "37  The Eugenics History of Statistics",
    "section": "37.3 Eugenics in America",
    "text": "37.3 Eugenics in America\nEugenic ideas found fertile ground in the United States. The Eugenics Record Office (ERO) was founded at Cold Spring Harbor in 1910, conducting research aimed at identifying “unfit” individuals who should be prevented from reproducing.\n\n\n\n\n\n\nFigure 37.2: Eugenics exhibit at a state fair promoting “fitter families” contests\n\n\n\nState fairs featured “fitter family” contests. Educational materials promoted eugenic thinking. Thirty states passed laws allowing forced sterilization of people deemed “unfit”—a category that disproportionately targeted poor people, immigrants, people with disabilities, and people of color.\nBetween 1907 and 1963, over 64,000 people were forcibly sterilized in the United States under eugenic legislation. California led the nation in forced sterilizations.",
    "crumbs": [
      "Historical Context",
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>The Eugenics History of Statistics</span>"
    ]
  },
  {
    "objectID": "chapters/A1-eugenics-history.html#r.a.-fisher-and-eugenics",
    "href": "chapters/A1-eugenics-history.html#r.a.-fisher-and-eugenics",
    "title": "37  The Eugenics History of Statistics",
    "section": "37.4 R.A. Fisher and Eugenics",
    "text": "37.4 R.A. Fisher and Eugenics\n\n\n\n\n\n\nFigure 37.3: R.A. Fisher (1890-1962), pioneering statistician and eugenics advocate\n\n\n\nRonald A. Fisher (1890–1962) is one of the most influential statisticians in history. He developed analysis of variance (ANOVA), the concept of statistical significance, maximum likelihood estimation, and experimental design principles that remain standard today.\nFisher was also deeply committed to eugenics throughout his career. He was the founding chairman of the Cambridge University Eugenics Society. Approximately one-third of his landmark book “The Genetical Theory of Natural Selection” (1930) addresses eugenics, arguing that the fall of civilizations was caused by differential fertility between social classes.\nFisher used his statistical methods to analyze human variation in ways meant to support racial hierarchies. His scientific work and his eugenic advocacy were not separate activities—they were intertwined.",
    "crumbs": [
      "Historical Context",
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>The Eugenics History of Statistics</span>"
    ]
  },
  {
    "objectID": "chapters/A1-eugenics-history.html#connections-to-nazi-germany",
    "href": "chapters/A1-eugenics-history.html#connections-to-nazi-germany",
    "title": "37  The Eugenics History of Statistics",
    "section": "37.5 Connections to Nazi Germany",
    "text": "37.5 Connections to Nazi Germany\nAmerican eugenics directly influenced Nazi Germany. The Nazi sterilization program, which forcibly sterilized hundreds of thousands of people, was explicitly modeled on American laws, particularly California’s.\n\n\n\n\n\n\nFigure 37.4: Nazi Germany’s sterilization program was modeled on American eugenic laws\n\n\n\nThe Holocaust itself was the most extreme expression of eugenic ideology—the belief that human populations could and should be “improved” through preventing certain people from reproducing, taken to its murderous conclusion.\nAfter World War II, the horrors of Nazi eugenics discredited the movement publicly, but eugenic practices continued. Forced sterilizations continued in the United States into the 1970s. California did not pass legislation explicitly banning sterilization of prison inmates until 2014.",
    "crumbs": [
      "Historical Context",
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>The Eugenics History of Statistics</span>"
    ]
  },
  {
    "objectID": "chapters/A1-eugenics-history.html#what-do-we-do-with-this-history",
    "href": "chapters/A1-eugenics-history.html#what-do-we-do-with-this-history",
    "title": "37  The Eugenics History of Statistics",
    "section": "37.6 What Do We Do With This History?",
    "text": "37.6 What Do We Do With This History?\nThe statistical methods developed by Galton, Fisher, and others are mathematically sound. A t-test does not care about the motives of the person who developed it. These tools have been used to improve medicine, agriculture, and countless other fields.\nBut acknowledging this history serves several purposes:\nIt reminds us that science is not neutral. The questions scientists ask are shaped by their values and social context. Eugenic statistics were developed to answer eugenic questions.\nIt encourages vigilance. Similar misuses of science can occur today. Claims about genetic differences between groups, about who deserves resources or rights, should be scrutinized carefully.\nIt honors the victims. Tens of thousands of people were forcibly sterilized, and millions were murdered, under policies justified by scientific authority. Acknowledging this history recognizes their suffering.",
    "crumbs": [
      "Historical Context",
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>The Eugenics History of Statistics</span>"
    ]
  },
  {
    "objectID": "chapters/A1-eugenics-history.html#moving-forward",
    "href": "chapters/A1-eugenics-history.html#moving-forward",
    "title": "37  The Eugenics History of Statistics",
    "section": "37.7 Moving Forward",
    "text": "37.7 Moving Forward\nUnderstanding this history does not mean abandoning statistics—it means using these tools thoughtfully and ethically. It means being skeptical when scientific claims align too conveniently with existing prejudices. It means recognizing that data and analysis can be weaponized.\nScience has the potential to improve human welfare, but only when practiced with awareness of its history and constant attention to its ethics.",
    "crumbs": [
      "Historical Context",
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>The Eugenics History of Statistics</span>"
    ]
  },
  {
    "objectID": "chapters/A1-eugenics-history.html#further-reading",
    "href": "chapters/A1-eugenics-history.html#further-reading",
    "title": "37  The Eugenics History of Statistics",
    "section": "37.8 Further Reading",
    "text": "37.8 Further Reading\nFor those interested in exploring this history further:\n\n“Superior: The Return of Race Science” by Angela Saini\n“The Gene: An Intimate History” by Siddhartha Mukherjee\n\n“Control: The Dark History and Troubling Present of Eugenics” by Adam Rutherford\n\n\n\n\n\n\n\nFigure 37.5: Book covers for further reading on the history and ethics of eugenics and race science",
    "crumbs": [
      "Historical Context",
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>The Eugenics History of Statistics</span>"
    ]
  },
  {
    "objectID": "chapters/A8-keyboard-shortcuts.html",
    "href": "chapters/A8-keyboard-shortcuts.html",
    "title": "38  Keyboard Shortcuts Reference",
    "section": "",
    "text": "38.1 Bash/Terminal Shortcuts\nThis appendix provides essential keyboard shortcuts for the tools commonly used in biostatistics and data science workflows.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>Keyboard Shortcuts Reference</span>"
    ]
  },
  {
    "objectID": "chapters/A8-keyboard-shortcuts.html#bashterminal-shortcuts",
    "href": "chapters/A8-keyboard-shortcuts.html#bashterminal-shortcuts",
    "title": "38  Keyboard Shortcuts Reference",
    "section": "",
    "text": "Navigation and Editing\n\n\n\nShortcut\nDescription\n\n\n\n\nCtrl + A\nMove cursor to beginning of line\n\n\nCtrl + E\nMove cursor to end of line\n\n\nCtrl + B\nMove cursor back one character\n\n\nCtrl + F\nMove cursor forward one character\n\n\nAlt + B\nMove cursor back one word\n\n\nAlt + F\nMove cursor forward one word\n\n\nCtrl + U\nDelete from cursor to beginning of line\n\n\nCtrl + K\nDelete from cursor to end of line\n\n\nCtrl + W\nDelete word before cursor\n\n\nAlt + D\nDelete word after cursor\n\n\nCtrl + Y\nPaste (yank) deleted text\n\n\nCtrl + _\nUndo last edit\n\n\n\n\n\nHistory and Search\n\n\n\nShortcut\nDescription\n\n\n\n\nCtrl + R\nSearch command history (reverse)\n\n\nCtrl + S\nSearch command history (forward)\n\n\nCtrl + G\nCancel history search\n\n\nCtrl + P or Up\nPrevious command in history\n\n\nCtrl + N or Down\nNext command in history\n\n\n!!\nRepeat last command\n\n\n!$\nLast argument of previous command\n\n\n!n\nExecute command number n from history\n\n\n!string\nExecute last command starting with string\n\n\n\n\n\nProcess Control\n\n\n\nShortcut\nDescription\n\n\n\n\nCtrl + C\nInterrupt/kill current process\n\n\nCtrl + Z\nSuspend current process (use fg to resume)\n\n\nCtrl + D\nExit shell / End of input\n\n\nCtrl + L\nClear screen\n\n\nCtrl + S\nPause terminal output\n\n\nCtrl + Q\nResume terminal output",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>Keyboard Shortcuts Reference</span>"
    ]
  },
  {
    "objectID": "chapters/A8-keyboard-shortcuts.html#vim-shortcuts",
    "href": "chapters/A8-keyboard-shortcuts.html#vim-shortcuts",
    "title": "38  Keyboard Shortcuts Reference",
    "section": "38.2 Vim Shortcuts",
    "text": "38.2 Vim Shortcuts\n\nModes\n\n\n\nKey\nDescription\n\n\n\n\ni\nEnter Insert mode (before cursor)\n\n\na\nEnter Insert mode (after cursor)\n\n\nI\nInsert at beginning of line\n\n\nA\nInsert at end of line\n\n\no\nOpen new line below\n\n\nO\nOpen new line above\n\n\nv\nEnter Visual mode (character)\n\n\nV\nEnter Visual mode (line)\n\n\nCtrl + V\nEnter Visual Block mode\n\n\nEsc\nReturn to Normal mode\n\n\n:\nEnter Command mode\n\n\n\n\n\nNavigation (Normal Mode)\n\n\n\nKey\nDescription\n\n\n\n\nh, j, k, l\nLeft, down, up, right\n\n\nw\nMove to next word\n\n\nb\nMove to previous word\n\n\ne\nMove to end of word\n\n\n0\nMove to beginning of line\n\n\n$\nMove to end of line\n\n\n^\nMove to first non-blank character\n\n\ngg\nGo to first line\n\n\nG\nGo to last line\n\n\nnG or :n\nGo to line n\n\n\nCtrl + F\nPage down\n\n\nCtrl + B\nPage up\n\n\nCtrl + D\nHalf page down\n\n\nCtrl + U\nHalf page up\n\n\n%\nJump to matching bracket\n\n\n{\nJump to previous paragraph\n\n\n}\nJump to next paragraph\n\n\n\n\n\nEditing (Normal Mode)\n\n\n\nKey\nDescription\n\n\n\n\nx\nDelete character under cursor\n\n\nX\nDelete character before cursor\n\n\ndd\nDelete entire line\n\n\ndw\nDelete word\n\n\nd$ or D\nDelete to end of line\n\n\nd0\nDelete to beginning of line\n\n\nyy\nYank (copy) entire line\n\n\nyw\nYank word\n\n\np\nPaste after cursor\n\n\nP\nPaste before cursor\n\n\nr\nReplace single character\n\n\nR\nEnter Replace mode\n\n\ncc\nChange entire line\n\n\ncw\nChange word\n\n\nc$ or C\nChange to end of line\n\n\nu\nUndo\n\n\nCtrl + R\nRedo\n\n\n.\nRepeat last command\n\n\n~\nToggle case\n\n\n&gt;&gt;\nIndent line\n\n\n&lt;&lt;\nUnindent line\n\n\n\n\n\nSearch and Replace\n\n\n\nCommand\nDescription\n\n\n\n\n/pattern\nSearch forward for pattern\n\n\n?pattern\nSearch backward for pattern\n\n\nn\nFind next occurrence\n\n\nN\nFind previous occurrence\n\n\n*\nSearch for word under cursor\n\n\n:%s/old/new/g\nReplace all occurrences in file\n\n\n:s/old/new/g\nReplace all occurrences in line\n\n\n:%s/old/new/gc\nReplace with confirmation\n\n\n\n\n\nFile Operations\n\n\n\nCommand\nDescription\n\n\n\n\n:w\nSave file\n\n\n:w filename\nSave as filename\n\n\n:q\nQuit\n\n\n:q!\nQuit without saving\n\n\n:wq or :x\nSave and quit\n\n\nZZ\nSave and quit (Normal mode)\n\n\n:e filename\nOpen file\n\n\n:r filename\nInsert file contents",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>Keyboard Shortcuts Reference</span>"
    ]
  },
  {
    "objectID": "chapters/A8-keyboard-shortcuts.html#rstudio-shortcuts",
    "href": "chapters/A8-keyboard-shortcuts.html#rstudio-shortcuts",
    "title": "38  Keyboard Shortcuts Reference",
    "section": "38.3 RStudio Shortcuts",
    "text": "38.3 RStudio Shortcuts\n\nConsole\n\n\n\nShortcut\nDescription\n\n\n\n\nCtrl + Enter\nRun current line/selection\n\n\nCtrl + Shift + Enter\nRun entire script\n\n\nCtrl + Alt + R\nRun from beginning to current line\n\n\nCtrl + Alt + E\nRun from current line to end\n\n\nCtrl + Alt + B\nRun from beginning to cursor\n\n\nCtrl + L\nClear console\n\n\nEsc\nInterrupt R\n\n\nTab\nAutocomplete\n\n\nCtrl + Up\nCommand history popup\n\n\n\n\n\nEditor\n\n\n\nShortcut\nDescription\n\n\n\n\nCtrl + S\nSave current file\n\n\nCtrl + Shift + S\nSave all open files\n\n\nCtrl + Z\nUndo\n\n\nCtrl + Shift + Z\nRedo\n\n\nCtrl + F\nFind\n\n\nCtrl + H\nFind and replace\n\n\nCtrl + Shift + F\nFind in files\n\n\nCtrl + D\nDelete line\n\n\nCtrl + Shift + D\nDuplicate line\n\n\nAlt + Up/Down\nMove line up/down\n\n\nCtrl + Shift + C\nComment/uncomment line\n\n\nCtrl + Shift + /\nReflow comment\n\n\nCtrl + I\nReindent lines\n\n\nCtrl + Shift + A\nReformat code\n\n\nCtrl + Shift + M\nInsert pipe (%&gt;% or |&gt;)\n\n\nAlt + -\nInsert assignment (&lt;-)\n\n\nCtrl + Shift + R\nInsert code section\n\n\n\n\n\nNavigation\n\n\n\nShortcut\nDescription\n\n\n\n\nCtrl + .\nGo to file/function\n\n\nF1\nHelp on selected function\n\n\nF2\nGo to function definition\n\n\nCtrl + Shift + O\nDocument outline\n\n\nCtrl + Tab\nSwitch between open files\n\n\nCtrl + Shift + Tab\nSwitch to previous file\n\n\nCtrl + 1\nFocus on Source editor\n\n\nCtrl + 2\nFocus on Console\n\n\nCtrl + 3\nFocus on Help\n\n\nCtrl + 4\nFocus on History\n\n\nCtrl + 5\nFocus on Files\n\n\nCtrl + 6\nFocus on Plots\n\n\nCtrl + 7\nFocus on Packages\n\n\nCtrl + 8\nFocus on Environment\n\n\nCtrl + 9\nFocus on Viewer\n\n\n\n\n\nProjects and Sessions\n\n\n\nShortcut\nDescription\n\n\n\n\nCtrl + Shift + N\nNew file\n\n\nCtrl + O\nOpen file\n\n\nCtrl + Shift + P\nCommand palette\n\n\nCtrl + Shift + F10\nRestart R session\n\n\nCtrl + Q\nQuit RStudio",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>Keyboard Shortcuts Reference</span>"
    ]
  },
  {
    "objectID": "chapters/A8-keyboard-shortcuts.html#jupyter-notebook-shortcuts",
    "href": "chapters/A8-keyboard-shortcuts.html#jupyter-notebook-shortcuts",
    "title": "38  Keyboard Shortcuts Reference",
    "section": "38.4 Jupyter Notebook Shortcuts",
    "text": "38.4 Jupyter Notebook Shortcuts\n\nCommand Mode (Blue Border)\nPress Esc to enter Command mode.\n\n\n\nKey\nDescription\n\n\n\n\nEnter\nEnter Edit mode\n\n\nH\nShow keyboard shortcuts\n\n\nA\nInsert cell above\n\n\nB\nInsert cell below\n\n\nX\nCut selected cells\n\n\nC\nCopy selected cells\n\n\nV\nPaste cells below\n\n\nShift + V\nPaste cells above\n\n\nD, D\nDelete selected cells\n\n\nZ\nUndo cell deletion\n\n\nY\nChange cell to Code\n\n\nM\nChange cell to Markdown\n\n\nR\nChange cell to Raw\n\n\n1-6\nChange to heading level 1-6\n\n\nUp/K\nSelect cell above\n\n\nDown/J\nSelect cell below\n\n\nShift + Up/K\nExtend selection above\n\n\nShift + Down/J\nExtend selection below\n\n\nShift + M\nMerge selected cells\n\n\nO\nToggle output\n\n\nShift + O\nToggle output scrolling\n\n\nI, I\nInterrupt kernel\n\n\n0, 0\nRestart kernel\n\n\nS or Ctrl + S\nSave notebook\n\n\nL\nToggle line numbers\n\n\n\n\n\nEdit Mode (Green Border)\nPress Enter to enter Edit mode.\n\n\n\nShortcut\nDescription\n\n\n\n\nEsc\nEnter Command mode\n\n\nCtrl + Enter\nRun cell\n\n\nShift + Enter\nRun cell, select below\n\n\nAlt + Enter\nRun cell, insert below\n\n\nCtrl + Shift + -\nSplit cell at cursor\n\n\nTab\nCode completion\n\n\nShift + Tab\nTooltip/documentation\n\n\nCtrl + ]\nIndent\n\n\nCtrl + [\nDedent\n\n\nCtrl + A\nSelect all\n\n\nCtrl + Z\nUndo\n\n\nCtrl + Shift + Z\nRedo\n\n\nCtrl + D\nDelete whole line\n\n\nCtrl + /\nComment/uncomment\n\n\nCtrl + Home\nGo to cell start\n\n\nCtrl + End\nGo to cell end",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>Keyboard Shortcuts Reference</span>"
    ]
  },
  {
    "objectID": "chapters/A8-keyboard-shortcuts.html#vs-code-shortcuts",
    "href": "chapters/A8-keyboard-shortcuts.html#vs-code-shortcuts",
    "title": "38  Keyboard Shortcuts Reference",
    "section": "38.5 VS Code Shortcuts",
    "text": "38.5 VS Code Shortcuts\n\nGeneral\n\n\n\nShortcut\nDescription\n\n\n\n\nCtrl + Shift + P\nCommand palette\n\n\nCtrl + P\nQuick open file\n\n\nCtrl + Shift + N\nNew window\n\n\nCtrl + W\nClose editor\n\n\nCtrl + ,\nOpen settings\n\n\nCtrl + K Ctrl + S\nKeyboard shortcuts\n\n\nCtrl + ``\nToggle integrated terminal\n\n\nF11\nToggle full screen\n\n\nCtrl + B\nToggle sidebar\n\n\nCtrl + Shift + E\nExplorer focus\n\n\nCtrl + Shift + F\nSearch across files\n\n\nCtrl + Shift + G\nSource control\n\n\nCtrl + Shift + D\nDebug\n\n\nCtrl + Shift + X\nExtensions\n\n\n\n\n\nEditing\n\n\n\nShortcut\nDescription\n\n\n\n\nCtrl + X\nCut line (empty selection)\n\n\nCtrl + C\nCopy line (empty selection)\n\n\nCtrl + V\nPaste\n\n\nCtrl + Shift + K\nDelete line\n\n\nCtrl + Enter\nInsert line below\n\n\nCtrl + Shift + Enter\nInsert line above\n\n\nAlt + Up/Down\nMove line up/down\n\n\nShift + Alt + Up/Down\nCopy line up/down\n\n\nCtrl + D\nSelect word (repeat for next)\n\n\nCtrl + Shift + L\nSelect all occurrences\n\n\nCtrl + L\nSelect current line\n\n\nCtrl + /\nToggle line comment\n\n\nShift + Alt + A\nToggle block comment\n\n\nCtrl + ]\nIndent line\n\n\nCtrl + [\nOutdent line\n\n\nCtrl + Shift + \\\nJump to matching bracket\n\n\nCtrl + Z\nUndo\n\n\nCtrl + Shift + Z\nRedo\n\n\n\n\n\nNavigation\n\n\n\nShortcut\nDescription\n\n\n\n\nCtrl + G\nGo to line\n\n\nCtrl + P\nGo to file\n\n\nCtrl + Shift + O\nGo to symbol\n\n\nF12\nGo to definition\n\n\nAlt + F12\nPeek definition\n\n\nCtrl + Shift + \\\nGo to matching bracket\n\n\nCtrl + Home\nGo to beginning of file\n\n\nCtrl + End\nGo to end of file\n\n\nCtrl + Tab\nSwitch between open editors\n\n\nAlt + Left/Right\nGo back/forward\n\n\nCtrl + K Ctrl + Q\nGo to last edit location\n\n\n\n\n\nMulti-Cursor and Selection\n\n\n\nShortcut\nDescription\n\n\n\n\nAlt + Click\nInsert cursor\n\n\nCtrl + Alt + Up/Down\nInsert cursor above/below\n\n\nCtrl + U\nUndo cursor operation\n\n\nShift + Alt + I\nInsert cursor at end of each line\n\n\nCtrl + Shift + L\nSelect all occurrences of selection\n\n\nCtrl + F2\nSelect all occurrences of word\n\n\nShift + Alt + Drag\nColumn selection\n\n\nCtrl + Shift + Alt + Arrow\nColumn selection\n\n\n\n\n\nSearch and Replace\n\n\n\nShortcut\nDescription\n\n\n\n\nCtrl + F\nFind\n\n\nCtrl + H\nReplace\n\n\nF3 / Shift + F3\nFind next/previous\n\n\nAlt + Enter\nSelect all occurrences of find match\n\n\nCtrl + D\nAdd selection to next find match\n\n\nCtrl + K Ctrl + D\nMove last selection to next find match\n\n\n\n\n\nIntegrated Terminal\n\n\n\nShortcut\nDescription\n\n\n\n\nCtrl + ``\nToggle terminal\n\n\nCtrl + Shift + ``\nCreate new terminal\n\n\nCtrl + Shift + 5\nSplit terminal\n\n\nCtrl + PageUp/Down\nScroll terminal\n\n\nShift + PageUp/Down\nScroll terminal by page\n\n\n\n\n\n\n\n\n\nCustomizing Shortcuts\n\n\n\nMost applications allow you to customize keyboard shortcuts:\n\nRStudio: Tools → Modify Keyboard Shortcuts\nVS Code: File → Preferences → Keyboard Shortcuts (or Ctrl + K Ctrl + S)\nJupyter: Help → Edit Keyboard Shortcuts\nBash: Edit ~/.inputrc for readline customizations\n\nLearning the default shortcuts first helps when working on different machines, but customization can boost your personal productivity significantly.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>Keyboard Shortcuts Reference</span>"
    ]
  },
  {
    "objectID": "chapters/A8-keyboard-shortcuts.html#platform-differences",
    "href": "chapters/A8-keyboard-shortcuts.html#platform-differences",
    "title": "38  Keyboard Shortcuts Reference",
    "section": "38.6 Platform Differences",
    "text": "38.6 Platform Differences\nMost shortcuts in this appendix use Ctrl as the modifier key (Windows/Linux). On macOS:\n\n\n\nWindows/Linux\nmacOS\n\n\n\n\nCtrl\nCmd (⌘)\n\n\nAlt\nOption (⌥)\n\n\nCtrl + Alt\nCmd + Option\n\n\n\nSome applications maintain the Ctrl key on macOS for terminal-related shortcuts (Bash, Vim).",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>Keyboard Shortcuts Reference</span>"
    ]
  },
  {
    "objectID": "chapters/A6-unix-reference.html",
    "href": "chapters/A6-unix-reference.html",
    "title": "39  Unix Command Reference",
    "section": "",
    "text": "39.1 Navigation Commands\nThis appendix provides a comprehensive reference for Unix/Linux commands covered throughout the book.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>39</span>  <span class='chapter-title'>Unix Command Reference</span>"
    ]
  },
  {
    "objectID": "chapters/A6-unix-reference.html#sec-unix-nav",
    "href": "chapters/A6-unix-reference.html#sec-unix-nav",
    "title": "39  Unix Command Reference",
    "section": "",
    "text": "Command\nDescription\nExample\n\n\n\n\npwd\nPrint working directory\npwd\n\n\nls\nList directory contents\nls -la\n\n\nls -l\nLong format listing\nls -l *.txt\n\n\nls -a\nShow hidden files\nls -a\n\n\nls -h\nHuman-readable sizes\nls -lh\n\n\nls -R\nRecursive listing\nls -R projects/\n\n\nls -t\nSort by modification time\nls -lt\n\n\ncd\nChange directory\ncd ~/projects\n\n\ncd ..\nGo up one directory\ncd ..\n\n\ncd -\nGo to previous directory\ncd -\n\n\ncd ~\nGo to home directory\ncd ~\n\n\nmkdir\nCreate directory\nmkdir data\n\n\nmkdir -p\nCreate nested directories\nmkdir -p data/raw/2024\n\n\nrmdir\nRemove empty directory\nrmdir old_folder\n\n\ntree\nDisplay directory tree\ntree -L 2",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>39</span>  <span class='chapter-title'>Unix Command Reference</span>"
    ]
  },
  {
    "objectID": "chapters/A6-unix-reference.html#sec-unix-files",
    "href": "chapters/A6-unix-reference.html#sec-unix-files",
    "title": "39  Unix Command Reference",
    "section": "39.2 File Operations",
    "text": "39.2 File Operations\n\n\n\n\n\n\n\n\nCommand\nDescription\nExample\n\n\n\n\ncp\nCopy files\ncp file.txt backup/\n\n\ncp -r\nCopy directories recursively\ncp -r data/ backup/\n\n\ncp -i\nInteractive (prompt before overwrite)\ncp -i *.txt dest/\n\n\ncp -v\nVerbose output\ncp -v file.txt backup/\n\n\nmv\nMove or rename files\nmv old.txt new.txt\n\n\nmv -i\nInteractive move\nmv -i *.txt archive/\n\n\nrm\nRemove files\nrm unwanted.txt\n\n\nrm -r\nRemove directories recursively\nrm -r old_directory/\n\n\nrm -i\nInteractive removal\nrm -i *.log\n\n\nrm -f\nForce removal (no prompts)\nrm -f temp*.txt\n\n\ntouch\nCreate empty file / update timestamp\ntouch notes.txt\n\n\nln -s\nCreate symbolic link\nln -s /path/to/file link_name\n\n\nfile\nDetermine file type\nfile mystery_file\n\n\nstat\nDisplay file status\nstat data.csv",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>39</span>  <span class='chapter-title'>Unix Command Reference</span>"
    ]
  },
  {
    "objectID": "chapters/A6-unix-reference.html#sec-unix-view",
    "href": "chapters/A6-unix-reference.html#sec-unix-view",
    "title": "39  Unix Command Reference",
    "section": "39.3 Viewing File Contents",
    "text": "39.3 Viewing File Contents\n\n\n\nCommand\nDescription\nExample\n\n\n\n\ncat\nDisplay entire file\ncat data.csv\n\n\ncat -n\nDisplay with line numbers\ncat -n script.sh\n\n\nhead\nShow first lines (default 10)\nhead file.txt\n\n\nhead -n\nShow first n lines\nhead -n 20 file.txt\n\n\ntail\nShow last lines (default 10)\ntail file.txt\n\n\ntail -n\nShow last n lines\ntail -n 50 log.txt\n\n\ntail -f\nFollow file (live updates)\ntail -f server.log\n\n\nless\nPage through file\nless huge_file.txt\n\n\nmore\nSimple pager\nmore file.txt\n\n\ndiff\nCompare files\ndiff file1.txt file2.txt\n\n\ndiff -u\nUnified diff format\ndiff -u old.txt new.txt\n\n\ncmp\nCompare files byte by byte\ncmp file1 file2\n\n\nmd5sum\nCalculate MD5 checksum\nmd5sum file.txt\n\n\nsha256sum\nCalculate SHA256 checksum\nsha256sum file.txt",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>39</span>  <span class='chapter-title'>Unix Command Reference</span>"
    ]
  },
  {
    "objectID": "chapters/A6-unix-reference.html#sec-unix-text",
    "href": "chapters/A6-unix-reference.html#sec-unix-text",
    "title": "39  Unix Command Reference",
    "section": "39.4 Text Processing",
    "text": "39.4 Text Processing\n\n\n\n\n\n\n\n\nCommand\nDescription\nExample\n\n\n\n\ngrep\nSearch for patterns\ngrep \"error\" log.txt\n\n\ngrep -E\nExtended regex\ngrep -E \"gene[0-9]+\" data.txt\n\n\ngrep -v\nInvert match (exclude)\ngrep -v \"^#\" data.txt\n\n\ngrep -c\nCount matches\ngrep -c \"&gt;\" sequences.fa\n\n\ngrep -i\nCase insensitive\ngrep -i \"warning\" log.txt\n\n\ngrep -n\nShow line numbers\ngrep -n \"TODO\" code.py\n\n\ngrep -l\nList matching files only\ngrep -l \"error\" *.log\n\n\ngrep -r\nRecursive search\ngrep -r \"function\" src/\n\n\ngrep -A n\nShow n lines after match\ngrep -A 3 \"error\" log.txt\n\n\ngrep -B n\nShow n lines before match\ngrep -B 2 \"error\" log.txt\n\n\nsort\nSort lines\nsort names.txt\n\n\nsort -n\nNumeric sort\nsort -n numbers.txt\n\n\nsort -r\nReverse sort\nsort -r names.txt\n\n\nsort -k\nSort by column\nsort -k2 data.tsv\n\n\nsort -u\nSort and remove duplicates\nsort -u list.txt\n\n\nuniq\nRemove adjacent duplicates\nsort file | uniq\n\n\nuniq -c\nCount occurrences\nsort file | uniq -c\n\n\nuniq -d\nShow only duplicates\nsort file | uniq -d\n\n\ncut -f\nExtract fields (tab-delimited)\ncut -f2 data.tsv\n\n\ncut -d\nSpecify delimiter\ncut -d',' -f1,3 data.csv\n\n\ncut -c\nExtract characters\ncut -c1-10 file.txt\n\n\ntr\nTranslate characters\ntr 'a-z' 'A-Z'\n\n\ntr -d\nDelete characters\ntr -d '\\r' &lt; file.txt\n\n\ntr -s\nSqueeze repeats\ntr -s ' '\n\n\nsed\nStream editor\nsed 's/old/new/g' file.txt\n\n\nsed -i\nEdit file in place\nsed -i 's/old/new/g' file.txt\n\n\nsed -n\nSuppress output\nsed -n '10,20p' file.txt\n\n\nawk\nPattern processing\nawk '{print $1}' file.txt\n\n\nawk -F\nSpecify field separator\nawk -F',' '{print $2}' data.csv\n\n\nwc\nCount lines, words, bytes\nwc file.txt\n\n\nwc -l\nCount lines only\nwc -l data.csv\n\n\nwc -w\nCount words only\nwc -w essay.txt\n\n\nwc -c\nCount bytes only\nwc -c file.bin\n\n\npaste\nMerge files line by line\npaste file1.txt file2.txt\n\n\njoin\nJoin files on common field\njoin file1.txt file2.txt\n\n\nsplit\nSplit file into pieces\nsplit -l 1000 large.txt",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>39</span>  <span class='chapter-title'>Unix Command Reference</span>"
    ]
  },
  {
    "objectID": "chapters/A6-unix-reference.html#sec-unix-redirect",
    "href": "chapters/A6-unix-reference.html#sec-unix-redirect",
    "title": "39  Unix Command Reference",
    "section": "39.5 Redirection and Pipes",
    "text": "39.5 Redirection and Pipes\n\n\n\n\n\n\n\n\nOperator\nDescription\nExample\n\n\n\n\n&gt;\nRedirect output (overwrite)\nls &gt; files.txt\n\n\n&gt;&gt;\nRedirect output (append)\necho \"done\" &gt;&gt; log.txt\n\n\n&lt;\nRedirect input\nwc -l &lt; data.txt\n\n\n2&gt;\nRedirect stderr\ncmd 2&gt; errors.txt\n\n\n2&gt;&1\nRedirect stderr to stdout\ncmd &gt; out.txt 2&gt;&1\n\n\n&&gt;\nRedirect both stdout and stderr\ncmd &&gt; all.txt\n\n\n|\nPipe to next command\ncat file | sort | uniq\n\n\n|&\nPipe stdout and stderr\ncmd |& less\n\n\ntee\nWrite to file and stdout\ncmd | tee output.txt\n\n\nxargs\nBuild commands from input\nfind . -name \"*.txt\" | xargs grep \"pattern\"",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>39</span>  <span class='chapter-title'>Unix Command Reference</span>"
    ]
  },
  {
    "objectID": "chapters/A6-unix-reference.html#sec-unix-compress",
    "href": "chapters/A6-unix-reference.html#sec-unix-compress",
    "title": "39  Unix Command Reference",
    "section": "39.6 File Compression",
    "text": "39.6 File Compression\n\n\n\nCommand\nDescription\nExample\n\n\n\n\ngzip\nCompress file\ngzip large_file.txt\n\n\ngzip -k\nKeep original file\ngzip -k file.txt\n\n\ngzip -d\nDecompress\ngzip -d file.txt.gz\n\n\ngunzip\nDecompress .gz file\ngunzip file.txt.gz\n\n\nzcat\nView compressed file\nzcat data.gz | head\n\n\nzgrep\nSearch compressed file\nzgrep \"pattern\" file.gz\n\n\nzless\nPage through compressed file\nzless data.gz\n\n\nbzip2\nCompress (better ratio)\nbzip2 large_file.txt\n\n\nbunzip2\nDecompress .bz2 file\nbunzip2 file.txt.bz2\n\n\ntar -c\nCreate archive\ntar -cvf archive.tar dir/\n\n\ntar -x\nExtract archive\ntar -xvf archive.tar\n\n\ntar -z\nUse gzip compression\ntar -czvf archive.tar.gz dir/\n\n\ntar -j\nUse bzip2 compression\ntar -cjvf archive.tar.bz2 dir/\n\n\ntar -t\nList archive contents\ntar -tvf archive.tar.gz\n\n\nzip\nCreate zip archive\nzip -r archive.zip dir/\n\n\nunzip\nExtract zip archive\nunzip archive.zip\n\n\nunzip -l\nList zip contents\nunzip -l archive.zip",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>39</span>  <span class='chapter-title'>Unix Command Reference</span>"
    ]
  },
  {
    "objectID": "chapters/A6-unix-reference.html#sec-unix-permissions",
    "href": "chapters/A6-unix-reference.html#sec-unix-permissions",
    "title": "39  Unix Command Reference",
    "section": "39.7 File Permissions",
    "text": "39.7 File Permissions\n\n\n\n\n\n\n\n\nCommand\nDescription\nExample\n\n\n\n\nchmod\nChange file permissions\nchmod +x script.sh\n\n\nchmod u+x\nAdd execute for owner\nchmod u+x script.sh\n\n\nchmod go-w\nRemove write for group/others\nchmod go-w file.txt\n\n\nchmod 755\nSet rwxr-xr-x\nchmod 755 script.sh\n\n\nchmod 644\nSet rw-r–r–\nchmod 644 data.txt\n\n\nchmod -R\nRecursive permission change\nchmod -R 755 scripts/\n\n\nchown\nChange file owner\nchown user file.txt\n\n\nchown user:group\nChange owner and group\nchown user:group file.txt\n\n\nchown -R\nRecursive ownership change\nchown -R user:group dir/\n\n\nchgrp\nChange group ownership\nchgrp group file.txt\n\n\n\n\nPermission Codes\n\n\n\nMode\nMeaning\nNumeric\n\n\n\n\nr\nRead\n4\n\n\nw\nWrite\n2\n\n\nx\nExecute\n1\n\n\nrwx\nRead, write, execute\n7\n\n\nrw-\nRead, write\n6\n\n\nr-x\nRead, execute\n5\n\n\nr--\nRead only\n4\n\n\n\n\n\nCommon Permission Settings\n\n\n\nSetting\nMeaning\nUse Case\n\n\n\n\n755\nrwxr-xr-x\nExecutable scripts\n\n\n644\nrw-r–r–\nRegular files\n\n\n700\nrwx——\nPrivate scripts\n\n\n600\nrw——-\nPrivate files\n\n\n777\nrwxrwxrwx\n(Avoid - too permissive)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>39</span>  <span class='chapter-title'>Unix Command Reference</span>"
    ]
  },
  {
    "objectID": "chapters/A6-unix-reference.html#sec-unix-system",
    "href": "chapters/A6-unix-reference.html#sec-unix-system",
    "title": "39  Unix Command Reference",
    "section": "39.8 System Information",
    "text": "39.8 System Information\n\n\n\nCommand\nDescription\nExample\n\n\n\n\nwhoami\nCurrent username\nwhoami\n\n\nid\nUser and group IDs\nid\n\n\nhostname\nComputer name\nhostname\n\n\nuname -a\nSystem information\nuname -a\n\n\ndate\nCurrent date/time\ndate\n\n\ndate +%Y-%m-%d\nFormatted date\ndate +%Y-%m-%d\n\n\ncal\nDisplay calendar\ncal\n\n\nuptime\nSystem uptime\nuptime\n\n\ndf -h\nDisk space usage\ndf -h\n\n\ndu -sh\nDirectory size\ndu -sh folder/\n\n\ndu -h --max-depth=1\nSubdirectory sizes\ndu -h --max-depth=1\n\n\nfree -h\nMemory usage\nfree -h\n\n\ntop\nRunning processes (interactive)\ntop\n\n\nhtop\nEnhanced process viewer\nhtop\n\n\nps\nProcess status\nps aux\n\n\nps -ef\nAll processes\nps -ef\n\n\npgrep\nFind process by name\npgrep python\n\n\nkill\nTerminate process\nkill PID\n\n\nkill -9\nForce terminate\nkill -9 PID\n\n\nkillall\nKill by name\nkillall process_name\n\n\nnohup\nRun immune to hangups\nnohup script.sh &\n\n\nbg\nSend to background\nbg\n\n\nfg\nBring to foreground\nfg\n\n\njobs\nList background jobs\njobs\n\n\nwhich\nLocate command\nwhich python\n\n\nwhereis\nLocate binary and man page\nwhereis python\n\n\ntype\nDisplay command type\ntype ls",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>39</span>  <span class='chapter-title'>Unix Command Reference</span>"
    ]
  },
  {
    "objectID": "chapters/A6-unix-reference.html#sec-unix-find",
    "href": "chapters/A6-unix-reference.html#sec-unix-find",
    "title": "39  Unix Command Reference",
    "section": "39.9 Finding Files",
    "text": "39.9 Finding Files\n\n\n\n\n\n\n\n\nCommand\nDescription\nExample\n\n\n\n\nfind\nFind files by criteria\nfind . -name \"*.txt\"\n\n\nfind -type f\nFind files only\nfind . -type f -name \"*.py\"\n\n\nfind -type d\nFind directories only\nfind . -type d -name \"data\"\n\n\nfind -mtime\nFind by modification time\nfind . -mtime -7 (last 7 days)\n\n\nfind -size\nFind by size\nfind . -size +100M\n\n\nfind -exec\nExecute command on results\nfind . -name \"*.tmp\" -exec rm {} \\;\n\n\nfind -delete\nDelete matching files\nfind . -name \"*.tmp\" -delete\n\n\nlocate\nFast file search (uses database)\nlocate filename\n\n\nupdatedb\nUpdate locate database\nsudo updatedb",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>39</span>  <span class='chapter-title'>Unix Command Reference</span>"
    ]
  },
  {
    "objectID": "chapters/A6-unix-reference.html#sec-unix-network",
    "href": "chapters/A6-unix-reference.html#sec-unix-network",
    "title": "39  Unix Command Reference",
    "section": "39.10 Network and Remote",
    "text": "39.10 Network and Remote\n\n\n\n\n\n\n\n\nCommand\nDescription\nExample\n\n\n\n\nssh\nSecure shell connection\nssh user@host\n\n\nssh -p\nSpecify port\nssh -p 2222 user@host\n\n\nssh -i\nUse identity file\nssh -i key.pem user@host\n\n\nssh -L\nLocal port forwarding\nssh -L 8080:localhost:80 user@host\n\n\nscp\nSecure copy to remote\nscp file user@host:path/\n\n\nscp -r\nCopy directory recursively\nscp -r dir/ user@host:path/\n\n\nscp\nCopy from remote\nscp user@host:file local_path\n\n\nrsync\nSync files efficiently\nrsync -avz src/ dest/\n\n\nrsync --delete\nSync and delete extra files\nrsync -avz --delete src/ dest/\n\n\nwget\nDownload file\nwget URL\n\n\nwget -O\nDownload with custom name\nwget -O output.txt URL\n\n\nwget -c\nContinue interrupted download\nwget -c URL\n\n\ncurl\nTransfer data\ncurl URL\n\n\ncurl -O\nSave with remote filename\ncurl -O URL\n\n\ncurl -o\nSave with custom filename\ncurl -o output.txt URL\n\n\nping\nTest network connectivity\nping host\n\n\ntraceroute\nShow network path\ntraceroute host\n\n\nnetstat\nNetwork statistics\nnetstat -an\n\n\nifconfig\nNetwork interface config\nifconfig\n\n\nip addr\nShow IP addresses\nip addr",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>39</span>  <span class='chapter-title'>Unix Command Reference</span>"
    ]
  },
  {
    "objectID": "chapters/A6-unix-reference.html#sec-unix-env",
    "href": "chapters/A6-unix-reference.html#sec-unix-env",
    "title": "39  Unix Command Reference",
    "section": "39.11 Environment Variables",
    "text": "39.11 Environment Variables\n\n\n\n\n\n\n\n\nCommand\nDescription\nExample\n\n\n\n\necho $VAR\nDisplay variable value\necho $PATH\n\n\nexport VAR=value\nSet environment variable\nexport PATH=$PATH:/new/path\n\n\nenv\nDisplay all environment variables\nenv\n\n\nprintenv\nPrint environment variables\nprintenv HOME\n\n\nunset VAR\nRemove variable\nunset MYVAR\n\n\nsource\nExecute script in current shell\nsource ~/.bashrc\n\n\n.\nSame as source\n. ~/.bashrc\n\n\n\n\nCommon Environment Variables\n\n\n\nVariable\nDescription\n\n\n\n\n$HOME\nHome directory\n\n\n$USER\nCurrent username\n\n\n$PATH\nExecutable search path\n\n\n$PWD\nCurrent directory\n\n\n$SHELL\nCurrent shell\n\n\n$EDITOR\nDefault text editor\n\n\n$HOSTNAME\nComputer hostname\n\n\n$?\nExit status of last command\n\n\n$$\nCurrent process ID",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>39</span>  <span class='chapter-title'>Unix Command Reference</span>"
    ]
  },
  {
    "objectID": "chapters/A2-r-reference.html",
    "href": "chapters/A2-r-reference.html",
    "title": "40  R Command Reference",
    "section": "",
    "text": "40.1 Base R Fundamentals\nThis appendix provides a comprehensive reference for R commands, covering both base R and tidyverse functions.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>40</span>  <span class='chapter-title'>R Command Reference</span>"
    ]
  },
  {
    "objectID": "chapters/A2-r-reference.html#sec-r-base",
    "href": "chapters/A2-r-reference.html#sec-r-base",
    "title": "40  R Command Reference",
    "section": "",
    "text": "Assignment and Basic Operations\n\n\n\nOperation\nSyntax\nExample\n\n\n\n\nAssignment\n&lt;- or =\nx &lt;- 5\n\n\nPrint value\nVariable name or print()\nx or print(x)\n\n\nComment\n#\n# This is a comment\n\n\nHelp\n? or help()\n?mean\n\n\nExamples\nexample()\nexample(mean)\n\n\nSearch help\n?? or help.search()\n??regression\n\n\n\n\n\nArithmetic Operators\n\n\n\nOperator\nDescription\nExample\n\n\n\n\n+\nAddition\n5 + 3\n\n\n-\nSubtraction\n5 - 3\n\n\n*\nMultiplication\n5 * 3\n\n\n/\nDivision\n5 / 3\n\n\n^ or **\nExponentiation\n5^3\n\n\n%%\nModulo (remainder)\n5 %% 3\n\n\n%/%\nInteger division\n5 %/% 3\n\n\n\n\n\nComparison Operators\n\n\n\nOperator\nDescription\nExample\n\n\n\n\n==\nEqual to\nx == 5\n\n\n!=\nNot equal to\nx != 5\n\n\n&lt;\nLess than\nx &lt; 5\n\n\n&gt;\nGreater than\nx &gt; 5\n\n\n&lt;=\nLess than or equal\nx &lt;= 5\n\n\n&gt;=\nGreater than or equal\nx &gt;= 5\n\n\n\n\n\nLogical Operators\n\n\n\nOperator\nDescription\nExample\n\n\n\n\n&\nAND (element-wise)\nx &gt; 0 & x &lt; 10\n\n\n|\nOR (element-wise)\nx &lt; 0 | x &gt; 10\n\n\n!\nNOT\n!is.na(x)\n\n\n&&\nAND (single value)\ncond1 && cond2\n\n\n||\nOR (single value)\ncond1 || cond2\n\n\n%in%\nValue in set\nx %in% c(1, 2, 3)\n\n\nxor()\nExclusive OR\nxor(TRUE, FALSE)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>40</span>  <span class='chapter-title'>R Command Reference</span>"
    ]
  },
  {
    "objectID": "chapters/A2-r-reference.html#sec-r-vectors",
    "href": "chapters/A2-r-reference.html#sec-r-vectors",
    "title": "40  R Command Reference",
    "section": "40.2 Vector Operations",
    "text": "40.2 Vector Operations\n\nCreating Vectors\n\n\n\nFunction\nDescription\nExample\n\n\n\n\nc()\nCombine values\nc(1, 2, 3, 4, 5)\n\n\n:\nSequence\n1:10\n\n\nseq()\nSequence with step\nseq(0, 1, by = 0.1)\n\n\nseq_len()\nSequence of length n\nseq_len(10)\n\n\nseq_along()\nSequence along object\nseq_along(x)\n\n\nrep()\nRepeat values\nrep(1, times = 5)\n\n\nrep()\nRepeat each\nrep(1:3, each = 2)\n\n\nvector()\nCreate empty vector\nvector(\"numeric\", 10)\n\n\n\n\n\nVector Indexing\n\n\n\nSyntax\nDescription\nExample\n\n\n\n\nx[i]\nElement at position i\nx[3]\n\n\nx[c(i,j)]\nMultiple elements\nx[c(1, 3, 5)]\n\n\nx[-i]\nExclude element\nx[-1]\n\n\nx[condition]\nLogical subsetting\nx[x &gt; 5]\n\n\nx[1:n]\nRange of elements\nx[1:5]\n\n\nx[\"name\"]\nBy name\nx[\"first\"]\n\n\n\n\n\nVector Functions\n\n\n\nFunction\nDescription\nExample\n\n\n\n\nlength()\nNumber of elements\nlength(x)\n\n\nsum()\nSum of elements\nsum(x)\n\n\nmean()\nArithmetic mean\nmean(x)\n\n\nmedian()\nMedian value\nmedian(x)\n\n\nsd()\nStandard deviation\nsd(x)\n\n\nvar()\nVariance\nvar(x)\n\n\nmin()\nMinimum\nmin(x)\n\n\nmax()\nMaximum\nmax(x)\n\n\nrange()\nRange (min and max)\nrange(x)\n\n\nquantile()\nQuantiles\nquantile(x, 0.5)\n\n\nsort()\nSort values\nsort(x)\n\n\norder()\nIndices for sorting\norder(x)\n\n\nrev()\nReverse order\nrev(x)\n\n\nunique()\nUnique values\nunique(x)\n\n\ntable()\nFrequency table\ntable(x)\n\n\ncumsum()\nCumulative sum\ncumsum(x)\n\n\ndiff()\nDifferences\ndiff(x)\n\n\nwhich()\nIndices where TRUE\nwhich(x &gt; 5)\n\n\nwhich.max()\nIndex of max\nwhich.max(x)\n\n\nwhich.min()\nIndex of min\nwhich.min(x)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>40</span>  <span class='chapter-title'>R Command Reference</span>"
    ]
  },
  {
    "objectID": "chapters/A2-r-reference.html#sec-r-dataframes",
    "href": "chapters/A2-r-reference.html#sec-r-dataframes",
    "title": "40  R Command Reference",
    "section": "40.3 Data Frames",
    "text": "40.3 Data Frames\n\nCreating Data Frames\n\n\n\n\n\n\n\n\nFunction\nDescription\nExample\n\n\n\n\ndata.frame()\nCreate data frame\ndata.frame(x = 1:3, y = c(\"a\", \"b\", \"c\"))\n\n\ntibble()\nCreate tibble\ntibble(x = 1:3, y = c(\"a\", \"b\", \"c\"))\n\n\nas.data.frame()\nConvert to data frame\nas.data.frame(matrix)\n\n\nas_tibble()\nConvert to tibble\nas_tibble(df)\n\n\n\n\n\nData Frame Indexing\n\n\n\nSyntax\nDescription\nExample\n\n\n\n\ndf$col\nColumn by name\ndf$age\n\n\ndf[, \"col\"]\nColumn as vector\ndf[, \"age\"]\n\n\ndf[\"col\"]\nColumn as data frame\ndf[\"age\"]\n\n\ndf[i, ]\nRow by position\ndf[1, ]\n\n\ndf[i, j]\nElement\ndf[1, 2]\n\n\ndf[condition, ]\nFilter rows\ndf[df$age &gt; 25, ]\n\n\n\n\n\nData Frame Functions\n\n\n\nFunction\nDescription\nExample\n\n\n\n\nnrow()\nNumber of rows\nnrow(df)\n\n\nncol()\nNumber of columns\nncol(df)\n\n\ndim()\nDimensions\ndim(df)\n\n\nnames()\nColumn names\nnames(df)\n\n\ncolnames()\nColumn names\ncolnames(df)\n\n\nrownames()\nRow names\nrownames(df)\n\n\nhead()\nFirst rows\nhead(df, 10)\n\n\ntail()\nLast rows\ntail(df, 10)\n\n\nstr()\nStructure\nstr(df)\n\n\nsummary()\nSummary statistics\nsummary(df)\n\n\nglimpse()\nTidyverse structure\nglimpse(df)\n\n\nView()\nOpen viewer\nView(df)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>40</span>  <span class='chapter-title'>R Command Reference</span>"
    ]
  },
  {
    "objectID": "chapters/A2-r-reference.html#sec-r-io",
    "href": "chapters/A2-r-reference.html#sec-r-io",
    "title": "40  R Command Reference",
    "section": "40.4 Reading and Writing Data",
    "text": "40.4 Reading and Writing Data\n\nBase R I/O\n\n\n\n\n\n\n\n\nFunction\nDescription\nExample\n\n\n\n\nread.csv()\nRead CSV\nread.csv(\"file.csv\")\n\n\nread.table()\nRead table\nread.table(\"file.txt\", header = TRUE)\n\n\nread.delim()\nRead tab-delimited\nread.delim(\"file.tsv\")\n\n\nwrite.csv()\nWrite CSV\nwrite.csv(df, \"file.csv\", row.names = FALSE)\n\n\nwrite.table()\nWrite table\nwrite.table(df, \"file.txt\")\n\n\nsaveRDS()\nSave R object\nsaveRDS(obj, \"file.rds\")\n\n\nreadRDS()\nRead R object\nreadRDS(\"file.rds\")\n\n\nsave()\nSave multiple objects\nsave(x, y, file = \"data.RData\")\n\n\nload()\nLoad RData file\nload(\"data.RData\")\n\n\n\n\n\nTidyverse I/O (readr)\n\n\n\n\n\n\n\n\nFunction\nDescription\nExample\n\n\n\n\nread_csv()\nRead CSV (fast)\nread_csv(\"file.csv\")\n\n\nread_tsv()\nRead TSV\nread_tsv(\"file.tsv\")\n\n\nread_delim()\nRead with delimiter\nread_delim(\"file.txt\", delim = \"|\")\n\n\nread_fwf()\nRead fixed-width\nread_fwf(\"file.txt\", col_positions)\n\n\nwrite_csv()\nWrite CSV\nwrite_csv(df, \"file.csv\")\n\n\nwrite_tsv()\nWrite TSV\nwrite_tsv(df, \"file.tsv\")",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>40</span>  <span class='chapter-title'>R Command Reference</span>"
    ]
  },
  {
    "objectID": "chapters/A2-r-reference.html#sec-r-dplyr",
    "href": "chapters/A2-r-reference.html#sec-r-dplyr",
    "title": "40  R Command Reference",
    "section": "40.5 Tidyverse: dplyr",
    "text": "40.5 Tidyverse: dplyr\n\nCore Verbs\n\n\n\n\n\n\n\n\nFunction\nDescription\nExample\n\n\n\n\nfilter()\nFilter rows\nfilter(df, age &gt; 25)\n\n\nselect()\nSelect columns\nselect(df, name, age)\n\n\nmutate()\nCreate/modify columns\nmutate(df, age_sq = age^2)\n\n\narrange()\nSort rows\narrange(df, age)\n\n\nsummarise()\nSummarize data\nsummarise(df, mean_age = mean(age))\n\n\ngroup_by()\nGroup data\ngroup_by(df, category)\n\n\n\n\n\nSelection Helpers\n\n\n\n\n\n\n\n\nFunction\nDescription\nExample\n\n\n\n\nstarts_with()\nColumns starting with\nselect(df, starts_with(\"temp\"))\n\n\nends_with()\nColumns ending with\nselect(df, ends_with(\"_id\"))\n\n\ncontains()\nColumns containing\nselect(df, contains(\"score\"))\n\n\nmatches()\nColumns matching regex\nselect(df, matches(\"^x[0-9]\"))\n\n\neverything()\nAll columns\nselect(df, name, everything())\n\n\nwhere()\nColumns where condition\nselect(df, where(is.numeric))\n\n\nall_of()\nAll specified columns\nselect(df, all_of(col_names))\n\n\nany_of()\nAny of specified columns\nselect(df, any_of(col_names))\n\n\n\n\n\nAdditional dplyr Functions\n\n\n\n\n\n\n\n\nFunction\nDescription\nExample\n\n\n\n\ndistinct()\nUnique rows\ndistinct(df, category)\n\n\ncount()\nCount occurrences\ncount(df, category)\n\n\nslice()\nSelect rows by position\nslice(df, 1:10)\n\n\nslice_head()\nFirst n rows\nslice_head(df, n = 5)\n\n\nslice_tail()\nLast n rows\nslice_tail(df, n = 5)\n\n\nslice_sample()\nRandom rows\nslice_sample(df, n = 10)\n\n\nslice_max()\nRows with max values\nslice_max(df, age, n = 3)\n\n\nslice_min()\nRows with min values\nslice_min(df, age, n = 3)\n\n\npull()\nExtract column as vector\npull(df, name)\n\n\nrename()\nRename columns\nrename(df, new_name = old_name)\n\n\nrelocate()\nReorder columns\nrelocate(df, name, .before = age)\n\n\nacross()\nApply to multiple columns\nmutate(df, across(where(is.numeric), scale))\n\n\nrowwise()\nRow-wise operations\nrowwise(df)\n\n\nungroup()\nRemove grouping\nungroup(df)\n\n\nn()\nCount in group\nsummarise(df, count = n())\n\n\nn_distinct()\nCount unique values\nsummarise(df, unique = n_distinct(x))\n\n\nfirst()\nFirst value\nsummarise(df, first = first(x))\n\n\nlast()\nLast value\nsummarise(df, last = last(x))\n\n\nnth()\nNth value\nsummarise(df, third = nth(x, 3))\n\n\n\n\n\nJoins\n\n\n\n\n\n\n\n\nFunction\nDescription\nExample\n\n\n\n\nleft_join()\nKeep all left rows\nleft_join(df1, df2, by = \"id\")\n\n\nright_join()\nKeep all right rows\nright_join(df1, df2, by = \"id\")\n\n\ninner_join()\nKeep matching rows\ninner_join(df1, df2, by = \"id\")\n\n\nfull_join()\nKeep all rows\nfull_join(df1, df2, by = \"id\")\n\n\nsemi_join()\nFilter left by right\nsemi_join(df1, df2, by = \"id\")\n\n\nanti_join()\nFilter left, no match in right\nanti_join(df1, df2, by = \"id\")\n\n\nbind_rows()\nStack data frames\nbind_rows(df1, df2)\n\n\nbind_cols()\nCombine columns\nbind_cols(df1, df2)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>40</span>  <span class='chapter-title'>R Command Reference</span>"
    ]
  },
  {
    "objectID": "chapters/A2-r-reference.html#sec-r-tidyr",
    "href": "chapters/A2-r-reference.html#sec-r-tidyr",
    "title": "40  R Command Reference",
    "section": "40.6 Tidyverse: tidyr",
    "text": "40.6 Tidyverse: tidyr\n\n\n\n\n\n\n\n\nFunction\nDescription\nExample\n\n\n\n\npivot_longer()\nWide to long\npivot_longer(df, cols = -id, names_to = \"var\", values_to = \"val\")\n\n\npivot_wider()\nLong to wide\npivot_wider(df, names_from = var, values_from = val)\n\n\nseparate()\nSplit column\nseparate(df, col, into = c(\"a\", \"b\"), sep = \"_\")\n\n\nseparate_wider_delim()\nSplit by delimiter\nseparate_wider_delim(df, col, delim = \"_\", names = c(\"a\", \"b\"))\n\n\nunite()\nCombine columns\nunite(df, new_col, a, b, sep = \"_\")\n\n\ndrop_na()\nRemove NA rows\ndrop_na(df)\n\n\nfill()\nFill NA with previous\nfill(df, column, .direction = \"down\")\n\n\nreplace_na()\nReplace NA values\nreplace_na(df, list(x = 0))\n\n\ncomplete()\nComplete missing combinations\ncomplete(df, x, y)\n\n\nexpand()\nCreate all combinations\nexpand(df, x, y)\n\n\nnest()\nNest data\nnest(df, data = -group)\n\n\nunnest()\nUnnest data\nunnest(df, data)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>40</span>  <span class='chapter-title'>R Command Reference</span>"
    ]
  },
  {
    "objectID": "chapters/A2-r-reference.html#sec-r-ggplot",
    "href": "chapters/A2-r-reference.html#sec-r-ggplot",
    "title": "40  R Command Reference",
    "section": "40.7 Tidyverse: ggplot2",
    "text": "40.7 Tidyverse: ggplot2\n\nBasic Structure\nggplot(data, aes(x = x_var, y = y_var)) +\n  geom_*() +\n  labs() +\n  theme_*()\n\n\nGeometries\n\n\n\nFunction\nPlot Type\nUsage\n\n\n\n\ngeom_point()\nScatter plot\nContinuous x and y\n\n\ngeom_line()\nLine plot\nContinuous x and y\n\n\ngeom_smooth()\nSmoothed line\nAdd trend line\n\n\ngeom_bar()\nBar chart\nCounts\n\n\ngeom_col()\nBar chart\nValues\n\n\ngeom_histogram()\nHistogram\nDistribution\n\n\ngeom_density()\nDensity plot\nSmooth distribution\n\n\ngeom_boxplot()\nBox plot\nDistribution by group\n\n\ngeom_violin()\nViolin plot\nDistribution shape\n\n\ngeom_jitter()\nJittered points\nAvoid overplotting\n\n\ngeom_area()\nArea plot\nFilled area\n\n\ngeom_tile()\nHeatmap tiles\nGrid data\n\n\ngeom_text()\nText labels\nAdd text\n\n\ngeom_label()\nText with background\nLabeled points\n\n\ngeom_errorbar()\nError bars\nUncertainty\n\n\ngeom_hline()\nHorizontal line\nReference line\n\n\ngeom_vline()\nVertical line\nReference line\n\n\ngeom_abline()\nDiagonal line\ny = a + bx\n\n\n\n\n\nAesthetics\n\n\n\nAesthetic\nDescription\nExample\n\n\n\n\nx\nX-axis variable\naes(x = var)\n\n\ny\nY-axis variable\naes(y = var)\n\n\ncolor\nPoint/line color\naes(color = group)\n\n\nfill\nFill color\naes(fill = group)\n\n\nsize\nPoint/line size\naes(size = value)\n\n\nshape\nPoint shape\naes(shape = group)\n\n\nlinetype\nLine type\naes(linetype = group)\n\n\nalpha\nTransparency\naes(alpha = value)\n\n\ngroup\nGrouping\naes(group = id)\n\n\n\n\n\nScales and Labels\n\n\n\n\n\n\n\n\nFunction\nDescription\nExample\n\n\n\n\nlabs()\nAdd labels\nlabs(title = \"Title\", x = \"X\", y = \"Y\")\n\n\nscale_x_continuous()\nContinuous x scale\nscale_x_continuous(limits = c(0, 100))\n\n\nscale_y_continuous()\nContinuous y scale\nscale_y_continuous(breaks = seq(0, 10, 2))\n\n\nscale_x_log10()\nLog10 x scale\nscale_x_log10()\n\n\nscale_color_manual()\nManual colors\nscale_color_manual(values = c(\"red\", \"blue\"))\n\n\nscale_fill_brewer()\nColorBrewer palette\nscale_fill_brewer(palette = \"Set1\")\n\n\nscale_fill_viridis_d()\nViridis discrete\nscale_fill_viridis_d()\n\n\ncoord_flip()\nFlip coordinates\ncoord_flip()\n\n\ncoord_polar()\nPolar coordinates\ncoord_polar()\n\n\n\n\n\nFaceting\n\n\n\nFunction\nDescription\nExample\n\n\n\n\nfacet_wrap()\nWrap into panels\nfacet_wrap(~variable)\n\n\nfacet_grid()\nGrid of panels\nfacet_grid(rows ~ cols)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>40</span>  <span class='chapter-title'>R Command Reference</span>"
    ]
  },
  {
    "objectID": "chapters/A2-r-reference.html#sec-r-stats",
    "href": "chapters/A2-r-reference.html#sec-r-stats",
    "title": "40  R Command Reference",
    "section": "40.8 Statistical Functions",
    "text": "40.8 Statistical Functions\n\n\n\nFunction\nDescription\nExample\n\n\n\n\nt.test()\nT-test\nt.test(x, y)\n\n\ncor()\nCorrelation\ncor(x, y)\n\n\ncor.test()\nCorrelation test\ncor.test(x, y)\n\n\nlm()\nLinear model\nlm(y ~ x, data = df)\n\n\nglm()\nGeneralized linear model\nglm(y ~ x, family = binomial)\n\n\naov()\nANOVA\naov(y ~ group, data = df)\n\n\nchisq.test()\nChi-squared test\nchisq.test(table)\n\n\nwilcox.test()\nWilcoxon test\nwilcox.test(x, y)\n\n\nks.test()\nKolmogorov-Smirnov test\nks.test(x, y)\n\n\nsummary()\nModel summary\nsummary(model)\n\n\ncoef()\nModel coefficients\ncoef(model)\n\n\nresiduals()\nModel residuals\nresiduals(model)\n\n\npredict()\nPredictions\npredict(model, newdata)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>40</span>  <span class='chapter-title'>R Command Reference</span>"
    ]
  },
  {
    "objectID": "chapters/A2-r-reference.html#sec-r-stringr",
    "href": "chapters/A2-r-reference.html#sec-r-stringr",
    "title": "40  R Command Reference",
    "section": "40.9 String Functions (stringr)",
    "text": "40.9 String Functions (stringr)\n\n\n\n\n\n\n\n\nFunction\nDescription\nExample\n\n\n\n\nstr_length()\nString length\nstr_length(\"hello\")\n\n\nstr_sub()\nSubstring\nstr_sub(\"hello\", 1, 3)\n\n\nstr_c()\nConcatenate\nstr_c(\"a\", \"b\", sep = \"-\")\n\n\nstr_detect()\nDetect pattern\nstr_detect(x, \"pattern\")\n\n\nstr_replace()\nReplace first match\nstr_replace(x, \"old\", \"new\")\n\n\nstr_replace_all()\nReplace all matches\nstr_replace_all(x, \"old\", \"new\")\n\n\nstr_split()\nSplit string\nstr_split(x, \",\")\n\n\nstr_trim()\nRemove whitespace\nstr_trim(x)\n\n\nstr_to_lower()\nLowercase\nstr_to_lower(x)\n\n\nstr_to_upper()\nUppercase\nstr_to_upper(x)\n\n\nstr_to_title()\nTitle case\nstr_to_title(x)\n\n\nstr_extract()\nExtract match\nstr_extract(x, \"[0-9]+\")\n\n\nstr_count()\nCount matches\nstr_count(x, \"a\")",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>40</span>  <span class='chapter-title'>R Command Reference</span>"
    ]
  },
  {
    "objectID": "chapters/A2-r-reference.html#sec-r-control",
    "href": "chapters/A2-r-reference.html#sec-r-control",
    "title": "40  R Command Reference",
    "section": "40.10 Control Flow",
    "text": "40.10 Control Flow\n\nConditionals\n# if-else\nif (condition) {\n  # code if TRUE\n} else if (other_condition) {\n  # code if other TRUE\n} else {\n  # code if all FALSE\n}\n\n# Vectorized if-else\nifelse(condition, value_if_true, value_if_false)\n\n# dplyr case_when\ncase_when(\n  condition1 ~ value1,\n  condition2 ~ value2,\n  TRUE ~ default_value\n)\n\n\nLoops\n# for loop\nfor (i in 1:10) {\n  print(i)\n}\n\n# while loop\nwhile (condition) {\n  # code\n}\n\n# Apply functions (preferred)\nlapply(list, function)   # Returns list\nsapply(list, function)   # Returns vector\nmap(list, function)      # purrr, returns list\nmap_dbl(list, function)  # purrr, returns double vector",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>40</span>  <span class='chapter-title'>R Command Reference</span>"
    ]
  },
  {
    "objectID": "chapters/A2-r-reference.html#sec-r-packages",
    "href": "chapters/A2-r-reference.html#sec-r-packages",
    "title": "40  R Command Reference",
    "section": "40.11 Package Management",
    "text": "40.11 Package Management\n\n\n\n\n\n\n\n\nFunction\nDescription\nExample\n\n\n\n\ninstall.packages()\nInstall from CRAN\ninstall.packages(\"dplyr\")\n\n\nlibrary()\nLoad package\nlibrary(dplyr)\n\n\nrequire()\nLoad (returns TRUE/FALSE)\nrequire(dplyr)\n\n\ninstalled.packages()\nList installed\ninstalled.packages()\n\n\nupdate.packages()\nUpdate all\nupdate.packages()\n\n\nremove.packages()\nRemove package\nremove.packages(\"dplyr\")\n\n\npackageVersion()\nPackage version\npackageVersion(\"dplyr\")",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>40</span>  <span class='chapter-title'>R Command Reference</span>"
    ]
  },
  {
    "objectID": "chapters/A9-quarto-reference.html",
    "href": "chapters/A9-quarto-reference.html",
    "title": "41  Quarto Markdown Reference",
    "section": "",
    "text": "41.1 Introduction to Quarto\nThis appendix provides a comprehensive quick reference for Quarto Markdown syntax and features used throughout this book.\nQuarto is an open-source scientific and technical publishing system built on Pandoc. It allows you to:\nQuarto documents use the .qmd file extension and combine Markdown for text, code chunks for analysis, and YAML for configuration.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>41</span>  <span class='chapter-title'>Quarto Markdown Reference</span>"
    ]
  },
  {
    "objectID": "chapters/A9-quarto-reference.html#sec-quarto-intro",
    "href": "chapters/A9-quarto-reference.html#sec-quarto-intro",
    "title": "41  Quarto Markdown Reference",
    "section": "",
    "text": "Combine narrative text with executable code (R, Python, Julia, Observable)\nCreate reproducible documents in multiple formats (HTML, PDF, Word, EPUB)\nUse consistent syntax across different programming languages\nGenerate professional scientific documents with cross-references, citations, and equations",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>41</span>  <span class='chapter-title'>Quarto Markdown Reference</span>"
    ]
  },
  {
    "objectID": "chapters/A9-quarto-reference.html#sec-markdown-basics",
    "href": "chapters/A9-quarto-reference.html#sec-markdown-basics",
    "title": "41  Quarto Markdown Reference",
    "section": "41.2 Basic Markdown Syntax",
    "text": "41.2 Basic Markdown Syntax\n\nHeaders\n\n\n\nSyntax\nResult\nUsage\n\n\n\n\n# Header 1\nLargest heading\nChapter/section title\n\n\n## Header 2\nSecond level\nMajor section\n\n\n### Header 3\nThird level\nSubsection\n\n\n#### Header 4\nFourth level\nSub-subsection\n\n\n##### Header 5\nFifth level\nRarely used\n\n\n###### Header 6\nSmallest heading\nRarely used\n\n\n\nHeaders can include section labels for cross-referencing:\n## Methods {#sec-methods}\n\n\nText Formatting\n\n\n\nSyntax\nResult\nDescription\n\n\n\n\n*italic* or _italic_\nitalic\nEmphasized text\n\n\n**bold** or __bold__\nbold\nStrong emphasis\n\n\n***bold italic***\nbold italic\nBoth bold and italic\n\n\n~~strikethrough~~\nstrikethrough\nDeleted text\n\n\nsuperscript^2^\nsuperscript2\nSuperscript\n\n\nsubscript~2~\nsubscript2\nSubscript\n\n\n`code`\ncode\nInline code\n\n\n\n\n\nLists\nUnordered Lists:\n- Item 1\n- Item 2\n  - Nested item 2a\n  - Nested item 2b\n- Item 3\nOrdered Lists:\n1. First item\n2. Second item\n   a. Nested item\n   b. Another nested item\n3. Third item\nTask Lists:\n- [ ] Incomplete task\n- [x] Completed task\n\n\nLinks and Images\n\n\n\nSyntax\nResult\nDescription\n\n\n\n\n[text](url)\ntext\nHyperlink\n\n\n[text](url \"title\")\ntext\nLink with title\n\n\n&lt;url&gt;\n\nAutomatic link\n\n\n![alt text](image.png)\nImage\nEmbedded image\n\n\n![alt text](image.png \"caption\")\nImage\nImage with title\n\n\n\nExample:\nVisit the [Quarto website](https://quarto.org) for documentation.\n\n![Histogram of data](figures/histogram.png)\n\n\nBlockquotes\n&gt; This is a blockquote.\n&gt; It can span multiple lines.\n&gt;\n&gt; And multiple paragraphs.\nResult: &gt; This is a blockquote. &gt; It can span multiple lines.\n\n\nCode\nInline Code:\nUse the `mean()` function to calculate averages.\nCode Blocks:\n```\nPlain code block without syntax highlighting\n```\n\n```r\n# R code with syntax highlighting\nx &lt;- c(1, 2, 3, 4, 5)\nmean(x)\n```\n\n```python\n# Python code with syntax highlighting\nimport numpy as np\nx = np.array([1, 2, 3, 4, 5])\nnp.mean(x)\n```",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>41</span>  <span class='chapter-title'>Quarto Markdown Reference</span>"
    ]
  },
  {
    "objectID": "chapters/A9-quarto-reference.html#sec-quarto-features",
    "href": "chapters/A9-quarto-reference.html#sec-quarto-features",
    "title": "41  Quarto Markdown Reference",
    "section": "41.10 Quarto-Specific Features",
    "text": "41.10 Quarto-Specific Features\n\nYAML Header\nEvery Quarto document begins with a YAML header enclosed in ---:\n---\ntitle: \"My Analysis\"\nauthor: \"Your Name\"\ndate: \"2024-01-15\"\nformat:\n  html:\n    toc: true\n    code-fold: true\n    theme: cosmo\n  pdf:\n    documentclass: article\n    geometry: margin=1in\nexecute:\n  echo: true\n  warning: false\n  message: false\nbibliography: references.bib\n---\nCommon YAML Options:\n\n\n\nOption\nDescription\nExample\n\n\n\n\ntitle\nDocument title\n\"My Report\"\n\n\nauthor\nAuthor name(s)\n\"Jane Doe\"\n\n\ndate\nDocument date\ntoday or \"2024-01-15\"\n\n\nformat\nOutput format(s)\nhtml, pdf, docx\n\n\ntoc\nTable of contents\ntrue or false\n\n\nnumber-sections\nNumber sections\ntrue or false\n\n\nbibliography\nBibliography file\nreferences.bib\n\n\nexecute\nGlobal execution options\nSee below\n\n\n\n\n\nCode Chunk Options\nCode chunks in Quarto use #| (hashpipe) notation for options:\nquarto-executable-code-5450563D\n\n```r\n#| label: fig-scatter\n#| fig-cap: \"Relationship between variables\"\n#| echo: false\n#| warning: false\n#| fig-width: 6\n#| fig-height: 4\n\nplot(mtcars$mpg, mtcars$hp)\n```\nCommon Chunk Options:\n\n\n\nOption\nDescription\nValues\n\n\n\n\nlabel\nUnique chunk identifier\nfig-name, tbl-name\n\n\necho\nShow code\ntrue, false\n\n\neval\nExecute code\ntrue, false\n\n\ninclude\nInclude output\ntrue, false\n\n\nwarning\nShow warnings\ntrue, false\n\n\nmessage\nShow messages\ntrue, false\n\n\nerror\nShow errors\ntrue, false\n\n\nfig-cap\nFigure caption\n\"Caption text\"\n\n\nfig-width\nFigure width (inches)\n6, 8, etc.\n\n\nfig-height\nFigure height (inches)\n4, 6, etc.\n\n\ntbl-cap\nTable caption\n\"Caption text\"\n\n\noutput\nControl output\ntrue, false, asis\n\n\n\n\n\nCross-References\nQuarto provides automatic numbering and cross-referencing:\nFigures:\nquarto-executable-code-5450563D\n\n```r\n#| label: fig-histogram\n#| fig-cap: \"Distribution of sample data\"\n\nhist(rnorm(100))\n```\n\nSee @fig-histogram for the distribution.\nTables:\nquarto-executable-code-5450563D\n\n```r\n#| label: tbl-summary\n#| tbl-cap: \"Summary statistics\"\n\nknitr::kable(summary(mtcars))\n```\n\nResults are shown in @tbl-summary.\nSections:\n## Methods {#sec-methods}\n\nAs described in @sec-methods, we used...\nEquations:\n$$\nE = mc^2\n$$ {#eq-einstein}\n\nAccording to @eq-einstein, energy equals...\nCross-Reference Prefixes:\n\n\n\nType\nPrefix\nExample\n\n\n\n\nFigure\nfig-\n@fig-scatter\n\n\nTable\ntbl-\n@tbl-results\n\n\nSection\nsec-\n@sec-intro\n\n\nEquation\neq-\n@eq-model\n\n\nTheorem\nthm-\n@thm-central-limit\n\n\nLemma\nlem-\n@lem-property\n\n\nProof\nprp-\n@prp-main\n\n\n\n\n\nCitations\nReference bibliography entries using @key:\nAccording to @fisher1925 and @student1908, the t-test...\n\nMultiple citations [@fisher1925; @student1908].\n\nCitation with page number [@fisher1925, p. 42].\n\nCitation with prefix [see @fisher1925, p. 42].\nBibliography File (BibTeX):\n@article{fisher1925,\n  author = {Fisher, R. A.},\n  title = {Statistical Methods for Research Workers},\n  journal = {Oliver and Boyd},\n  year = {1925}\n}\n\n\nCallout Blocks\nCallout blocks highlight important information:\nNote:\n:::{.callout-note}\nThis is a note callout block.\n:::\nTip:\n:::{.callout-tip}\n## Pro Tip\nThis is a helpful tip for users.\n:::\nWarning:\n:::{.callout-warning}\nBe careful with this approach!\n:::\nImportant:\n:::{.callout-important}\nThis is crucial information.\n:::\nCaution:\n:::{.callout-caution}\nProceed with caution.\n:::\nCallout Types:\n\n\n\nType\nIcon\nColor\nUsage\n\n\n\n\nnote\nInfo\nBlue\nGeneral information\n\n\ntip\nLightbulb\nGreen\nHelpful suggestions\n\n\nwarning\nWarning\nOrange\nPotential issues\n\n\nimportant\nExclamation\nRed\nCritical information\n\n\ncaution\nFire\nRed\nDangerous operations\n\n\n\nCollapsible Callouts:\n:::{.callout-note collapse=\"true\"}\n## Expandable Note\nThis content is hidden by default.\n:::\n\n\nTabsets\nOrganize content into tabs:\n::: {.panel-tabset}\n\n## R Code\nquarto-executable-code-5450563D\n\n```r\nmean(c(1, 2, 3, 4, 5))",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>41</span>  <span class='chapter-title'>Quarto Markdown Reference</span>"
    ]
  },
  {
    "objectID": "chapters/A9-quarto-reference.html#base-r-alternative",
    "href": "chapters/A9-quarto-reference.html#base-r-alternative",
    "title": "41  Quarto Markdown Reference",
    "section": "41.4 Base R Alternative",
    "text": "41.4 Base R Alternative\n\n\nCode\nsum(c(1, 2, 3, 4, 5)) / length(c(1, 2, 3, 4, 5))\n\n\n[1] 3\n\n\n:::\n\n### Equations {#sec-equations}\n\n**Inline Math:**\n```markdown\nThe mean is calculated as $\\bar{x} = \\frac{1}{n}\\sum_{i=1}^{n}x_i$.\nDisplay Math:\n$$\n\\sigma^2 = \\frac{1}{n-1}\\sum_{i=1}^{n}(x_i - \\bar{x})^2\n$$\nNumbered Equations:\n$$\nf(x) = \\frac{1}{\\sigma\\sqrt{2\\pi}} e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}\n$$ {#eq-normal}\n\nThe normal distribution (@eq-normal) is...\nAligned Equations:\n$$\n\\begin{aligned}\n\\bar{x} &= \\frac{1}{n}\\sum_{i=1}^{n}x_i \\\\\n&= \\frac{x_1 + x_2 + \\cdots + x_n}{n}\n\\end{aligned}\n$$ {#eq-mean}",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>41</span>  <span class='chapter-title'>Quarto Markdown Reference</span>"
    ]
  },
  {
    "objectID": "chapters/A9-quarto-reference.html#sec-figures-tables",
    "href": "chapters/A9-quarto-reference.html#sec-figures-tables",
    "title": "41  Quarto Markdown Reference",
    "section": "41.12 Figures and Tables",
    "text": "41.12 Figures and Tables\n\nFigure Syntax\nBasic Figure:\nquarto-executable-code-5450563D\n\n```r\n#| label: fig-plot\n#| fig-cap: \"Scatter plot of mpg vs hp\"\n\nplot(mtcars$mpg, mtcars$hp,\n     xlab = \"Miles per Gallon\",\n     ylab = \"Horsepower\")\n```\nMultiple Figures (Subfigures):\nquarto-executable-code-5450563D\n\n```r\n#| label: fig-multiple\n#| fig-cap: \"Comparison of distributions\"\n#| fig-subcap:\n#|   - \"Normal distribution\"\n#|   - \"Exponential distribution\"\n#| layout-ncol: 2\n\nhist(rnorm(1000))\nhist(rexp(1000))\n```\n\nSee @fig-multiple-1 and @fig-multiple-2.\nMarkdown Images with Labels:\n![Experimental setup](images/setup.png){#fig-setup}\n\nThe apparatus shown in @fig-setup was used...\n\n\nFigure Size and Alignment\n\n\n\nOption\nDescription\nExample\n\n\n\n\nfig-width\nWidth in inches\nfig-width: 6\n\n\nfig-height\nHeight in inches\nfig-height: 4\n\n\nfig-align\nAlignment\nleft, center, right\n\n\nout-width\nOutput width (%)\nout-width: \"80%\"\n\n\nfig-dpi\nResolution\nfig-dpi: 300\n\n\n\nExample:\nquarto-executable-code-5450563D\n\n```r\n#| fig-width: 8\n#| fig-height: 5\n#| fig-align: center\n#| out-width: \"90%\"\n\nplot(pressure)\n```\n\n\nTable Syntax\nMarkdown Tables (Pipe Tables):\n| Column 1 | Column 2 | Column 3 |\n|:---------|:--------:|---------:|\n| Left     | Center   | Right    |\n| aligned  | aligned  | aligned  |\n| text     | text     | text     |\nTable Alignment: - :--- = Left aligned - :--: = Center aligned - ---: = Right aligned\nGrid Tables:\n+----------+----------+----------+\n| Column 1 | Column 2 | Column 3 |\n+==========+==========+==========+\n| Row 1    | Data     | Values   |\n+----------+----------+----------+\n| Row 2    | More     | Content  |\n+----------+----------+----------+\nCode-Generated Tables:\nquarto-executable-code-5450563D\n\n```r\n#| label: tbl-mtcars\n#| tbl-cap: \"Summary of mtcars dataset\"\n\nknitr::kable(head(mtcars[, 1:4]))\n```\nCaption and Label:\n: Summary statistics for experimental groups {#tbl-summary}\n\n| Group | Mean | SD   | N  |\n|:------|-----:|-----:|---:|\n| A     | 5.2  | 1.1  | 20 |\n| B     | 6.8  | 1.4  | 20 |",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>41</span>  <span class='chapter-title'>Quarto Markdown Reference</span>"
    ]
  },
  {
    "objectID": "chapters/A9-quarto-reference.html#sec-output-options",
    "href": "chapters/A9-quarto-reference.html#sec-output-options",
    "title": "41  Quarto Markdown Reference",
    "section": "41.13 Document Output Options",
    "text": "41.13 Document Output Options\n\nHTML Output\nformat:\n  html:\n    toc: true\n    toc-depth: 3\n    toc-location: left\n    number-sections: true\n    code-fold: true\n    code-tools: true\n    code-copy: true\n    theme: cosmo\n    highlight-style: github\n    embed-resources: false\n    fig-width: 7\n    fig-height: 5\nCommon HTML Options:\n\n\n\nOption\nDescription\nValues\n\n\n\n\ntoc\nTable of contents\ntrue, false\n\n\ntoc-depth\nTOC depth\n1, 2, 3, etc.\n\n\ntoc-location\nTOC position\nleft, right, body\n\n\ncode-fold\nFoldable code\ntrue, false, show\n\n\ncode-tools\nCode tools menu\ntrue, false\n\n\ntheme\nVisual theme\ndefault, cosmo, flatly, etc.\n\n\nembed-resources\nSelf-contained\ntrue, false\n\n\n\n\n\nPDF Output\nformat:\n  pdf:\n    documentclass: article\n    geometry:\n      - margin=1in\n      - letterpaper\n    number-sections: true\n    colorlinks: true\n    toc: true\n    toc-depth: 2\n    keep-tex: false\nCommon PDF Options:\n\n\n\nOption\nDescription\nValues\n\n\n\n\ndocumentclass\nLaTeX class\narticle, report, book\n\n\ngeometry\nPage layout\nmargin=1in, etc.\n\n\nfontsize\nText size\n10pt, 11pt, 12pt\n\n\ncolorlinks\nColored links\ntrue, false\n\n\nkeep-tex\nKeep .tex file\ntrue, false\n\n\n\n\n\nWord Output\nformat:\n  docx:\n    toc: true\n    number-sections: true\n    highlight-style: github\n    reference-doc: custom-template.docx\n\n\nMultiple Formats\nformat:\n  html:\n    toc: true\n    theme: cosmo\n  pdf:\n    documentclass: article\n  docx:\n    toc: true\nRender specific format:\nquarto render document.qmd --to html\nquarto render document.qmd --to pdf",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>41</span>  <span class='chapter-title'>Quarto Markdown Reference</span>"
    ]
  },
  {
    "objectID": "chapters/A9-quarto-reference.html#sec-quarto-commands",
    "href": "chapters/A9-quarto-reference.html#sec-quarto-commands",
    "title": "41  Quarto Markdown Reference",
    "section": "41.14 Common Quarto Commands",
    "text": "41.14 Common Quarto Commands\n\nBasic Commands\n\n\n\nCommand\nDescription\n\n\n\n\nquarto preview file.qmd\nLive preview in browser\n\n\nquarto render file.qmd\nRender to default format\n\n\nquarto render file.qmd --to html\nRender to HTML\n\n\nquarto render file.qmd --to pdf\nRender to PDF\n\n\nquarto render file.qmd --to docx\nRender to Word\n\n\nquarto render\nRender entire project\n\n\nquarto publish\nPublish document online\n\n\n\n\n\nProject Commands\n\n\n\nCommand\nDescription\n\n\n\n\nquarto create project\nCreate new project\n\n\nquarto preview\nPreview project\n\n\nquarto render\nRender all project files\n\n\nquarto check\nCheck installation\n\n\nquarto tools install\nInstall required tools\n\n\n\n\n\nPreview Options\n# Preview on specific port\nquarto preview document.qmd --port 8080\n\n# Preview without opening browser\nquarto preview document.qmd --no-browser\n\n# Preview specific format\nquarto preview document.qmd --to pdf\n\n\nRender Options\n# Render with parameters\nquarto render document.qmd -P alpha:0.05 -P n:100\n\n# Render to specific output file\nquarto render document.qmd --output report.html\n\n# Render all formats\nquarto render document.qmd --to all\n\n# Execute code even if frozen\nquarto render document.qmd --execute",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>41</span>  <span class='chapter-title'>Quarto Markdown Reference</span>"
    ]
  },
  {
    "objectID": "chapters/A9-quarto-reference.html#sec-quarto-shortcuts",
    "href": "chapters/A9-quarto-reference.html#sec-quarto-shortcuts",
    "title": "41  Quarto Markdown Reference",
    "section": "41.15 Keyboard Shortcuts",
    "text": "41.15 Keyboard Shortcuts\nWhen using Quarto in RStudio or VS Code:\n\n\n\nShortcut\nAction\nPlatform\n\n\n\n\nCmd/Ctrl + Shift + K\nRender document\nAll\n\n\nCmd/Ctrl + Shift + Enter\nRun current chunk\nAll\n\n\nCmd/Ctrl + Alt + I\nInsert code chunk\nRStudio\n\n\nCmd/Ctrl + Alt + P\nRun all chunks above\nRStudio",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>41</span>  <span class='chapter-title'>Quarto Markdown Reference</span>"
    ]
  },
  {
    "objectID": "chapters/A9-quarto-reference.html#sec-quick-summary",
    "href": "chapters/A9-quarto-reference.html#sec-quick-summary",
    "title": "41  Quarto Markdown Reference",
    "section": "41.16 Quick Reference Summary",
    "text": "41.16 Quick Reference Summary\nDocument Structure:\n---\ntitle: \"Title\"\nformat: html\n---\n\n# Section\n\nText with **bold** and *italic*.\n\nquarto-executable-code-5450563D\n\n```r\n#| label: fig-name\n#| fig-cap: \"Caption\"\ncode here\nSee ?fig-name and ?sec-other.\n\n**Essential Syntax:**\n- Headers: `#`, `##`, `###`\n- Bold: `**text**`\n- Italic: `*text*`\n- Code: `` `code` ``\n- Link: `[text](url)`\n- Image: `![caption](file.png)`\n- Equation: `$math$` or `$$display$$`\n- Citation: `@reference`\n- Cross-ref: `@fig-name`, `@tbl-name`, `@sec-name`\n\n**Key Features:**\n- Use YAML for configuration\n- Use `#|` for chunk options\n- Label figures/tables/sections with `{#prefix-name}`\n- Reference with `@prefix-name`\n- Use callouts for important info\n- Organize with tabsets and divs\n:::",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>41</span>  <span class='chapter-title'>Quarto Markdown Reference</span>"
    ]
  },
  {
    "objectID": "chapters/A9-quarto-reference.html#base-r-alternative-1",
    "href": "chapters/A9-quarto-reference.html#base-r-alternative-1",
    "title": "41  Quarto Markdown Reference",
    "section": "41.11 Base R Alternative",
    "text": "41.11 Base R Alternative\nquarto-executable-code-5450563D\nsum(c(1, 2, 3, 4, 5)) / length(c(1, 2, 3, 4, 5))\n:::\n\n### Equations {#sec-equations}\n\n**Inline Math:**\n```markdown\nThe mean is calculated as $\\bar{x} = \\frac{1}{n}\\sum_{i=1}^{n}x_i$.\nDisplay Math:\n$$\n\\sigma^2 = \\frac{1}{n-1}\\sum_{i=1}^{n}(x_i - \\bar{x})^2\n$$\nNumbered Equations:\n$$\nf(x) = \\frac{1}{\\sigma\\sqrt{2\\pi}} e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}\n$$ {#eq-normal}\n\nThe normal distribution (@eq-normal) is...\nAligned Equations:\n$$\n\\begin{aligned}\n\\bar{x} &= \\frac{1}{n}\\sum_{i=1}^{n}x_i \\\\\n&= \\frac{x_1 + x_2 + \\cdots + x_n}{n}\n\\end{aligned}\n$$ {#eq-mean}",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>41</span>  <span class='chapter-title'>Quarto Markdown Reference</span>"
    ]
  },
  {
    "objectID": "chapters/A7-latex-reference.html",
    "href": "chapters/A7-latex-reference.html",
    "title": "42  LaTeX Command Reference",
    "section": "",
    "text": "42.1 Greek Letters\nThis appendix provides a comprehensive reference for LaTeX mathematical notation commands commonly used in Quarto and R Markdown documents.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>42</span>  <span class='chapter-title'>LaTeX Command Reference</span>"
    ]
  },
  {
    "objectID": "chapters/A7-latex-reference.html#sec-latex-greek",
    "href": "chapters/A7-latex-reference.html#sec-latex-greek",
    "title": "42  LaTeX Command Reference",
    "section": "",
    "text": "Lowercase Greek Letters\n\n\n\n\n\n\n\n\n\n\n\nSymbol\nCommand\nSymbol\nCommand\nSymbol\nCommand\n\n\n\n\n\\(\\alpha\\)\n\\alpha\n\\(\\iota\\)\n\\iota\n\\(\\rho\\)\n\\rho\n\n\n\\(\\beta\\)\n\\beta\n\\(\\kappa\\)\n\\kappa\n\\(\\sigma\\)\n\\sigma\n\n\n\\(\\gamma\\)\n\\gamma\n\\(\\lambda\\)\n\\lambda\n\\(\\tau\\)\n\\tau\n\n\n\\(\\delta\\)\n\\delta\n\\(\\mu\\)\n\\mu\n\\(\\upsilon\\)\n\\upsilon\n\n\n\\(\\epsilon\\)\n\\epsilon\n\\(\\nu\\)\n\\nu\n\\(\\phi\\)\n\\phi\n\n\n\\(\\varepsilon\\)\n\\varepsilon\n\\(\\xi\\)\n\\xi\n\\(\\varphi\\)\n\\varphi\n\n\n\\(\\zeta\\)\n\\zeta\n\\(\\pi\\)\n\\pi\n\\(\\chi\\)\n\\chi\n\n\n\\(\\eta\\)\n\\eta\n\\(\\varpi\\)\n\\varpi\n\\(\\psi\\)\n\\psi\n\n\n\\(\\theta\\)\n\\theta\n\\(\\omega\\)\n\\omega\n\\(\\varsigma\\)\n\\varsigma\n\n\n\\(\\vartheta\\)\n\\vartheta\n\n\n\n\n\n\n\n\n\nUppercase Greek Letters\n\n\n\nSymbol\nCommand\nSymbol\nCommand\nSymbol\nCommand\n\n\n\n\n\\(\\Gamma\\)\n\\Gamma\n\\(\\Xi\\)\n\\Xi\n\\(\\Phi\\)\n\\Phi\n\n\n\\(\\Delta\\)\n\\Delta\n\\(\\Pi\\)\n\\Pi\n\\(\\Psi\\)\n\\Psi\n\n\n\\(\\Theta\\)\n\\Theta\n\\(\\Sigma\\)\n\\Sigma\n\\(\\Omega\\)\n\\Omega\n\n\n\\(\\Lambda\\)\n\\Lambda\n\\(\\Upsilon\\)\n\\Upsilon",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>42</span>  <span class='chapter-title'>LaTeX Command Reference</span>"
    ]
  },
  {
    "objectID": "chapters/A7-latex-reference.html#sec-latex-arithmetic",
    "href": "chapters/A7-latex-reference.html#sec-latex-arithmetic",
    "title": "42  LaTeX Command Reference",
    "section": "42.2 Arithmetic Operations",
    "text": "42.2 Arithmetic Operations\n\n\n\nSymbol\nCommand\nDescription\nExample\n\n\n\n\n\\(+\\)\n+\nAddition\n$a + b$\n\n\n\\(-\\)\n-\nSubtraction\n$a - b$\n\n\n\\(\\times\\)\n\\times\nMultiplication\n$a \\times b$\n\n\n\\(\\cdot\\)\n\\cdot\nCentered dot\n$a \\cdot b$\n\n\n\\(\\div\\)\n\\div\nDivision\n$a \\div b$\n\n\n\\(\\frac{a}{b}\\)\n\\frac{a}{b}\nFraction\n$\\frac{x}{y}$\n\n\n\\(\\pm\\)\n\\pm\nPlus or minus\n$\\pm 5$\n\n\n\\(\\mp\\)\n\\mp\nMinus or plus\n$\\mp 5$",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>42</span>  <span class='chapter-title'>LaTeX Command Reference</span>"
    ]
  },
  {
    "objectID": "chapters/A7-latex-reference.html#sec-latex-powers",
    "href": "chapters/A7-latex-reference.html#sec-latex-powers",
    "title": "42  LaTeX Command Reference",
    "section": "42.3 Powers and Indices",
    "text": "42.3 Powers and Indices\n\n\n\nSymbol\nCommand\nDescription\nExample\n\n\n\n\n\\(x^n\\)\nx^n\nSuperscript\n$x^2$\n\n\n\\(x_n\\)\nx_n\nSubscript\n$x_1$\n\n\n\\(x^{n+1}\\)\nx^{n+1}\nMulti-char superscript\n$x^{ab}$\n\n\n\\(x_{i,j}\\)\nx_{i,j}\nMulti-char subscript\n$x_{ij}$\n\n\n\\(x_n^2\\)\nx_n^2\nCombined\n$x_i^2$\n\n\n\\(\\sqrt{x}\\)\n\\sqrt{x}\nSquare root\n$\\sqrt{16}$\n\n\n\\(\\sqrt[n]{x}\\)\n\\sqrt[n]{x}\nnth root\n$\\sqrt[3]{8}$",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>42</span>  <span class='chapter-title'>LaTeX Command Reference</span>"
    ]
  },
  {
    "objectID": "chapters/A7-latex-reference.html#sec-latex-relations",
    "href": "chapters/A7-latex-reference.html#sec-latex-relations",
    "title": "42  LaTeX Command Reference",
    "section": "42.4 Relation Operators",
    "text": "42.4 Relation Operators\n\n\n\nSymbol\nCommand\nDescription\n\n\n\n\n\\(=\\)\n=\nEqual to\n\n\n\\(\\neq\\)\n\\neq\nNot equal to\n\n\n\\(&lt;\\)\n&lt;\nLess than\n\n\n\\(&gt;\\)\n&gt;\nGreater than\n\n\n\\(\\leq\\)\n\\leq\nLess than or equal\n\n\n\\(\\geq\\)\n\\geq\nGreater than or equal\n\n\n\\(\\ll\\)\n\\ll\nMuch less than\n\n\n\\(\\gg\\)\n\\gg\nMuch greater than\n\n\n\\(\\approx\\)\n\\approx\nApproximately equal\n\n\n\\(\\sim\\)\n\\sim\nSimilar to / distributed as\n\n\n\\(\\simeq\\)\n\\simeq\nSimilar or equal\n\n\n\\(\\cong\\)\n\\cong\nCongruent\n\n\n\\(\\equiv\\)\n\\equiv\nEquivalent\n\n\n\\(\\propto\\)\n\\propto\nProportional to\n\n\n\\(\\doteq\\)\n\\doteq\nApproaches the limit",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>42</span>  <span class='chapter-title'>LaTeX Command Reference</span>"
    ]
  },
  {
    "objectID": "chapters/A7-latex-reference.html#sec-latex-sets",
    "href": "chapters/A7-latex-reference.html#sec-latex-sets",
    "title": "42  LaTeX Command Reference",
    "section": "42.5 Set Theory",
    "text": "42.5 Set Theory\n\n\n\nSymbol\nCommand\nDescription\n\n\n\n\n\\(\\in\\)\n\\in\nElement of\n\n\n\\(\\notin\\)\n\\notin\nNot element of\n\n\n\\(\\ni\\)\n\\ni\nContains as element\n\n\n\\(\\subset\\)\n\\subset\nProper subset\n\n\n\\(\\subseteq\\)\n\\subseteq\nSubset or equal\n\n\n\\(\\supset\\)\n\\supset\nProper superset\n\n\n\\(\\supseteq\\)\n\\supseteq\nSuperset or equal\n\n\n\\(\\cap\\)\n\\cap\nIntersection\n\n\n\\(\\cup\\)\n\\cup\nUnion\n\n\n\\(\\setminus\\)\n\\setminus\nSet difference\n\n\n\\(\\emptyset\\)\n\\emptyset\nEmpty set\n\n\n\\(\\varnothing\\)\n\\varnothing\nEmpty set (variant)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>42</span>  <span class='chapter-title'>LaTeX Command Reference</span>"
    ]
  },
  {
    "objectID": "chapters/A7-latex-reference.html#sec-latex-logic",
    "href": "chapters/A7-latex-reference.html#sec-latex-logic",
    "title": "42  LaTeX Command Reference",
    "section": "42.6 Logic Symbols",
    "text": "42.6 Logic Symbols\n\n\n\nSymbol\nCommand\nDescription\n\n\n\n\n\\(\\land\\)\n\\land\nLogical AND\n\n\n\\(\\lor\\)\n\\lor\nLogical OR\n\n\n\\(\\neg\\)\n\\neg\nLogical NOT\n\n\n\\(\\forall\\)\n\\forall\nFor all\n\n\n\\(\\exists\\)\n\\exists\nThere exists\n\n\n\\(\\nexists\\)\n\\nexists\nThere does not exist\n\n\n\\(\\Rightarrow\\)\n\\Rightarrow\nImplies\n\n\n\\(\\Leftarrow\\)\n\\Leftarrow\nIs implied by\n\n\n\\(\\Leftrightarrow\\)\n\\Leftrightarrow\nIf and only if\n\n\n\\(\\therefore\\)\n\\therefore\nTherefore\n\n\n\\(\\because\\)\n\\because\nBecause",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>42</span>  <span class='chapter-title'>LaTeX Command Reference</span>"
    ]
  },
  {
    "objectID": "chapters/A7-latex-reference.html#sec-latex-arrows",
    "href": "chapters/A7-latex-reference.html#sec-latex-arrows",
    "title": "42  LaTeX Command Reference",
    "section": "42.7 Arrows",
    "text": "42.7 Arrows\n\n\n\n\n\n\n\n\n\nSymbol\nCommand\nSymbol\nCommand\n\n\n\n\n\\(\\rightarrow\\)\n\\rightarrow\n\\(\\leftarrow\\)\n\\leftarrow\n\n\n\\(\\Rightarrow\\)\n\\Rightarrow\n\\(\\Leftarrow\\)\n\\Leftarrow\n\n\n\\(\\leftrightarrow\\)\n\\leftrightarrow\n\\(\\Leftrightarrow\\)\n\\Leftrightarrow\n\n\n\\(\\longrightarrow\\)\n\\longrightarrow\n\\(\\longleftarrow\\)\n\\longleftarrow\n\n\n\\(\\uparrow\\)\n\\uparrow\n\\(\\downarrow\\)\n\\downarrow\n\n\n\\(\\Uparrow\\)\n\\Uparrow\n\\(\\Downarrow\\)\n\\Downarrow\n\n\n\\(\\nearrow\\)\n\\nearrow\n\\(\\searrow\\)\n\\searrow\n\n\n\\(\\nwarrow\\)\n\\nwarrow\n\\(\\swarrow\\)\n\\swarrow\n\n\n\\(\\mapsto\\)\n\\mapsto\n\\(\\to\\)\n\\to",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>42</span>  <span class='chapter-title'>LaTeX Command Reference</span>"
    ]
  },
  {
    "objectID": "chapters/A7-latex-reference.html#sec-latex-big-operators",
    "href": "chapters/A7-latex-reference.html#sec-latex-big-operators",
    "title": "42  LaTeX Command Reference",
    "section": "42.8 Sums, Products, and Integrals",
    "text": "42.8 Sums, Products, and Integrals\n\n\n\nSymbol\nCommand\nExample\n\n\n\n\n\\(\\sum\\)\n\\sum\n$\\sum_{i=1}^{n} x_i$\n\n\n\\(\\prod\\)\n\\prod\n$\\prod_{i=1}^{n} x_i$\n\n\n\\(\\coprod\\)\n\\coprod\n$\\coprod_{i=1}^{n}$\n\n\n\\(\\int\\)\n\\int\n$\\int_a^b f(x) dx$\n\n\n\\(\\iint\\)\n\\iint\n$\\iint_D f(x,y) dA$\n\n\n\\(\\iiint\\)\n\\iiint\n$\\iiint_V f dV$\n\n\n\\(\\oint\\)\n\\oint\n$\\oint_C F \\cdot dr$\n\n\n\\(\\bigcup\\)\n\\bigcup\n$\\bigcup_{i=1}^{n} A_i$\n\n\n\\(\\bigcap\\)\n\\bigcap\n$\\bigcap_{i=1}^{n} A_i$\n\n\n\\(\\bigoplus\\)\n\\bigoplus\n$\\bigoplus_{i=1}^{n}$\n\n\n\\(\\bigotimes\\)\n\\bigotimes\n$\\bigotimes_{i=1}^{n}$\n\n\n\\(\\lim\\)\n\\lim\n$\\lim_{x \\to 0}$\n\n\n\\(\\limsup\\)\n\\limsup\n$\\limsup_{n \\to \\infty}$\n\n\n\\(\\liminf\\)\n\\liminf\n$\\liminf_{n \\to \\infty}$",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>42</span>  <span class='chapter-title'>LaTeX Command Reference</span>"
    ]
  },
  {
    "objectID": "chapters/A7-latex-reference.html#sec-latex-functions",
    "href": "chapters/A7-latex-reference.html#sec-latex-functions",
    "title": "42  LaTeX Command Reference",
    "section": "42.9 Mathematical Functions",
    "text": "42.9 Mathematical Functions\n\n\n\nSymbol\nCommand\nSymbol\nCommand\n\n\n\n\n\\(\\sin\\)\n\\sin\n\\(\\arcsin\\)\n\\arcsin\n\n\n\\(\\cos\\)\n\\cos\n\\(\\arccos\\)\n\\arccos\n\n\n\\(\\tan\\)\n\\tan\n\\(\\arctan\\)\n\\arctan\n\n\n\\(\\cot\\)\n\\cot\n\\(\\sec\\)\n\\sec\n\n\n\\(\\csc\\)\n\\csc\n\\(\\sinh\\)\n\\sinh\n\n\n\\(\\cosh\\)\n\\cosh\n\\(\\tanh\\)\n\\tanh\n\n\n\\(\\log\\)\n\\log\n\\(\\ln\\)\n\\ln\n\n\n\\(\\lg\\)\n\\lg\n\\(\\exp\\)\n\\exp\n\n\n\\(\\max\\)\n\\max\n\\(\\min\\)\n\\min\n\n\n\\(\\sup\\)\n\\sup\n\\(\\inf\\)\n\\inf\n\n\n\\(\\arg\\)\n\\arg\n\\(\\det\\)\n\\det\n\n\n\\(\\dim\\)\n\\dim\n\\(\\ker\\)\n\\ker\n\n\n\\(\\gcd\\)\n\\gcd\n\\(\\deg\\)\n\\deg\n\n\n\\(\\hom\\)\n\\hom\n\\(\\Pr\\)\n\\Pr",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>42</span>  <span class='chapter-title'>LaTeX Command Reference</span>"
    ]
  },
  {
    "objectID": "chapters/A7-latex-reference.html#sec-latex-accents",
    "href": "chapters/A7-latex-reference.html#sec-latex-accents",
    "title": "42  LaTeX Command Reference",
    "section": "42.10 Accents and Decorations",
    "text": "42.10 Accents and Decorations\n\n\n\nSymbol\nCommand\nDescription\n\n\n\n\n\\(\\hat{x}\\)\n\\hat{x}\nHat (estimator)\n\n\n\\(\\widehat{xyz}\\)\n\\widehat{xyz}\nWide hat\n\n\n\\(\\bar{x}\\)\n\\bar{x}\nBar (mean)\n\n\n\\(\\overline{xyz}\\)\n\\overline{xyz}\nOverline\n\n\n\\(\\tilde{x}\\)\n\\tilde{x}\nTilde\n\n\n\\(\\widetilde{xyz}\\)\n\\widetilde{xyz}\nWide tilde\n\n\n\\(\\vec{x}\\)\n\\vec{x}\nVector arrow\n\n\n\\(\\overrightarrow{AB}\\)\n\\overrightarrow{AB}\nVector from A to B\n\n\n\\(\\dot{x}\\)\n\\dot{x}\nDot (time derivative)\n\n\n\\(\\ddot{x}\\)\n\\ddot{x}\nDouble dot\n\n\n\\(\\acute{x}\\)\n\\acute{x}\nAcute accent\n\n\n\\(\\grave{x}\\)\n\\grave{x}\nGrave accent\n\n\n\\(\\breve{x}\\)\n\\breve{x}\nBreve\n\n\n\\(\\check{x}\\)\n\\check{x}\nCheck mark",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>42</span>  <span class='chapter-title'>LaTeX Command Reference</span>"
    ]
  },
  {
    "objectID": "chapters/A7-latex-reference.html#sec-latex-fonts",
    "href": "chapters/A7-latex-reference.html#sec-latex-fonts",
    "title": "42  LaTeX Command Reference",
    "section": "42.11 Font Styles",
    "text": "42.11 Font Styles\n\n\n\nSymbol\nCommand\nDescription\n\n\n\n\n\\(\\mathbf{x}\\)\n\\mathbf{x}\nBold (vectors/matrices)\n\n\n\\(\\mathit{x}\\)\n\\mathit{x}\nItalic\n\n\n\\(\\mathrm{x}\\)\n\\mathrm{x}\nRoman (upright)\n\n\n\\(\\mathsf{x}\\)\n\\mathsf{x}\nSans-serif\n\n\n\\(\\mathtt{x}\\)\n\\mathtt{x}\nTypewriter\n\n\n\\(\\mathcal{X}\\)\n\\mathcal{X}\nCalligraphic\n\n\n\\(\\mathbb{R}\\)\n\\mathbb{R}\nBlackboard bold\n\n\n\\(\\mathfrak{X}\\)\n\\mathfrak{X}\nFraktur\n\n\n\\(\\boldsymbol{\\alpha}\\)\n\\boldsymbol{\\alpha}\nBold symbol (Greek)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>42</span>  <span class='chapter-title'>LaTeX Command Reference</span>"
    ]
  },
  {
    "objectID": "chapters/A7-latex-reference.html#sec-latex-number-sets",
    "href": "chapters/A7-latex-reference.html#sec-latex-number-sets",
    "title": "42  LaTeX Command Reference",
    "section": "42.12 Common Number Sets",
    "text": "42.12 Common Number Sets\n\n\n\nSymbol\nCommand\nDescription\n\n\n\n\n\\(\\mathbb{N}\\)\n\\mathbb{N}\nNatural numbers\n\n\n\\(\\mathbb{Z}\\)\n\\mathbb{Z}\nIntegers\n\n\n\\(\\mathbb{Q}\\)\n\\mathbb{Q}\nRational numbers\n\n\n\\(\\mathbb{R}\\)\n\\mathbb{R}\nReal numbers\n\n\n\\(\\mathbb{C}\\)\n\\mathbb{C}\nComplex numbers\n\n\n\\(\\mathbb{P}\\)\n\\mathbb{P}\nPrime numbers",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>42</span>  <span class='chapter-title'>LaTeX Command Reference</span>"
    ]
  },
  {
    "objectID": "chapters/A7-latex-reference.html#sec-latex-brackets",
    "href": "chapters/A7-latex-reference.html#sec-latex-brackets",
    "title": "42  LaTeX Command Reference",
    "section": "42.13 Brackets and Delimiters",
    "text": "42.13 Brackets and Delimiters\n\n\n\nSymbol\nCommand\nDescription\n\n\n\n\n\\((x)\\)\n(x)\nParentheses\n\n\n\\([x]\\)\n[x]\nSquare brackets\n\n\n\\(\\{x\\}\\)\n\\{x\\}\nCurly braces\n\n\n\\(\\langle x \\rangle\\)\n\\langle x \\rangle\nAngle brackets\n\n\n\\(|x|\\)\n|x|\nVertical bars (absolute value)\n\n\n\\(\\|x\\|\\)\n\\|x\\|\nDouble bars (norm)\n\n\n\\(\\lfloor x \\rfloor\\)\n\\lfloor x \\rfloor\nFloor\n\n\n\\(\\lceil x \\rceil\\)\n\\lceil x \\rceil\nCeiling\n\n\n\n\nAuto-sizing Delimiters\nUse \\left and \\right for automatically sized delimiters:\n\n\n\n\n\n\n\nExample\nCode\n\n\n\n\n\\(\\left( \\frac{a}{b} \\right)\\)\n\\left( \\frac{a}{b} \\right)\n\n\n\\(\\left[ \\sum_{i=1}^{n} x_i \\right]\\)\n\\left[ \\sum_{i=1}^{n} x_i \\right]\n\n\n\\(\\left\\{ \\frac{a}{b} \\right\\}\\)\n\\left\\{ \\frac{a}{b} \\right\\}\n\n\n\\(\\left| \\frac{a}{b} \\right|\\)\n\\left| \\frac{a}{b} \\right|",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>42</span>  <span class='chapter-title'>LaTeX Command Reference</span>"
    ]
  },
  {
    "objectID": "chapters/A7-latex-reference.html#sec-latex-matrices",
    "href": "chapters/A7-latex-reference.html#sec-latex-matrices",
    "title": "42  LaTeX Command Reference",
    "section": "42.14 Matrix Environments",
    "text": "42.14 Matrix Environments\n\n\n\n\n\n\n\n\nEnvironment\nResult\nBrackets\n\n\n\n\nmatrix\n\\(\\begin{matrix} a & b \\\\ c & d \\end{matrix}\\)\nNone\n\n\npmatrix\n\\(\\begin{pmatrix} a & b \\\\ c & d \\end{pmatrix}\\)\nParentheses\n\n\nbmatrix\n\\(\\begin{bmatrix} a & b \\\\ c & d \\end{bmatrix}\\)\nSquare brackets\n\n\nBmatrix\n\\(\\begin{Bmatrix} a & b \\\\ c & d \\end{Bmatrix}\\)\nCurly braces\n\n\nvmatrix\n\\(\\begin{vmatrix} a & b \\\\ c & d \\end{vmatrix}\\)\nVertical bars\n\n\nVmatrix\n\\(\\begin{Vmatrix} a & b \\\\ c & d \\end{Vmatrix}\\)\nDouble vertical\n\n\n\nSyntax:\n$$\\begin{bmatrix}\na & b & c \\\\\nd & e & f\n\\end{bmatrix}$$",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>42</span>  <span class='chapter-title'>LaTeX Command Reference</span>"
    ]
  },
  {
    "objectID": "chapters/A7-latex-reference.html#sec-latex-spacing",
    "href": "chapters/A7-latex-reference.html#sec-latex-spacing",
    "title": "42  LaTeX Command Reference",
    "section": "42.15 Spacing",
    "text": "42.15 Spacing\n\n\n\nCommand\nWidth\nExample\n\n\n\n\n\\,\n3/18 em (thin)\n\\(a\\,b\\)\n\n\n\\:\n4/18 em (medium)\n\\(a\\:b\\)\n\n\n\\;\n5/18 em (thick)\n\\(a\\;b\\)\n\n\n\\quad\n1 em\n\\(a\\quad b\\)\n\n\n\\qquad\n2 em\n\\(a\\qquad b\\)\n\n\n\\!\n-3/18 em (negative)\n\\(a\\!b\\)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>42</span>  <span class='chapter-title'>LaTeX Command Reference</span>"
    ]
  },
  {
    "objectID": "chapters/A7-latex-reference.html#sec-latex-dots",
    "href": "chapters/A7-latex-reference.html#sec-latex-dots",
    "title": "42  LaTeX Command Reference",
    "section": "42.16 Dots and Ellipses",
    "text": "42.16 Dots and Ellipses\n\n\n\nSymbol\nCommand\nUsage\n\n\n\n\n\\(\\ldots\\)\n\\ldots\nBaseline dots\n\n\n\\(\\cdots\\)\n\\cdots\nCentered dots\n\n\n\\(\\vdots\\)\n\\vdots\nVertical dots\n\n\n\\(\\ddots\\)\n\\ddots\nDiagonal dots\n\n\n\\(\\cdot\\)\n\\cdot\nSingle centered dot",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>42</span>  <span class='chapter-title'>LaTeX Command Reference</span>"
    ]
  },
  {
    "objectID": "chapters/A7-latex-reference.html#sec-latex-misc",
    "href": "chapters/A7-latex-reference.html#sec-latex-misc",
    "title": "42  LaTeX Command Reference",
    "section": "42.17 Miscellaneous Symbols",
    "text": "42.17 Miscellaneous Symbols\n\n\n\nSymbol\nCommand\nDescription\n\n\n\n\n\\(\\infty\\)\n\\infty\nInfinity\n\n\n\\(\\partial\\)\n\\partial\nPartial derivative\n\n\n\\(\\nabla\\)\n\\nabla\nNabla / Del\n\n\n\\(\\prime\\)\n\\prime\nPrime\n\n\n\\(\\angle\\)\n\\angle\nAngle\n\n\n\\(\\triangle\\)\n\\triangle\nTriangle\n\n\n\\(\\square\\)\n\\square\nSquare\n\n\n\\(\\circ\\)\n\\circ\nCircle / composition\n\n\n\\(\\bullet\\)\n\\bullet\nBullet\n\n\n\\(\\star\\)\n\\star\nStar\n\n\n\\(\\dagger\\)\n\\dagger\nDagger\n\n\n\\(\\ddagger\\)\n\\ddagger\nDouble dagger\n\n\n\\(\\ell\\)\n\\ell\nScript l\n\n\n\\(\\hbar\\)\n\\hbar\nh-bar (Planck’s constant)\n\n\n\\(\\Re\\)\n\\Re\nReal part\n\n\n\\(\\Im\\)\n\\Im\nImaginary part\n\n\n\\(\\aleph\\)\n\\aleph\nAleph",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>42</span>  <span class='chapter-title'>LaTeX Command Reference</span>"
    ]
  },
  {
    "objectID": "chapters/A7-latex-reference.html#sec-latex-stats",
    "href": "chapters/A7-latex-reference.html#sec-latex-stats",
    "title": "42  LaTeX Command Reference",
    "section": "42.18 Statistical Notation",
    "text": "42.18 Statistical Notation\n\n\n\nExpression\nCode\nDescription\n\n\n\n\n\\(\\bar{x}\\)\n\\bar{x}\nSample mean\n\n\n\\(\\hat{\\mu}\\)\n\\hat{\\mu}\nEstimated parameter\n\n\n\\(\\tilde{x}\\)\n\\tilde{x}\nMedian\n\n\n\\(X \\sim N(\\mu, \\sigma^2)\\)\nX \\sim N(\\mu, \\sigma^2)\nDistribution\n\n\n\\({n \\choose k}\\)\n{n \\choose k}\nBinomial coefficient\n\n\n\\(\\binom{n}{k}\\)\n\\binom{n}{k}\nBinomial coefficient (alternate)\n\n\n\\(P(A \\mid B)\\)\nP(A \\mid B)\nConditional probability\n\n\n\\(\\mathbb{E}[X]\\)\n\\mathbb{E}[X]\nExpected value\n\n\n\\(\\text{Var}(X)\\)\n\\text{Var}(X)\nVariance\n\n\n\\(\\text{Cov}(X, Y)\\)\n\\text{Cov}(X, Y)\nCovariance",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>42</span>  <span class='chapter-title'>LaTeX Command Reference</span>"
    ]
  },
  {
    "objectID": "chapters/A7-latex-reference.html#sec-latex-stat-formulas",
    "href": "chapters/A7-latex-reference.html#sec-latex-stat-formulas",
    "title": "42  LaTeX Command Reference",
    "section": "42.19 Common Statistical Formulas",
    "text": "42.19 Common Statistical Formulas\n\nSample Statistics\n\n\n\n\n\n\n\nStatistic\nFormula\n\n\n\n\nSample mean\n\\(\\bar{x} = \\frac{1}{n}\\sum_{i=1}^{n} x_i\\)\n\n\nSample variance\n\\(s^2 = \\frac{1}{n-1}\\sum_{i=1}^{n}(x_i - \\bar{x})^2\\)\n\n\nStandard error\n\\(SE = \\frac{s}{\\sqrt{n}}\\)\n\n\nZ-score\n\\(z = \\frac{x - \\mu}{\\sigma}\\)\n\n\n\n\n\nProbability Distributions\n\n\n\n\n\n\n\nDistribution\nProbability Function\n\n\n\n\nBinomial\n\\(P(X=k) = \\binom{n}{k} p^k (1-p)^{n-k}\\)\n\n\nPoisson\n\\(P(X=k) = \\frac{e^{-\\lambda}\\lambda^k}{k!}\\)\n\n\nNormal\n\\(f(x) = \\frac{1}{\\sigma\\sqrt{2\\pi}} e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}\\)\n\n\nExponential\n\\(f(x) = \\lambda e^{-\\lambda x}\\)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>42</span>  <span class='chapter-title'>LaTeX Command Reference</span>"
    ]
  },
  {
    "objectID": "chapters/A7-latex-reference.html#sec-latex-numbering",
    "href": "chapters/A7-latex-reference.html#sec-latex-numbering",
    "title": "42  LaTeX Command Reference",
    "section": "42.20 Equation Numbering in Quarto",
    "text": "42.20 Equation Numbering in Quarto\nAdd equation labels for cross-referencing:\n$$\nE = mc^2\n$$ {#eq-einstein}\n\nSee @eq-einstein for the mass-energy equivalence.\nMultiple equations with alignment:\n$$\n\\begin{aligned}\ny &= mx + b \\\\\n  &= 2x + 3\n\\end{aligned}\n$$ {#eq-line}",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>42</span>  <span class='chapter-title'>LaTeX Command Reference</span>"
    ]
  },
  {
    "objectID": "chapters/A4-greek-letters.html",
    "href": "chapters/A4-greek-letters.html",
    "title": "43  Greek Letters in Mathematics and Statistics",
    "section": "",
    "text": "43.1 The Greek Alphabet\nGreek letters are ubiquitous in mathematics and statistics. This appendix provides a reference for the Greek alphabet, including pronunciation and common uses in statistical contexts.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>43</span>  <span class='chapter-title'>Greek Letters in Mathematics and Statistics</span>"
    ]
  },
  {
    "objectID": "chapters/A4-greek-letters.html#the-greek-alphabet",
    "href": "chapters/A4-greek-letters.html#the-greek-alphabet",
    "title": "43  Greek Letters in Mathematics and Statistics",
    "section": "",
    "text": "Letter\nLowercase\nUppercase\nName\nPronunciation\nCommon Statistical Uses\n\n\n\n\n1\nα\nΑ\nAlpha\nAL-fah\nSignificance level (Type I error rate); regression intercept\n\n\n2\nβ\nΒ\nBeta\nBAY-tah\nType II error rate; regression coefficients; slope parameters\n\n\n3\nγ\nΓ\nGamma\nGAM-ah\nGamma distribution; shape parameter\n\n\n4\nδ\nΔ\nDelta\nDEL-tah\nChange or difference; effect size (Cohen’s d uses Roman d)\n\n\n5\nε\nΕ\nEpsilon\nEP-si-lon\nError term in models; small quantity approaching zero\n\n\n6\nζ\nΖ\nZeta\nZAY-tah\nRarely used in statistics\n\n\n7\nη\nΗ\nEta\nAY-tah\nEffect size (η²); learning rate\n\n\n8\nθ\nΘ\nTheta\nTHAY-tah\nGeneric parameter; angle\n\n\n9\nι\nΙ\nIota\neye-OH-tah\nRarely used in statistics\n\n\n10\nκ\nΚ\nKappa\nKAP-ah\nCohen’s kappa (agreement); condition number\n\n\n11\nλ\nΛ\nLambda\nLAM-dah\nRate parameter (Poisson, exponential); eigenvalue; Wilks’ lambda\n\n\n12\nμ\nΜ\nMu\nMYOO\nPopulation mean\n\n\n13\nν\nΝ\nNu\nNOO\nDegrees of freedom\n\n\n14\nξ\nΞ\nXi\nKSEE or ZIGH\nRarely used; sometimes for random variables\n\n\n15\nο\nΟ\nOmicron\nOM-i-kron\nRarely used (resembles Roman O)\n\n\n16\nπ\nΠ\nPi\nPIE\nMathematical constant (≈ 3.14159); product notation (Π)\n\n\n17\nρ\nΡ\nRho\nROW\nPopulation correlation coefficient; autocorrelation\n\n\n18\nσ\nΣ\nSigma\nSIG-mah\nPopulation standard deviation (σ); summation (Σ)\n\n\n19\nτ\nΤ\nTau\nTAW (rhymes with cow)\nKendall’s tau; time constant\n\n\n20\nυ\nΥ\nUpsilon\nOOP-si-lon\nRarely used in statistics\n\n\n21\nφ\nΦ\nPhi\nFYE or FEE\nPhi coefficient; standard normal PDF (φ); golden ratio\n\n\n22\nχ\nΧ\nChi\nKYE (rhymes with sky)\nChi-square distribution and test (χ²)\n\n\n23\nψ\nΨ\nPsi\nSIGH or PSEE\nRarely used; sometimes for angles or wave functions\n\n\n24\nω\nΩ\nOmega\noh-MAY-gah\nEffect size (ω²); angular frequency",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>43</span>  <span class='chapter-title'>Greek Letters in Mathematics and Statistics</span>"
    ]
  },
  {
    "objectID": "chapters/A4-greek-letters.html#most-commonly-used-letters-in-statistics",
    "href": "chapters/A4-greek-letters.html#most-commonly-used-letters-in-statistics",
    "title": "43  Greek Letters in Mathematics and Statistics",
    "section": "43.2 Most Commonly Used Letters in Statistics",
    "text": "43.2 Most Commonly Used Letters in Statistics\n\nPopulation Parameters\nThe following Greek letters conventionally represent population parameters (true but unknown values):\n\nμ (mu): Population mean\nσ (sigma): Population standard deviation\nσ² (sigma squared): Population variance\nρ (rho): Population correlation coefficient\nπ (pi): Population proportion (also the mathematical constant)\nβ (beta): Population regression coefficients\n\n\n\nHypothesis Testing\n\nα (alpha): Significance level; probability of Type I error (rejecting a true null hypothesis). Commonly set to 0.05.\nβ (beta): Probability of Type II error (failing to reject a false null hypothesis). Power = 1 - β.\nχ² (chi-square): Test statistic for categorical data analysis\n\n\n\nEffect Sizes\n\nη² (eta squared): Proportion of variance explained in ANOVA\nω² (omega squared): Less biased estimate of variance explained\nφ (phi): Effect size for 2×2 contingency tables\n\n\n\nDistributions\n\nλ (lambda): Rate parameter for Poisson and exponential distributions\nΓ (Gamma): The Gamma function and Gamma distribution\nθ (theta): Generic parameter in probability distributions\n\n\n\nSummation and Products\n\nΣ (capital sigma): Summation notation: \\(\\sum_{i=1}^{n} x_i\\)\nΠ (capital pi): Product notation: \\(\\prod_{i=1}^{n} x_i\\)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>43</span>  <span class='chapter-title'>Greek Letters in Mathematics and Statistics</span>"
    ]
  },
  {
    "objectID": "chapters/A4-greek-letters.html#writing-greek-letters",
    "href": "chapters/A4-greek-letters.html#writing-greek-letters",
    "title": "43  Greek Letters in Mathematics and Statistics",
    "section": "43.3 Writing Greek Letters",
    "text": "43.3 Writing Greek Letters\n\nIn LaTeX and Quarto\nGreek letters are written in LaTeX (and thus in Quarto documents) using backslash commands:\n$\\alpha$    → α        $\\Alpha$    → Α\n$\\beta$     → β        $\\Beta$     → Β\n$\\gamma$    → γ        $\\Gamma$    → Γ\n$\\delta$    → δ        $\\Delta$    → Δ\n$\\epsilon$  → ε        $\\mu$       → μ\n$\\sigma$    → σ        $\\Sigma$    → Σ\n$\\chi$      → χ        $\\lambda$   → λ\n$\\theta$    → θ        $\\rho$      → ρ\n\n\nIn R\nR supports Greek letters in plots using the expression() function:\n\n\nCode\n# Axis labels with Greek letters\nplot(x, y,\n     xlab = expression(mu),\n     ylab = expression(sigma^2))\n\n# More complex expressions\ntitle(expression(paste(\"Mean = \", mu, \", SD = \", sigma)))\n\n# In ggplot2\nlibrary(ggplot2)\nggplot(data, aes(x, y)) +\n  geom_point() +\n  labs(x = expression(alpha),\n       y = expression(beta))",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>43</span>  <span class='chapter-title'>Greek Letters in Mathematics and Statistics</span>"
    ]
  },
  {
    "objectID": "chapters/A4-greek-letters.html#conventions-and-mnemonics",
    "href": "chapters/A4-greek-letters.html#conventions-and-mnemonics",
    "title": "43  Greek Letters in Mathematics and Statistics",
    "section": "43.4 Conventions and Mnemonics",
    "text": "43.4 Conventions and Mnemonics\n\nRoman vs. Greek Letters\nA useful convention in statistics:\n\nGreek letters represent population parameters (unknown, fixed values)\nRoman (Latin) letters represent sample statistics (calculated from data)\n\n\n\n\nPopulation Parameter\nSample Statistic\n\n\n\n\nμ (mu) - population mean\n\\(\\bar{x}\\) - sample mean\n\n\nσ (sigma) - population SD\ns - sample SD\n\n\nρ (rho) - population correlation\nr - sample correlation\n\n\nβ (beta) - population slope\nb - sample slope\n\n\nπ (pi) - population proportion\np̂ - sample proportion\n\n\n\n\n\nMemory Aids\n\nα (alpha) for significance level: “A” comes first, and we set alpha first before testing\nβ (beta) for slope: “B” is for the coefficient “B” in y = a + bx\nμ (mu) for mean: Think “μ” looks like a “u” turned sideways—“u” for “average of you all”\nσ (sigma) for standard deviation: “S” for Sigma, “S” for Standard deviation\nΣ (capital sigma) for sum: “S” for Sum\nρ (rho) for correlation: “R” for Rho, “R” for R-value (correlation)\nχ (chi) for chi-square: Looks like an “X”—think “X marks the spot” for testing categories",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>43</span>  <span class='chapter-title'>Greek Letters in Mathematics and Statistics</span>"
    ]
  },
  {
    "objectID": "chapters/A4-greek-letters.html#common-formulas-using-greek-letters",
    "href": "chapters/A4-greek-letters.html#common-formulas-using-greek-letters",
    "title": "43  Greek Letters in Mathematics and Statistics",
    "section": "43.5 Common Formulas Using Greek Letters",
    "text": "43.5 Common Formulas Using Greek Letters\n\nNormal Distribution\n\\[f(x) = \\frac{1}{\\sigma\\sqrt{2\\pi}} e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}\\]\n\n\nZ-score\n\\[z = \\frac{x - \\mu}{\\sigma}\\]\n\n\nLinear Regression Model\n\\[Y_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i\\]\nwhere \\(\\epsilon_i \\sim N(0, \\sigma^2)\\)\n\n\nCorrelation Coefficient\n\\[\\rho = \\frac{\\text{Cov}(X, Y)}{\\sigma_X \\sigma_Y}\\]\n\n\nChi-Square Test Statistic\n\\[\\chi^2 = \\sum \\frac{(O - E)^2}{E}\\]\n\n\nANOVA F-ratio\n\\[F = \\frac{MS_{\\text{between}}}{MS_{\\text{within}}} = \\frac{\\sigma^2_{\\text{between}}}{\\sigma^2_{\\text{within}}}\\]",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>43</span>  <span class='chapter-title'>Greek Letters in Mathematics and Statistics</span>"
    ]
  },
  {
    "objectID": "chapters/A4-greek-letters.html#summary",
    "href": "chapters/A4-greek-letters.html#summary",
    "title": "43  Greek Letters in Mathematics and Statistics",
    "section": "43.6 Summary",
    "text": "43.6 Summary\nMastering Greek letters is essential for reading and writing statistical notation. The most important letters to memorize are:\n\nμ, σ, ρ: Population mean, standard deviation, and correlation\nα, β: Significance level and Type II error (or regression coefficients)\nΣ: Summation\nχ²: Chi-square\n\nWith practice, these symbols become as natural as the Roman alphabet, and they provide a universal language for expressing statistical concepts precisely and concisely.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>43</span>  <span class='chapter-title'>Greek Letters in Mathematics and Statistics</span>"
    ]
  },
  {
    "objectID": "chapters/A3-probability-distributions.html",
    "href": "chapters/A3-probability-distributions.html",
    "title": "44  Common Probability Distributions",
    "section": "",
    "text": "44.1 Discrete Distributions\nThis appendix provides a reference for probability distributions commonly encountered in biological and bioengineering applications. Each distribution is characterized by its probability function, parameters, mean, variance, and typical applications.\nDiscrete distributions describe random variables that take on countable values (integers).",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>44</span>  <span class='chapter-title'>Common Probability Distributions</span>"
    ]
  },
  {
    "objectID": "chapters/A3-probability-distributions.html#discrete-distributions",
    "href": "chapters/A3-probability-distributions.html#discrete-distributions",
    "title": "44  Common Probability Distributions",
    "section": "",
    "text": "Bernoulli Distribution\nThe Bernoulli distribution describes a single trial with two possible outcomes (success/failure).\nProbability mass function: \\[P(X = k) = p^k(1-p)^{1-k}, \\quad k \\in \\{0, 1\\}\\]\nParameters: \\(p\\) = probability of success (0 ≤ p ≤ 1)\nMean: \\(E[X] = p\\)\nVariance: \\(\\text{Var}(X) = p(1-p)\\)\nApplications:\n\nSingle coin flip\nWhether a patient responds to treatment\nWhether an allele is inherited\n\n\n\nCode\n# Bernoulli with different p values\np_vals &lt;- c(0.2, 0.5, 0.8)\ndata.frame(\n  outcome = rep(c(\"Failure\", \"Success\"), 3),\n  p = rep(p_vals, each = 2),\n  prob = c(1-p_vals[1], p_vals[1], 1-p_vals[2], p_vals[2], 1-p_vals[3], p_vals[3])\n) %&gt;%\n  ggplot(aes(x = outcome, y = prob, fill = factor(p))) +\n  geom_col(position = \"dodge\") +\n  labs(title = \"Bernoulli Distribution\", y = \"Probability\", fill = \"p\")\n\n\n\n\n\n\n\n\nFigure 44.1: Bernoulli distribution showing probability of success and failure for different values of p\n\n\n\n\n\n\n\nSampling Process Example\nConsider recording whether a neuron fires in response to a stimulus. Each trial represents exposing the neuron to a brief stimulus pulse and observing whether it fires (success = 1) or not (failure = 0). If the neuron has a baseline firing probability of \\(p = 0.3\\) in response to the stimulus, each observation is a Bernoulli trial.\n\n\nCode\n# Simulate 10 neuron stimulation trials\nset.seed(123)\np_fire &lt;- 0.3\nn_trials &lt;- 10\n\n# Sample from Bernoulli distribution\nneuron_responses &lt;- rbinom(n_trials, size = 1, prob = p_fire)\ncat(\"Neuron firing pattern (1=fire, 0=no fire):\", neuron_responses, \"\\n\")\n\n\nNeuron firing pattern (1=fire, 0=no fire): 0 1 0 1 1 0 0 1 0 0 \n\n\nCode\ncat(\"Total fires:\", sum(neuron_responses), \"out of\", n_trials, \"trials\")\n\n\nTotal fires: 4 out of 10 trials\n\n\n\n\nBinomial Distribution\nThe binomial distribution describes the number of successes in \\(n\\) independent Bernoulli trials.\nProbability mass function: \\[P(X = k) = \\binom{n}{k} p^k (1-p)^{n-k}\\]\nParameters:\n\n\\(n\\) = number of trials\n\\(p\\) = probability of success on each trial\n\nMean: \\(E[X] = np\\)\nVariance: \\(\\text{Var}(X) = np(1-p)\\)\nApplications:\n\nNumber of heads in \\(n\\) coin flips\nNumber of mutant alleles in offspring\nNumber of patients responding to treatment out of \\(n\\) treated\n\n\n\nCode\n# Binomial distributions with n=20\npar(mfrow = c(1, 3))\nfor (p in c(0.2, 0.5, 0.8)) {\n  x &lt;- 0:20\n  barplot(dbinom(x, 20, p), names.arg = x,\n          main = paste(\"Binomial(n=20, p=\", p, \")\"),\n          xlab = \"k\", ylab = \"P(X=k)\", col = \"steelblue\")\n}\n\n\n\n\n\n\n\n\nFigure 44.2: Binomial distribution with n=20 trials for different probabilities of success\n\n\n\n\n\nR functions: dbinom(), pbinom(), qbinom(), rbinom()\n\n\nSampling Process Example\nA plant biologist plants 100 seeds of a particular species and wants to know how many will germinate. Based on previous experiments, each seed has a probability \\(p = 0.75\\) of germinating independently. The number of seeds that germinate follows a binomial distribution with \\(n = 100\\) and \\(p = 0.75\\).\n\n\nCode\n# Simulate seed germination experiment\nset.seed(456)\nn_seeds &lt;- 100\np_germinate &lt;- 0.75\n\n# Sample from binomial distribution\ngerminated_seeds &lt;- rbinom(1, size = n_seeds, prob = p_germinate)\ncat(\"Number of seeds germinated:\", germinated_seeds, \"out of\", n_seeds, \"\\n\")\n\n\nNumber of seeds germinated: 81 out of 100 \n\n\nCode\n# Simulate multiple experiments\nn_experiments &lt;- 1000\ngermination_counts &lt;- rbinom(n_experiments, size = n_seeds, prob = p_germinate)\ncat(\"Mean across\", n_experiments, \"experiments:\", mean(germination_counts), \"\\n\")\n\n\nMean across 1000 experiments: 74.723 \n\n\nCode\ncat(\"Expected value (np):\", n_seeds * p_germinate)\n\n\nExpected value (np): 75\n\n\n\n\nGeometric Distribution\nThe geometric distribution describes the number of trials needed to get the first success.\nProbability mass function: \\[P(X = k) = (1-p)^{k-1}p, \\quad k = 1, 2, 3, \\ldots\\]\nParameters: \\(p\\) = probability of success\nMean: \\(E[X] = \\frac{1}{p}\\)\nVariance: \\(\\text{Var}(X) = \\frac{1-p}{p^2}\\)\nApplications:\n\nNumber of trials until first success\nTime until extinction of an endangered population\nNumber of reads until finding a specific sequence\n\n\n\nCode\n# Geometric distribution example\n# If extinction probability is 0.1 per year, expected time to extinction?\np &lt;- 0.1\nx &lt;- 1:50\nplot(x, dgeom(x-1, p), type = \"h\", lwd = 2, col = \"steelblue\",\n     xlab = \"Years until extinction\", ylab = \"Probability\",\n     main = paste(\"Geometric Distribution (p =\", p, \")\\nMean =\", 1/p, \"years\"))\n\n\n\n\n\n\n\n\nFigure 44.3: Geometric distribution showing the probability of years until extinction when p = 0.1\n\n\n\n\n\nR functions: dgeom(), pgeom(), qgeom(), rgeom()\n\n\nSampling Process Example\nIn a PCR amplification experiment, a researcher is attempting to amplify a rare target sequence. Each PCR cycle has a probability \\(p = 0.15\\) of successfully amplifying the target to detectable levels. The number of cycles until the first successful amplification follows a geometric distribution.\n\n\nCode\n# Simulate PCR amplification trials\nset.seed(789)\np_success &lt;- 0.15\n\n# Sample number of cycles until first success\ncycles_to_success &lt;- rgeom(1, prob = p_success) + 1  # +1 because rgeom gives failures before success\ncat(\"Cycles until first successful amplification:\", cycles_to_success, \"\\n\")\n\n\nCycles until first successful amplification: 1 \n\n\nCode\n# Simulate multiple PCR experiments\nn_experiments &lt;- 1000\ncycles_distribution &lt;- rgeom(n_experiments, prob = p_success) + 1\ncat(\"Mean cycles across\", n_experiments, \"experiments:\", mean(cycles_distribution), \"\\n\")\n\n\nMean cycles across 1000 experiments: 6.428 \n\n\nCode\ncat(\"Expected value (1/p):\", 1/p_success)\n\n\nExpected value (1/p): 6.666667\n\n\n\n\nNegative Binomial Distribution\nThe negative binomial extends the geometric distribution to describe the number of trials needed to achieve \\(r\\) successes.\nProbability mass function: \\[P(X = k) = \\binom{k-1}{r-1} p^r (1-p)^{k-r}, \\quad k = r, r+1, r+2, \\ldots\\]\nParameters:\n\n\\(r\\) = number of successes needed\n\\(p\\) = probability of success\n\nMean: \\(E[X] = \\frac{r}{p}\\)\nVariance: \\(\\text{Var}(X) = \\frac{r(1-p)}{p^2}\\)\nApplications:\n\nOverdispersed count data (variance &gt; mean)\nRNA-seq count modeling\nNumber of trials until \\(r\\) successes\n\n\n\nCode\n# Negative binomial example\n# Predator must capture 10 prey before reproduction\nr &lt;- 10\np &lt;- 0.1\nx &lt;- r:100\nplot(x, dnbinom(x - r, size = r, prob = p), type = \"h\", lwd = 2, col = \"steelblue\",\n     xlab = \"Days until reproduction\", ylab = \"Probability\",\n     main = paste(\"Negative Binomial (r=10, p=0.1)\\nMean =\", r/p, \"days\"))\n\n\n\n\n\n\n\n\nFigure 44.4: Negative binomial distribution showing days until a predator captures 10 prey (r=10, p=0.1)\n\n\n\n\n\nR functions: dnbinom(), pnbinom(), qnbinom(), rnbinom()\n\n\nSampling Process Example\nA genomics researcher is sequencing DNA to identify rare somatic mutations. Each sequencing read has a probability \\(p = 0.05\\) of covering a region containing a mutation of interest. The researcher needs to find \\(r = 10\\) such mutations to have sufficient statistical power. The number of reads required follows a negative binomial distribution.\n\n\nCode\n# Simulate sequencing until finding r mutations\nset.seed(101)\nr_mutations &lt;- 10\np_mutation &lt;- 0.05\n\n# Sample number of reads until r mutations found\n# rnbinom gives number of failures before r successes\nreads_needed &lt;- rnbinom(1, size = r_mutations, prob = p_mutation) + r_mutations\ncat(\"Reads needed to find\", r_mutations, \"mutations:\", reads_needed, \"\\n\")\n\n\nReads needed to find 10 mutations: 177 \n\n\nCode\n# Simulate multiple sequencing experiments\nn_simulations &lt;- 1000\nreads_distribution &lt;- rnbinom(n_simulations, size = r_mutations, prob = p_mutation) + r_mutations\ncat(\"Mean reads across\", n_simulations, \"simulations:\", mean(reads_distribution), \"\\n\")\n\n\nMean reads across 1000 simulations: 199.935 \n\n\nCode\ncat(\"Expected value (r/p):\", r_mutations/p_mutation)\n\n\nExpected value (r/p): 200\n\n\n\n\nPoisson Distribution\nThe Poisson distribution describes the number of events occurring in a fixed interval when events occur independently at a constant average rate.\nProbability mass function: \\[P(X = k) = \\frac{e^{-\\lambda}\\lambda^k}{k!}, \\quad k = 0, 1, 2, \\ldots\\]\nParameters: \\(\\lambda\\) = rate parameter (expected count)\nMean: \\(E[X] = \\lambda\\)\nVariance: \\(\\text{Var}(X) = \\lambda\\)\nNote: For a Poisson distribution, mean equals variance. When observed variance exceeds the mean, the data are called “overdispersed.”\nApplications:\n\nNumber of mutations per gene\nNumber of cells in a microscope field\nNumber of organisms per sample area\nRare event counts in genomics\n\n\n\nCode\n# Poisson distributions with different lambda values\npar(mfrow = c(1, 3))\nfor (lambda in c(1, 5, 15)) {\n  x &lt;- 0:30\n  barplot(dpois(x, lambda), names.arg = x,\n          main = paste(\"Poisson(λ =\", lambda, \")\"),\n          xlab = \"k\", ylab = \"P(X=k)\", col = \"steelblue\")\n}\n\n\n\n\n\n\n\n\nFigure 44.5: Poisson distribution for different rate parameters (λ = 1, 5, and 15)\n\n\n\n\n\nR functions: dpois(), ppois(), qpois(), rpois()\n\n\nSampling Process Example\nIn evolutionary genomics, mutations occur randomly along a gene sequence. Consider a gene of fixed length where mutations occur at an average rate of \\(\\lambda = 3.5\\) mutations per gene per million years. The number of mutations observed in a particular gene follows a Poisson distribution.\n\n\nCode\n# Simulate mutation counts in genes\nset.seed(202)\nlambda_mutations &lt;- 3.5\n\n# Sample mutation count for a single gene\nmutations_in_gene &lt;- rpois(1, lambda = lambda_mutations)\ncat(\"Mutations observed in one gene:\", mutations_in_gene, \"\\n\")\n\n\nMutations observed in one gene: 1 \n\n\nCode\n# Simulate mutation counts across multiple genes\nn_genes &lt;- 100\nmutation_counts &lt;- rpois(n_genes, lambda = lambda_mutations)\ncat(\"Mean mutations across\", n_genes, \"genes:\", mean(mutation_counts), \"\\n\")\n\n\nMean mutations across 100 genes: 3.49 \n\n\nCode\ncat(\"Variance across\", n_genes, \"genes:\", var(mutation_counts), \"\\n\")\n\n\nVariance across 100 genes: 3.141313 \n\n\nCode\ncat(\"Expected value (λ):\", lambda_mutations, \"\\n\")\n\n\nExpected value (λ): 3.5 \n\n\nCode\ncat(\"Note: For Poisson, mean ≈ variance\")\n\n\nNote: For Poisson, mean ≈ variance",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>44</span>  <span class='chapter-title'>Common Probability Distributions</span>"
    ]
  },
  {
    "objectID": "chapters/A3-probability-distributions.html#continuous-distributions",
    "href": "chapters/A3-probability-distributions.html#continuous-distributions",
    "title": "44  Common Probability Distributions",
    "section": "44.2 Continuous Distributions",
    "text": "44.2 Continuous Distributions\nContinuous distributions describe random variables that can take any value in some range.\n\nUniform Distribution\nThe uniform distribution assigns equal probability to all values in a specified range.\nProbability density function: \\[f(x) = \\frac{1}{b-a}, \\quad a \\leq x \\leq b\\]\nParameters:\n\n\\(a\\) = minimum value\n\\(b\\) = maximum value\n\nMean: \\(E[X] = \\frac{a+b}{2}\\)\nVariance: \\(\\text{Var}(X) = \\frac{(b-a)^2}{12}\\)\nApplications:\n\nRandom number generation\nUninformative priors in Bayesian analysis\nEqual probability among alternatives\n\n\n\nCode\nx &lt;- seq(-1, 6, length.out = 200)\nplot(x, dunif(x, min = 0, max = 5), type = \"l\", lwd = 2, col = \"steelblue\",\n     xlab = \"x\", ylab = \"f(x)\", main = \"Uniform Distribution (a=0, b=5)\",\n     ylim = c(0, 0.3))\npolygon(c(0, 0, 5, 5), c(0, 0.2, 0.2, 0), col = rgb(0, 0, 1, 0.3), border = NA)\n\n\n\n\n\n\n\n\nFigure 44.6: Uniform distribution with equal probability between a=0 and b=5\n\n\n\n\n\nR functions: dunif(), punif(), qunif(), runif()\n\n\nSampling Process Example\nIn a cell culture experiment, cells are synchronized to divide during a specific 60-minute window. Within this window, the exact timing of any individual cell’s division is equally likely at any moment. If we measure the time (in minutes from the start of the window) when each cell divides, these times follow a uniform distribution between \\(a = 0\\) and \\(b = 60\\) minutes.\n\n\nCode\n# Simulate cell division times within a synchronized window\nset.seed(303)\nwindow_start &lt;- 0\nwindow_end &lt;- 60\n\n# Sample division times for 20 cells\nn_cells &lt;- 20\ndivision_times &lt;- runif(n_cells, min = window_start, max = window_end)\ncat(\"Division times (minutes):\", round(division_times, 1), \"\\n\")\n\n\nDivision times (minutes): 18.6 50.1 36.6 44.4 39 58.8 31.6 28.1 16.9 16.7 54.3 52.4 6.2 2.2 14 12.8 24.5 14 43.7 39.3 \n\n\nCode\ncat(\"Mean division time:\", round(mean(division_times), 1), \"minutes\\n\")\n\n\nMean division time: 30.2 minutes\n\n\nCode\ncat(\"Expected mean:\", (window_start + window_end)/2, \"minutes\")\n\n\nExpected mean: 30 minutes\n\n\n\n\nExponential Distribution\nThe exponential distribution describes the time between events in a Poisson process.\nProbability density function: \\[f(x) = \\lambda e^{-\\lambda x}, \\quad x \\geq 0\\]\nParameters: \\(\\lambda\\) = rate parameter\nMean: \\(E[X] = \\frac{1}{\\lambda}\\)\nVariance: \\(\\text{Var}(X) = \\frac{1}{\\lambda^2}\\)\nApplications:\n\nTime until next event (radioactive decay, mutations)\nLifespan distributions (constant hazard rate)\nWaiting times\n\n\n\nCode\n# Exponential with different rate parameters\nx &lt;- seq(0, 5, length.out = 200)\nplot(x, dexp(x, rate = 0.5), type = \"l\", lwd = 2, col = \"blue\",\n     xlab = \"x\", ylab = \"f(x)\", main = \"Exponential Distribution\",\n     ylim = c(0, 2))\nlines(x, dexp(x, rate = 1), lwd = 2, col = \"red\")\nlines(x, dexp(x, rate = 2), lwd = 2, col = \"darkgreen\")\nlegend(\"topright\", c(\"λ = 0.5\", \"λ = 1\", \"λ = 2\"),\n       col = c(\"blue\", \"red\", \"darkgreen\"), lwd = 2)\n\n\n\n\n\n\n\n\nFigure 44.7: Exponential distribution for different rate parameters (λ = 0.5, 1, and 2)\n\n\n\n\n\nR functions: dexp(), pexp(), qexp(), rexp()\n\n\nSampling Process Example\nBacterial cells divide asynchronously in exponential growth phase. If cells divide at a constant average rate of \\(\\lambda = 0.5\\) divisions per hour (one division every 2 hours on average), the waiting time between successive cell divisions follows an exponential distribution.\n\n\nCode\n# Simulate times between cell divisions\nset.seed(404)\nlambda_rate &lt;- 0.5  # divisions per hour\n\n# Sample inter-division times for 10 successive divisions\nn_divisions &lt;- 10\ninter_division_times &lt;- rexp(n_divisions, rate = lambda_rate)\ncat(\"Time between divisions (hours):\", round(inter_division_times, 2), \"\\n\")\n\n\nTime between divisions (hours): 1.12 0.21 0.1 0.72 2.39 2.13 1.83 8.49 0.31 0.56 \n\n\nCode\ncat(\"Mean inter-division time:\", round(mean(inter_division_times), 2), \"hours\\n\")\n\n\nMean inter-division time: 1.79 hours\n\n\nCode\ncat(\"Expected mean (1/λ):\", 1/lambda_rate, \"hours\\n\")\n\n\nExpected mean (1/λ): 2 hours\n\n\nCode\n# Total time for all divisions\ncat(\"Total time for\", n_divisions, \"divisions:\", round(sum(inter_division_times), 2), \"hours\")\n\n\nTotal time for 10 divisions: 17.87 hours\n\n\n\n\nGamma Distribution\nThe gamma distribution generalizes the exponential distribution to describe waiting time until the \\(r\\)th event.\nProbability density function: \\[f(x) = \\frac{\\lambda^r}{\\Gamma(r)} x^{r-1} e^{-\\lambda x}, \\quad x \\geq 0\\]\nParameters:\n\n\\(r\\) = shape parameter (number of events)\n\\(\\lambda\\) = rate parameter\n\nMean: \\(E[X] = \\frac{r}{\\lambda}\\)\nVariance: \\(\\text{Var}(X) = \\frac{r}{\\lambda^2}\\)\nApplications:\n\nTime for multiple events to occur\nDuration of processes with multiple stages\nPrior distributions in Bayesian analysis\n\n\n\nCode\n# Gamma distribution with different shape parameters\nx &lt;- seq(0, 15, length.out = 200)\nplot(x, dgamma(x, shape = 1, rate = 0.5), type = \"l\", lwd = 2, col = \"blue\",\n     xlab = \"x\", ylab = \"f(x)\", main = \"Gamma Distribution (rate = 0.5)\",\n     ylim = c(0, 0.5))\nlines(x, dgamma(x, shape = 2, rate = 0.5), lwd = 2, col = \"red\")\nlines(x, dgamma(x, shape = 5, rate = 0.5), lwd = 2, col = \"darkgreen\")\nlegend(\"topright\", c(\"shape = 1\", \"shape = 2\", \"shape = 5\"),\n       col = c(\"blue\", \"red\", \"darkgreen\"), lwd = 2)\n\n\n\n\n\n\n\n\nFigure 44.8: Gamma distribution with rate = 0.5 for different shape parameters (1, 2, and 5)\n\n\n\n\n\nR functions: dgamma(), pgamma(), qgamma(), rgamma()\n\n\nSampling Process Example\nIn developmental biology, a cell must complete \\(r = 5\\) distinct checkpoints before differentiating into a specialized cell type. Each checkpoint is passed at an average rate of \\(\\lambda = 2\\) checkpoints per day. The total time until the cell completes all 5 checkpoints and differentiates follows a gamma distribution.\n\n\nCode\n# Simulate time to complete multiple checkpoints\nset.seed(505)\nr_checkpoints &lt;- 5\nlambda_rate &lt;- 2  # checkpoints per day\n\n# Sample differentiation times for 15 cells\nn_cells &lt;- 15\ndifferentiation_times &lt;- rgamma(n_cells, shape = r_checkpoints, rate = lambda_rate)\ncat(\"Time to differentiation (days):\", round(differentiation_times, 2), \"\\n\")\n\n\nTime to differentiation (days): 1.22 1.06 1.37 1.74 1.39 2.26 3.64 3.09 2.42 2.24 2.04 4.06 1.8 3.93 3.21 \n\n\nCode\ncat(\"Mean differentiation time:\", round(mean(differentiation_times), 2), \"days\\n\")\n\n\nMean differentiation time: 2.36 days\n\n\nCode\ncat(\"Expected mean (r/λ):\", r_checkpoints/lambda_rate, \"days\")\n\n\nExpected mean (r/λ): 2.5 days\n\n\n\n\nNormal (Gaussian) Distribution\nThe normal distribution is the most important continuous distribution in statistics. It arises naturally when many independent factors contribute additively to an outcome.\nProbability density function: \\[f(x) = \\frac{1}{\\sqrt{2\\pi}\\sigma} e^{-\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)^2}\\]\nParameters:\n\n\\(\\mu\\) = mean (location)\n\\(\\sigma\\) = standard deviation (scale)\n\nMean: \\(E[X] = \\mu\\)\nVariance: \\(\\text{Var}(X) = \\sigma^2\\)\nNotation: \\(X \\sim \\mathcal{N}(\\mu, \\sigma^2)\\)\nApplications:\n\nHeights, weights, and other biological measurements\nMeasurement errors\nSampling distributions (via Central Limit Theorem)\nBasis for many statistical tests\n\n\n\nCode\nx &lt;- seq(-4, 8, length.out = 200)\nplot(x, dnorm(x, mean = 2, sd = 1), type = \"l\", lwd = 2, col = \"blue\",\n     xlab = \"x\", ylab = \"f(x)\", main = \"Normal Distribution\",\n     ylim = c(0, 0.45))\nlines(x, dnorm(x, mean = 2, sd = 0.5), lwd = 2, col = \"red\")\nlines(x, dnorm(x, mean = 2, sd = 2), lwd = 2, col = \"darkgreen\")\nlegend(\"topright\", c(\"μ=2, σ=1\", \"μ=2, σ=0.5\", \"μ=2, σ=2\"),\n       col = c(\"blue\", \"red\", \"darkgreen\"), lwd = 2)\n\n\n\n\n\n\n\n\nFigure 44.9: Normal distribution with mean μ=2 for different standard deviations (σ = 0.5, 1, and 2)\n\n\n\n\n\nKey properties:\n\n68% of values fall within 1 SD of the mean\n95% of values fall within 2 SDs of the mean\n99.7% of values fall within 3 SDs of the mean\n\nR functions: dnorm(), pnorm(), qnorm(), rnorm()\n\n\nSampling Process Example\nHuman height is influenced by many genetic and environmental factors that contribute additively. In a population of adult males, height follows approximately a normal distribution with mean \\(\\mu = 175\\) cm and standard deviation \\(\\sigma = 7\\) cm. When we measure heights of individuals, we are sampling from this distribution.\n\n\nCode\n# Simulate measuring heights in a population\nset.seed(606)\nmu_height &lt;- 175  # cm\nsigma_height &lt;- 7  # cm\n\n# Sample heights of 30 individuals\nn_individuals &lt;- 30\nheights &lt;- rnorm(n_individuals, mean = mu_height, sd = sigma_height)\ncat(\"Sample heights (cm):\", round(heights, 1), \"\\n\")\n\n\nSample heights (cm): 175.8 168.5 166.2 174 171 172.9 175.2 176.9 181.3 183.9 169.3 173.5 161.7 176.2 183.6 169.5 187.2 187.1 182.9 176.9 172 164.3 192.5 167.8 176.9 170.5 187.4 171.4 177.8 177.5 \n\n\nCode\ncat(\"Sample mean:\", round(mean(heights), 1), \"cm\\n\")\n\n\nSample mean: 175.7 cm\n\n\nCode\ncat(\"Sample SD:\", round(sd(heights), 1), \"cm\\n\")\n\n\nSample SD: 7.5 cm\n\n\nCode\ncat(\"Population parameters: μ =\", mu_height, \"cm, σ =\", sigma_height, \"cm\")\n\n\nPopulation parameters: μ = 175 cm, σ = 7 cm\n\n\n\n\nStandard Normal Distribution\nThe standard normal distribution is a normal distribution with \\(\\mu = 0\\) and \\(\\sigma = 1\\).\nAny normal variable \\(X \\sim \\mathcal{N}(\\mu, \\sigma^2)\\) can be converted to a standard normal \\(Z\\) using:\n\\[Z = \\frac{X - \\mu}{\\sigma}\\]\nThis z-score tells us how many standard deviations a value is from the mean.\n\n\nCode\nx &lt;- seq(-4, 4, length.out = 200)\ny &lt;- dnorm(x)\n\nplot(x, y, type = \"l\", lwd = 2,\n     xlab = \"z\", ylab = \"f(z)\",\n     main = \"Standard Normal Distribution\")\n\n# Shade areas\nx_1sd &lt;- seq(-1, 1, length.out = 100)\npolygon(c(-1, x_1sd, 1), c(0, dnorm(x_1sd), 0),\n        col = rgb(0, 0, 1, 0.3), border = NA)\n\nx_2sd &lt;- c(seq(-2, -1, length.out = 50), seq(1, 2, length.out = 50))\nfor (region in list(c(-2, -1), c(1, 2))) {\n  xx &lt;- seq(region[1], region[2], length.out = 50)\n  polygon(c(region[1], xx, region[2]), c(0, dnorm(xx), 0),\n          col = rgb(0, 1, 0, 0.3), border = NA)\n}\n\nlegend(\"topright\", c(\"68% within 1σ\", \"95% within 2σ\"),\n       fill = c(rgb(0, 0, 1, 0.3), rgb(0, 1, 0, 0.3)))\n\n\n\n\n\n\n\n\nFigure 44.10: Standard normal distribution showing 68% of values within 1 standard deviation and 95% within 2 standard deviations\n\n\n\n\n\n\n\nLog-Normal Distribution\nIf \\(\\ln(X)\\) follows a normal distribution, then \\(X\\) follows a log-normal distribution. This arises when effects are multiplicative rather than additive.\nProbability density function: \\[f(x) = \\frac{1}{x\\sigma\\sqrt{2\\pi}} e^{-\\frac{(\\ln x - \\mu)^2}{2\\sigma^2}}, \\quad x &gt; 0\\]\nParameters:\n\n\\(\\mu\\) = mean of the log-transformed variable\n\\(\\sigma\\) = SD of the log-transformed variable\n\nMean: \\(E[X] = e^{\\mu + \\sigma^2/2}\\)\nVariance: \\(\\text{Var}(X) = (e^{\\sigma^2} - 1)e^{2\\mu + \\sigma^2}\\)\nApplications:\n\nGene expression levels\nOrganism sizes\nConcentrations\nIncome distributions\n\n\n\nCode\npar(mfrow = c(1, 2))\n\n# Log-normal data\nset.seed(42)\nx &lt;- rlnorm(1000, meanlog = 1, sdlog = 0.5)\n\nhist(x, breaks = 30, main = \"Log-Normal Data\", col = \"lightblue\",\n     xlab = \"x\", probability = TRUE)\ncurve(dlnorm(x, meanlog = 1, sdlog = 0.5), add = TRUE, col = \"red\", lwd = 2)\n\n# After log transformation\nhist(log(x), breaks = 30, main = \"After Log Transform\", col = \"lightblue\",\n     xlab = \"log(x)\", probability = TRUE)\ncurve(dnorm(x, mean = 1, sd = 0.5), add = TRUE, col = \"red\", lwd = 2)\n\n\n\n\n\n\n\n\nFigure 44.11: Log-normal distribution (left) and its transformation to normal distribution after log transformation (right)\n\n\n\n\n\nR functions: dlnorm(), plnorm(), qlnorm(), rlnorm()\n\n\nSampling Process Example\nGene expression levels often follow a log-normal distribution because they result from multiplicative processes (transcription, translation, and degradation rates). When measuring mRNA levels of a particular gene across cells, if the log-transformed expression follows \\(\\mathcal{N}(\\mu = 2, \\sigma = 0.6)\\), the raw expression levels follow a log-normal distribution.\n\n\nCode\n# Simulate gene expression measurements\nset.seed(707)\nmu_log &lt;- 2\nsigma_log &lt;- 0.6\n\n# Sample expression levels from 50 cells\nn_cells &lt;- 50\nexpression_levels &lt;- rlnorm(n_cells, meanlog = mu_log, sdlog = sigma_log)\ncat(\"Expression levels (arbitrary units):\", round(expression_levels, 2), \"\\n\")\n\n\nExpression levels (arbitrary units): 3.42 5.1 11.24 14.64 4.51 3.97 25.53 9.63 3.63 21.63 9.5 14.48 8.16 4.31 4.81 3.68 18.24 9.31 7.71 13.99 5.13 4.18 7.56 7.94 11.77 28.58 6.69 16.92 5.49 9.77 10.61 15.75 12.96 5.43 7.88 11.54 6.1 6.11 8.73 7.17 3.86 10.07 8.18 9.41 4.12 6.45 25.62 8.32 13.5 5.49 \n\n\nCode\ncat(\"Mean expression:\", round(mean(expression_levels), 2), \"\\n\")\n\n\nMean expression: 9.78 \n\n\nCode\ncat(\"Median expression:\", round(median(expression_levels), 2), \"\\n\")\n\n\nMedian expression: 8.17 \n\n\nCode\n# Log-transformed data should be approximately normal\nlog_expression &lt;- log(expression_levels)\ncat(\"Mean of log-expression:\", round(mean(log_expression), 2), \"\\n\")\n\n\nMean of log-expression: 2.12 \n\n\nCode\ncat(\"SD of log-expression:\", round(sd(log_expression), 2))\n\n\nSD of log-expression: 0.55\n\n\n\n\nChi-Square Distribution\nThe chi-square distribution is the distribution of a sum of squared standard normal variables. It is fundamental to many statistical tests.\nParameters: \\(k\\) = degrees of freedom\nMean: \\(E[X] = k\\)\nVariance: \\(\\text{Var}(X) = 2k\\)\nApplications:\n\nGoodness-of-fit tests\nTests of independence\nVariance estimation\n\n\n\nCode\nx &lt;- seq(0, 20, length.out = 200)\nplot(x, dchisq(x, df = 2), type = \"l\", lwd = 2, col = \"blue\",\n     xlab = \"x\", ylab = \"f(x)\", main = \"Chi-Square Distribution\",\n     ylim = c(0, 0.5))\nlines(x, dchisq(x, df = 5), lwd = 2, col = \"red\")\nlines(x, dchisq(x, df = 10), lwd = 2, col = \"darkgreen\")\nlegend(\"topright\", c(\"df = 2\", \"df = 5\", \"df = 10\"),\n       col = c(\"blue\", \"red\", \"darkgreen\"), lwd = 2)\n\n\n\n\n\n\n\n\nFigure 44.12: Chi-square distribution for different degrees of freedom (df = 2, 5, and 10)\n\n\n\n\n\nR functions: dchisq(), pchisq(), qchisq(), rchisq()\n\n\nSampling Process Example\nIn a quality control study, a bioengineering lab measures the precision of a fabrication process by taking repeated measurements of device dimensions. The sample variance \\(s^2\\) from \\(n = 10\\) measurements is related to the chi-square distribution. Specifically, \\((n-1)s^2/\\sigma^2\\) follows a chi-square distribution with \\(k = n-1 = 9\\) degrees of freedom.\n\n\nCode\n# Simulate sampling process to understand variance distribution\nset.seed(808)\nn_measurements &lt;- 10\ntrue_sigma &lt;- 2.5  # true population standard deviation\n\n# Generate many samples and compute their variances\nn_samples &lt;- 1000\nsample_variances &lt;- replicate(n_samples, {\n  measurements &lt;- rnorm(n_measurements, mean = 100, sd = true_sigma)\n  var(measurements)\n})\n\n# The scaled variances follow chi-square\nscaled_variances &lt;- (n_measurements - 1) * sample_variances / true_sigma^2\ncat(\"Sample variance from one experiment:\", round(sample_variances[1], 2), \"\\n\")\n\n\nSample variance from one experiment: 11.59 \n\n\nCode\ncat(\"Mean of\", n_samples, \"sample variances:\", round(mean(sample_variances), 2), \"\\n\")\n\n\nMean of 1000 sample variances: 6.21 \n\n\nCode\ncat(\"True variance (σ²):\", true_sigma^2, \"\\n\")\n\n\nTrue variance (σ²): 6.25 \n\n\nCode\ncat(\"Mean of scaled variances:\", round(mean(scaled_variances), 2), \"\\n\")\n\n\nMean of scaled variances: 8.94 \n\n\nCode\ncat(\"Expected (df = \", n_measurements - 1, \"):\", n_measurements - 1)\n\n\nExpected (df =  9 ): 9\n\n\n\n\nStudent’s t Distribution\nThe t-distribution arises when estimating the mean of a normally distributed population when the sample size is small and the population standard deviation is unknown.\nParameters: \\(\\nu\\) = degrees of freedom\nAs \\(\\nu \\to \\infty\\), the t-distribution approaches the standard normal.\nApplications:\n\nt-tests\nConfidence intervals for means\nRobust regression\n\n\n\nCode\nx &lt;- seq(-4, 4, length.out = 200)\nplot(x, dnorm(x), type = \"l\", lwd = 2, col = \"black\", lty = 2,\n     xlab = \"x\", ylab = \"f(x)\", main = \"t-Distribution vs Normal\")\nlines(x, dt(x, df = 2), lwd = 2, col = \"blue\")\nlines(x, dt(x, df = 5), lwd = 2, col = \"red\")\nlines(x, dt(x, df = 30), lwd = 2, col = \"darkgreen\")\nlegend(\"topright\", c(\"Normal\", \"t (df=2)\", \"t (df=5)\", \"t (df=30)\"),\n       col = c(\"black\", \"blue\", \"red\", \"darkgreen\"), lwd = 2, lty = c(2, 1, 1, 1))\n\n\n\n\n\n\n\n\nFigure 44.13: Student’s t-distribution compared to normal distribution for different degrees of freedom\n\n\n\n\n\nR functions: dt(), pt(), qt(), rt()\n\n\nSampling Process Example\nA pharmacologist is testing the effect of a new drug on blood pressure. She samples \\(n = 8\\) patients and measures their blood pressure after treatment. The population mean \\(\\mu\\) and standard deviation \\(\\sigma\\) are unknown. To test whether the mean differs from a reference value, she computes a t-statistic: \\(t = \\frac{\\bar{x} - \\mu_0}{s/\\sqrt{n}}\\), which follows a t-distribution with \\(\\nu = n-1 = 7\\) degrees of freedom under the null hypothesis.\n\n\nCode\n# Simulate t-statistic sampling distribution\nset.seed(909)\nn_patients &lt;- 8\ntrue_mu &lt;- 120  # true mean blood pressure\ntrue_sigma &lt;- 12  # true SD\nmu_0 &lt;- 120  # null hypothesis value (no change)\n\n# Simulate many experiments\nn_experiments &lt;- 1000\nt_statistics &lt;- replicate(n_experiments, {\n  blood_pressures &lt;- rnorm(n_patients, mean = true_mu, sd = true_sigma)\n  sample_mean &lt;- mean(blood_pressures)\n  sample_sd &lt;- sd(blood_pressures)\n  # Compute t-statistic\n  (sample_mean - mu_0) / (sample_sd / sqrt(n_patients))\n})\n\ncat(\"First 10 t-statistics:\", round(t_statistics[1:10], 2), \"\\n\")\n\n\nFirst 10 t-statistics: -1.22 2.32 2.34 0.1 -2.1 -0.81 1 -2.88 0.9 -0.91 \n\n\nCode\ncat(\"Mean of t-statistics:\", round(mean(t_statistics), 2), \"\\n\")\n\n\nMean of t-statistics: 0.02 \n\n\nCode\ncat(\"SD of t-statistics:\", round(sd(t_statistics), 2), \"\\n\")\n\n\nSD of t-statistics: 1.19 \n\n\nCode\ncat(\"Expected SD for t with df=7:\", round(sqrt(7/(7-2)), 2))\n\n\nExpected SD for t with df=7: 1.18\n\n\n\n\nF Distribution\nThe F-distribution is the ratio of two chi-square distributions divided by their degrees of freedom. It is used in ANOVA and comparing variances.\nParameters:\n\n\\(d_1\\) = numerator degrees of freedom\n\\(d_2\\) = denominator degrees of freedom\n\nApplications:\n\nANOVA F-tests\nComparing variances\nRegression significance tests\n\n\n\nCode\nx &lt;- seq(0, 5, length.out = 200)\nplot(x, df(x, df1 = 5, df2 = 20), type = \"l\", lwd = 2, col = \"blue\",\n     xlab = \"x\", ylab = \"f(x)\", main = \"F Distribution\",\n     ylim = c(0, 1))\nlines(x, df(x, df1 = 10, df2 = 20), lwd = 2, col = \"red\")\nlines(x, df(x, df1 = 20, df2 = 20), lwd = 2, col = \"darkgreen\")\nlegend(\"topright\", c(\"F(5,20)\", \"F(10,20)\", \"F(20,20)\"),\n       col = c(\"blue\", \"red\", \"darkgreen\"), lwd = 2)\n\n\n\n\n\n\n\n\nFigure 44.14: F distribution for different numerator and denominator degrees of freedom\n\n\n\n\n\nR functions: df(), pf(), qf(), rf()\n\n\nSampling Process Example\nAn ecologist is comparing the variability in body mass between two populations of birds. Population 1 has \\(n_1 = 12\\) individuals and population 2 has \\(n_2 = 15\\) individuals. To test whether the two populations have equal variances, she computes the F-statistic: \\(F = \\frac{s_1^2}{s_2^2}\\), which follows an F-distribution with \\(d_1 = n_1 - 1 = 11\\) and \\(d_2 = n_2 - 1 = 14\\) degrees of freedom when the true variances are equal.\n\n\nCode\n# Simulate F-statistic sampling distribution\nset.seed(1010)\nn1 &lt;- 12\nn2 &lt;- 15\ntrue_sigma1 &lt;- 5  # true SD for population 1\ntrue_sigma2 &lt;- 5  # true SD for population 2 (equal variances)\n\n# Simulate many paired samples\nn_comparisons &lt;- 1000\nF_statistics &lt;- replicate(n_comparisons, {\n  pop1_masses &lt;- rnorm(n1, mean = 100, sd = true_sigma1)\n  pop2_masses &lt;- rnorm(n2, mean = 100, sd = true_sigma2)\n  var1 &lt;- var(pop1_masses)\n  var2 &lt;- var(pop2_masses)\n  # Compute F-statistic\n  var1 / var2\n})\n\ncat(\"First 10 F-statistics:\", round(F_statistics[1:10], 2), \"\\n\")\n\n\nFirst 10 F-statistics: 2.64 1.26 1.01 1.55 0.55 0.46 0.88 1.67 2.88 1.16 \n\n\nCode\ncat(\"Mean of F-statistics:\", round(mean(F_statistics), 2), \"\\n\")\n\n\nMean of F-statistics: 1.14 \n\n\nCode\ncat(\"Expected mean for F(11,14):\", round(14/(14-2), 2))\n\n\nExpected mean for F(11,14): 1.17\n\n\n\n\nBeta Distribution\nThe beta distribution is defined on the interval [0, 1] and is useful for modeling proportions and probabilities.\nParameters:\n\n\\(\\alpha\\) = shape parameter 1\n\\(\\beta\\) = shape parameter 2\n\nMean: \\(E[X] = \\frac{\\alpha}{\\alpha + \\beta}\\)\nVariance: \\(\\text{Var}(X) = \\frac{\\alpha\\beta}{(\\alpha+\\beta)^2(\\alpha+\\beta+1)}\\)\nApplications:\n\nPrior distributions for probabilities (Bayesian analysis)\nModeling proportions\nAllele frequencies\n\n\n\nCode\nx &lt;- seq(0, 1, length.out = 200)\nplot(x, dbeta(x, 2, 5), type = \"l\", lwd = 2, col = \"blue\",\n     xlab = \"x\", ylab = \"f(x)\", main = \"Beta Distribution\",\n     ylim = c(0, 3))\nlines(x, dbeta(x, 5, 2), lwd = 2, col = \"red\")\nlines(x, dbeta(x, 2, 2), lwd = 2, col = \"darkgreen\")\nlines(x, dbeta(x, 0.5, 0.5), lwd = 2, col = \"purple\")\nlegend(\"top\", c(\"Beta(2,5)\", \"Beta(5,2)\", \"Beta(2,2)\", \"Beta(0.5,0.5)\"),\n       col = c(\"blue\", \"red\", \"darkgreen\", \"purple\"), lwd = 2)\n\n\n\n\n\n\n\n\nFigure 44.15: Beta distribution for different shape parameters showing various distribution shapes\n\n\n\n\n\nR functions: dbeta(), pbeta(), qbeta(), rbeta()\n\n\nSampling Process Example\nIn population genetics, a researcher is estimating the frequency of a particular allele in a population. Based on prior studies, she has some information about the likely allele frequency, which can be encoded as a beta prior distribution. For example, if previous evidence suggests the allele is moderately common but variable, she might use a Beta(5, 3) distribution to represent her prior beliefs about the allele frequency \\(p\\) before collecting new data.\n\n\nCode\n# Simulate prior distributions for allele frequencies\nset.seed(1111)\nalpha_param &lt;- 5\nbeta_param &lt;- 3\n\n# Sample possible allele frequencies from prior\nn_simulations &lt;- 1000\nprior_frequencies &lt;- rbeta(n_simulations, shape1 = alpha_param, shape2 = beta_param)\ncat(\"Sample of prior allele frequencies:\", round(prior_frequencies[1:10], 3), \"\\n\")\n\n\nSample of prior allele frequencies: 0.642 0.337 0.371 0.602 0.484 0.384 0.322 0.796 0.873 0.905 \n\n\nCode\ncat(\"Mean prior frequency:\", round(mean(prior_frequencies), 3), \"\\n\")\n\n\nMean prior frequency: 0.628 \n\n\nCode\ncat(\"Expected mean (α/(α+β)):\", round(alpha_param/(alpha_param + beta_param), 3), \"\\n\")\n\n\nExpected mean (α/(α+β)): 0.625 \n\n\nCode\n# In practice, these could be updated with observed data using Bayesian inference\n# For example: observing 15 allele A and 5 allele B would update to Beta(5+15, 3+5)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>44</span>  <span class='chapter-title'>Common Probability Distributions</span>"
    ]
  },
  {
    "objectID": "chapters/A3-probability-distributions.html#summary-table",
    "href": "chapters/A3-probability-distributions.html#summary-table",
    "title": "44  Common Probability Distributions",
    "section": "44.3 Summary Table",
    "text": "44.3 Summary Table\n\n\n\n\n\n\n\n\n\n\n\nDistribution\nType\nParameters\nMean\nVariance\nR prefix\n\n\n\n\nBernoulli\nDiscrete\np\np\np(1-p)\nbinom (n=1)\n\n\nBinomial\nDiscrete\nn, p\nnp\nnp(1-p)\nbinom\n\n\nGeometric\nDiscrete\np\n1/p\n(1-p)/p²\ngeom\n\n\nNeg. Binomial\nDiscrete\nr, p\nr/p\nr(1-p)/p²\nnbinom\n\n\nPoisson\nDiscrete\nλ\nλ\nλ\npois\n\n\nUniform\nContinuous\na, b\n(a+b)/2\n(b-a)²/12\nunif\n\n\nExponential\nContinuous\nλ\n1/λ\n1/λ²\nexp\n\n\nGamma\nContinuous\nr, λ\nr/λ\nr/λ²\ngamma\n\n\nNormal\nContinuous\nμ, σ\nμ\nσ²\nnorm\n\n\nLog-normal\nContinuous\nμ, σ\nexp(μ+σ²/2)\n(exp(σ²)-1)exp(2μ+σ²)\nlnorm\n\n\nChi-square\nContinuous\nk\nk\n2k\nchisq\n\n\nt\nContinuous\nν\n0 (ν&gt;1)\nν/(ν-2) (ν&gt;2)\nt\n\n\nF\nContinuous\nd₁, d₂\nd₂/(d₂-2)\n…\nf\n\n\nBeta\nContinuous\nα, β\nα/(α+β)\n…\nbeta",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>44</span>  <span class='chapter-title'>Common Probability Distributions</span>"
    ]
  },
  {
    "objectID": "chapters/A3-probability-distributions.html#r-function-naming-convention",
    "href": "chapters/A3-probability-distributions.html#r-function-naming-convention",
    "title": "44  Common Probability Distributions",
    "section": "44.4 R Function Naming Convention",
    "text": "44.4 R Function Naming Convention\nR uses a consistent naming convention for distribution functions:\n\ndxxx - density/mass function (PDF/PMF)\npxxx - cumulative distribution function (CDF)\nqxxx - quantile function (inverse CDF)\nrxxx - random number generation\n\nFor example, for the normal distribution:\n\n\nCode\n# Density at x = 0 for standard normal\ndnorm(0)\n\n\n[1] 0.3989423\n\n\nCode\n# Probability that X ≤ 1.96\npnorm(1.96)\n\n\n[1] 0.9750021\n\n\nCode\n# Value where P(X ≤ q) = 0.975\nqnorm(0.975)\n\n\n[1] 1.959964\n\n\nCode\n# Generate 5 random normal values\nset.seed(42)\nrnorm(5)\n\n\n[1]  1.3709584 -0.5646982  0.3631284  0.6328626  0.4042683",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>44</span>  <span class='chapter-title'>Common Probability Distributions</span>"
    ]
  },
  {
    "objectID": "chapters/A3-probability-distributions.html#choosing-the-right-distribution",
    "href": "chapters/A3-probability-distributions.html#choosing-the-right-distribution",
    "title": "44  Common Probability Distributions",
    "section": "44.5 Choosing the Right Distribution",
    "text": "44.5 Choosing the Right Distribution\n\n\n\n\n\n\n\n\nFigure 44.16: Decision flowchart for choosing the appropriate probability distribution based on data type and characteristics\n\n\n\n\n\nGuidelines for selection:\n\nIs your variable discrete or continuous?\n\nDiscrete: Binomial, Poisson, Negative Binomial\nContinuous: Normal, Gamma, Beta, etc.\n\nFor counts:\n\nFixed number of trials with yes/no: Binomial\nEvents per unit time/space, rare: Poisson\nOverdispersed counts (variance &gt; mean): Negative Binomial\n\nFor continuous measurements:\n\nSymmetric, unbounded: Normal\nPositive, right-skewed: Log-normal or Gamma\nBounded between 0 and 1: Beta\nWaiting times: Exponential or Gamma\n\nFor testing:\n\nMeans (unknown variance): t-distribution\nVariances: Chi-square or F-distribution",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>44</span>  <span class='chapter-title'>Common Probability Distributions</span>"
    ]
  },
  {
    "objectID": "chapters/A5-sampling-distributions.html",
    "href": "chapters/A5-sampling-distributions.html",
    "title": "45  Sampling Distributions in Hypothesis Testing",
    "section": "",
    "text": "45.1 Why Sampling Distributions Matter\nThis appendix provides a reference for the statistical distributions used in hypothesis testing. While Chapter 44 covers probability distributions for modeling data, this appendix focuses on sampling distributions—the theoretical distributions that test statistics follow under the null hypothesis.\nWhen we conduct a hypothesis test, we calculate a test statistic from our sample data. To determine whether this statistic is “unusual,” we need to know what values to expect if the null hypothesis were true. The sampling distribution tells us exactly this—it’s the distribution of the test statistic across all possible samples.\nCode\n# Demonstrate: sampling distribution of the mean\nset.seed(42)\npopulation &lt;- rnorm(100000, mean = 100, sd = 15)\n\n# Take many samples and compute means\nsample_means &lt;- replicate(5000, mean(sample(population, 30)))\n\nhist(sample_means, breaks = 40, col = \"lightblue\",\n     main = \"Sampling Distribution of the Mean\",\n     xlab = \"Sample Mean (n = 30)\",\n     probability = TRUE)\n\n# Overlay theoretical normal\nx &lt;- seq(min(sample_means), max(sample_means), length.out = 100)\nlines(x, dnorm(x, mean = 100, sd = 15/sqrt(30)), col = \"red\", lwd = 2)\n\nlegend(\"topright\",\n       legend = c(\"Simulated\", \"Theoretical\"),\n       fill = c(\"lightblue\", NA),\n       border = c(\"black\", NA),\n       lty = c(NA, 1), lwd = c(NA, 2),\n       col = c(NA, \"red\"))\n\n\n\n\n\n\n\n\nFigure 45.1: Sampling distribution of the mean based on 5000 samples of size 30, showing close agreement with the theoretical normal distribution",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>45</span>  <span class='chapter-title'>Sampling Distributions in Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "chapters/A5-sampling-distributions.html#the-standard-normal-z-distribution",
    "href": "chapters/A5-sampling-distributions.html#the-standard-normal-z-distribution",
    "title": "45  Sampling Distributions in Hypothesis Testing",
    "section": "45.2 The Standard Normal (Z) Distribution",
    "text": "45.2 The Standard Normal (Z) Distribution\n\nWhen It’s Used\nThe standard normal distribution is used when:\n\nTesting means with known population variance\nLarge samples (n &gt; 30) where CLT applies\nTesting proportions with large samples\n\n\n\nThe Distribution\n\\[Z = \\frac{\\bar{X} - \\mu}{\\sigma / \\sqrt{n}}\\]\nUnder \\(H_0\\), \\(Z \\sim N(0, 1)\\)\n\n\nCode\nx &lt;- seq(-4, 4, length.out = 200)\ny &lt;- dnorm(x)\n\nplot(x, y, type = \"l\", lwd = 2,\n     xlab = \"z\", ylab = \"Density\",\n     main = \"Standard Normal Distribution\")\n\n# Shade rejection regions (two-tailed, α = 0.05)\nx_left &lt;- seq(-4, -1.96, length.out = 50)\nx_right &lt;- seq(1.96, 4, length.out = 50)\n\npolygon(c(-4, x_left, -1.96), c(0, dnorm(x_left), 0),\n        col = rgb(1, 0, 0, 0.3), border = NA)\npolygon(c(1.96, x_right, 4), c(0, dnorm(x_right), 0),\n        col = rgb(1, 0, 0, 0.3), border = NA)\n\nabline(v = c(-1.96, 1.96), lty = 2, col = \"red\")\ntext(0, 0.15, \"95%\\nAcceptance\\nRegion\", cex = 0.9)\ntext(-2.8, 0.05, \"2.5%\", col = \"red\")\ntext(2.8, 0.05, \"2.5%\", col = \"red\")\n\n\n\n\n\n\n\n\nFigure 45.2: Standard normal distribution with rejection regions for a two-tailed test at α = 0.05\n\n\n\n\n\n\n\nCritical Values\n\n\n\nConfidence Level\nTwo-tailed α\nCritical Z\n\n\n\n\n90%\n0.10\n±1.645\n\n\n95%\n0.05\n±1.960\n\n\n99%\n0.01\n±2.576\n\n\n\n\n\nCode\n# R functions for Z distribution\nqnorm(0.975)  # 97.5th percentile (for two-tailed 95% CI)\n\n\n[1] 1.959964\n\n\nCode\npnorm(1.96)   # Probability below z = 1.96\n\n\n[1] 0.9750021",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>45</span>  <span class='chapter-title'>Sampling Distributions in Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "chapters/A5-sampling-distributions.html#students-t-distribution",
    "href": "chapters/A5-sampling-distributions.html#students-t-distribution",
    "title": "45  Sampling Distributions in Hypothesis Testing",
    "section": "45.3 Student’s t-Distribution",
    "text": "45.3 Student’s t-Distribution\n\nWhen It’s Used\nThe t-distribution is used when:\n\nTesting means with unknown population variance (estimated from sample)\nComparing two means (two-sample t-test)\nTesting regression coefficients\nSmall to moderate sample sizes\n\n\n\nThe Distribution\n\\[t = \\frac{\\bar{X} - \\mu}{s / \\sqrt{n}}\\]\nUnder \\(H_0\\), \\(t \\sim t_{df}\\) where \\(df = n - 1\\) for one-sample tests.\n\n\nEffect of Degrees of Freedom\nThe t-distribution has heavier tails than the normal, reflecting additional uncertainty from estimating variance. As df increases, t approaches normal:\n\n\nCode\nx &lt;- seq(-4, 4, length.out = 200)\n\nplot(x, dnorm(x), type = \"l\", lwd = 2, col = \"black\",\n     xlab = \"t\", ylab = \"Density\",\n     main = \"t-Distribution: Effect of Degrees of Freedom\")\nlines(x, dt(x, df = 3), lwd = 2, col = \"red\")\nlines(x, dt(x, df = 10), lwd = 2, col = \"blue\")\nlines(x, dt(x, df = 30), lwd = 2, col = \"darkgreen\")\n\nlegend(\"topright\",\n       legend = c(\"Normal (df = ∞)\", \"t (df = 3)\", \"t (df = 10)\", \"t (df = 30)\"),\n       col = c(\"black\", \"red\", \"blue\", \"darkgreen\"),\n       lwd = 2)\n\n\n\n\n\n\n\n\nFigure 45.3: t-distribution for different degrees of freedom showing convergence to the normal distribution as df increases\n\n\n\n\n\nNotice how df = 3 has much heavier tails (more extreme values expected), while df = 30 is nearly indistinguishable from the normal.\n\n\nCritical Values Change with df\n\n\nCode\n# Critical t-values for 95% CI (two-tailed)\ndfs &lt;- c(5, 10, 20, 30, 50, 100, Inf)\nt_crits &lt;- qt(0.975, df = dfs)\n\ndata.frame(\n  df = dfs,\n  critical_t = round(t_crits, 3)\n)\n\n\n   df critical_t\n1   5      2.571\n2  10      2.228\n3  20      2.086\n4  30      2.042\n5  50      2.009\n6 100      1.984\n7 Inf      1.960\n\n\n\n\nPractical Implications\n\n\nCode\n# How confidence interval width depends on sample size\nn_values &lt;- seq(5, 100, by = 5)\nci_multipliers &lt;- qt(0.975, df = n_values - 1)\n\nplot(n_values, ci_multipliers, type = \"b\", pch = 19, col = \"blue\",\n     xlab = \"Sample Size (n)\", ylab = \"t Critical Value (α = 0.05)\",\n     main = \"Why Larger Samples Give Narrower CIs\")\nabline(h = 1.96, lty = 2, col = \"red\")\ntext(80, 2.05, \"Z = 1.96 (infinite df)\", col = \"red\")\n\n\n\n\n\n\n\n\nFigure 45.4: Critical t-values decrease as sample size increases, approaching the Z critical value of 1.96\n\n\n\n\n\nWith small samples, we need a larger critical value to achieve the same confidence level, making confidence intervals wider.\n\n\nR Functions\n\n\nCode\n# t-distribution functions\nqt(0.975, df = 10)    # Critical value for 95% CI with df = 10\n\n\n[1] 2.228139\n\n\nCode\npt(2.228, df = 10)    # Probability below t = 2.228\n\n\n[1] 0.9749941\n\n\nCode\ndt(0, df = 10)        # Density at t = 0\n\n\n[1] 0.3891084",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>45</span>  <span class='chapter-title'>Sampling Distributions in Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "chapters/A5-sampling-distributions.html#chi-square-χ²-distribution",
    "href": "chapters/A5-sampling-distributions.html#chi-square-χ²-distribution",
    "title": "45  Sampling Distributions in Hypothesis Testing",
    "section": "45.4 Chi-Square (χ²) Distribution",
    "text": "45.4 Chi-Square (χ²) Distribution\n\nWhen It’s Used\nThe chi-square distribution is used for:\n\nGoodness of fit tests (observed vs. expected frequencies)\nTests of independence (contingency tables)\nTesting variance (one population)\nModel fit in regression (deviance tests)\n\n\n\nThe Distribution\nThe chi-square distribution is the sum of squared standard normal variables:\n\\[\\chi^2 = \\sum_{i=1}^{k} Z_i^2\\]\nThe distribution is always positive and right-skewed. As df increases, it becomes more symmetric and approaches normality.\n\n\nCode\nx &lt;- seq(0, 30, length.out = 200)\n\nplot(x, dchisq(x, df = 2), type = \"l\", lwd = 2, col = \"red\",\n     xlab = expression(chi^2), ylab = \"Density\",\n     main = expression(paste(chi^2, \" Distribution\")),\n     ylim = c(0, 0.3))\nlines(x, dchisq(x, df = 5), lwd = 2, col = \"blue\")\nlines(x, dchisq(x, df = 10), lwd = 2, col = \"darkgreen\")\nlines(x, dchisq(x, df = 20), lwd = 2, col = \"purple\")\n\nlegend(\"topright\",\n       legend = c(\"df = 2\", \"df = 5\", \"df = 10\", \"df = 20\"),\n       col = c(\"red\", \"blue\", \"darkgreen\", \"purple\"),\n       lwd = 2)\n\n\n\n\n\n\n\n\nFigure 45.5: Chi-square distribution for different degrees of freedom showing increasing symmetry with higher df\n\n\n\n\n\n\n\nProperties\n\nMean: \\(E[\\chi^2] = df\\)\nVariance: \\(Var(\\chi^2) = 2 \\times df\\)\nAlways positive (sums of squares)\nRight-skewed, especially for small df\n\n\n\nCritical Values for Common Tests\n\n\nCode\n# Chi-square critical values (right-tail, α = 0.05)\ndfs &lt;- c(1, 2, 3, 5, 10, 20)\nchi_crits &lt;- qchisq(0.95, df = dfs)\n\ndata.frame(\n  df = dfs,\n  critical_chi_sq = round(chi_crits, 3),\n  mean = dfs  # Note: critical value is close to df + 2*sqrt(2*df)\n)\n\n\n  df critical_chi_sq mean\n1  1           3.841    1\n2  2           5.991    2\n3  3           7.815    3\n4  5          11.070    5\n5 10          18.307   10\n6 20          31.410   20\n\n\n\n\nR Functions\n\n\nCode\n# Chi-square distribution functions\nqchisq(0.95, df = 5)      # Critical value (right-tail α = 0.05)\n\n\n[1] 11.0705\n\n\nCode\n1 - pchisq(11.07, df = 5) # p-value for chi-square = 11.07\n\n\n[1] 0.05000962\n\n\nCode\ndchisq(5, df = 5)         # Density at chi-square = 5\n\n\n[1] 0.1220415",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>45</span>  <span class='chapter-title'>Sampling Distributions in Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "chapters/A5-sampling-distributions.html#f-distribution",
    "href": "chapters/A5-sampling-distributions.html#f-distribution",
    "title": "45  Sampling Distributions in Hypothesis Testing",
    "section": "45.5 F Distribution",
    "text": "45.5 F Distribution\n\nWhen It’s Used\nThe F distribution is used for:\n\nComparing two variances (F-test)\nANOVA (comparing means of multiple groups)\nTesting overall significance in regression\nComparing nested models\n\n\n\nThe Distribution\nThe F distribution is the ratio of two chi-square distributions:\n\\[F = \\frac{\\chi^2_1 / df_1}{\\chi^2_2 / df_2}\\]\n\n\\(df_1\\): numerator degrees of freedom (between-groups)\n\\(df_2\\): denominator degrees of freedom (within-groups or error)\n\n\n\nCode\nx &lt;- seq(0, 5, length.out = 200)\n\nplot(x, df(x, df1 = 1, df2 = 10), type = \"l\", lwd = 2, col = \"red\",\n     xlab = \"F\", ylab = \"Density\",\n     main = \"F Distribution\",\n     ylim = c(0, 1))\nlines(x, df(x, df1 = 5, df2 = 10), lwd = 2, col = \"blue\")\nlines(x, df(x, df1 = 10, df2 = 10), lwd = 2, col = \"darkgreen\")\nlines(x, df(x, df1 = 10, df2 = 50), lwd = 2, col = \"purple\")\n\nlegend(\"topright\",\n       legend = c(\"F(1,10)\", \"F(5,10)\", \"F(10,10)\", \"F(10,50)\"),\n       col = c(\"red\", \"blue\", \"darkgreen\", \"purple\"),\n       lwd = 2)\n\n\n\n\n\n\n\n\nFigure 45.6: F distribution for different combinations of numerator and denominator degrees of freedom\n\n\n\n\n\n\n\nUnderstanding F in ANOVA\nIn ANOVA, F is the ratio of between-group variance to within-group variance:\n\\[F = \\frac{MS_{between}}{MS_{within}} = \\frac{\\text{Signal}}{\\text{Noise}}\\]\n\nLarge F: Groups differ more than expected from random variation\nF ≈ 1: Group differences are similar to within-group variation\n\n\n\nCode\n# Visualize rejection region for ANOVA\nx &lt;- seq(0, 6, length.out = 200)\ny &lt;- df(x, df1 = 3, df2 = 20)  # 4 groups, total n = 24\n\nplot(x, y, type = \"l\", lwd = 2,\n     xlab = \"F\", ylab = \"Density\",\n     main = \"F(3, 20) Distribution for One-Way ANOVA\")\n\n# Critical value and rejection region\nf_crit &lt;- qf(0.95, df1 = 3, df2 = 20)\nx_reject &lt;- seq(f_crit, 6, length.out = 50)\npolygon(c(f_crit, x_reject, 6), c(0, df(x_reject, 3, 20), 0),\n        col = rgb(1, 0, 0, 0.3), border = NA)\n\nabline(v = f_crit, lty = 2, col = \"red\")\ntext(f_crit + 0.3, 0.3, paste(\"F* =\", round(f_crit, 2)), col = \"red\")\ntext(4.5, 0.05, \"Rejection\\nRegion\\n(α = 0.05)\", col = \"red\")\n\n\n\n\n\n\n\n\nFigure 45.7: F distribution with df1=3 and df2=20 showing the rejection region for a one-way ANOVA at α = 0.05\n\n\n\n\n\n\n\nCritical Values Table\n\n\nCode\n# F critical values for α = 0.05 (common ANOVA scenarios)\n# Rows: numerator df (groups - 1)\n# Columns: denominator df (total n - groups)\n\ndf1_vals &lt;- c(1, 2, 3, 4, 5)\ndf2_vals &lt;- c(10, 20, 30, 60, 120)\n\nf_table &lt;- outer(df1_vals, df2_vals,\n                 function(d1, d2) round(qf(0.95, d1, d2), 2))\nrownames(f_table) &lt;- paste(\"df1 =\", df1_vals)\ncolnames(f_table) &lt;- paste(\"df2 =\", df2_vals)\n\ncat(\"F Critical Values (α = 0.05)\\n\")\n\n\nF Critical Values (α = 0.05)\n\n\nCode\nprint(f_table)\n\n\n        df2 = 10 df2 = 20 df2 = 30 df2 = 60 df2 = 120\ndf1 = 1     4.96     4.35     4.17     4.00      3.92\ndf1 = 2     4.10     3.49     3.32     3.15      3.07\ndf1 = 3     3.71     3.10     2.92     2.76      2.68\ndf1 = 4     3.48     2.87     2.69     2.53      2.45\ndf1 = 5     3.33     2.71     2.53     2.37      2.29\n\n\n\n\nR Functions\n\n\nCode\n# F distribution functions\nqf(0.95, df1 = 3, df2 = 20)  # Critical F for ANOVA\n\n\n[1] 3.098391\n\n\nCode\n1 - pf(3.5, df1 = 3, df2 = 20)  # p-value for F = 3.5\n\n\n[1] 0.0344931",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>45</span>  <span class='chapter-title'>Sampling Distributions in Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "chapters/A5-sampling-distributions.html#relationships-between-distributions",
    "href": "chapters/A5-sampling-distributions.html#relationships-between-distributions",
    "title": "45  Sampling Distributions in Hypothesis Testing",
    "section": "45.6 Relationships Between Distributions",
    "text": "45.6 Relationships Between Distributions\nThese distributions are mathematically related:\n\n\n\n\n\n\n\n\nFigure 45.8: Mathematical relationships between the normal, chi-square, t, and F distributions\n\n\n\n\n\nKey relationships:\n\nt² = F(1, df): A squared t-statistic follows an F distribution with 1 numerator df\nχ² → Normal: As df increases, chi-square approaches normality\nt → Z: As df → ∞, t-distribution becomes standard normal\nF(1, ∞) = χ²(1): Limiting case of F distribution",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>45</span>  <span class='chapter-title'>Sampling Distributions in Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "chapters/A5-sampling-distributions.html#choosing-the-right-distribution",
    "href": "chapters/A5-sampling-distributions.html#choosing-the-right-distribution",
    "title": "45  Sampling Distributions in Hypothesis Testing",
    "section": "45.7 Choosing the Right Distribution",
    "text": "45.7 Choosing the Right Distribution\n\n\n\nTest\nDistribution\nDegrees of Freedom\n\n\n\n\nZ-test (known σ)\nNormal\nN/A\n\n\nOne-sample t-test\nt\nn - 1\n\n\nTwo-sample t-test\nt\nn₁ + n₂ - 2 (pooled)\n\n\nPaired t-test\nt\nn - 1\n\n\nChi-square GOF\nχ²\nk - 1\n\n\nChi-square independence\nχ²\n(r-1)(c-1)\n\n\nOne-way ANOVA\nF\nk-1, N-k\n\n\nRegression F-test\nF\np, n-p-1\n\n\nRegression coefficient\nt\nn - p - 1\n\n\n\nwhere k = number of groups/categories, n = sample size, p = number of predictors",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>45</span>  <span class='chapter-title'>Sampling Distributions in Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "chapters/A5-sampling-distributions.html#degrees-of-freedom-intuition",
    "href": "chapters/A5-sampling-distributions.html#degrees-of-freedom-intuition",
    "title": "45  Sampling Distributions in Hypothesis Testing",
    "section": "45.8 Degrees of Freedom: Intuition",
    "text": "45.8 Degrees of Freedom: Intuition\nDegrees of freedom represent the number of independent pieces of information available for estimation. They decrease when we estimate parameters from the data:\n\nSample mean: Uses 1 df → leaves n-1 for variance estimation\nTwo groups: Estimate 2 means → lose 2 df from total\nRegression: Estimate p+1 coefficients → leaves n-p-1 error df\n\n\n\nCode\n# Demonstration: Why df matters\n# Sampling distribution of sample variance with different n\n\nset.seed(42)\ntrue_variance &lt;- 100\n\nsimulate_s2 &lt;- function(n, reps = 5000) {\n  replicate(reps, var(rnorm(n, mean = 0, sd = 10)))\n}\n\ns2_small &lt;- simulate_s2(5)    # df = 4\ns2_medium &lt;- simulate_s2(20)  # df = 19\ns2_large &lt;- simulate_s2(50)   # df = 49\n\npar(mfrow = c(1, 3))\nhist(s2_small, breaks = 30, main = \"n = 5 (df = 4)\",\n     xlab = \"Sample Variance\", col = \"lightblue\", xlim = c(0, 400))\nabline(v = 100, col = \"red\", lwd = 2)\n\nhist(s2_medium, breaks = 30, main = \"n = 20 (df = 19)\",\n     xlab = \"Sample Variance\", col = \"lightblue\", xlim = c(0, 400))\nabline(v = 100, col = \"red\", lwd = 2)\n\nhist(s2_large, breaks = 30, main = \"n = 50 (df = 49)\",\n     xlab = \"Sample Variance\", col = \"lightblue\", xlim = c(0, 400))\nabline(v = 100, col = \"red\", lwd = 2)\n\n\n\n\n\n\n\n\nFigure 45.9: Sampling distributions of sample variance for different sample sizes, showing increased precision with higher degrees of freedom\n\n\n\n\n\nWith more degrees of freedom: - Variance estimates are more precise (narrower distribution) - More likely to be close to the true value - Critical values move closer to their limiting values",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>45</span>  <span class='chapter-title'>Sampling Distributions in Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "chapters/A5-sampling-distributions.html#summary",
    "href": "chapters/A5-sampling-distributions.html#summary",
    "title": "45  Sampling Distributions in Hypothesis Testing",
    "section": "45.9 Summary",
    "text": "45.9 Summary\n\n\n\nDistribution\nParameters\nMean\nUse For\n\n\n\n\nNormal (Z)\nNone\n0\nMeans (known σ), proportions\n\n\nt\ndf\n0\nMeans (unknown σ), regression\n\n\nChi-square\ndf\ndf\nFrequencies, variance, GOF\n\n\nF\ndf₁, df₂\ndf₂/(df₂-2)\nANOVA, comparing variances\n\n\n\nRemember: - More data (higher df) → distributions approach their limits - The t approaches Z, χ² becomes symmetric, F becomes more peaked - Heavier tails in t and F require larger critical values for small df - These distributions assume normality of underlying data (robustness varies)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>45</span>  <span class='chapter-title'>Sampling Distributions in Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "chapters/A10-matrix-algebra.html",
    "href": "chapters/A10-matrix-algebra.html",
    "title": "46  Matrix Algebra Fundamentals",
    "section": "",
    "text": "46.1 Introduction\nMatrix algebra (linear algebra) is the mathematical foundation for many statistical and machine learning methods. Understanding matrices is essential for:\nThis appendix introduces the key concepts and R operations you need to work with matrices effectively.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>46</span>  <span class='chapter-title'>Matrix Algebra Fundamentals</span>"
    ]
  },
  {
    "objectID": "chapters/A10-matrix-algebra.html#introduction",
    "href": "chapters/A10-matrix-algebra.html#introduction",
    "title": "46  Matrix Algebra Fundamentals",
    "section": "",
    "text": "Dimensionality reduction techniques like PCA (Chapter 33)\nClassification methods (Chapter 31)\nRegression and regularization (Chapter 29)\nWorking efficiently with large datasets",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>46</span>  <span class='chapter-title'>Matrix Algebra Fundamentals</span>"
    ]
  },
  {
    "objectID": "chapters/A10-matrix-algebra.html#basic-objects-scalars-vectors-and-matrices",
    "href": "chapters/A10-matrix-algebra.html#basic-objects-scalars-vectors-and-matrices",
    "title": "46  Matrix Algebra Fundamentals",
    "section": "46.2 Basic Objects: Scalars, Vectors, and Matrices",
    "text": "46.2 Basic Objects: Scalars, Vectors, and Matrices\n\nScalars\nA scalar is a single number. In matrix notation, scalars are typically denoted with lowercase letters:\n\\[a = 5, \\quad b = 3.14\\]\n\n\nVectors\nA vector is an ordered collection of numbers. In R, we create vectors with c():\n\n\nCode\nx &lt;- c(2, 5, 1, 8, 3)\nx\n\n\n[1] 2 5 1 8 3\n\n\nMathematically, we write a column vector as:\n\\[\n\\mathbf{x} = \\begin{pmatrix}\nx_1 \\\\\nx_2 \\\\\n\\vdots \\\\\nx_n\n\\end{pmatrix}\n\\]\nVectors are typically denoted with bold lowercase letters. The transpose operation \\(^\\top\\) converts a column vector to a row vector:\n\\[\\mathbf{x}^\\top = (x_1, x_2, \\ldots, x_n)\\]\n\n\nMatrices\nA matrix is a rectangular array of numbers arranged in rows and columns. We can create matrices in R by binding vectors:\n\n\nCode\nx_1 &lt;- 1:5\nx_2 &lt;- 6:10\nX &lt;- cbind(x_1, x_2)\nX\n\n\n     x_1 x_2\n[1,]   1   6\n[2,]   2   7\n[3,]   3   8\n[4,]   4   9\n[5,]   5  10\n\n\nMathematically, an \\(n \\times p\\) matrix (n rows, p columns) is written as:\n\\[\n\\mathbf{X} = \\begin{pmatrix}\nx_{1,1} & x_{1,2} & \\cdots & x_{1,p} \\\\\nx_{2,1} & x_{2,2} & \\cdots & x_{2,p} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nx_{n,1} & x_{n,2} & \\cdots & x_{n,p}\n\\end{pmatrix}\n\\]\nMatrices are denoted with bold uppercase letters. The element in row \\(i\\) and column \\(j\\) is written as \\(x_{i,j}\\).\n\n\nMatrix Dimensions\nThe dimension of a matrix is its number of rows × number of columns:\n\n\nCode\n# Load MNIST data for examples\nif (!exists(\"mnist\")) mnist &lt;- read_mnist()\nx &lt;- mnist$train$images[1:1000, ]\ny &lt;- mnist$train$labels[1:1000]\n\ndim(x)\n\n\n[1] 1000  784\n\n\nThis matrix has 1,000 rows (observations) and 784 columns (features—one per pixel).\n\n\nCode\nnrow(x)  # Number of rows\n\n\n[1] 1000\n\n\nCode\nncol(x)  # Number of columns\n\n\n[1] 784\n\n\n\n\n\n\n\n\nVectors vs. Matrices in R\n\n\n\nR vectors don’t have dimensions:\n\n\nCode\nv &lt;- 1:5\ndim(v)\n\n\nNULL\n\n\nTo treat a vector as a matrix, use as.matrix():\n\n\nCode\ndim(as.matrix(v))\n\n\n[1] 5 1",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>46</span>  <span class='chapter-title'>Matrix Algebra Fundamentals</span>"
    ]
  },
  {
    "objectID": "chapters/A10-matrix-algebra.html#creating-matrices",
    "href": "chapters/A10-matrix-algebra.html#creating-matrices",
    "title": "46  Matrix Algebra Fundamentals",
    "section": "46.3 Creating Matrices",
    "text": "46.3 Creating Matrices\n\nThe matrix() Function\nCreate a matrix from a vector by specifying dimensions:\n\n\nCode\nmy_vector &lt;- 1:12\nmat &lt;- matrix(my_vector, nrow = 3, ncol = 4)\nmat\n\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    4    7   10\n[2,]    2    5    8   11\n[3,]    3    6    9   12\n\n\nBy default, matrices are filled by column. Use byrow = TRUE to fill by row:\n\n\nCode\nmat_byrow &lt;- matrix(my_vector, nrow = 3, ncol = 4, byrow = TRUE)\nmat_byrow\n\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    2    3    4\n[2,]    5    6    7    8\n[3,]    9   10   11   12\n\n\n\n\nConverting Vectors to Grid Images\nA powerful application is reshaping a vector into a 2D image. Each row of the MNIST data is a 784-element vector representing a 28×28 pixel image:\n\n\nCode\n# Convert the 3rd digit to a grid\ngrid &lt;- matrix(x[3, ], 28, 28)\n\n# Display (flip to show correctly)\nimage(1:28, 1:28, grid[, 28:1], col = gray.colors(256, start = 1, end = 0),\n      xlab = \"\", ylab = \"\", main = paste(\"Digit:\", y[3]))\n\n\n\n\n\n\n\n\nFigure 46.1: A digit from MNIST data displayed as a 28×28 pixel grid\n\n\n\n\n\n\n\nTranspose\nThe transpose swaps rows and columns:\n\n\nCode\nmat &lt;- matrix(1:6, nrow = 2, ncol = 3)\nmat\n\n\n     [,1] [,2] [,3]\n[1,]    1    3    5\n[2,]    2    4    6\n\n\nCode\nt(mat)\n\n\n     [,1] [,2]\n[1,]    1    2\n[2,]    3    4\n[3,]    5    6",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>46</span>  <span class='chapter-title'>Matrix Algebra Fundamentals</span>"
    ]
  },
  {
    "objectID": "chapters/A10-matrix-algebra.html#matrix-indexing",
    "href": "chapters/A10-matrix-algebra.html#matrix-indexing",
    "title": "46  Matrix Algebra Fundamentals",
    "section": "46.4 Matrix Indexing",
    "text": "46.4 Matrix Indexing\n\nExtracting Rows and Columns\nUse [row, column] notation:\n\n\nCode\nmat &lt;- matrix(1:12, nrow = 3, ncol = 4)\n\n# Single element\nmat[2, 3]\n\n\n[1] 8\n\n\nCode\n# Entire row\nmat[2, ]\n\n\n[1]  2  5  8 11\n\n\nCode\n# Entire column\nmat[, 3]\n\n\n[1] 7 8 9\n\n\nCode\n# Multiple rows/columns\nmat[1:2, 3:4]\n\n\n     [,1] [,2]\n[1,]    7   10\n[2,]    8   11\n\n\n\n\nPreserving Matrix Structure\nExtracting a single row or column returns a vector by default:\n\n\nCode\nclass(mat[, 1])\n\n\n[1] \"integer\"\n\n\nUse drop = FALSE to keep it as a matrix:\n\n\nCode\nclass(mat[, 1, drop = FALSE])\n\n\n[1] \"matrix\" \"array\" \n\n\n\n\nLogical Indexing\nSelect elements based on conditions:\n\n\nCode\nmat &lt;- matrix(1:12, nrow = 3, ncol = 4)\nmat[mat &gt; 6] &lt;- 0  # Set values &gt; 6 to zero\nmat\n\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    4    0    0\n[2,]    2    5    0    0\n[3,]    3    6    0    0\n\n\nThis is useful for operations like binarizing data:\n\n\nCode\n# Binarize MNIST data (convert to 0/1)\nbin_x &lt;- (x &gt; 127) * 1",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>46</span>  <span class='chapter-title'>Matrix Algebra Fundamentals</span>"
    ]
  },
  {
    "objectID": "chapters/A10-matrix-algebra.html#row-and-column-operations",
    "href": "chapters/A10-matrix-algebra.html#row-and-column-operations",
    "title": "46  Matrix Algebra Fundamentals",
    "section": "46.5 Row and Column Operations",
    "text": "46.5 Row and Column Operations\n\nSummary Functions\nCompute summaries across rows or columns:\n\n\nCode\n# Row sums and means\nrow_sums &lt;- rowSums(x)\nrow_means &lt;- rowMeans(x)\n\n# Column sums and means\ncol_sums &lt;- colSums(x)\ncol_means &lt;- colMeans(x)\n\n\nFor standard deviations, use the matrixStats package:\n\n\nCode\nlibrary(matrixStats)\nrow_sds &lt;- rowSds(x)\ncol_sds &lt;- colSds(x)\n\n\n\n\nThe apply() Function\nApply any function across rows (margin = 1) or columns (margin = 2):\n\n\nCode\n# Row medians\nrow_medians &lt;- apply(x, 1, median)\n\n# Column ranges\ncol_ranges &lt;- apply(x, 2, function(col) max(col) - min(col))\n\n\n\n\nFiltering Based on Summaries\nRemove low-variance columns (uninformative pixels):\n\n\nCode\n# Keep only columns with SD &gt; 60\nsds &lt;- colSds(x)\nx_filtered &lt;- x[, sds &gt; 60]\ndim(x_filtered)\n\n\n[1] 1000  314\n\n\nThis removes over half the predictors while keeping informative features.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>46</span>  <span class='chapter-title'>Matrix Algebra Fundamentals</span>"
    ]
  },
  {
    "objectID": "chapters/A10-matrix-algebra.html#vectorized-operations",
    "href": "chapters/A10-matrix-algebra.html#vectorized-operations",
    "title": "46  Matrix Algebra Fundamentals",
    "section": "46.6 Vectorized Operations",
    "text": "46.6 Vectorized Operations\n\nElement-wise Operations\nStandard arithmetic operates element-by-element:\n\n\nCode\nA &lt;- matrix(1:4, 2, 2)\nB &lt;- matrix(5:8, 2, 2)\n\nA + B   # Addition\n\n\n     [,1] [,2]\n[1,]    6   10\n[2,]    8   12\n\n\nCode\nA * B   # Element-wise multiplication\n\n\n     [,1] [,2]\n[1,]    5   21\n[2,]   12   32\n\n\nCode\nA / B   # Element-wise division\n\n\n          [,1]      [,2]\n[1,] 0.2000000 0.4285714\n[2,] 0.3333333 0.5000000\n\n\n\n\nBroadcasting\nWhen operating with vectors, R recycles along columns:\n\n\nCode\nmat &lt;- matrix(1:6, nrow = 2, ncol = 3)\nvec &lt;- c(10, 20)\nmat + vec  # Adds 10 to row 1, 20 to row 2\n\n\n     [,1] [,2] [,3]\n[1,]   11   13   15\n[2,]   22   24   26\n\n\n\n\nThe sweep() Function\nFor more control over row/column operations:\n\n\nCode\n# Subtract column means (center the data)\nx_centered &lt;- sweep(x, 2, colMeans(x))\n\n# Divide by column SDs (standardize)\nx_standardized &lt;- sweep(x_centered, 2, colSds(x), FUN = \"/\")\n\n\nFor row operations, use margin = 1:\n\n\nCode\n# Subtract row means\nx_row_centered &lt;- sweep(x, 1, rowMeans(x))",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>46</span>  <span class='chapter-title'>Matrix Algebra Fundamentals</span>"
    ]
  },
  {
    "objectID": "chapters/A10-matrix-algebra.html#matrix-multiplication",
    "href": "chapters/A10-matrix-algebra.html#matrix-multiplication",
    "title": "46  Matrix Algebra Fundamentals",
    "section": "46.7 Matrix Multiplication",
    "text": "46.7 Matrix Multiplication\n\nDefinition\nMatrix multiplication is not element-wise. For matrices \\(\\mathbf{A}\\) (\\(n \\times k\\)) and \\(\\mathbf{B}\\) (\\(k \\times p\\)), the product \\(\\mathbf{C} = \\mathbf{AB}\\) is an \\(n \\times p\\) matrix where:\n\\[c_{i,j} = \\sum_{l=1}^{k} a_{i,l} \\cdot b_{l,j}\\]\nThe number of columns in \\(\\mathbf{A}\\) must equal the number of rows in \\(\\mathbf{B}\\).\nIn R, use %*% for matrix multiplication:\n\n\nCode\nA &lt;- matrix(1:6, nrow = 2, ncol = 3)\nB &lt;- matrix(1:6, nrow = 3, ncol = 2)\n\nA %*% B\n\n\n     [,1] [,2]\n[1,]   22   49\n[2,]   28   64\n\n\n\n\nCross Product\nThe cross product \\(\\mathbf{X}^\\top\\mathbf{X}\\) is fundamental in statistics:\n\n\nCode\n# Using matrix multiplication\nXtX &lt;- t(x) %*% x\n\n# More efficient built-in function\nXtX &lt;- crossprod(x)\n\n\nThe cross product creates a \\(p \\times p\\) matrix where element \\((i,j)\\) is the dot product of columns \\(i\\) and \\(j\\).\n\n\nMatrix-Vector Multiplication\nMultiplying a matrix by a vector transforms the vector:\n\n\nCode\nA &lt;- matrix(c(2, 0, 0, 3), nrow = 2)  # Scaling matrix\nv &lt;- c(1, 1)\nA %*% v\n\n\n     [,1]\n[1,]    2\n[2,]    3\n\n\nThis concept is central to understanding eigenanalysis and PCA.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>46</span>  <span class='chapter-title'>Matrix Algebra Fundamentals</span>"
    ]
  },
  {
    "objectID": "chapters/A10-matrix-algebra.html#the-covariance-matrix",
    "href": "chapters/A10-matrix-algebra.html#the-covariance-matrix",
    "title": "46  Matrix Algebra Fundamentals",
    "section": "46.8 The Covariance Matrix",
    "text": "46.8 The Covariance Matrix\nFor centered data \\(\\mathbf{X}\\) (column means = 0), the covariance matrix is:\n\\[\\mathbf{\\Sigma} = \\frac{1}{n-1} \\mathbf{X}^\\top \\mathbf{X}\\]\n\n\nCode\n# Center the data\nx_centered &lt;- scale(x, center = TRUE, scale = FALSE)\n\n# Compute covariance matrix\ncov_matrix &lt;- cov(x_centered)\ndim(cov_matrix)\n\n\n[1] 784 784\n\n\nThe covariance matrix is: - Symmetric: \\(\\Sigma_{ij} = \\Sigma_{ji}\\) - Positive semi-definite: All eigenvalues ≥ 0 - Diagonal elements: Variances of each variable - Off-diagonal elements: Covariances between variables",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>46</span>  <span class='chapter-title'>Matrix Algebra Fundamentals</span>"
    ]
  },
  {
    "objectID": "chapters/A10-matrix-algebra.html#sec-eigenanalysis",
    "href": "chapters/A10-matrix-algebra.html#sec-eigenanalysis",
    "title": "46  Matrix Algebra Fundamentals",
    "section": "46.9 Eigenanalysis",
    "text": "46.9 Eigenanalysis\nEigenanalysis is the mathematical foundation of Principal Component Analysis (PCA). Understanding eigenvalues and eigenvectors helps you interpret PCA results correctly.\n\nWhat are Eigenvectors and Eigenvalues?\nFor a square matrix \\(\\mathbf{A}\\), an eigenvector \\(\\mathbf{v}\\) and its corresponding eigenvalue \\(\\lambda\\) satisfy:\n\\[\\mathbf{A}\\mathbf{v} = \\lambda\\mathbf{v}\\]\nThis equation says that when we multiply \\(\\mathbf{A}\\) by \\(\\mathbf{v}\\), the result is just \\(\\mathbf{v}\\) scaled by \\(\\lambda\\). The eigenvector’s direction doesn’t change—only its magnitude.\n\n\nGeometric Intuition\nMost vectors change direction when multiplied by a matrix. Eigenvectors are special—they only get stretched or compressed:\n\n\nCode\n# Define a transformation matrix\nA &lt;- matrix(c(2, 1, 1, 2), nrow = 2)\n\n# Compute eigenvectors\neig &lt;- eigen(A)\n\n# Regular vector (changes direction)\nv_regular &lt;- c(1, 0)\nv_transformed &lt;- A %*% v_regular\n\n# Eigenvector (only scales)\nv_eigen &lt;- eig$vectors[, 1]\nv_eigen_transformed &lt;- A %*% v_eigen\n\n# Plot\nplot(NULL, xlim = c(-3, 3), ylim = c(-3, 3), asp = 1,\n     xlab = \"x\", ylab = \"y\", main = \"Eigenvector vs Regular Vector\")\nabline(h = 0, v = 0, col = \"gray80\")\n\n# Regular vector and its transform\narrows(0, 0, v_regular[1], v_regular[2], col = \"blue\", lwd = 2)\narrows(0, 0, v_transformed[1], v_transformed[2], col = \"blue\", lwd = 2, lty = 2)\n\n# Eigenvector and its transform\narrows(0, 0, v_eigen[1], v_eigen[2], col = \"red\", lwd = 2)\narrows(0, 0, v_eigen_transformed[1], v_eigen_transformed[2], col = \"red\", lwd = 2, lty = 2)\n\nlegend(\"topleft\", c(\"Regular (original)\", \"Regular (transformed)\",\n                    \"Eigenvector (original)\", \"Eigenvector (transformed)\"),\n       col = c(\"blue\", \"blue\", \"red\", \"red\"), lty = c(1, 2, 1, 2), lwd = 2)\n\n\n\n\n\n\n\n\nFigure 46.2: Eigenvectors maintain their direction when transformed by a matrix. The red vector is an eigenvector (only scaled), while the blue vector changes direction.\n\n\n\n\n\n\n\nComputing Eigenvalues and Eigenvectors in R\n\n\nCode\n# Symmetric matrix example\nA &lt;- matrix(c(4, 2, 2, 3), nrow = 2)\neig &lt;- eigen(A)\n\n# Eigenvalues\neig$values\n\n\n[1] 5.561553 1.438447\n\n\nCode\n# Eigenvectors (as columns)\neig$vectors\n\n\n           [,1]       [,2]\n[1,] -0.7882054  0.6154122\n[2,] -0.6154122 -0.7882054\n\n\nVerify the eigenvalue equation \\(\\mathbf{A}\\mathbf{v} = \\lambda\\mathbf{v}\\):\n\n\nCode\nv1 &lt;- eig$vectors[, 1]\nlambda1 &lt;- eig$values[1]\n\n# These should be equal\nA %*% v1\n\n\n          [,1]\n[1,] -4.383646\n[2,] -3.422648\n\n\nCode\nlambda1 * v1\n\n\n[1] -4.383646 -3.422648\n\n\n\n\nProperties for Symmetric Matrices\nFor symmetric matrices (like covariance matrices):\n\nAll eigenvalues are real (not complex)\nEigenvectors are orthogonal (perpendicular to each other)\nEigenvalues are non-negative (for positive semi-definite matrices)\n\n\n\nCode\n# Verify orthogonality: dot product = 0\nsum(eig$vectors[, 1] * eig$vectors[, 2])\n\n\n[1] 0\n\n\n\n\nEigenanalysis and PCA\nPCA performs eigenanalysis on the covariance (or correlation) matrix. Here’s the connection:\n\nEigenvectors of the covariance matrix → Principal component directions (loadings)\nEigenvalues → Variance explained by each component\n\n\n\nCode\n# Simple example with iris data\niris_centered &lt;- scale(iris[, 1:4], center = TRUE, scale = FALSE)\ncov_iris &lt;- cov(iris_centered)\n\n# Eigenanalysis\neig_iris &lt;- eigen(cov_iris)\n\n# Compare to PCA\npca_iris &lt;- prcomp(iris[, 1:4], center = TRUE, scale. = FALSE)\n\n# Eigenvalues = squared singular values = variances\neig_iris$values\n\n\n[1] 4.22824171 0.24267075 0.07820950 0.02383509\n\n\nCode\npca_iris$sdev^2\n\n\n[1] 4.22824171 0.24267075 0.07820950 0.02383509\n\n\nThe loadings from PCA match the eigenvectors (possibly with sign flips, which are arbitrary):\n\n\nCode\n# Eigenvectors\nround(eig_iris$vectors, 3)\n\n\n       [,1]   [,2]   [,3]   [,4]\n[1,]  0.361 -0.657 -0.582  0.315\n[2,] -0.085 -0.730  0.598 -0.320\n[3,]  0.857  0.173  0.076 -0.480\n[4,]  0.358  0.075  0.546  0.754\n\n\nCode\n# PCA loadings\nround(pca_iris$rotation, 3)\n\n\n                PC1    PC2    PC3    PC4\nSepal.Length  0.361 -0.657  0.582  0.315\nSepal.Width  -0.085 -0.730 -0.598 -0.320\nPetal.Length  0.857  0.173 -0.076 -0.480\nPetal.Width   0.358  0.075 -0.546  0.754\n\n\n\n\nInterpreting Eigenvalues\nEigenvalues tell you how much variance each principal component captures.\n\n\nCode\n# Compute proportion of variance\nvar_explained &lt;- eig_iris$values / sum(eig_iris$values)\ncumulative_var &lt;- cumsum(var_explained)\n\npar(mfrow = c(1, 2))\nbarplot(var_explained * 100, names.arg = paste0(\"PC\", 1:4),\n        ylab = \"Variance Explained (%)\", main = \"Individual Variance\")\n\nbarplot(cumulative_var * 100, names.arg = paste0(\"PC\", 1:4),\n        ylab = \"Cumulative Variance (%)\", main = \"Cumulative Variance\")\nabline(h = 90, col = \"red\", lty = 2)\n\n\n\n\n\n\n\n\nFigure 46.3: Eigenvalues represent variance captured by each principal component. The first PC captures the most variance.\n\n\n\n\n\nKey interpretations:\n\nLarge eigenvalue: The corresponding PC captures substantial variance\nSmall eigenvalue: The PC captures little variance (often noise)\nSum of eigenvalues: Equals total variance in the data\n\n\n\nInterpreting Eigenvectors (Loadings)\nEigenvectors tell you how original variables combine to form each PC.\n\n\nCode\nloadings_df &lt;- data.frame(\n  Variable = rep(colnames(iris)[1:4], 4),\n  PC = rep(paste0(\"PC\", 1:4), each = 4),\n  Loading = as.vector(pca_iris$rotation)\n)\n\nggplot(loadings_df, aes(x = Variable, y = Loading, fill = Loading &gt; 0)) +\n  geom_bar(stat = \"identity\") +\n  facet_wrap(~PC, nrow = 1) +\n  scale_fill_manual(values = c(\"steelblue\", \"coral\"), guide = \"none\") +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +\n  labs(title = \"PCA Loadings for Iris Data\")\n\n\n\n\n\n\n\n\nFigure 46.4: PCA loadings show how original variables contribute to each principal component\n\n\n\n\n\nKey interpretations:\n\nLarge positive loading: Variable increases as PC increases\nLarge negative loading: Variable decreases as PC increases\nSmall loading: Variable contributes little to that PC\nSimilar loadings: Variables that load together are correlated\n\n\n\nThe Spectral Decomposition\nA symmetric matrix can be decomposed as:\n\\[\\mathbf{A} = \\mathbf{V} \\mathbf{\\Lambda} \\mathbf{V}^\\top\\]\nwhere: - \\(\\mathbf{V}\\) is the matrix of eigenvectors (columns) - \\(\\mathbf{\\Lambda}\\) is a diagonal matrix of eigenvalues\n\n\nCode\n# Verify spectral decomposition\nV &lt;- eig_iris$vectors\nLambda &lt;- diag(eig_iris$values)\nreconstructed &lt;- V %*% Lambda %*% t(V)\n\n# Should match original covariance matrix\nmax(abs(cov_iris - reconstructed))\n\n\n[1] 8.881784e-16\n\n\nThis decomposition is the mathematical basis for PCA’s ability to reduce dimensionality while preserving variance.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>46</span>  <span class='chapter-title'>Matrix Algebra Fundamentals</span>"
    ]
  },
  {
    "objectID": "chapters/A10-matrix-algebra.html#matrix-inverse",
    "href": "chapters/A10-matrix-algebra.html#matrix-inverse",
    "title": "46  Matrix Algebra Fundamentals",
    "section": "46.10 Matrix Inverse",
    "text": "46.10 Matrix Inverse\nThe inverse of a square matrix \\(\\mathbf{A}\\), denoted \\(\\mathbf{A}^{-1}\\), satisfies:\n\\[\\mathbf{A}\\mathbf{A}^{-1} = \\mathbf{A}^{-1}\\mathbf{A} = \\mathbf{I}\\]\nwhere \\(\\mathbf{I}\\) is the identity matrix.\n\n\nCode\nA &lt;- matrix(c(4, 2, 2, 3), nrow = 2)\nA_inv &lt;- solve(A)\n\n# Verify\nA %*% A_inv\n\n\n     [,1] [,2]\n[1,]    1    0\n[2,]    0    1\n\n\n\nApplications\nMatrix inversion is used in:\n\nSolving linear systems: \\(\\mathbf{x} = \\mathbf{A}^{-1}\\mathbf{b}\\)\nLinear regression: \\(\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^\\top\\mathbf{X})^{-1}\\mathbf{X}^\\top\\mathbf{y}\\)\n\n\n\nCode\n# Solve linear system Ax = b\nA &lt;- matrix(c(2, 1, 1, 3), nrow = 2)\nb &lt;- c(5, 7)\nx &lt;- solve(A, b)  # More efficient than solve(A) %*% b\nx\n\n\n[1] 1.6 1.8\n\n\nCode\n# Verify\nA %*% x\n\n\n     [,1]\n[1,]    5\n[2,]    7",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>46</span>  <span class='chapter-title'>Matrix Algebra Fundamentals</span>"
    ]
  },
  {
    "objectID": "chapters/A10-matrix-algebra.html#qr-decomposition",
    "href": "chapters/A10-matrix-algebra.html#qr-decomposition",
    "title": "46  Matrix Algebra Fundamentals",
    "section": "46.11 QR Decomposition",
    "text": "46.11 QR Decomposition\nThe QR decomposition factors a matrix as \\(\\mathbf{X} = \\mathbf{QR}\\) where:\n\n\\(\\mathbf{Q}\\) is orthogonal (\\(\\mathbf{Q}^\\top\\mathbf{Q} = \\mathbf{I}\\))\n\\(\\mathbf{R}\\) is upper triangular\n\nThis is numerically more stable than matrix inversion for solving linear systems.\n\n\nCode\nqr_result &lt;- qr(A)\nQ &lt;- qr.Q(qr_result)\nR &lt;- qr.R(qr_result)\n\n# Verify\nQ %*% R\n\n\n     [,1] [,2]\n[1,]    2    1\n[2,]    1    3",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>46</span>  <span class='chapter-title'>Matrix Algebra Fundamentals</span>"
    ]
  },
  {
    "objectID": "chapters/A10-matrix-algebra.html#summary",
    "href": "chapters/A10-matrix-algebra.html#summary",
    "title": "46  Matrix Algebra Fundamentals",
    "section": "46.12 Summary",
    "text": "46.12 Summary\nKey matrix algebra concepts for statistics:\n\n\n\nOperation\nR Function\nDescription\n\n\n\n\nTranspose\nt(X)\nSwap rows and columns\n\n\nMatrix multiply\nX %*% Y\nMatrix multiplication\n\n\nCross product\ncrossprod(X)\n\\(\\mathbf{X}^\\top\\mathbf{X}\\)\n\n\nElement-wise ops\nX * Y, X + Y\nApply to each element\n\n\nRow/column stats\nrowMeans(), colSds()\nSummary statistics\n\n\nStandardize\nsweep(), scale()\nCenter and scale\n\n\nEigenanalysis\neigen()\nEigenvalues and eigenvectors\n\n\nInverse\nsolve()\nMatrix inverse\n\n\nQR decomposition\nqr()\nOrthogonal decomposition\n\n\n\nFor PCA (see Chapter 33):\n\nEigenvalues = variance captured by each principal component\nEigenvectors = loadings (weights for combining original variables)\nLarger eigenvalues indicate more important components\nOrthogonal eigenvectors ensure uncorrelated components",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>46</span>  <span class='chapter-title'>Matrix Algebra Fundamentals</span>"
    ]
  },
  {
    "objectID": "chapters/A10-matrix-algebra.html#exercises",
    "href": "chapters/A10-matrix-algebra.html#exercises",
    "title": "46  Matrix Algebra Fundamentals",
    "section": "46.13 Exercises",
    "text": "46.13 Exercises\n\n\n\n\n\n\nExercise MA.1: Matrix Creation and Properties\n\n\n\n\nCreate a 100 × 10 matrix of randomly generated normal numbers. Compute its dimensions, number of rows, and number of columns.\nAdd the scalar 1 to row 1, the scalar 2 to row 2, and so on, to the matrix from Exercise 1. Hint: create a vector 1:100 and add it.\n\n\n\n\n\n\n\n\n\nExercise MA.2: Covariance and Symmetry\n\n\n\n\nCompute the covariance matrix of the iris numeric variables. Verify it is symmetric.\n\n\n\n\n\n\n\n\n\nExercise MA.3: Eigenanalysis\n\n\n\n\nPerform eigenanalysis on the iris covariance matrix. Which variable contributes most to PC1?\nVerify that the eigenvectors are orthogonal by computing their pairwise dot products.\n\n\n\n\n\n\n\n\n\nExercise MA.4: Image Data Analysis\n\n\n\n\nUsing the MNIST data, compute the proportion of pixels in a “grey area” (values between 50 and 205) for each digit. Make a boxplot by digit class.\n\n\n\n\n\n\n\n\n\nExercise MA.5: Matrix Standardization\n\n\n\n\nStandardize the columns of a matrix (subtract mean, divide by SD) using sweep().",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>46</span>  <span class='chapter-title'>Matrix Algebra Fundamentals</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Breiman, Leo. 2001. “Random Forests.” Machine\nLearning 45 (1): 5–32.\n\n\nCleveland, William S. 1979. “Robust Locally Weighted Regression\nand Smoothing Scatterplots.” Journal of the American\nStatistical Association 74 (368): 829–36.\n\n\nCohen, Jacob. 1988. Statistical Power Analysis for the Behavioral\nSciences. 2nd ed. Lawrence Erlbaum Associates.\n\n\nCortes, Corinna, and Vladimir Vapnik. 1995. “Support-Vector\nNetworks.” Machine Learning 20 (3): 273–97.\n\n\nCrawley, Michael J. 2007. The r Book. John Wiley & Sons.\n\n\nEfron, Bradley. 1979. “Bootstrap Methods: Another Look at the\nJackknife.” The Annals of Statistics 7 (1): 1–26.\n\n\nFisher, Ronald A. 1925. Statistical Methods for Research\nWorkers. Edinburgh: Oliver; Boyd.\n\n\n———. 1935. The Design of Experiments. Edinburgh: Oliver; Boyd.\n\n\nFriedman, Lawrence M., Curt D. Furberg, David L. DeMets, David M.\nReboussin, and Christopher B. Granger. 2015. Fundamentals of\nClinical Trials. 5th ed. New York: Springer.\n\n\nGelman, Andrew, John B. Carlin, Hal S. Stern, David B. Dunson, Aki\nVehtari, and Donald B. Rubin. 2013. Bayesian Data Analysis. 3rd\ned. Boca Raton, FL: CRC Press.\n\n\nHastie, Trevor, Robert Tibshirani, and Jerome Friedman. 2009. The\nElements of Statistical Learning: Data Mining, Inference, and\nPrediction. 2nd ed. New York: Springer.\n\n\nHoerl, Arthur E., and Robert W. Kennard. 1970. “Ridge Regression:\nBiased Estimation for Nonorthogonal Problems.”\nTechnometrics 12 (1): 55–67.\n\n\nHotelling, Harold. 1933. “Analysis of a Complex of Statistical\nVariables into Principal Components.” Journal of Educational\nPsychology 24 (6): 417–41.\n\n\nIrizarry, Rafael A. 2019. Introduction to Data Science.\nSelf-published. https://rafalab.github.io/dsbook/.\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani.\n2023. An Introduction to Statistical Learning with Applications in\nr. 2nd ed. Springer. https://www.statlearning.com.\n\n\nKruschke, John K. 2014. Doing Bayesian Data Analysis: A Tutorial\nwith r, JAGS, and Stan. 2nd ed. Boston: Academic Press.\n\n\nLogan, Murray. 2010. Biostatistical Design and Analysis Using\nr. Wiley-Blackwell.\n\n\nMaaten, Laurens van der, and Geoffrey Hinton. 2008. “Visualizing\nData Using t-SNE.” Journal of Machine Learning Research\n9: 2579–2605.\n\n\nMann, Henry B., and Donald R. Whitney. 1947. “On a Test of Whether\nOne of Two Random Variables Is Stochastically Larger Than the\nOther.” The Annals of Mathematical Statistics 18 (1):\n50–60.\n\n\nMcElreath, Richard. 2020. Statistical Rethinking: A Bayesian Course\nwith Examples in r and Stan. 2nd ed. Boca Raton, FL: CRC Press.\n\n\nMcInnes, Leland, John Healy, and James Melville. 2018. “UMAP:\nUniform Manifold Approximation and Projection for Dimension\nReduction.” arXiv Preprint arXiv:1802.03426.\n\n\nMontgomery, Douglas C. 2017. Design and Analysis of\nExperiments. 9th ed. Hoboken, NJ: Wiley.\n\n\nPearson, Karl. 1901. “On Lines and Planes of Closest Fit to\nSystems of Points in Space.” The London, Edinburgh, and\nDublin Philosophical Magazine and Journal of Science 2 (11):\n559–72.\n\n\nPocock, Stuart J. 2004. Clinical Trials: A Practical Approach.\nChichester: Wiley.\n\n\nRubin, Donald B. 2008. Matched Sampling for Causal Effects.\nCambridge: Cambridge University Press.\n\n\nStudent. 1908. “The Probable Error of a Mean.”\nBiometrika 6 (1): 1–25.\n\n\nThulin, Måns. 2025. Modern Statistics with r. CRC Press. https://www.modernstatisticswithr.com.\n\n\nTibshirani, Robert. 1996. “Regression Shrinkage and Selection via\nthe Lasso.” Journal of the Royal Statistical Society: Series\nB (Methodological) 58 (1): 267–88.\n\n\nTukey, John W. 1977. Exploratory Data Analysis. Reading, MA:\nAddison-Wesley.\n\n\nWilcoxon, Frank. 1945. “Individual Comparisons by Ranking\nMethods.” Biometrics Bulletin 1 (6): 80–83.",
    "crumbs": [
      "References"
    ]
  }
]