[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Statistics for Bioengineering",
    "section": "",
    "text": "Preface\nWelcome to Statistics for Bioengineering, a comprehensive guide designed for graduate students at the University of Oregon Knight Campus. This book provides the foundational skills needed for a successful scientific career in bioengineering and related fields, combining rigorous statistical theory with practical computational implementation in R.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#why-this-book",
    "href": "index.html#why-this-book",
    "title": "Statistics for Bioengineering",
    "section": "Why This Book?",
    "text": "Why This Book?\nModern bioengineering research generates vast amounts of data—from RNA sequencing experiments to biomaterial characterization studies, from neural recordings to clinical trial outcomes. Making sense of this data requires more than just running statistical tests; it demands a deep understanding of the principles underlying those tests and the computational skills to implement them properly.\nThis course takes a practical, hands-on approach to learning statistics. Rather than deriving every formula from first principles, we focus on understanding when and why to apply particular methods, how to implement them in R, and how to interpret and communicate results. Throughout the book, you’ll work with real biological data and develop the skills to analyze your own research questions.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#what-you-will-learn",
    "href": "index.html#what-you-will-learn",
    "title": "Statistics for Bioengineering",
    "section": "What You Will Learn",
    "text": "What You Will Learn\nThe material spans several interconnected domains:\nComputational Foundations. You will develop proficiency in Unix command-line operations and R programming, including the tidyverse ecosystem for data manipulation and visualization. These tools form the backbone of modern reproducible research practices.\nStatistical Theory. The book covers probability distributions, parameter estimation, hypothesis testing, and the logic of statistical inference. Understanding these concepts allows you to choose appropriate methods and interpret results correctly.\nPractical Analysis Methods. From t-tests to linear regression to analysis of variance, you’ll learn to implement a wide range of statistical techniques. Each method is presented with clear guidance on assumptions, implementation in R, and interpretation of output.\nReproducible Research. Using Markdown, Git, and GitHub, you’ll learn to document your analyses in ways that others (including your future self) can understand and reproduce.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#prerequisites",
    "href": "index.html#prerequisites",
    "title": "Statistics for Bioengineering",
    "section": "Prerequisites",
    "text": "Prerequisites\nThis book assumes no prior programming experience, though familiarity with basic algebra and an introductory statistics course will be helpful. You should have access to a computer running Windows, MacOS, or Linux with R and RStudio installed. Instructions for software setup are provided in the opening chapters.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#course-materials",
    "href": "index.html#course-materials",
    "title": "Statistics for Bioengineering",
    "section": "Course Materials",
    "text": "Course Materials\n\nRequired Texts (Free Online)\nThe following textbooks are available freely online and provide excellent supplementary material:\n\nR for Data Science (RDS). 2025. Wickham, Çetinkaya-Rundel, and Grolemund. O’Reilly Press.\nModern Statistics with R (MSR). 2025. Måns Thulin, CRC Press.\nAn Introduction to Statistical Learning (ISLR). 2023. James, Witten, Hastie, Tibshirani. Springer.\n\n\n\nAdditional Resources\n\nModern Statistics for Modern Biology. 2019. Holmes and Huber. Cambridge University Press.\nggPlot2: Elegant Graphics for Data Analysis, 3rd Edition. Wickham, Navarro, Pedersen. Springer.\nThe Visual Display of Quantitative Information. Tufte, E.R. Graphics Press.\n\n\n\nSoftware Requirements\n\nLatest version of R (install here)\nLatest version of RStudio (install here)\nA terminal with SSH capabilities for connecting to computing clusters\nGit installed and a GitHub account\nLaTeX for document preparation",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#how-to-use-this-book",
    "href": "index.html#how-to-use-this-book",
    "title": "Statistics for Bioengineering",
    "section": "How to Use This Book",
    "text": "How to Use This Book\nEach chapter builds on previous material, so working through the book sequentially is recommended for beginners. However, the modular organization also allows readers with some background to jump to specific topics of interest.\nCode examples are provided throughout the text, and you are strongly encouraged to type them yourself rather than copying and pasting. The act of typing reinforces learning and helps you notice details that might otherwise slip by. When you encounter errors—and you will—treat them as learning opportunities.\nThe exercises at the end of each chapter progress from straightforward applications of the material to more challenging problems requiring synthesis across topics. Attempting these exercises, even when difficult, is essential for developing genuine competence.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#acknowledgments",
    "href": "index.html#acknowledgments",
    "title": "Statistics for Bioengineering",
    "section": "Acknowledgments",
    "text": "Acknowledgments\nThis book grew out of many years of teaching biostatistics at the University of Oregon. I am grateful to the many students whose questions and struggles have shaped how I present this material, and to colleagues who have shared their insights on effective teaching of statistics.\n\nBill Cresko\nKnight Campus for Accelerating Scientific Impact\nUniversity of Oregon",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "chapters/01-introduction.html",
    "href": "chapters/01-introduction.html",
    "title": "1  Introduction",
    "section": "",
    "text": "1.1 The Role of Statistics in Bioengineering\nBioengineering sits at the intersection of biology, engineering, and medicine, a field where understanding complex systems requires both precise measurement and rigorous analysis. Whether you are developing new biomaterials, engineering tissues, designing medical devices, or analyzing genomic data, you will encounter situations where you need to draw conclusions from imperfect data. Statistics provides the framework for doing this responsibly.\nAt its core, statistics addresses a fundamental problem: we almost never know the world perfectly, yet we still need to make decisions and draw conclusions. When you measure the mechanical properties of a hydrogel, characterize the response of neurons to a stimulus, or quantify gene expression in different treatment groups, you obtain samples from larger populations. Statistics gives us the tools to estimate underlying parameters from these samples and to quantify our uncertainty about those estimates.",
    "crumbs": [
      "Foundations of Data Science",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapters/01-introduction.html#why-statistics-matters-for-your-career",
    "href": "chapters/01-introduction.html#why-statistics-matters-for-your-career",
    "title": "1  Introduction",
    "section": "1.2 Why Statistics Matters for Your Career",
    "text": "1.2 Why Statistics Matters for Your Career\nThe importance of statistical literacy for bioengineers cannot be overstated. Experimental design, data analysis, and the interpretation of results form the backbone of scientific research. Understanding statistics allows you to design experiments that can actually answer your questions, analyze data appropriately, and communicate your findings clearly and honestly.\nBeyond research, statistical thinking is increasingly important in industry applications. Quality control in biomanufacturing relies on statistical process control. Clinical trials require sophisticated statistical designs. Machine learning algorithms that power diagnostic tools and drug discovery pipelines are fundamentally statistical methods. Familiarity with these concepts will serve you throughout your career.",
    "crumbs": [
      "Foundations of Data Science",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapters/01-introduction.html#coding-and-scripting-for-data-analysis",
    "href": "chapters/01-introduction.html#coding-and-scripting-for-data-analysis",
    "title": "1  Introduction",
    "section": "1.3 Coding and Scripting for Data Analysis",
    "text": "1.3 Coding and Scripting for Data Analysis\nThis course emphasizes computational approaches to statistics. While it is possible to perform many statistical calculations by hand or using spreadsheet software, modern data analysis almost always involves programming. The ability to write code opens up enormous possibilities.\nProgramming is incredibly fast and powerful, particularly for repeated actions. A single command can accomplish what would require thousands of mouse clicks. You gain the ability to analyze large datasets that spreadsheet software cannot handle efficiently. You have access to thousands of free programs created by and for scientists. Your analyses become reproducible—you can document exactly what you did and share that documentation with others.\n\n\n\n\n\nWe distinguish between coding and scripting, though the line between them has blurred considerably. Coding generally refers to programming in compiled languages like C++ or Fortran, where source code is translated into machine code before execution. Scripting typically involves interpreted languages like Python, R, or Julia, where commands are executed on the fly without a separate compilation step. Compiled code tends to run faster but is less flexible during development; scripting languages offer more interactivity at some cost in execution speed. Modern analytical pipelines typically combine both approaches, using scripting languages for data manipulation and visualization while calling compiled code for computationally intensive operations.",
    "crumbs": [
      "Foundations of Data Science",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapters/01-introduction.html#what-you-will-learn",
    "href": "chapters/01-introduction.html#what-you-will-learn",
    "title": "1  Introduction",
    "section": "1.4 What You Will Learn",
    "text": "1.4 What You Will Learn\nThis course provides broad coverage of the core components of modern statistics while giving you the computational tools necessary to carry out your work. By the end, you will be able to read and write code in Unix and R, implement reproducible research practices through Markdown, GitHub, and cloud computing platforms, perform exploratory data analysis and visualization, understand probability in the context of distributions and sampling, and conduct a wide range of statistical analyses from t-tests to linear models to machine learning methods.\nThe course is organized around progressive skill building. We start with the computational foundations—Unix, R, and tools for reproducible research. We then develop the probability theory needed to understand statistical inference. With these foundations in place, we cover classical hypothesis testing and parametric methods before moving to more advanced topics like linear models and statistical learning.",
    "crumbs": [
      "Foundations of Data Science",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapters/01-introduction.html#course-philosophy",
    "href": "chapters/01-introduction.html#course-philosophy",
    "title": "1  Introduction",
    "section": "1.5 Course Philosophy",
    "text": "1.5 Course Philosophy\nThis is a practical course, and we will learn by doing. Class time will be devoted primarily to hands-on coding practice rather than traditional lecturing. You will work through exercises, debug code, and analyze real data. This active approach to learning is more challenging than passive note-taking, but it produces much deeper understanding.\nExpect to struggle at times. Programming is frustrating, especially when you are learning. Error messages will seem cryptic. Code that should work will not work. Problems that seem simple will prove difficult. This is normal, and working through these challenges is how you develop genuine competence. The goal is not to avoid mistakes but to develop the skills to diagnose and fix them.\nThroughout the course, we emphasize reproducibility and transparency. Your analyses should be documented in ways that allow others to understand and verify what you did. This is not just good practice for collaboration; it also helps you when you return to your own work months or years later.",
    "crumbs": [
      "Foundations of Data Science",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapters/01-introduction.html#statistical-thinking",
    "href": "chapters/01-introduction.html#statistical-thinking",
    "title": "1  Introduction",
    "section": "1.6 Statistical Thinking",
    "text": "1.6 Statistical Thinking\nStatistics ultimately aims to turn data into conclusions about the world. We want to make point estimates and construct confidence intervals that quantify our uncertainty. We design experiments that can distinguish between competing hypotheses. We test those hypotheses using data. When dealing with high-dimensional data, we need methods to reduce complexity while preserving important information.\nAll of this requires a firm understanding of probability, sampling, and distributions. Probability provides the mathematical framework for reasoning about uncertainty. Understanding how samples relate to populations allows us to make inferences about things we cannot directly observe. Knowledge of common probability distributions tells us what to expect under various conditions and helps us identify when data deviate from expectations.\nWe will explore two major approaches to statistical inference. Frequentist statistics, the classical approach taught in most introductory courses, interprets probabilities as long-run frequencies and uses null hypothesis testing as its primary framework. Hierarchical probabilistic modeling, including maximum likelihood estimation and Bayesian methods, provides complementary tools that are increasingly important in modern statistical practice. Both perspectives have their uses, and understanding both will make you a more versatile analyst.",
    "crumbs": [
      "Foundations of Data Science",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapters/01-introduction.html#getting-started",
    "href": "chapters/01-introduction.html#getting-started",
    "title": "1  Introduction",
    "section": "1.7 Getting Started",
    "text": "1.7 Getting Started\nThe remainder of this chapter covers the practical matters of getting your computational environment set up. In subsequent chapters, we will dive into the material itself, beginning with Unix and the command line before moving to R and RStudio. With these tools in place, we will begin our exploration of probability, inference, and statistical modeling.\nThe journey ahead requires effort, but the skills you develop will serve you throughout your career. Let’s begin.",
    "crumbs": [
      "Foundations of Data Science",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapters/02-unix-command-line.html",
    "href": "chapters/02-unix-command-line.html",
    "title": "2  Unix and the Command Line",
    "section": "",
    "text": "2.1 What is Unix?\nUnix is a family of operating systems that originated at Bell Labs in 1969 and was released publicly in 1973. Its design philosophy emphasizes modularity—small programs that do one thing well and can be combined to accomplish complex tasks. This approach has proven remarkably durable, and Unix-based systems remain dominant in scientific computing, web servers, and high-performance computing environments.\nLinux is an open-source implementation of Unix that runs on everything from embedded devices to the world’s fastest supercomputers. MacOS is built on a Unix foundation, which means Mac users have native access to Unix commands. Windows historically used a different approach, but recent versions include the Windows Subsystem for Linux (WSL), allowing Windows users to run Linux environments alongside their Windows applications.\nUnderstanding Unix is essential for modern data science. You will need it to access remote computing resources like supercomputer clusters, to run bioinformatics software that is only available through the command line, and to automate repetitive tasks. The skills you develop here will transfer across platforms and remain relevant throughout your career.",
    "crumbs": [
      "Foundations of Data Science",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Unix and the Command Line</span>"
    ]
  },
  {
    "objectID": "chapters/02-unix-command-line.html#the-shell-and-terminal",
    "href": "chapters/02-unix-command-line.html#the-shell-and-terminal",
    "title": "2  Unix and the Command Line",
    "section": "2.2 The Shell and Terminal",
    "text": "2.2 The Shell and Terminal\nThe shell is a program that interprets your commands and communicates with the operating system. When you type a command, the shell parses it, figures out what you want to do, and tells the operating system to do it. The results are then displayed back to you.\nBash (Bourne Again SHell) is the most common shell on Linux systems and was the default on MacOS until recently (MacOS now defaults to zsh, which is very similar). The shell runs inside a terminal application, which provides the window where you type commands and see output.\n\n\n\n\n\nOn Mac, you can access the terminal by opening the Terminal app or a third-party alternative like iTerm2. On Linux, look for a Terminal application in your system menus. Windows users should install the Windows Subsystem for Linux following Microsoft’s documentation, then access it through the Ubuntu app or similar.\nRStudio also includes a terminal pane, which can be convenient when you want shell access without leaving your R development environment.",
    "crumbs": [
      "Foundations of Data Science",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Unix and the Command Line</span>"
    ]
  },
  {
    "objectID": "chapters/02-unix-command-line.html#anatomy-of-a-shell-command",
    "href": "chapters/02-unix-command-line.html#anatomy-of-a-shell-command",
    "title": "2  Unix and the Command Line",
    "section": "2.3 Anatomy of a Shell Command",
    "text": "2.3 Anatomy of a Shell Command\nShell commands follow a consistent structure. You type a command name, possibly followed by options that modify its behavior, and arguments that specify what the command should operate on. The shell waits at a prompt—typically $ for regular users or # for administrators—indicating it is ready to accept input.\n\n\n\n\n\nConsider the command ls -l Documents. Here, ls is the command (list directory contents), -l is an option (use long format), and Documents is the argument (the directory to list). Options usually begin with a dash and can often be combined: ls -la combines the -l (long format) and -a (show hidden files) options.",
    "crumbs": [
      "Foundations of Data Science",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Unix and the Command Line</span>"
    ]
  },
  {
    "objectID": "chapters/02-unix-command-line.html#file-system-organization",
    "href": "chapters/02-unix-command-line.html#file-system-organization",
    "title": "2  Unix and the Command Line",
    "section": "2.4 File System Organization",
    "text": "2.4 File System Organization\nUnix organizes files in a hierarchical structure of directories (folders) and files. The root directory, represented by a single forward slash /, sits at the top of this hierarchy and contains all other directories.\n\n\n\n\n\nYour home directory is your personal workspace, typically located at /Users/yourusername on Mac or /home/yourusername on Linux. The tilde character ~ serves as a shorthand for your home directory, so ~/Documents refers to the Documents folder in your home directory.\nEvery file and directory has a path—a specification of its location in the file system. Absolute paths start from the root directory and give the complete location, like /Users/wcresko/Documents/data.csv. Relative paths specify location relative to your current directory, so if you are in your home directory, Documents/data.csv refers to the same file.",
    "crumbs": [
      "Foundations of Data Science",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Unix and the Command Line</span>"
    ]
  },
  {
    "objectID": "chapters/02-unix-command-line.html#navigation-commands",
    "href": "chapters/02-unix-command-line.html#navigation-commands",
    "title": "2  Unix and the Command Line",
    "section": "2.5 Navigation Commands",
    "text": "2.5 Navigation Commands\nThe most fundamental navigation command is pwd (print working directory), which tells you where you currently are in the file system. This is often the first thing you type when opening a terminal to orient yourself.\n\n\n\n\n\npwd\nThe ls command lists the contents of a directory. Without arguments, it lists the current directory. With a path argument, it lists that location.\nls                  # list current directory\nls Documents        # list the Documents folder\nls -l               # long format with details\nls -a               # include hidden files (starting with .)\nls -la              # combine long format and hidden files\nls -lS              # long format, sorted by size\n\n\n\n\n\nThe cd command (change directory) moves you to a different location.\ncd Documents        # move into Documents\ncd ..               # move up one level (parent directory)\ncd ~                # move to home directory\ncd /                # move to root directory\ncd -                # move to previous location",
    "crumbs": [
      "Foundations of Data Science",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Unix and the Command Line</span>"
    ]
  },
  {
    "objectID": "chapters/02-unix-command-line.html#working-with-files-and-directories",
    "href": "chapters/02-unix-command-line.html#working-with-files-and-directories",
    "title": "2  Unix and the Command Line",
    "section": "2.6 Working with Files and Directories",
    "text": "2.6 Working with Files and Directories\nCreating new directories uses the mkdir command.\nmkdir project_data\nmkdir -p analysis/results/figures  # create nested directories\nThe -p flag tells mkdir to create parent directories as needed, which is useful for creating nested folder structures in one command.\nMoving and renaming files uses the mv command.\nmv old_name.txt new_name.txt       # rename a file\nmv file.txt Documents/             # move file to Documents\nmv file.txt Documents/newname.txt  # move and rename\nCopying files uses cp.\ncp original.txt copy.txt           # copy a file\ncp -r folder/ backup/              # copy a directory recursively\nRemoving files uses rm. Be careful with this command—there is no trash can or undo in the shell.\nrm unwanted_file.txt               # remove a file\nrm -r unwanted_folder/             # remove a directory and contents\nrm -i file.txt                     # ask for confirmation before removing",
    "crumbs": [
      "Foundations of Data Science",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Unix and the Command Line</span>"
    ]
  },
  {
    "objectID": "chapters/02-unix-command-line.html#viewing-file-contents",
    "href": "chapters/02-unix-command-line.html#viewing-file-contents",
    "title": "2  Unix and the Command Line",
    "section": "2.7 Viewing File Contents",
    "text": "2.7 Viewing File Contents\nSeveral commands let you examine file contents without opening them in an editor.\nThe cat command displays the entire contents of a file.\ncat data.txt\nFor longer files, head and tail show the beginning and end.\nhead data.csv          # first 10 lines\nhead -n 20 data.csv    # first 20 lines\ntail data.csv          # last 10 lines\ntail -f logfile.txt    # follow a file as it grows\nThe less command opens an interactive viewer that lets you scroll through large files.\nless large_data.txt\nInside less, use arrow keys to scroll, / to search, and q to quit.\nThe wc command counts lines, words, and characters.\nwc data.txt            # lines, words, characters\nwc -l data.txt         # just lines",
    "crumbs": [
      "Foundations of Data Science",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Unix and the Command Line</span>"
    ]
  },
  {
    "objectID": "chapters/02-unix-command-line.html#getting-help",
    "href": "chapters/02-unix-command-line.html#getting-help",
    "title": "2  Unix and the Command Line",
    "section": "2.8 Getting Help",
    "text": "2.8 Getting Help\nUnix provides documentation through manual pages, accessible with the man command.\nman ls                 # manual page for ls command\nManual pages can be dense, but they are comprehensive. Use the spacebar to page through, / to search, and q to exit. Many commands also accept a --help flag that provides a shorter summary.\nls --help\nOf course, the internet provides extensive resources. When you encounter an unfamiliar command or error message, searching online often leads to helpful explanations and examples.",
    "crumbs": [
      "Foundations of Data Science",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Unix and the Command Line</span>"
    ]
  },
  {
    "objectID": "chapters/02-unix-command-line.html#pipes-and-redirection",
    "href": "chapters/02-unix-command-line.html#pipes-and-redirection",
    "title": "2  Unix and the Command Line",
    "section": "2.9 Pipes and Redirection",
    "text": "2.9 Pipes and Redirection\nOne of Unix’s most powerful features is the ability to combine simple commands into complex pipelines. The pipe operator | sends the output of one command to another command as input.\nls -l | head -n 5           # list files, show only first 5\ncat data.txt | wc -l        # count lines in file\nRedirection operators send output to files instead of the screen.\nls -l &gt; file_list.txt       # write output to file (overwrite)\nls -l &gt;&gt; file_list.txt      # append output to file\nThese features enable powerful text processing. Combined with tools like grep (search for patterns), sort, and cut (extract columns), you can accomplish sophisticated data manipulation with compact commands.\ngrep \"gene\" data.txt                    # find lines containing \"gene\"\ngrep -c \"gene\" data.txt                 # count matching lines\nsort data.txt                           # sort lines alphabetically\nsort -n numbers.txt                     # sort numerically\ncut -f1,3 data.tsv                      # extract columns 1 and 3 from tab-separated file",
    "crumbs": [
      "Foundations of Data Science",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Unix and the Command Line</span>"
    ]
  },
  {
    "objectID": "chapters/02-unix-command-line.html#connecting-to-remote-systems",
    "href": "chapters/02-unix-command-line.html#connecting-to-remote-systems",
    "title": "2  Unix and the Command Line",
    "section": "2.10 Connecting to Remote Systems",
    "text": "2.10 Connecting to Remote Systems\nThe ssh command (secure shell) lets you connect to remote computers.\nssh username@server.university.edu\nYou will use this to connect to computing clusters like Talapas for computationally intensive work. Once connected, you work in a shell environment on the remote system just as you would locally.\nThe scp command copies files between your computer and remote systems.\nscp local_file.txt username@server.edu:~/destination/\nscp username@server.edu:~/remote_file.txt ./local_copy.txt",
    "crumbs": [
      "Foundations of Data Science",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Unix and the Command Line</span>"
    ]
  },
  {
    "objectID": "chapters/02-unix-command-line.html#practice-exercises",
    "href": "chapters/02-unix-command-line.html#practice-exercises",
    "title": "2  Unix and the Command Line",
    "section": "2.11 Practice Exercises",
    "text": "2.11 Practice Exercises\nThe best way to learn command-line skills is through practice. Create a project folder structure for organizing your course work. Navigate through the file system and examine the contents of various directories. Create some text files and practice viewing, copying, moving, and removing them.\nTry combining commands with pipes. For example, you might list all files in a directory, filter for those with a particular extension, and count how many there are. Start simple and gradually build more complex pipelines as you become comfortable with the individual commands.",
    "crumbs": [
      "Foundations of Data Science",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Unix and the Command Line</span>"
    ]
  },
  {
    "objectID": "chapters/03-r-rstudio.html",
    "href": "chapters/03-r-rstudio.html",
    "title": "3  R and RStudio",
    "section": "",
    "text": "3.1 Why R?\nR is a programming language designed specifically for statistical computing and graphics. Created in the early 1990s as an open-source implementation of the S language, R has become the lingua franca of statistical analysis in academia and is widely used in industry as well.\nSeveral features make R particularly well-suited for data analysis. It provides an extensive collection of statistical and graphical techniques built into the language. It is powerful, flexible, and completely free. It runs on Windows, Mac, and Linux, so your code will work across platforms. New capabilities are constantly being added through packages contributed by the community, with thousands of packages available for specialized analyses.\nR excels at reproducibility. You can keep your scripts to document exactly what analyses you performed. Unlike point-and-click software where actions leave no trace, R code provides a complete record of your analytical workflow. This record can be shared with collaborators, included in publications, and revisited years later when you need to remember how you produced a particular result.\nYou can write your own functions in R, extending the language to meet your specific needs. Extensive online help and active user communities mean that answers to most questions are a web search away. The RStudio integrated development environment makes working with R much more pleasant, especially for newcomers. And with tools like R Markdown and Quarto, you can embed your analyses in polished documents, presentations, websites, and books—this book itself was created with these tools.",
    "crumbs": [
      "Foundations of Data Science",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>R and RStudio</span>"
    ]
  },
  {
    "objectID": "chapters/03-r-rstudio.html#installing-r-and-rstudio",
    "href": "chapters/03-r-rstudio.html#installing-r-and-rstudio",
    "title": "3  R and RStudio",
    "section": "3.2 Installing R and RStudio",
    "text": "3.2 Installing R and RStudio\nR must be installed before RStudio. Download R from https://www.r-project.org, selecting the version appropriate for your operating system. Follow the installation instructions for your platform.\nRStudio is an integrated development environment (IDE) that makes working with R much easier. Download the free RStudio Desktop from https://www.rstudio.com. RStudio provides a console for running R commands, an editor for writing scripts, tools for viewing plots and data, and integration with version control systems.\nAfter installing both programs, launch RStudio. You will see a window divided into panes, each serving a different purpose. The console pane is where R commands are executed. The source pane is where you edit scripts and documents. The environment pane shows what objects currently exist in your R session. The files/plots/packages/help pane provides access to various utilities.",
    "crumbs": [
      "Foundations of Data Science",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>R and RStudio</span>"
    ]
  },
  {
    "objectID": "chapters/03-r-rstudio.html#r-basics",
    "href": "chapters/03-r-rstudio.html#r-basics",
    "title": "3  R and RStudio",
    "section": "3.3 R Basics",
    "text": "3.3 R Basics\nR evaluates expressions and returns results. You can use it as a calculator by typing arithmetic expressions at the console.\n\n\nCode\n4 * 4\n\n\n[1] 16\n\n\nCode\n(4 + 3 * 2^2)\n\n\n[1] 16\n\n\nNotice that R follows standard mathematical order of operations: exponentiation before multiplication and division, which come before addition and subtraction. Parentheses can override this ordering.",
    "crumbs": [
      "Foundations of Data Science",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>R and RStudio</span>"
    ]
  },
  {
    "objectID": "chapters/03-r-rstudio.html#variables-and-assignment",
    "href": "chapters/03-r-rstudio.html#variables-and-assignment",
    "title": "3  R and RStudio",
    "section": "3.4 Variables and Assignment",
    "text": "3.4 Variables and Assignment\nMore useful than evaluating isolated expressions is storing values in variables for later use. Variables are assigned using the &lt;- operator (a less-than sign followed by a hyphen).\n\n\nCode\nx &lt;- 2\nx * 3\n\n\n[1] 6\n\n\nCode\ny &lt;- x * 3\ny - 2\n\n\n[1] 4\n\n\nVariable names must begin with a letter but can contain letters, numbers, periods, and underscores after the first character. R is case-sensitive, so myVariable, MyVariable, and myvariable are three different names. Choose descriptive names that make your code readable.\nNote that when you assign a value to a variable, R does not print anything. To see a variable’s value, type its name alone or use the print() function.\n\n\nCode\nz &lt;- 100\nz\n\n\n[1] 100\n\n\nCode\nprint(z)\n\n\n[1] 100",
    "crumbs": [
      "Foundations of Data Science",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>R and RStudio</span>"
    ]
  },
  {
    "objectID": "chapters/03-r-rstudio.html#functions",
    "href": "chapters/03-r-rstudio.html#functions",
    "title": "3  R and RStudio",
    "section": "3.5 Functions",
    "text": "3.5 Functions\nFunctions are the workhorses of R. A function takes inputs (called arguments), performs some operation, and returns an output. R has many built-in functions, and packages provide thousands more.\n\n\nCode\nlog(10)\n\n\n[1] 2.302585\n\n\nCode\nsqrt(16)\n\n\n[1] 4\n\n\nCode\nexp(1)\n\n\n[1] 2.718282\n\n\nFunctions are called by typing their name followed by parentheses containing their arguments. Many functions accept multiple arguments, separated by commas. Arguments can be specified by position or by name.\n\n\nCode\nround(3.14159, digits = 2)\n\n\n[1] 3.14\n\n\nCode\nround(3.14159, 2)  # same result, argument specified by position\n\n\n[1] 3.14\n\n\nTo learn about a function, use the help system. Type ?functionname or help(functionname) to open the documentation.\n\n\nCode\n?round\nhelp(sqrt)",
    "crumbs": [
      "Foundations of Data Science",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>R and RStudio</span>"
    ]
  },
  {
    "objectID": "chapters/03-r-rstudio.html#vectors",
    "href": "chapters/03-r-rstudio.html#vectors",
    "title": "3  R and RStudio",
    "section": "3.6 Vectors",
    "text": "3.6 Vectors\nThe fundamental data structure in R is the vector, an ordered collection of values of the same type. You create vectors using the c() function (for concatenate or combine).\n\n\nCode\nnumbers &lt;- c(1, 2, 3, 4, 5)\nnumbers\n\n\n[1] 1 2 3 4 5\n\n\nCode\nnames &lt;- c(\"Alice\", \"Bob\", \"Carol\")\nnames\n\n\n[1] \"Alice\" \"Bob\"   \"Carol\"\n\n\nMany operations in R are vectorized, meaning they operate on entire vectors at once rather than requiring you to loop through elements.\n\n\nCode\nnumbers * 2\n\n\n[1]  2  4  6  8 10\n\n\nCode\nnumbers + 10\n\n\n[1] 11 12 13 14 15\n\n\nCode\nnumbers^2\n\n\n[1]  1  4  9 16 25\n\n\nYou can access individual elements using square brackets with an index (R uses 1-based indexing, so the first element is at position 1).\n\n\nCode\nnumbers[1]\n\n\n[1] 1\n\n\nCode\nnumbers[3]\n\n\n[1] 3\n\n\nCode\nnumbers[c(1, 3, 5)]\n\n\n[1] 1 3 5",
    "crumbs": [
      "Foundations of Data Science",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>R and RStudio</span>"
    ]
  },
  {
    "objectID": "chapters/03-r-rstudio.html#creating-sequences",
    "href": "chapters/03-r-rstudio.html#creating-sequences",
    "title": "3  R and RStudio",
    "section": "3.7 Creating Sequences",
    "text": "3.7 Creating Sequences\nR provides convenient functions for creating regular sequences.\n\n\nCode\n1:10\n\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\n\nCode\nseq(0, 10, by = 2)\n\n\n[1]  0  2  4  6  8 10\n\n\nCode\nseq(0, 1, length.out = 5)\n\n\n[1] 0.00 0.25 0.50 0.75 1.00\n\n\nCode\nrep(1, times = 5)\n\n\n[1] 1 1 1 1 1\n\n\nCode\nrep(c(1, 2), times = 3)\n\n\n[1] 1 2 1 2 1 2",
    "crumbs": [
      "Foundations of Data Science",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>R and RStudio</span>"
    ]
  },
  {
    "objectID": "chapters/03-r-rstudio.html#generating-random-numbers",
    "href": "chapters/03-r-rstudio.html#generating-random-numbers",
    "title": "3  R and RStudio",
    "section": "3.8 Generating Random Numbers",
    "text": "3.8 Generating Random Numbers\nR can generate random numbers from various probability distributions, which is invaluable for simulation and understanding statistical concepts.\n\n\nCode\n# Draw 1000 values from a normal distribution with mean 0 and SD 10\nx &lt;- rnorm(1000, mean = 0, sd = 10)\nhist(x)\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Draw from a binomial distribution: 1000 experiments, 20 trials each, p=0.5\nheads &lt;- rbinom(n = 1000, size = 20, prob = 0.5)\nhist(heads)\n\n\n\n\n\n\n\n\n\nThe set.seed() function allows you to make random simulations reproducible by initializing the random number generator to a known state.\n\n\nCode\nset.seed(42)\nrnorm(5)\n\n\n[1]  1.3709584 -0.5646982  0.3631284  0.6328626  0.4042683\n\n\nCode\nset.seed(42)  # same seed produces same \"random\" numbers\nrnorm(5)\n\n\n[1]  1.3709584 -0.5646982  0.3631284  0.6328626  0.4042683",
    "crumbs": [
      "Foundations of Data Science",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>R and RStudio</span>"
    ]
  },
  {
    "objectID": "chapters/03-r-rstudio.html#data-frames",
    "href": "chapters/03-r-rstudio.html#data-frames",
    "title": "3  R and RStudio",
    "section": "3.9 Data Frames",
    "text": "3.9 Data Frames\nData frames are R’s structure for tabular data—rows of observations and columns of variables. Each column can contain a different type of data (numeric, character, logical), but all values within a column must be the same type.\n\n\nCode\n# Create a data frame from vectors\nhydrogel_concentration &lt;- factor(c(\"low\", \"high\", \"high\", \"high\", \n                                    \"medium\", \"medium\", \"medium\", \"low\"))\ncompression &lt;- c(3.4, 3.4, 8.4, 3, 5.6, 8.1, 8.3, 4.5)\nconductivity &lt;- c(0, 9.2, 3.8, 5, 5.6, 4.1, 7.1, 5.3)\n\nmydata &lt;- data.frame(hydrogel_concentration, compression, conductivity)\nmydata\n\n\n  hydrogel_concentration compression conductivity\n1                    low         3.4          0.0\n2                   high         3.4          9.2\n3                   high         8.4          3.8\n4                   high         3.0          5.0\n5                 medium         5.6          5.6\n6                 medium         8.1          4.1\n7                 medium         8.3          7.1\n8                    low         4.5          5.3\n\n\nAccess columns using the $ operator or square brackets.\n\n\nCode\nmydata$compression\n\n\n[1] 3.4 3.4 8.4 3.0 5.6 8.1 8.3 4.5\n\n\nCode\nmydata[, 2]  # second column\n\n\n[1] 3.4 3.4 8.4 3.0 5.6 8.1 8.3 4.5\n\n\nCode\nmydata[1, ]  # first row\n\n\n  hydrogel_concentration compression conductivity\n1                    low         3.4            0\n\n\nCode\nmydata[1, 2] # first row, second column\n\n\n[1] 3.4",
    "crumbs": [
      "Foundations of Data Science",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>R and RStudio</span>"
    ]
  },
  {
    "objectID": "chapters/03-r-rstudio.html#reading-and-writing-data",
    "href": "chapters/03-r-rstudio.html#reading-and-writing-data",
    "title": "3  R and RStudio",
    "section": "3.10 Reading and Writing Data",
    "text": "3.10 Reading and Writing Data\nReal analyses typically begin by reading data from external files. R provides functions for various file formats.\n\n\nCode\n# Read comma-separated values\ndata &lt;- read.csv(\"mydata.csv\")\n\n# Read tab-separated values\ndata &lt;- read.table(\"mydata.txt\", header = TRUE, sep = \"\\t\")\n\n# Read Excel files (requires readxl package)\nlibrary(readxl)\ndata &lt;- read_excel(\"mydata.xlsx\")\n\n\nSimilarly, you can write data to files.\n\n\nCode\nwrite.csv(mydata, \"output.csv\", row.names = FALSE)\nwrite.table(mydata, \"output.txt\", sep = \"\\t\", row.names = FALSE)",
    "crumbs": [
      "Foundations of Data Science",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>R and RStudio</span>"
    ]
  },
  {
    "objectID": "chapters/03-r-rstudio.html#basic-plotting",
    "href": "chapters/03-r-rstudio.html#basic-plotting",
    "title": "3  R and RStudio",
    "section": "3.11 Basic Plotting",
    "text": "3.11 Basic Plotting\nR has extensive graphics capabilities. The base plot() function creates scatterplots and other basic visualizations.\n\n\nCode\nx &lt;- 1:10\ny &lt;- x^2\nplot(x, y, \n     xlab = \"X values\", \n     ylab = \"Y squared\",\n     main = \"A Simple Plot\",\n     col = \"blue\",\n     pch = 19)\n\n\n\n\n\n\n\n\n\nHistograms visualize the distribution of a single variable.\n\n\nCode\ndata &lt;- rnorm(1000)\nhist(data, breaks = 30, col = \"lightblue\", main = \"Normal Distribution\")\n\n\n\n\n\n\n\n\n\nBoxplots compare distributions across groups.\n\n\nCode\nboxplot(compression ~ hydrogel_concentration, data = mydata,\n        xlab = \"Concentration\", ylab = \"Compression\")\n\n\n\n\n\n\n\n\n\nWe will explore the more sophisticated ggplot2 package for graphics in a later chapter.",
    "crumbs": [
      "Foundations of Data Science",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>R and RStudio</span>"
    ]
  },
  {
    "objectID": "chapters/03-r-rstudio.html#scripts-and-reproducibility",
    "href": "chapters/03-r-rstudio.html#scripts-and-reproducibility",
    "title": "3  R and RStudio",
    "section": "3.12 Scripts and Reproducibility",
    "text": "3.12 Scripts and Reproducibility\nWhile you can type commands directly at the console, for anything beyond simple explorations you should write scripts—text files containing R commands that can be saved, edited, and rerun.\nIn RStudio, create a new script with File &gt; New File &gt; R Script. Type your commands in the script editor, and run them by placing your cursor on a line and pressing Ctrl+Enter (Cmd+Enter on Mac) or by selecting code and clicking Run.\nScripts should be self-contained, including all the commands needed to reproduce your analysis from start to finish. Begin scripts by loading required packages, then reading data, then performing analyses. Add comments (lines beginning with #) to explain what your code does and why.\n\n\nCode\n# Analysis of hydrogel mechanical properties\n# Author: Your Name\n# Date: 2025-04-01\n\n# Load required packages\nlibrary(tidyverse)\n\n# Read data\ndata &lt;- read.csv(\"hydrogel_data.csv\")\n\n# Calculate summary statistics\nsummary(data)\n\n# Create visualization\nggplot(data, aes(x = concentration, y = compression)) +\n  geom_boxplot()",
    "crumbs": [
      "Foundations of Data Science",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>R and RStudio</span>"
    ]
  },
  {
    "objectID": "chapters/03-r-rstudio.html#getting-help",
    "href": "chapters/03-r-rstudio.html#getting-help",
    "title": "3  R and RStudio",
    "section": "3.13 Getting Help",
    "text": "3.13 Getting Help\nWhen you encounter problems, R provides several resources. The ? operator opens documentation for functions. The help.search() function searches the help system for topics. The example() function runs examples from a function’s documentation.\n\n\nCode\n?mean\nhelp.search(\"regression\")\nexample(plot)\n\n\nBeyond R’s built-in help, the internet offers vast resources. Stack Overflow has answers to almost any R question you can imagine. Package vignettes provide tutorials for specific packages. The RStudio community forums are welcoming to beginners.\nWhen asking for help online, provide a minimal reproducible example—the smallest piece of code that demonstrates your problem, including sample data. This makes it much easier for others to understand and solve your issue.",
    "crumbs": [
      "Foundations of Data Science",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>R and RStudio</span>"
    ]
  },
  {
    "objectID": "chapters/04-markdown-latex.html",
    "href": "chapters/04-markdown-latex.html",
    "title": "4  Markdown and LaTeX",
    "section": "",
    "text": "4.1 The Power of Plain Text\nScientific communication requires more than just words—we need formatted text, mathematical equations, code, figures, and tables. Traditionally, researchers used word processors for this purpose, but word processors have significant limitations for technical writing. They obscure the structure of documents behind visual formatting. They make collaboration difficult because different versions become hard to reconcile. They separate code from the documents that describe it, making it easy for analyses and their descriptions to get out of sync.\nMarkup languages offer a different approach. You write in plain text, adding simple annotations that specify how the document should be formatted. A processor then converts this annotated text into beautifully formatted output. Because the source is plain text, it can be version-controlled, compared across versions, and edited with any text editor.",
    "crumbs": [
      "Foundations of Data Science",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Markdown and LaTeX</span>"
    ]
  },
  {
    "objectID": "chapters/04-markdown-latex.html#what-is-markdown",
    "href": "chapters/04-markdown-latex.html#what-is-markdown",
    "title": "4  Markdown and LaTeX",
    "section": "4.2 What is Markdown?",
    "text": "4.2 What is Markdown?\nMarkdown is a lightweight markup language designed to be easy to read and write. Created by John Gruber in 2004, it uses intuitive syntax that looks reasonable even before processing. A document written in Markdown can be rendered into HTML, PDF, Word documents, presentations, websites, and more.\nThe basic philosophy is that common formatting should be quick to type and not interrupt the flow of writing. Bold text is wrapped in double asterisks. Italics use single asterisks. Headers are indicated by hash marks at the start of a line.\n\n4.2.1 Text Formatting\nTo make text italic, wrap it in single asterisks or underscores:\n*italic text* or _italic text_\nFor bold text, use double asterisks or underscores:\n**bold text** or __bold text__\nYou can combine them for bold italic:\n***bold and italic***\n\n\n4.2.2 Headers\nHeaders structure your document into sections. Use hash marks at the start of a line, with more hashes indicating lower-level headers:\n# First-Level Header\n## Second-Level Header\n### Third-Level Header\n\n\n4.2.3 Lists\nCreate bullet lists by starting lines with dashes, asterisks, or plus signs:\n- First item\n- Second item\n    - Sub-item (indent with spaces or tabs)\n    - Another sub-item\n- Third item\nFor numbered lists, use numbers followed by periods:\n1. First step\n2. Second step\n3. Third step\n\n\n4.2.4 Block Quotes\nBlock quotes are useful for highlighting important passages or attributing quotes:\n&gt; \"You know the greatest danger facing us is ourselves, \n&gt; an irrational fear of the unknown. But there's no such \n&gt; thing as the unknown — only things temporarily hidden, \n&gt; temporarily not understood.\"\n&gt;\n&gt; --- Captain James T. Kirk\nThis renders as:\n\n“You know the greatest danger facing us is ourselves, an irrational fear of the unknown. But there’s no such thing as the unknown — only things temporarily hidden, temporarily not understood.”\n— Captain James T. Kirk\n\n\n\n4.2.5 Links and Images\nLinks use square brackets for the text and parentheses for the URL:\n[Link text](https://www.example.com)\nImages use the same syntax with an exclamation mark prefix:\n![Image caption](path/to/image.png)\n\n\n4.2.6 Code\nInline code is wrapped in single backticks: `code`. Code blocks use triple backticks, optionally specifying the language for syntax highlighting:\n```r\nx &lt;- rnorm(100)\nmean(x)\n```",
    "crumbs": [
      "Foundations of Data Science",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Markdown and LaTeX</span>"
    ]
  },
  {
    "objectID": "chapters/04-markdown-latex.html#what-is-latex",
    "href": "chapters/04-markdown-latex.html#what-is-latex",
    "title": "4  Markdown and LaTeX",
    "section": "4.3 What is LaTeX?",
    "text": "4.3 What is LaTeX?\nLaTeX (pronounced “LAH-tek” or “LAY-tek”) is a document preparation system for high-quality typesetting, particularly of technical and scientific documents. Originally created by Leslie Lamport in the 1980s, LaTeX builds on the TeX typesetting system developed by Donald Knuth.\nLaTeX excels at mathematical notation. Complex equations that would be tedious or impossible to create in a word processor can be expressed elegantly in LaTeX. The system handles numbering, cross-references, bibliographies, and other scholarly apparatus automatically.\nMost importantly for our purposes, LaTeX can be embedded directly in Markdown documents. This gives us the best of both worlds: the simplicity of Markdown for prose and the power of LaTeX for mathematics.",
    "crumbs": [
      "Foundations of Data Science",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Markdown and LaTeX</span>"
    ]
  },
  {
    "objectID": "chapters/04-markdown-latex.html#mathematical-notation-with-latex",
    "href": "chapters/04-markdown-latex.html#mathematical-notation-with-latex",
    "title": "4  Markdown and LaTeX",
    "section": "4.4 Mathematical Notation with LaTeX",
    "text": "4.4 Mathematical Notation with LaTeX\n\n4.4.1 Inline and Display Math\nMathematical expressions can appear inline with text or as centered display equations. Inline math is wrapped in single dollar signs: $x^2$ produces \\(x^2\\). Display equations use double dollar signs:\n$$E = mc^2$$\nProduces:\n\\[E = mc^2\\]\n\n\n4.4.2 Greek Letters and Symbols\nGreek letters are typed as their names preceded by a backslash:\n$$\\alpha, \\beta, \\gamma, \\delta, \\epsilon$$\n$$\\mu, \\sigma, \\pi, \\theta, \\lambda$$\n\\[\\alpha, \\beta, \\gamma, \\delta, \\epsilon\\] \\[\\mu, \\sigma, \\pi, \\theta, \\lambda\\]\nCommon mathematical symbols:\n$$\\neq, \\approx, \\leq, \\geq, \\pm, \\times, \\div$$\n\\[\\neq, \\approx, \\leq, \\geq, \\pm, \\times, \\div\\]\n\n\n4.4.3 Superscripts and Subscripts\nSuperscripts use the caret ^ and subscripts use the underscore _:\n$$x^2, x_i, x_i^2, x_{ij}$$\n\\[x^2, x_i, x_i^2, x_{ij}\\]\nFor multi-character superscripts or subscripts, use curly braces:\n$$x^{n+1}, x_{i,j}$$\n\\[x^{n+1}, x_{i,j}\\]\n\n\n4.4.4 Fractions\nFractions use the \\frac{numerator}{denominator} command:\n$$\\frac{a}{b}, \\frac{x^2 + 1}{x - 1}$$\n\\[\\frac{a}{b}, \\frac{x^2 + 1}{x - 1}\\]\n\n\n4.4.5 Square Roots\nSquare roots and nth roots:\n$$\\sqrt{x}, \\sqrt[3]{x}, \\sqrt[n]{x}$$\n\\[\\sqrt{x}, \\sqrt[3]{x}, \\sqrt[n]{x}\\]\n\n\n4.4.6 Summation and Products\nSums and products with limits:\n$$\\sum_{i=1}^{n} x_i, \\prod_{i=1}^{n} x_i$$\n\\[\\sum_{i=1}^{n} x_i, \\prod_{i=1}^{n} x_i\\]\n\n\n4.4.7 Integrals\nIntegrals with limits:\n$$\\int_{a}^{b} f(x) \\, dx$$\n$$\\iint f(x,y) \\, dx \\, dy$$\n\\[\\int_{a}^{b} f(x) \\, dx\\] \\[\\iint f(x,y) \\, dx \\, dy\\]\n\n\n4.4.8 Statistical Formulas\nHere are some common statistical formulas in LaTeX:\nThe sample mean:\n$$\\bar{x} = \\frac{1}{n}\\sum_{i=1}^{n} x_i$$\n\\[\\bar{x} = \\frac{1}{n}\\sum_{i=1}^{n} x_i\\]\nThe sample variance:\n$$s^2 = \\frac{1}{n-1}\\sum_{i=1}^{n} (x_i - \\bar{x})^2$$\n\\[s^2 = \\frac{1}{n-1}\\sum_{i=1}^{n} (x_i - \\bar{x})^2\\]\nThe binomial probability formula:\n$$P(X=k) = \\binom{n}{k} p^{k} (1-p)^{n-k}$$\n\\[P(X=k) = \\binom{n}{k} p^{k} (1-p)^{n-k}\\]\nThe Poisson probability formula:\n$$P(Y=r) = \\frac{e^{-\\mu}\\mu^r}{r!}$$\n\\[P(Y=r) = \\frac{e^{-\\mu}\\mu^r}{r!}\\]\nThe normal distribution density:\n$$f(x) = \\frac{1}{\\sigma\\sqrt{2\\pi}} e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}$$\n\\[f(x) = \\frac{1}{\\sigma\\sqrt{2\\pi}} e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}\\]\n\n\n4.4.9 Matrices\nMatrices are created with the matrix environment:\n$$\\begin{matrix}\na & b \\\\\nc & d\n\\end{matrix}$$\n\\[\\begin{matrix}\na & b \\\\\nc & d\n\\end{matrix}\\]\nFor brackets around the matrix, use pmatrix (parentheses) or bmatrix (square brackets):\n$$\\begin{pmatrix}\na & b \\\\\nc & d\n\\end{pmatrix}$$\n\\[\\begin{pmatrix}\na & b \\\\\nc & d\n\\end{pmatrix}\\]",
    "crumbs": [
      "Foundations of Data Science",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Markdown and LaTeX</span>"
    ]
  },
  {
    "objectID": "chapters/04-markdown-latex.html#quarto-and-r-markdown",
    "href": "chapters/04-markdown-latex.html#quarto-and-r-markdown",
    "title": "4  Markdown and LaTeX",
    "section": "4.5 Quarto and R Markdown",
    "text": "4.5 Quarto and R Markdown\nQuarto and R Markdown extend Markdown by allowing you to embed executable code chunks. When the document is rendered, the code runs and its output—whether text, tables, or figures—is automatically included in the final document.\nA code chunk in Quarto looks like this:\n```{r}\nx &lt;- rnorm(100)\nmean(x)\n```\nWhen rendered, this shows both the code and its output:\n\n\nCode\nx &lt;- rnorm(100)\nmean(x)\n\n\n[1] 0.04921364\n\n\nChunk options control how code is displayed and executed. Common options include echo (whether to show the code), eval (whether to run the code), and fig-width and fig-height (figure dimensions).\n```{r}\n#| echo: false\n#| fig-width: 6\n#| fig-height: 4\nhist(rnorm(1000))\n```",
    "crumbs": [
      "Foundations of Data Science",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Markdown and LaTeX</span>"
    ]
  },
  {
    "objectID": "chapters/04-markdown-latex.html#why-this-matters",
    "href": "chapters/04-markdown-latex.html#why-this-matters",
    "title": "4  Markdown and LaTeX",
    "section": "4.6 Why This Matters",
    "text": "4.6 Why This Matters\nThe combination of Markdown, LaTeX, and executable code enables truly reproducible research. Your analysis code lives in the same document as your prose. When data change, you re-render the document and everything updates automatically. Collaborators can see exactly what you did. Future you can understand past you’s work.\nThese tools have become standard in data science. Learning them now will pay dividends throughout your career, whether you are writing homework assignments, thesis chapters, journal articles, or technical reports.",
    "crumbs": [
      "Foundations of Data Science",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Markdown and LaTeX</span>"
    ]
  },
  {
    "objectID": "chapters/05-tidy-data.html",
    "href": "chapters/05-tidy-data.html",
    "title": "5  Tidy Data and Data Wrangling",
    "section": "",
    "text": "5.1 What is Tidy Data?\nData comes in many shapes, and not all shapes are equally convenient for analysis. The concept of “tidy data” provides a standard way to organize data that works well with R and makes many analyses straightforward. In tidy data, each variable forms a column, each observation forms a row, and each type of observational unit forms a table.\nThis structure might seem obvious, but real-world data rarely arrives in tidy form. Spreadsheets often encode information in column names, spread a single variable across multiple columns, or combine multiple variables in a single column. Data wrangling is the process of transforming messy data into tidy data.",
    "crumbs": [
      "Foundations of Data Science",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Tidy Data and Data Wrangling</span>"
    ]
  },
  {
    "objectID": "chapters/05-tidy-data.html#rules-of-thumb-for-data-organization",
    "href": "chapters/05-tidy-data.html#rules-of-thumb-for-data-organization",
    "title": "5  Tidy Data and Data Wrangling",
    "section": "5.2 Rules of Thumb for Data Organization",
    "text": "5.2 Rules of Thumb for Data Organization\nWhether you are creating a new dataset or cleaning an existing one, following these principles will save you time and frustration.\nStore a copy of your data in nonproprietary formats like plain text CSV files. Proprietary formats can become unreadable as software changes. Keep an uncorrected copy of your original data separate from any cleaned or processed versions. Use descriptive names for files and variables that convey meaning without requiring external documentation. Include a header row with variable names. Maintain metadata—a data dictionary explaining what each variable means, how it was measured, and what units it uses.\nWhen you add new observations, add rows. When you add new variables, add columns. A column should contain only one data type—don’t mix numbers and text in the same column. Dates should be in a consistent format. Missing values should be represented consistently, typically as empty cells or NA in R.",
    "crumbs": [
      "Foundations of Data Science",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Tidy Data and Data Wrangling</span>"
    ]
  },
  {
    "objectID": "chapters/05-tidy-data.html#types-of-data",
    "href": "chapters/05-tidy-data.html#types-of-data",
    "title": "5  Tidy Data and Data Wrangling",
    "section": "5.3 Types of Data",
    "text": "5.3 Types of Data\nUnderstanding the types of data you are working with guides how you analyze them.\nCategorical data classify observations into groups. Nominal categorical data have no inherent order—for example, species names, treatment groups, or colors. Ordinal categorical data have a meaningful order—ratings like “low,” “medium,” and “high,” or educational levels. In R, categorical data are often represented as factors, which store both the values and the set of possible levels.\nQuantitative data are numerical measurements. Interval data have meaningful differences between values but no true zero point—temperature in Celsius, where 0° does not mean “no temperature.” Ratio data have a true zero and meaningful ratios—mass, length, counts, where zero means “none” and twice as much is twice as much.\n\n\n\n\n\n\n\n\n\nCategorical\n\nQuantitative\n\n\n\n\n\nOrdinal\nNominal\nRatio\nInterval\n\n\nsmall, medium, large\napples, oranges\nkilograms, dollars\ntemperature, calendar year\n\n\nordered character\ncharacter\nnumeric\ninteger",
    "crumbs": [
      "Foundations of Data Science",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Tidy Data and Data Wrangling</span>"
    ]
  },
  {
    "objectID": "chapters/05-tidy-data.html#the-tidyverse",
    "href": "chapters/05-tidy-data.html#the-tidyverse",
    "title": "5  Tidy Data and Data Wrangling",
    "section": "5.4 The Tidyverse",
    "text": "5.4 The Tidyverse\nThe tidyverse is a collection of R packages designed for data science. These packages share a common philosophy and are designed to work together seamlessly. The core tidyverse packages include ggplot2 for visualization, dplyr for data manipulation, tidyr for reshaping data, readr for reading data files, and several others.\n\n\nCode\nlibrary(tidyverse)\n\n\nLoading the tidyverse loads all core packages at once. The message shows which packages are attached and notes any functions that conflict with base R or other packages.",
    "crumbs": [
      "Foundations of Data Science",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Tidy Data and Data Wrangling</span>"
    ]
  },
  {
    "objectID": "chapters/05-tidy-data.html#tibbles",
    "href": "chapters/05-tidy-data.html#tibbles",
    "title": "5  Tidy Data and Data Wrangling",
    "section": "5.5 Tibbles",
    "text": "5.5 Tibbles\nTibbles are the tidyverse’s enhanced data frames. They print more informatively, showing only the first few rows and as many columns as fit on screen, along with the dimensions and column types.\n\n\n\n\n\n\n\nCode\n# Create a tibble\nmy_tibble &lt;- tibble(\n  name = c(\"Alice\", \"Bob\", \"Carol\"),\n  score = c(85, 92, 78),\n  passed = c(TRUE, TRUE, TRUE)\n)\nmy_tibble\n\n\n# A tibble: 3 × 3\n  name  score passed\n  &lt;chr&gt; &lt;dbl&gt; &lt;lgl&gt; \n1 Alice    85 TRUE  \n2 Bob      92 TRUE  \n3 Carol    78 TRUE  \n\n\nThe column types are shown below the column names: &lt;chr&gt; for character, &lt;dbl&gt; for double (numeric), and &lt;lgl&gt; for logical.",
    "crumbs": [
      "Foundations of Data Science",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Tidy Data and Data Wrangling</span>"
    ]
  },
  {
    "objectID": "chapters/05-tidy-data.html#key-dplyr-verbs",
    "href": "chapters/05-tidy-data.html#key-dplyr-verbs",
    "title": "5  Tidy Data and Data Wrangling",
    "section": "5.6 Key dplyr Verbs",
    "text": "5.6 Key dplyr Verbs\nThe dplyr package provides a grammar for data manipulation. Five key verbs handle most data manipulation tasks.\n\n5.6.1 filter(): Subset Rows\nfilter() selects rows that meet specified conditions.\n\n\nCode\n# Flights in November or December\nfilter(flights, month == 11 | month == 12)\n\n# Flights with arrival delay greater than 2 hours\nfilter(flights, arr_delay &gt; 120)\n\n\nConditions use comparison operators: == (equals), != (not equals), &lt;, &gt;, &lt;=, &gt;=. Combine conditions with & (and) and | (or). The %in% operator checks membership in a set.\n\n\n5.6.2 select(): Choose Columns\nselect() picks columns by name.\n\n\nCode\n# Select specific columns\nselect(flights, year, month, day)\n\n# Select a range of columns\nselect(flights, year:day)\n\n# Drop columns\nselect(flights, -year, -month)\n\n\n\n\n5.6.3 arrange(): Sort Rows\narrange() reorders rows by column values.\n\n\nCode\n# Sort by year, then month, then day\narrange(flights, year, month, day)\n\n# Sort in descending order\narrange(flights, desc(dep_delay))\n\n\n\n\n5.6.4 mutate(): Create New Columns\nmutate() adds new columns that are functions of existing columns.\n\n\nCode\nmutate(flights,\n  gain = arr_delay - dep_delay,\n  hours = air_time / 60,\n  gain_per_hour = gain / hours\n)\n\n\n\n\n5.6.5 summarize(): Aggregate Data\nsummarize() collapses multiple rows into summary values.\n\n\nCode\nsummarize(flights, \n  mean_delay = mean(dep_delay, na.rm = TRUE),\n  n = n()\n)\n\n\nThe na.rm = TRUE argument tells mean() to ignore missing values. The n() function counts rows.",
    "crumbs": [
      "Foundations of Data Science",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Tidy Data and Data Wrangling</span>"
    ]
  },
  {
    "objectID": "chapters/05-tidy-data.html#grouping-with-group_by",
    "href": "chapters/05-tidy-data.html#grouping-with-group_by",
    "title": "5  Tidy Data and Data Wrangling",
    "section": "5.7 Grouping with group_by()",
    "text": "5.7 Grouping with group_by()\nThe real power of summarize() emerges when combined with group_by(), which splits data into groups for separate analysis.\n\n\nCode\n# Group by destination, then summarize\nby_dest &lt;- group_by(flights, dest)\nsummarize(by_dest, \n  count = n(),\n  mean_delay = mean(arr_delay, na.rm = TRUE)\n)\n\n\nThis calculates summary statistics separately for each destination.",
    "crumbs": [
      "Foundations of Data Science",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Tidy Data and Data Wrangling</span>"
    ]
  },
  {
    "objectID": "chapters/05-tidy-data.html#the-pipe-operator",
    "href": "chapters/05-tidy-data.html#the-pipe-operator",
    "title": "5  Tidy Data and Data Wrangling",
    "section": "5.8 The Pipe Operator",
    "text": "5.8 The Pipe Operator\nChaining multiple operations together can become unwieldy with nested function calls. The pipe operator |&gt; (or the tidyverse’s %&gt;%) passes the result of one operation as the first argument of the next, allowing you to read operations left-to-right, top-to-bottom.\n\n\nCode\n# Without pipe: nested and hard to read\nsummarize(group_by(filter(flights, !is.na(arr_delay)), dest), \n          mean_delay = mean(arr_delay))\n\n# With pipe: clear sequence of operations\nflights |&gt;\n  filter(!is.na(arr_delay)) |&gt;\n  group_by(dest) |&gt;\n  summarize(mean_delay = mean(arr_delay))\n\n\nRead the pipe as “then”—take flights, then filter, then group, then summarize.",
    "crumbs": [
      "Foundations of Data Science",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Tidy Data and Data Wrangling</span>"
    ]
  },
  {
    "objectID": "chapters/05-tidy-data.html#handling-missing-values",
    "href": "chapters/05-tidy-data.html#handling-missing-values",
    "title": "5  Tidy Data and Data Wrangling",
    "section": "5.9 Handling Missing Values",
    "text": "5.9 Handling Missing Values\nMissing values are a fact of life in real data. In R, missing values are represented as NA. Most operations involving NA return NA, which can cause problems if you are not careful.\n\n\nCode\nx &lt;- c(1, 2, NA, 4)\nmean(x)\n\n\n[1] NA\n\n\nCode\nmean(x, na.rm = TRUE)\n\n\n[1] 2.333333\n\n\nCheck for missing values with is.na():\n\n\nCode\nis.na(x)\n\n\n[1] FALSE FALSE  TRUE FALSE\n\n\nCode\nsum(is.na(x))  # count missing values\n\n\n[1] 1\n\n\nFilter out missing values:\n\n\nCode\nx[!is.na(x)]\n\n\n[1] 1 2 4\n\n\nOr use tidyr functions:\n\n\nCode\n# Remove rows with any missing values\ndrop_na(data)\n\n# Remove rows with missing values in specific columns\ndrop_na(data, column_name)",
    "crumbs": [
      "Foundations of Data Science",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Tidy Data and Data Wrangling</span>"
    ]
  },
  {
    "objectID": "chapters/05-tidy-data.html#reshaping-data",
    "href": "chapters/05-tidy-data.html#reshaping-data",
    "title": "5  Tidy Data and Data Wrangling",
    "section": "5.10 Reshaping Data",
    "text": "5.10 Reshaping Data\nSometimes data is not in the right shape for your analysis. The tidyr package provides functions to reshape data.\npivot_longer() takes wide data (variables spread across columns) and makes it long (variables stacked in rows). pivot_wider() does the reverse.\n\n\nCode\n# Example: wide data\nwide_data &lt;- tibble(\n  sample = c(\"A\", \"B\", \"C\"),\n  treatment_1 = c(10, 15, 12),\n  treatment_2 = c(8, 14, 11)\n)\nwide_data\n\n\n# A tibble: 3 × 3\n  sample treatment_1 treatment_2\n  &lt;chr&gt;        &lt;dbl&gt;       &lt;dbl&gt;\n1 A               10           8\n2 B               15          14\n3 C               12          11\n\n\nCode\n# Convert to long format\nlong_data &lt;- wide_data |&gt;\n  pivot_longer(\n    cols = starts_with(\"treatment\"),\n    names_to = \"treatment\",\n    values_to = \"response\"\n  )\nlong_data\n\n\n# A tibble: 6 × 3\n  sample treatment   response\n  &lt;chr&gt;  &lt;chr&gt;          &lt;dbl&gt;\n1 A      treatment_1       10\n2 A      treatment_2        8\n3 B      treatment_1       15\n4 B      treatment_2       14\n5 C      treatment_1       12\n6 C      treatment_2       11",
    "crumbs": [
      "Foundations of Data Science",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Tidy Data and Data Wrangling</span>"
    ]
  },
  {
    "objectID": "chapters/05-tidy-data.html#joining-data",
    "href": "chapters/05-tidy-data.html#joining-data",
    "title": "5  Tidy Data and Data Wrangling",
    "section": "5.11 Joining Data",
    "text": "5.11 Joining Data\nOften data comes in multiple tables that need to be combined. Join operations merge tables based on matching values in key columns.\n\n\nCode\n# Example tables\nsamples &lt;- tibble(\n  sample_id = c(\"S1\", \"S2\", \"S3\"),\n  concentration = c(0.1, 0.5, 1.0)\n)\n\nmeasurements &lt;- tibble(\n  sample_id = c(\"S1\", \"S1\", \"S2\", \"S2\", \"S3\", \"S3\"),\n  replicate = c(1, 2, 1, 2, 1, 2),\n  value = c(2.3, 2.1, 5.4, 5.6, 10.2, 10.8)\n)\n\n# Join tables\nleft_join(measurements, samples, by = \"sample_id\")\n\n\n# A tibble: 6 × 4\n  sample_id replicate value concentration\n  &lt;chr&gt;         &lt;dbl&gt; &lt;dbl&gt;         &lt;dbl&gt;\n1 S1                1   2.3           0.1\n2 S1                2   2.1           0.1\n3 S2                1   5.4           0.5\n4 S2                2   5.6           0.5\n5 S3                1  10.2           1  \n6 S3                2  10.8           1",
    "crumbs": [
      "Foundations of Data Science",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Tidy Data and Data Wrangling</span>"
    ]
  },
  {
    "objectID": "chapters/05-tidy-data.html#practice-exercise",
    "href": "chapters/05-tidy-data.html#practice-exercise",
    "title": "5  Tidy Data and Data Wrangling",
    "section": "5.12 Practice Exercise",
    "text": "5.12 Practice Exercise\nHere is a workflow to practice these concepts:\n\nRead a dataset into R\nConvert it to a tibble with as_tibble()\nSelect the columns you need\nFilter to the observations of interest\nCreate new variables with mutate\nGroup by categorical variables and summarize\nVisualize the results\n\nWorking through this process with your own data will cement these concepts better than any number of examples.",
    "crumbs": [
      "Foundations of Data Science",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Tidy Data and Data Wrangling</span>"
    ]
  },
  {
    "objectID": "chapters/06-data-visualization.html",
    "href": "chapters/06-data-visualization.html",
    "title": "6  Data Visualization",
    "section": "",
    "text": "6.1 The Grammar of Graphics\nData visualization is both an art and a science. A well-designed graphic can reveal patterns, communicate findings, and guide analysis in ways that tables of numbers cannot. The ggplot2 package implements a coherent system for creating graphics based on Leland Wilkinson’s “Grammar of Graphics”—a framework that describes the fundamental components from which all statistical graphics can be built.\nJust as grammar provides rules for constructing sentences from words, the grammar of graphics provides rules for constructing visualizations from components. Every graphic is composed of data, aesthetic mappings that connect variables to visual properties, and geometric objects that represent data points. Additional components like scales, statistical transformations, coordinate systems, and facets allow for sophisticated customizations.",
    "crumbs": [
      "Foundations of Data Science",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "chapters/06-data-visualization.html#building-plots-with-ggplot2",
    "href": "chapters/06-data-visualization.html#building-plots-with-ggplot2",
    "title": "6  Data Visualization",
    "section": "6.2 Building Plots with ggplot2",
    "text": "6.2 Building Plots with ggplot2\nThe basic structure of a ggplot2 call begins with the ggplot() function, which creates a coordinate system. You add layers to this foundation using the + operator.\n\n\nCode\nggplot(data = mpg, mapping = aes(x = displ, y = hwy)) +\n  geom_point()\n\n\n\n\n\n\n\n\n\nThis creates a scatterplot of highway fuel efficiency against engine displacement using the built-in mpg dataset. The aes() function establishes the aesthetic mapping—which variables map to which visual properties. Here, displ maps to the x-axis and hwy to the y-axis. The geom_point() function adds a layer of points.",
    "crumbs": [
      "Foundations of Data Science",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "chapters/06-data-visualization.html#aesthetic-mappings",
    "href": "chapters/06-data-visualization.html#aesthetic-mappings",
    "title": "6  Data Visualization",
    "section": "6.3 Aesthetic Mappings",
    "text": "6.3 Aesthetic Mappings\nAesthetics are visual properties of the plot. Beyond position (x and y), common aesthetics include color, size, shape, and transparency (alpha). You can map variables to these aesthetics to encode additional information.\n\n\nCode\nggplot(mpg, aes(x = displ, y = hwy, color = class)) + \n  geom_point(size = 3, alpha = 0.7)\n\n\n\n\n\n\n\n\n\nNow the color of each point indicates the vehicle class. The legend is created automatically. Note that aesthetics defined inside aes() are mapped to variables, while those defined outside (like size = 3) apply uniformly to all points.",
    "crumbs": [
      "Foundations of Data Science",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "chapters/06-data-visualization.html#geometric-objects",
    "href": "chapters/06-data-visualization.html#geometric-objects",
    "title": "6  Data Visualization",
    "section": "6.4 Geometric Objects",
    "text": "6.4 Geometric Objects\nGeometric objects, or geoms, determine what type of plot you create. Different geoms represent data in different ways.\n\n6.4.1 Scatterplots with geom_point()\nPoints are good for showing the relationship between two continuous variables:\n\n\nCode\nggplot(mpg, aes(x = displ, y = hwy)) +\n  geom_point()\n\n\n\n\n\n\n\n\n\n\n\n6.4.2 Line Plots with geom_line() and geom_smooth()\nLines connect points in order, useful for time series or showing trends:\n\n\nCode\nggplot(mpg, aes(x = displ, y = hwy)) +\n  geom_point() +\n  geom_smooth()\n\n\n\n\n\n\n\n\n\nThe geom_smooth() function adds a smoothed conditional mean with confidence interval.\n\n\n6.4.3 Bar Charts with geom_bar()\nBar charts show counts or summaries of categorical data:\n\n\nCode\nggplot(diamonds, aes(x = cut)) +\n  geom_bar()\n\n\n\n\n\n\n\n\n\nUse fill to color bars by another variable:\n\n\nCode\nggplot(diamonds, aes(x = cut, fill = clarity)) +\n  geom_bar(position = \"dodge\")\n\n\n\n\n\n\n\n\n\n\n\n6.4.4 Histograms with geom_histogram()\nHistograms show the distribution of a continuous variable:\n\n\nCode\nggplot(diamonds, aes(x = carat)) +\n  geom_histogram(binwidth = 0.1, fill = \"steelblue\", color = \"white\")\n\n\n\n\n\n\n\n\n\n\n\n6.4.5 Boxplots with geom_boxplot()\nBoxplots summarize distributions and highlight outliers:\n\n\nCode\nggplot(mpg, aes(x = class, y = hwy)) +\n  geom_boxplot()",
    "crumbs": [
      "Foundations of Data Science",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "chapters/06-data-visualization.html#combining-multiple-geoms",
    "href": "chapters/06-data-visualization.html#combining-multiple-geoms",
    "title": "6  Data Visualization",
    "section": "6.5 Combining Multiple Geoms",
    "text": "6.5 Combining Multiple Geoms\nYou can layer multiple geoms to create richer visualizations:\n\n\nCode\nggplot(mpg, aes(x = displ, y = hwy)) +\n  geom_point(aes(color = class)) +\n  geom_smooth(se = FALSE, color = \"black\")",
    "crumbs": [
      "Foundations of Data Science",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "chapters/06-data-visualization.html#faceting",
    "href": "chapters/06-data-visualization.html#faceting",
    "title": "6  Data Visualization",
    "section": "6.6 Faceting",
    "text": "6.6 Faceting\nFaceting creates small multiples—separate panels for subsets of the data. This is powerful for comparing patterns across groups.\n\n\nCode\nggplot(mpg, aes(x = displ, y = hwy)) +\n  geom_point() +\n  facet_wrap(~ class, nrow = 2)\n\n\n\n\n\n\n\n\n\nUse facet_grid() for two-variable faceting:\n\n\nCode\nggplot(mpg, aes(x = displ, y = hwy)) +\n  geom_point() +\n  facet_grid(drv ~ cyl)",
    "crumbs": [
      "Foundations of Data Science",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "chapters/06-data-visualization.html#labels-and-titles",
    "href": "chapters/06-data-visualization.html#labels-and-titles",
    "title": "6  Data Visualization",
    "section": "6.7 Labels and Titles",
    "text": "6.7 Labels and Titles\nAdd informative labels with the labs() function:\n\n\nCode\nggplot(mpg, aes(x = displ, y = hwy, color = class)) +\n  geom_point() +\n  geom_smooth(se = FALSE) +\n  labs(\n    title = \"Fuel Efficiency Decreases with Engine Size\",\n    subtitle = \"Data from EPA fuel economy tests\",\n    caption = \"Source: fueleconomy.gov\",\n    x = \"Engine Displacement (liters)\",\n    y = \"Highway Fuel Efficiency (mpg)\",\n    color = \"Vehicle Class\"\n  )",
    "crumbs": [
      "Foundations of Data Science",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "chapters/06-data-visualization.html#themes",
    "href": "chapters/06-data-visualization.html#themes",
    "title": "6  Data Visualization",
    "section": "6.8 Themes",
    "text": "6.8 Themes\nThemes control the non-data aspects of the plot—background, grid lines, fonts, etc. ggplot2 includes several built-in themes:\n\n\nCode\nggplot(mpg, aes(x = displ, y = hwy)) +\n  geom_point() +\n  theme_classic()\n\n\n\n\n\n\n\n\n\nOther built-in themes include theme_minimal(), theme_bw(), theme_light(), and theme_dark(). The ggthemes package provides many additional themes.",
    "crumbs": [
      "Foundations of Data Science",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "chapters/06-data-visualization.html#choosing-the-right-plot",
    "href": "chapters/06-data-visualization.html#choosing-the-right-plot",
    "title": "6  Data Visualization",
    "section": "6.9 Choosing the Right Plot",
    "text": "6.9 Choosing the Right Plot\nChoosing an appropriate visualization depends on the types of variables you want to display and the message you want to convey.\n\n\n\n\n\nFor one categorical variable, use bar charts. For one continuous variable, use histograms or density plots. For two continuous variables, use scatterplots. For one continuous and one categorical, use boxplots or violin plots. For two categorical variables, use stacked or grouped bar charts or heat maps.",
    "crumbs": [
      "Foundations of Data Science",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "chapters/06-data-visualization.html#principles-of-effective-visualization",
    "href": "chapters/06-data-visualization.html#principles-of-effective-visualization",
    "title": "6  Data Visualization",
    "section": "6.10 Principles of Effective Visualization",
    "text": "6.10 Principles of Effective Visualization\nEdward Tufte articulated principles of graphical excellence that remain influential: “Graphical excellence is that which gives to the viewer the greatest number of ideas in the shortest time with the least ink in the smallest space.”\nKey principles include:\nShow the data. Above all else, make the data visible. Avoid chart junk that obscures what you are trying to communicate.\nEncourage comparison. Design graphics to facilitate comparison of different groups or conditions.\nRepresent magnitudes honestly. The visual representation should be proportional to the numerical quantities being represented. Avoid truncated axes that exaggerate differences.\nMinimize clutter. Remove unnecessary grid lines, borders, and decorations. Every element should serve a purpose.\nMake displays easy to interpret. Use clear labels, appropriate colors, and logical organization.",
    "crumbs": [
      "Foundations of Data Science",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "chapters/06-data-visualization.html#examples-of-poor-graphics",
    "href": "chapters/06-data-visualization.html#examples-of-poor-graphics",
    "title": "6  Data Visualization",
    "section": "6.11 Examples of Poor Graphics",
    "text": "6.11 Examples of Poor Graphics\nRecognizing bad graphics helps you avoid making them.\n\n\n\n\n\nTicker-tape style displays make it hard to see patterns. Lines connecting unrelated points mislead. Pie charts make comparisons difficult because humans are poor at judging angles. Three-dimensional effects distort perception without adding information.",
    "crumbs": [
      "Foundations of Data Science",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "chapters/06-data-visualization.html#a-famous-good-example",
    "href": "chapters/06-data-visualization.html#a-famous-good-example",
    "title": "6  Data Visualization",
    "section": "6.12 A Famous Good Example",
    "text": "6.12 A Famous Good Example\nCharles Minard’s 1869 map of Napoleon’s Russian campaign is often cited as one of the best statistical graphics ever made. It displays six variables: the size of the army, its location (latitude and longitude), direction of movement, temperature, and date—all in a single coherent image.\n\n\n\n\n\nThe graphic tells a story. You can see the army shrink as it advances, the devastating losses during the retreat, and the correlation with plummeting temperatures. No legend is needed; the meaning is immediately apparent.",
    "crumbs": [
      "Foundations of Data Science",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "chapters/06-data-visualization.html#saving-plots",
    "href": "chapters/06-data-visualization.html#saving-plots",
    "title": "6  Data Visualization",
    "section": "6.13 Saving Plots",
    "text": "6.13 Saving Plots\nSave plots with ggsave():\n\n\nCode\n# Create and save a plot\np &lt;- ggplot(mpg, aes(x = displ, y = hwy)) +\n  geom_point()\n\nggsave(\"my_plot.png\", p, width = 8, height = 6, dpi = 300)\nggsave(\"my_plot.pdf\", p, width = 8, height = 6)\n\n\nThe function infers the format from the file extension. Specify dimensions and resolution for publication-quality output.",
    "crumbs": [
      "Foundations of Data Science",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "chapters/06-data-visualization.html#practice",
    "href": "chapters/06-data-visualization.html#practice",
    "title": "6  Data Visualization",
    "section": "6.14 Practice",
    "text": "6.14 Practice\nThe best way to learn ggplot2 is to use it. Take a dataset you care about and try different visualizations. Experiment with aesthetics, geoms, and facets. Read error messages carefully—they often point directly to the problem. The ggplot2 documentation and the R Graph Gallery (r-graph-gallery.com) provide extensive examples to learn from.",
    "crumbs": [
      "Foundations of Data Science",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "chapters/07-probability-foundations.html",
    "href": "chapters/07-probability-foundations.html",
    "title": "7  Foundations of Probability",
    "section": "",
    "text": "7.1 Why Probability Matters\nStatistics is fundamentally about quantifying and managing uncertainty. We rarely know the world perfectly, yet we need to draw conclusions and make decisions. Probability provides the mathematical language for reasoning about uncertainty.\nWhen you take a sample from a population, the values you obtain are subject to random variation. When you run an experiment, the outcomes vary even under identical conditions. Probability theory tells us what to expect from this variation and provides the foundation for statistical inference.",
    "crumbs": [
      "Probability and Distributions",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Foundations of Probability</span>"
    ]
  },
  {
    "objectID": "chapters/07-probability-foundations.html#two-interpretations-of-probability",
    "href": "chapters/07-probability-foundations.html#two-interpretations-of-probability",
    "title": "7  Foundations of Probability",
    "section": "7.2 Two Interpretations of Probability",
    "text": "7.2 Two Interpretations of Probability\nThere are two main ways to think about what probability means.\nThe frequentist interpretation views probabilities as long-run relative frequencies. If we say the probability of heads when flipping a fair coin is 0.5, we mean that if we flipped the coin many, many times, about half the flips would come up heads. This interpretation grounds probability in observable, repeatable phenomena.\nThe subjective (Bayesian) interpretation views probability as a measure of belief or uncertainty. A probability statement expresses how confident someone is that an event will occur, given their current information. This interpretation allows us to assign probabilities to one-time events and to update beliefs as we gather evidence.\nBoth interpretations have their uses, and modern statistics draws on both perspectives. For now, the frequentist interpretation provides good intuition for the concepts we will develop.",
    "crumbs": [
      "Probability and Distributions",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Foundations of Probability</span>"
    ]
  },
  {
    "objectID": "chapters/07-probability-foundations.html#random-variables-and-sample-spaces",
    "href": "chapters/07-probability-foundations.html#random-variables-and-sample-spaces",
    "title": "7  Foundations of Probability",
    "section": "7.3 Random Variables and Sample Spaces",
    "text": "7.3 Random Variables and Sample Spaces\nA random variable is a quantity that can take different values with different probabilities. The outcome of a coin flip, the number of bacterial colonies on a plate, and the expression level of a gene are all random variables.\nThe sample space of a random variable is the set of all possible values it can take. For a coin flip, the sample space is {Heads, Tails}. For a die roll, it is {1, 2, 3, 4, 5, 6}. For the concentration of a protein, it might be any non-negative real number.\nA probability distribution describes how likely each value in the sample space is. For discrete random variables (those that take distinct values), we use a probability mass function that gives the probability of each possible value. For continuous random variables (those that can take any value in a range), we use a probability density function from which probabilities are calculated by integration.\nOne fundamental rule: the probabilities across the entire sample space must sum (or integrate) to 1. Something from the sample space must happen.",
    "crumbs": [
      "Probability and Distributions",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Foundations of Probability</span>"
    ]
  },
  {
    "objectID": "chapters/07-probability-foundations.html#the-bernoulli-distribution",
    "href": "chapters/07-probability-foundations.html#the-bernoulli-distribution",
    "title": "7  Foundations of Probability",
    "section": "7.4 The Bernoulli Distribution",
    "text": "7.4 The Bernoulli Distribution\nThe simplest probability distribution describes a single event with two possible outcomes—success or failure, yes or no, heads or tails. This is the Bernoulli distribution.\nConsider flipping a fair coin once. The probability of heads is:\n\\[P(X = \\text{Head}) = \\frac{1}{2} = 0.5 = p\\]\nAnd the probability of tails is:\n\\[P(X = \\text{Tail}) = \\frac{1}{2} = 0.5 = 1 - p = q\\]\nIf the coin is not fair, \\(p\\) might differ from 0.5, but the probabilities still sum to 1:\n\\[p + (1-p) = 1\\] \\[p + q = 1\\]\nThis same framework applies to any binary outcome: whether a patient responds to treatment, whether an electronic component fails, whether an allele is inherited from a parent.",
    "crumbs": [
      "Probability and Distributions",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Foundations of Probability</span>"
    ]
  },
  {
    "objectID": "chapters/07-probability-foundations.html#probability-rules",
    "href": "chapters/07-probability-foundations.html#probability-rules",
    "title": "7  Foundations of Probability",
    "section": "7.5 Probability Rules",
    "text": "7.5 Probability Rules\nTwo fundamental rules govern how probabilities combine.\n\n7.5.1 The AND Rule (Multiplication)\nThe probability that two independent events both occur is the product of their individual probabilities. If you flip a coin twice:\n\\[P(\\text{First = Head AND Second = Head}) = p \\times p = p^2\\]\nMore generally, for independent events A and B:\n\\[P(A \\text{ and } B) = P(A) \\times P(B)\\]\nFor a fair coin with \\(p = 0.5\\):\n\\[P(\\text{HH}) = 0.5 \\times 0.5 = 0.25\\] \\[P(\\text{HT}) = 0.5 \\times 0.5 = 0.25\\] \\[P(\\text{TH}) = 0.5 \\times 0.5 = 0.25\\] \\[P(\\text{TT}) = 0.5 \\times 0.5 = 0.25\\]\n\n\n7.5.2 The OR Rule (Addition)\nThe probability that at least one of two mutually exclusive events occurs is the sum of their probabilities. The probability of getting heads or tails on a single flip:\n\\[P(\\text{Head OR Tail}) = p + q = 1\\]\nThe probability of getting exactly one head in two flips (either HT or TH):\n\\[P(\\text{one head}) = P(\\text{HT}) + P(\\text{TH}) = 0.25 + 0.25 = 0.5\\]\nFor mutually exclusive events A and B:\n\\[P(A \\text{ or } B) = P(A) + P(B)\\]\nThese simple rules—AND means multiply, OR means add—underlie most probability calculations. Complex probability distributions can be built up from these basic principles.",
    "crumbs": [
      "Probability and Distributions",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Foundations of Probability</span>"
    ]
  },
  {
    "objectID": "chapters/07-probability-foundations.html#simulating-coin-flips-in-r",
    "href": "chapters/07-probability-foundations.html#simulating-coin-flips-in-r",
    "title": "7  Foundations of Probability",
    "section": "7.6 Simulating Coin Flips in R",
    "text": "7.6 Simulating Coin Flips in R\nWe can explore these concepts through simulation:\n\n\nCode\n# Simulate a fair coin\ncoin &lt;- c(\"heads\", \"tails\")\n\n# Flip once\nsample(coin, size = 1)\n\n\n[1] \"tails\"\n\n\n\n\nCode\n# Flip many times and see the distribution\nset.seed(42)\nflips &lt;- sample(coin, prob = c(0.5, 0.5), size = 100, replace = TRUE)\nbarplot(table(flips), main = \"100 Fair Coin Flips\", col = \"steelblue\")\n\n\n\n\n\n\n\n\n\nTry changing the probabilities or sample size to see how the results change. With small samples, randomness can produce results far from the expected proportions. With large samples, the observed proportions converge to the true probabilities—a manifestation of the law of large numbers.",
    "crumbs": [
      "Probability and Distributions",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Foundations of Probability</span>"
    ]
  },
  {
    "objectID": "chapters/07-probability-foundations.html#joint-and-conditional-probability",
    "href": "chapters/07-probability-foundations.html#joint-and-conditional-probability",
    "title": "7  Foundations of Probability",
    "section": "7.7 Joint and Conditional Probability",
    "text": "7.7 Joint and Conditional Probability\nWhen we consider two random variables together, we encounter joint and conditional probabilities.\nThe joint probability \\(P(X, Y)\\) is the probability that both X and Y take particular values. For independent events:\n\\[P(X, Y) = P(X) \\times P(Y)\\]\nHowever, many pairs of variables are not independent. The conditional probability \\(P(Y|X)\\) is the probability of Y given that X has occurred. It captures how knowing X changes our beliefs about Y.\nFor independent variables, knowing X tells us nothing about Y:\n\\[P(Y|X) = P(Y) \\quad \\text{and} \\quad P(X|Y) = P(X)\\]\nFor non-independent (dependent) variables:\n\\[P(Y|X) \\neq P(Y) \\quad \\text{and} \\quad P(X|Y) \\neq P(X)\\]\nThe relationship between joint and conditional probability is given by:\n\\[P(X, Y) = P(Y|X) \\times P(X) = P(X|Y) \\times P(Y)\\]",
    "crumbs": [
      "Probability and Distributions",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Foundations of Probability</span>"
    ]
  },
  {
    "objectID": "chapters/07-probability-foundations.html#bayes-theorem",
    "href": "chapters/07-probability-foundations.html#bayes-theorem",
    "title": "7  Foundations of Probability",
    "section": "7.8 Bayes’ Theorem",
    "text": "7.8 Bayes’ Theorem\nRearranging the above relationship yields Bayes’ theorem, a cornerstone of probabilistic reasoning:\n\\[P(X|Y) = \\frac{P(Y|X) \\times P(X)}{P(Y)}\\]\nBayes’ theorem tells us how to update our beliefs about X after observing Y. It forms the foundation of Bayesian statistics, where we start with prior beliefs about parameters, observe data, and compute posterior beliefs.",
    "crumbs": [
      "Probability and Distributions",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Foundations of Probability</span>"
    ]
  },
  {
    "objectID": "chapters/07-probability-foundations.html#likelihood-vs.-probability",
    "href": "chapters/07-probability-foundations.html#likelihood-vs.-probability",
    "title": "7  Foundations of Probability",
    "section": "7.9 Likelihood vs. Probability",
    "text": "7.9 Likelihood vs. Probability\nA subtle but important distinction exists between probability and likelihood.\nProbability refers to the chance of observing particular data given a model or parameter value. If we know a coin has \\(p = 0.5\\), what is the probability of observing 7 heads in 10 flips?\nLikelihood refers to how well a parameter value explains observed data. Given that we observed 7 heads in 10 flips, how likely is it that the true probability is \\(p = 0.5\\) versus \\(p = 0.7\\)?\nMathematically, the likelihood function uses the same formula as probability, but we think of it differently:\n\\[L(\\text{parameter} | \\text{data}) = P(\\text{data} | \\text{parameter})\\]\nMaximum likelihood estimation finds the parameter value that makes the observed data most probable—the value that maximizes the likelihood function.",
    "crumbs": [
      "Probability and Distributions",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Foundations of Probability</span>"
    ]
  },
  {
    "objectID": "chapters/07-probability-foundations.html#covariance-and-correlation",
    "href": "chapters/07-probability-foundations.html#covariance-and-correlation",
    "title": "7  Foundations of Probability",
    "section": "7.10 Covariance and Correlation",
    "text": "7.10 Covariance and Correlation\nWhen two variables are not independent, they share information—knowing one tells you something about the other. This shared information is quantified by covariance, a measure of how two variables vary together.\nIf high values of X tend to occur with high values of Y (and low with low), the covariance is positive. If high values of X tend to occur with low values of Y, the covariance is negative. If there is no relationship, the covariance is near zero.\nCorrelation is covariance standardized to fall between -1 and 1, making it easier to interpret. A correlation of 1 means perfect positive linear relationship; -1 means perfect negative linear relationship; 0 means no linear relationship.\nThese concepts will become central when we discuss regression and other methods for relating variables to each other.",
    "crumbs": [
      "Probability and Distributions",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Foundations of Probability</span>"
    ]
  },
  {
    "objectID": "chapters/07-probability-foundations.html#looking-ahead",
    "href": "chapters/07-probability-foundations.html#looking-ahead",
    "title": "7  Foundations of Probability",
    "section": "7.11 Looking Ahead",
    "text": "7.11 Looking Ahead\nThis chapter introduced the language of probability—random variables, sample spaces, probability distributions, and rules for combining probabilities. In the following chapters, we will explore specific probability distributions that arise frequently in practice, both discrete (binomial, Poisson) and continuous (normal, exponential). Understanding these distributions provides the foundation for statistical inference.",
    "crumbs": [
      "Probability and Distributions",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Foundations of Probability</span>"
    ]
  },
  {
    "objectID": "chapters/08-discrete-distributions.html",
    "href": "chapters/08-discrete-distributions.html",
    "title": "8  Discrete Probability Distributions",
    "section": "",
    "text": "8.1 What Are Discrete Distributions?\nDiscrete probability distributions describe random variables that take on distinct, countable values. The number of heads in ten coin flips, the count of bacterial colonies on a plate, and the number of defective items in a batch are all discrete random variables. Understanding these distributions allows you to model count data, calculate probabilities of specific outcomes, and perform statistical tests.",
    "crumbs": [
      "Probability and Distributions",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Discrete Probability Distributions</span>"
    ]
  },
  {
    "objectID": "chapters/08-discrete-distributions.html#the-binomial-distribution",
    "href": "chapters/08-discrete-distributions.html#the-binomial-distribution",
    "title": "8  Discrete Probability Distributions",
    "section": "8.2 The Binomial Distribution",
    "text": "8.2 The Binomial Distribution\nThe binomial distribution arises when you perform a fixed number of independent trials, each with the same probability of success. It answers questions like: If I flip a coin 20 times, what is the probability of getting exactly 12 heads?\nThe probability of observing exactly \\(k\\) successes in \\(n\\) trials, when each trial has success probability \\(p\\), is:\n\\[P(X = k) = \\binom{n}{k} p^k (1-p)^{n-k}\\]\nThe binomial coefficient \\(\\binom{n}{k}\\) counts the number of ways to arrange \\(k\\) successes among \\(n\\) trials.\nThe mean of a binomial distribution is \\(\\mu = np\\) and the variance is \\(\\sigma^2 = np(1-p)\\).\n\n\nCode\n# Simulate 1000 experiments of 20 coin flips each\nset.seed(42)\nheads &lt;- rbinom(n = 1000, size = 20, prob = 0.5)\n\nhist(heads, breaks = 0:20, col = \"steelblue\", \n     main = \"Distribution of Heads in 20 Coin Flips\",\n     xlab = \"Number of Heads\")\n\n\n\n\n\n\n\n\n\nWith a fair coin (\\(p = 0.5\\)) and 20 flips, we expect about 10 heads on average. The distribution is symmetric and centered at 10.\nIn R, functions for the binomial distribution include:\n\ndbinom(k, n, p) - probability of exactly k successes\npbinom(k, n, p) - probability of k or fewer successes (cumulative)\nqbinom(q, n, p) - quantile function (inverse of cumulative)\nrbinom(n, size, p) - generate random samples\n\n\n\nCode\n# Probability of exactly 10 heads in 20 flips\ndbinom(10, size = 20, prob = 0.5)\n\n\n[1] 0.1761971\n\n\nCode\n# Probability of 10 or fewer heads\npbinom(10, size = 20, prob = 0.5)\n\n\n[1] 0.5880985",
    "crumbs": [
      "Probability and Distributions",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Discrete Probability Distributions</span>"
    ]
  },
  {
    "objectID": "chapters/08-discrete-distributions.html#the-poisson-distribution",
    "href": "chapters/08-discrete-distributions.html#the-poisson-distribution",
    "title": "8  Discrete Probability Distributions",
    "section": "8.3 The Poisson Distribution",
    "text": "8.3 The Poisson Distribution\nThe Poisson distribution models the number of events occurring in a fixed interval of time or space, when events occur independently at a constant average rate. It is appropriate for count data like the number of mutations in a DNA sequence, phone calls received per hour, or organisms per quadrat in an ecological survey.\nThe probability of observing exactly \\(r\\) events when the average rate is \\(\\lambda\\) is:\n\\[P(Y = r) = \\frac{e^{-\\lambda} \\lambda^r}{r!}\\]\nA remarkable property of the Poisson distribution is that the mean and variance are both equal to \\(\\lambda\\). This provides a simple check: if your count data has variance much larger than its mean, a simple Poisson model may not be appropriate (a situation called overdispersion, common in biological data).\n\n\n\n\n\n\n\nCode\n# Show Poisson distributions with different lambda values\npar(mfrow = c(2, 2))\nfor (lambda in c(1, 3, 5, 10)) {\n  x &lt;- 0:20\n  plot(x, dpois(x, lambda), type = \"h\", lwd = 3, col = \"steelblue\",\n       main = paste(\"Poisson, λ =\", lambda),\n       xlab = \"Count\", ylab = \"Probability\")\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAs \\(\\lambda\\) increases, the Poisson distribution becomes more symmetric and approaches a normal distribution.\n\n\nCode\n# Probability of exactly 2 events when lambda = 1\ndpois(x = 2, lambda = 1)\n\n\n[1] 0.1839397\n\n\nCode\n# Plot Poisson probabilities\nplot(dpois(x = 0:10, lambda = 3), type = \"h\", lwd = 3,\n     xlab = \"Count\", ylab = \"Probability\",\n     main = \"Poisson Distribution (λ = 3)\")",
    "crumbs": [
      "Probability and Distributions",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Discrete Probability Distributions</span>"
    ]
  },
  {
    "objectID": "chapters/08-discrete-distributions.html#the-geometric-distribution",
    "href": "chapters/08-discrete-distributions.html#the-geometric-distribution",
    "title": "8  Discrete Probability Distributions",
    "section": "8.4 The Geometric Distribution",
    "text": "8.4 The Geometric Distribution\nThe geometric distribution describes the number of trials needed to achieve the first success. If each trial has success probability \\(p\\), the probability that the first success occurs on trial \\(k\\) is:\n\\[P(X = k) = (1-p)^{k-1} p\\]\nThe mean is \\(1/p\\) and the variance is \\((1-p)/p^2\\).\nFor example, if the probability of a cell successfully transfecting is 0.1, the geometric distribution tells us how many cells we need to attempt before getting our first successful transfection.\n\n\nCode\n# Probability of first success on each trial\np &lt;- 0.1\ntrials &lt;- 1:30\nprobs &lt;- dgeom(trials - 1, prob = p)  # dgeom counts failures before first success\n\nplot(trials, probs, type = \"h\", lwd = 2, col = \"steelblue\",\n     xlab = \"Trial Number of First Success\",\n     ylab = \"Probability\",\n     main = \"Geometric Distribution (p = 0.1)\")",
    "crumbs": [
      "Probability and Distributions",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Discrete Probability Distributions</span>"
    ]
  },
  {
    "objectID": "chapters/08-discrete-distributions.html#the-negative-binomial-distribution",
    "href": "chapters/08-discrete-distributions.html#the-negative-binomial-distribution",
    "title": "8  Discrete Probability Distributions",
    "section": "8.5 The Negative Binomial Distribution",
    "text": "8.5 The Negative Binomial Distribution\nThe negative binomial distribution generalizes the geometric distribution. It describes the number of trials needed to achieve \\(r\\) successes. If each trial has success probability \\(p\\), the probability that the \\(r\\)th success occurs on trial \\(k\\) is:\n\\[P(X = k) = \\binom{k-1}{r-1} p^r (1-p)^{k-r}\\]\nThe mean is \\(r/p\\) and the variance is \\(r(1-p)/p^2\\).\nConsider a predator that must capture 10 prey to reach reproductive maturity. If the daily probability of catching prey is 0.1, the negative binomial distribution describes when the predator will be ready to reproduce.\n\n\n\n\n\nThe negative binomial is also commonly used to model overdispersed count data—counts with variance greater than their mean—which the simple Poisson cannot accommodate.",
    "crumbs": [
      "Probability and Distributions",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Discrete Probability Distributions</span>"
    ]
  },
  {
    "objectID": "chapters/08-discrete-distributions.html#common-pattern-in-r",
    "href": "chapters/08-discrete-distributions.html#common-pattern-in-r",
    "title": "8  Discrete Probability Distributions",
    "section": "8.6 Common Pattern in R",
    "text": "8.6 Common Pattern in R\nR uses a consistent naming convention for distribution functions:\n\n\n\nPrefix\nPurpose\nExample\n\n\n\n\nd\nProbability mass/density function\ndbinom(), dpois()\n\n\np\nCumulative distribution function\npbinom(), ppois()\n\n\nq\nQuantile function\nqbinom(), qpois()\n\n\nr\nRandom number generation\nrbinom(), rpois()\n\n\n\nThis pattern applies to all distributions in R:\n\n\n\nDistribution\nFunctions\n\n\n\n\nBinomial\ndbinom, pbinom, qbinom, rbinom\n\n\nPoisson\ndpois, ppois, qpois, rpois\n\n\nGeometric\ndgeom, pgeom, qgeom, rgeom\n\n\nNegative Binomial\ndnbinom, pnbinom, qnbinom, rnbinom",
    "crumbs": [
      "Probability and Distributions",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Discrete Probability Distributions</span>"
    ]
  },
  {
    "objectID": "chapters/08-discrete-distributions.html#choosing-the-right-distribution",
    "href": "chapters/08-discrete-distributions.html#choosing-the-right-distribution",
    "title": "8  Discrete Probability Distributions",
    "section": "8.7 Choosing the Right Distribution",
    "text": "8.7 Choosing the Right Distribution\nSelecting the appropriate distribution depends on the nature of your data and the process generating it.\nUse the binomial when you have a fixed number of independent trials with constant success probability and you are counting successes. Examples include the number of patients responding to treatment out of a fixed sample, the number of correct answers on a test, or the number of defective items in a batch.\nUse the Poisson when you are counting events in a fixed interval of time or space, events occur independently, and the average rate is constant. Examples include mutations per gene, radioactive decays per minute, or organisms per quadrat. Remember that for Poisson data, mean should approximately equal variance.\nUse the geometric when you are counting trials until the first success. Examples include the number of attempts until a successful measurement or the number of patients screened until finding one eligible for a trial.\nUse the negative binomial when counting trials until a specified number of successes, or when modeling overdispersed count data (variance exceeds mean).",
    "crumbs": [
      "Probability and Distributions",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Discrete Probability Distributions</span>"
    ]
  },
  {
    "objectID": "chapters/08-discrete-distributions.html#practice-with-simulations",
    "href": "chapters/08-discrete-distributions.html#practice-with-simulations",
    "title": "8  Discrete Probability Distributions",
    "section": "8.8 Practice with Simulations",
    "text": "8.8 Practice with Simulations\nUnderstanding distributions deepens through simulation. Generate data from each distribution, visualize it, and calculate summary statistics. Compare the theoretical mean and variance to what you observe in your simulated samples.\n\n\nCode\n# Compare theoretical and empirical properties\nset.seed(123)\n\n# Poisson with lambda = 5\npois_sample &lt;- rpois(10000, lambda = 5)\n\ncat(\"Poisson (λ = 5):\\n\")\n\n\nPoisson (λ = 5):\n\n\nCode\ncat(\"Theoretical mean:\", 5, \"  Observed:\", mean(pois_sample), \"\\n\")\n\n\nTheoretical mean: 5   Observed: 4.9746 \n\n\nCode\ncat(\"Theoretical var:\", 5, \"  Observed:\", var(pois_sample), \"\\n\\n\")\n\n\nTheoretical var: 5   Observed: 4.896444 \n\n\nCode\n# Binomial with n = 20, p = 0.3\nbinom_sample &lt;- rbinom(10000, size = 20, prob = 0.3)\n\ncat(\"Binomial (n = 20, p = 0.3):\\n\")\n\n\nBinomial (n = 20, p = 0.3):\n\n\nCode\ncat(\"Theoretical mean:\", 20 * 0.3, \"  Observed:\", mean(binom_sample), \"\\n\")\n\n\nTheoretical mean: 6   Observed: 5.9732 \n\n\nCode\ncat(\"Theoretical var:\", 20 * 0.3 * 0.7, \"  Observed:\", var(binom_sample), \"\\n\")\n\n\nTheoretical var: 4.2   Observed: 4.149097 \n\n\nThis kind of simulation-based exploration builds intuition that complements formal mathematical understanding.",
    "crumbs": [
      "Probability and Distributions",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Discrete Probability Distributions</span>"
    ]
  },
  {
    "objectID": "chapters/09-continuous-distributions.html",
    "href": "chapters/09-continuous-distributions.html",
    "title": "9  Continuous Probability Distributions",
    "section": "",
    "text": "9.1 From Discrete to Continuous\nMany quantities we measure—weight, concentration, time, temperature—can take any value within a range, not just discrete counts. These continuous random variables require a different mathematical treatment. Instead of probability mass functions that assign probabilities to specific values, we use probability density functions (PDFs) where probabilities come from integrating over intervals.\nFor a continuous random variable, the probability that it falls within an interval \\([a, b]\\) is:\n\\[P(a \\leq X \\leq b) = \\int_a^b f(x) \\, dx\\]\nwhere \\(f(x)\\) is the probability density function. The total area under the density curve must equal 1:\n\\[\\int_{-\\infty}^{\\infty} f(x) \\, dx = 1\\]\nNote that for continuous variables, the probability of any exact value is zero—only intervals have non-zero probability.",
    "crumbs": [
      "Probability and Distributions",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Continuous Probability Distributions</span>"
    ]
  },
  {
    "objectID": "chapters/09-continuous-distributions.html#the-uniform-distribution",
    "href": "chapters/09-continuous-distributions.html#the-uniform-distribution",
    "title": "9  Continuous Probability Distributions",
    "section": "9.2 The Uniform Distribution",
    "text": "9.2 The Uniform Distribution\nThe simplest continuous distribution is the uniform distribution, where all values in an interval are equally likely. If \\(X\\) is uniformly distributed between \\(a\\) and \\(b\\):\n\\[f(x) = \\frac{1}{b-a} \\quad \\text{for } a \\leq x \\leq b\\]\nThe mean is \\((a+b)/2\\) and the variance is \\((b-a)^2/12\\).\n\n\n\n\n\n\n\nCode\n# Uniform distribution between 0 and 10\nx &lt;- seq(0, 10, length.out = 100)\nplot(x, dunif(x, min = 0, max = 10), type = \"l\", lwd = 2,\n     xlab = \"x\", ylab = \"Density\",\n     main = \"Uniform Distribution (0, 10)\")\n\n\n\n\n\n\n\n\n\nThe uniform distribution is often used to model random number generation and situations where no outcome is favored over another within a range.",
    "crumbs": [
      "Probability and Distributions",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Continuous Probability Distributions</span>"
    ]
  },
  {
    "objectID": "chapters/09-continuous-distributions.html#the-exponential-distribution",
    "href": "chapters/09-continuous-distributions.html#the-exponential-distribution",
    "title": "9  Continuous Probability Distributions",
    "section": "9.3 The Exponential Distribution",
    "text": "9.3 The Exponential Distribution\nThe exponential distribution models waiting times between events in a Poisson process—the time until the next event when events occur randomly at a constant rate \\(\\lambda\\). Its density function is:\n\\[f(x) = \\lambda e^{-\\lambda x} \\quad \\text{for } x \\geq 0\\]\nThe mean is \\(1/\\lambda\\) and the variance is \\(1/\\lambda^2\\).\n\n\n\n\n\nIf a radioactive isotope has a decay rate of \\(\\lambda = 0.1\\) per minute, the time until the next decay follows an exponential distribution with mean 10 minutes.\n\n\nCode\n# Exponential distributions with different rates\nx &lt;- seq(0, 30, length.out = 200)\nplot(x, dexp(x, rate = 0.1), type = \"l\", lwd = 2, col = \"blue\",\n     xlab = \"Time\", ylab = \"Density\",\n     main = \"Exponential Distribution (λ = 0.1)\")\n\n\n\n\n\n\n\n\n\nA key property of the exponential distribution is memorylessness: the probability of waiting another \\(t\\) units does not depend on how long you have already waited.",
    "crumbs": [
      "Probability and Distributions",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Continuous Probability Distributions</span>"
    ]
  },
  {
    "objectID": "chapters/09-continuous-distributions.html#the-gamma-distribution",
    "href": "chapters/09-continuous-distributions.html#the-gamma-distribution",
    "title": "9  Continuous Probability Distributions",
    "section": "9.4 The Gamma Distribution",
    "text": "9.4 The Gamma Distribution\nThe gamma distribution generalizes the exponential distribution to model the waiting time until the \\(r\\)th event in a Poisson process. Its density function involves two parameters: shape \\(r\\) and rate \\(\\lambda\\):\n\\[f(x) = \\frac{\\lambda^r x^{r-1} e^{-\\lambda x}}{(r-1)!} \\quad \\text{for } x \\geq 0\\]\nThe mean is \\(r/\\lambda\\) and the variance is \\(r/\\lambda^2\\).\nWhen \\(r = 1\\), the gamma distribution reduces to the exponential. As \\(r\\) increases, the distribution becomes more symmetric and bell-shaped.",
    "crumbs": [
      "Probability and Distributions",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Continuous Probability Distributions</span>"
    ]
  },
  {
    "objectID": "chapters/09-continuous-distributions.html#the-normal-gaussian-distribution",
    "href": "chapters/09-continuous-distributions.html#the-normal-gaussian-distribution",
    "title": "9  Continuous Probability Distributions",
    "section": "9.5 The Normal (Gaussian) Distribution",
    "text": "9.5 The Normal (Gaussian) Distribution\nThe normal distribution is the most important continuous distribution in statistics. Its distinctive bell-shaped curve appears throughout nature, and the Central Limit Theorem explains why: the sum of many independent random effects tends toward normality regardless of the underlying distributions.\nThe normal distribution is characterized by two parameters: mean \\(\\mu\\) (center) and standard deviation \\(\\sigma\\) (spread):\n\\[f(x) = \\frac{1}{\\sigma\\sqrt{2\\pi}} e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}\\]\n\n\nCode\n# Normal distributions with different parameters\nx &lt;- seq(-10, 15, length.out = 200)\nplot(x, dnorm(x, mean = 0, sd = 1), type = \"l\", lwd = 2, col = \"blue\",\n     ylim = c(0, 0.5), xlab = \"x\", ylab = \"Density\",\n     main = \"Normal Distributions\")\nlines(x, dnorm(x, mean = 0, sd = 2), lwd = 2, col = \"red\")\nlines(x, dnorm(x, mean = 5, sd = 1), lwd = 2, col = \"darkgreen\")\nlegend(\"topright\", \n       legend = c(\"μ=0, σ=1\", \"μ=0, σ=2\", \"μ=5, σ=1\"),\n       col = c(\"blue\", \"red\", \"darkgreen\"), lwd = 2)\n\n\n\n\n\n\n\n\n\n\n9.5.1 Properties of the Normal Distribution\nThe normal distribution is symmetric around its mean. The mean, median, and mode are all equal. About 68% of the distribution falls within one standard deviation of the mean, 95% within two standard deviations, and 99.7% within three standard deviations (the “68-95-99.7 rule”).\n\n\n9.5.2 The Standard Normal Distribution\nWhen \\(\\mu = 0\\) and \\(\\sigma = 1\\), we have the standard normal distribution. Any normal variable can be converted to standard normal by subtracting the mean and dividing by the standard deviation:\n\\[Z = \\frac{X - \\mu}{\\sigma}\\]\nThis standardization, called computing a z-score, allows us to compare values from different normal distributions and to use tables of standard normal probabilities.\n\n\nCode\n# Probability calculations with the normal distribution\n# P(X &lt; 1.96) for standard normal\npnorm(1.96)\n\n\n[1] 0.9750021\n\n\nCode\n# P(-1.96 &lt; X &lt; 1.96)\npnorm(1.96) - pnorm(-1.96)\n\n\n[1] 0.9500042\n\n\nCode\n# What value has 97.5% of the distribution below it?\nqnorm(0.975)\n\n\n[1] 1.959964\n\n\nThe values 1.96 and -1.96 are particularly important because they bound the middle 95% of the standard normal distribution, forming the basis for 95% confidence intervals.\n\n\n9.5.3 Checking Normality\nMany statistical methods assume normally distributed data. Before applying these methods, you should check whether the assumption is reasonable.\nVisual methods include histograms and Q-Q (quantile-quantile) plots:\n\n\nCode\n# Generate some data\nset.seed(42)\nnormal_data &lt;- rnorm(200, mean = 50, sd = 10)\nskewed_data &lt;- rexp(200, rate = 0.1)\n\npar(mfrow = c(1, 2))\n\n# Q-Q plot for normal data\nqqnorm(normal_data, main = \"Normal Data\")\nqqline(normal_data, col = \"red\")\n\n# Q-Q plot for skewed data\nqqnorm(skewed_data, main = \"Skewed Data\")\nqqline(skewed_data, col = \"red\")\n\n\n\n\n\n\n\n\n\nIn a Q-Q plot, normally distributed data should fall approximately along the diagonal line. Systematic deviations indicate non-normality.",
    "crumbs": [
      "Probability and Distributions",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Continuous Probability Distributions</span>"
    ]
  },
  {
    "objectID": "chapters/09-continuous-distributions.html#the-central-limit-theorem",
    "href": "chapters/09-continuous-distributions.html#the-central-limit-theorem",
    "title": "9  Continuous Probability Distributions",
    "section": "9.6 The Central Limit Theorem",
    "text": "9.6 The Central Limit Theorem\nThe Central Limit Theorem (CLT) states that the sampling distribution of the mean approaches normality as sample size increases, regardless of the shape of the population distribution. This is why the normal distribution appears so frequently in statistics—we often work with means or other sums of random variables.\n\n\nCode\n# Demonstrate CLT with exponential distribution\nset.seed(123)\n\n# Exponential distribution is quite skewed\npar(mfrow = c(2, 2))\n\n# Original distribution\nhist(rexp(10000, rate = 1), breaks = 50, main = \"Original: Exponential\",\n     xlab = \"x\", col = \"lightblue\")\n\n# Means of samples of size 5\nmeans_5 &lt;- replicate(10000, mean(rexp(5, rate = 1)))\nhist(means_5, breaks = 50, main = \"Means of n=5\",\n     xlab = \"Sample Mean\", col = \"lightblue\")\n\n# Means of samples of size 30\nmeans_30 &lt;- replicate(10000, mean(rexp(30, rate = 1)))\nhist(means_30, breaks = 50, main = \"Means of n=30\",\n     xlab = \"Sample Mean\", col = \"lightblue\")\n\n# Means of samples of size 100\nmeans_100 &lt;- replicate(10000, mean(rexp(100, rate = 1)))\nhist(means_100, breaks = 50, main = \"Means of n=100\",\n     xlab = \"Sample Mean\", col = \"lightblue\")\n\n\n\n\n\n\n\n\n\nEven though the exponential distribution is strongly right-skewed, the distribution of sample means becomes increasingly normal as sample size grows. This is the Central Limit Theorem in action.",
    "crumbs": [
      "Probability and Distributions",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Continuous Probability Distributions</span>"
    ]
  },
  {
    "objectID": "chapters/09-continuous-distributions.html#summary-of-distribution-functions-in-r",
    "href": "chapters/09-continuous-distributions.html#summary-of-distribution-functions-in-r",
    "title": "9  Continuous Probability Distributions",
    "section": "9.7 Summary of Distribution Functions in R",
    "text": "9.7 Summary of Distribution Functions in R\nR provides consistent functions for all distributions:\n\n\n\n\n\n\n\n\n\n\nDistribution\nd (density)\np (cumulative)\nq (quantile)\nr (random)\n\n\n\n\nUniform\ndunif\npunif\nqunif\nrunif\n\n\nExponential\ndexp\npexp\nqexp\nrexp\n\n\nNormal\ndnorm\npnorm\nqnorm\nrnorm\n\n\nGamma\ndgamma\npgamma\nqgamma\nrgamma\n\n\n\nUnderstanding these distributions and their properties prepares you for statistical inference, where we use sampling distributions to make probabilistic statements about population parameters.",
    "crumbs": [
      "Probability and Distributions",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Continuous Probability Distributions</span>"
    ]
  },
  {
    "objectID": "chapters/10-sampling-estimation.html",
    "href": "chapters/10-sampling-estimation.html",
    "title": "10  Sampling and Parameter Estimation",
    "section": "",
    "text": "10.1 The Problem of Inference\nScience often works by measuring samples to learn about populations. We cannot measure every protein in a cell, every patient with a disease, or every fish in the ocean. Instead, we take samples and use statistical inference to draw conclusions about the larger populations from which they came.\nThis creates a fundamental challenge: sample statistics vary from sample to sample, even when samples come from the same population. If you take two different random samples from a population and calculate their means, you will almost certainly get two different values. How, then, can we say anything reliable about the population?\nThe answer lies in understanding the sampling distribution—the distribution of a statistic across all possible samples of a given size.",
    "crumbs": [
      "Probability and Distributions",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Sampling and Parameter Estimation</span>"
    ]
  },
  {
    "objectID": "chapters/10-sampling-estimation.html#parameters-and-statistics",
    "href": "chapters/10-sampling-estimation.html#parameters-and-statistics",
    "title": "10  Sampling and Parameter Estimation",
    "section": "10.2 Parameters and Statistics",
    "text": "10.2 Parameters and Statistics\nA parameter is a numerical characteristic of a population—the true population mean \\(\\mu\\), the true population standard deviation \\(\\sigma\\), the true proportion \\(p\\). Parameters are typically fixed but unknown.\nA statistic is a numerical characteristic of a sample—the sample mean \\(\\bar{x}\\), the sample standard deviation \\(s\\), the sample proportion \\(\\hat{p}\\). Statistics are calculated from data and vary from sample to sample.\nWe use statistics to estimate parameters. The sample mean \\(\\bar{x}\\) estimates the population mean \\(\\mu\\). The sample standard deviation \\(s\\) estimates the population standard deviation \\(\\sigma\\). These estimates will rarely equal the true parameter values exactly, but we can quantify how close they are likely to be.",
    "crumbs": [
      "Probability and Distributions",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Sampling and Parameter Estimation</span>"
    ]
  },
  {
    "objectID": "chapters/10-sampling-estimation.html#point-estimates",
    "href": "chapters/10-sampling-estimation.html#point-estimates",
    "title": "10  Sampling and Parameter Estimation",
    "section": "10.3 Point Estimates",
    "text": "10.3 Point Estimates\nA point estimate is a single number used as our best guess for a parameter. The sample mean is a natural point estimate for the population mean:\n\\[\\bar{x} = \\frac{1}{n} \\sum_{i=1}^{n} x_i\\]\nWhat makes a good estimator? Ideally, an estimator should be:\nUnbiased: On average, across many samples, the estimator equals the true parameter. The sample mean is an unbiased estimator of the population mean.\nEfficient: Among unbiased estimators, it has the smallest variance. The sample mean is the most efficient estimator of a normal mean.\nConsistent: As sample size increases, the estimator converges to the true parameter value.",
    "crumbs": [
      "Probability and Distributions",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Sampling and Parameter Estimation</span>"
    ]
  },
  {
    "objectID": "chapters/10-sampling-estimation.html#the-sampling-distribution-of-the-mean",
    "href": "chapters/10-sampling-estimation.html#the-sampling-distribution-of-the-mean",
    "title": "10  Sampling and Parameter Estimation",
    "section": "10.4 The Sampling Distribution of the Mean",
    "text": "10.4 The Sampling Distribution of the Mean\nImagine drawing all possible samples of size \\(n\\) from a population and calculating the mean of each. The distribution of these means is the sampling distribution of the mean.\nThe sampling distribution has remarkable properties:\n\nIts mean equals the population mean: \\(E[\\bar{X}] = \\mu\\)\nIts standard deviation (the standard error) equals: \\(SE = \\frac{\\sigma}{\\sqrt{n}}\\)\nFor large samples, it is approximately normal (Central Limit Theorem)\n\n\n\nCode\n# Demonstrate sampling distribution\nset.seed(32)\n\n# Create a population\ntrue_pop &lt;- rpois(n = 10000, lambda = 3)\npop_mean &lt;- mean(true_pop)\npop_sd &lt;- sd(true_pop)\n\n# Take many samples and compute their means\nsample_sizes &lt;- c(5, 20, 50, 200)\npar(mfrow = c(2, 2))\n\nfor (n in sample_sizes) {\n  sample_means &lt;- replicate(1000, mean(sample(true_pop, n)))\n  hist(sample_means, breaks = 30, main = paste(\"n =\", n),\n       xlab = \"Sample Mean\", col = \"steelblue\",\n       xlim = c(1, 5))\n  abline(v = pop_mean, col = \"red\", lwd = 2)\n}\n\n\n\n\n\n\n\n\n\nAs sample size increases, the sampling distribution becomes narrower (smaller standard error) and more normal in shape. This is why larger samples give more precise estimates.",
    "crumbs": [
      "Probability and Distributions",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Sampling and Parameter Estimation</span>"
    ]
  },
  {
    "objectID": "chapters/10-sampling-estimation.html#standard-error",
    "href": "chapters/10-sampling-estimation.html#standard-error",
    "title": "10  Sampling and Parameter Estimation",
    "section": "10.5 Standard Error",
    "text": "10.5 Standard Error\nThe standard error (SE) measures the variability of a statistic across samples. For the sample mean:\n\\[SE_{\\bar{x}} = \\frac{\\sigma}{\\sqrt{n}}\\]\nSince we usually do not know \\(\\sigma\\), we estimate the standard error using the sample standard deviation:\n\\[\\widehat{SE}_{\\bar{x}} = \\frac{s}{\\sqrt{n}}\\]\nThe standard error shrinks as sample size increases, but following a square root relationship. To halve the standard error, you need to quadruple the sample size.\n\n\nCode\n# Demonstrate how SE changes with sample size\nset.seed(32)\ntrue_pop &lt;- rpois(n = 1000, lambda = 5)\n\n# Sample size of 5\nsamps_5 &lt;- replicate(n = 50, sample(true_pop, size = 5))\nmeans_5 &lt;- apply(samps_5, 2, mean)\nse_5 &lt;- sd(means_5)\n\n# Sample size of 50\nsamps_50 &lt;- replicate(n = 50, sample(true_pop, size = 50))\nmeans_50 &lt;- apply(samps_50, 2, mean)\nse_50 &lt;- sd(means_50)\n\ncat(\"Standard error with n=5:\", round(se_5, 3), \"\\n\")\n\n\nStandard error with n=5: 0.919 \n\n\nCode\ncat(\"Standard error with n=50:\", round(se_50, 3), \"\\n\")\n\n\nStandard error with n=50: 0.305 \n\n\nCode\ncat(\"Ratio:\", round(se_5/se_50, 2), \"(theoretical: √10 =\", round(sqrt(10), 2), \")\\n\")\n\n\nRatio: 3.01 (theoretical: √10 = 3.16 )",
    "crumbs": [
      "Probability and Distributions",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Sampling and Parameter Estimation</span>"
    ]
  },
  {
    "objectID": "chapters/10-sampling-estimation.html#confidence-intervals",
    "href": "chapters/10-sampling-estimation.html#confidence-intervals",
    "title": "10  Sampling and Parameter Estimation",
    "section": "10.6 Confidence Intervals",
    "text": "10.6 Confidence Intervals\nA point estimate tells us our best guess, but not how uncertain we are. A confidence interval provides a range of plausible values for the parameter along with a measure of confidence.\nA 95% confidence interval for the population mean, when the population is normally distributed or the sample is large, is:\n\\[\\bar{x} \\pm t_{\\alpha/2} \\times \\frac{s}{\\sqrt{n}}\\]\nwhere \\(t_{\\alpha/2}\\) is the critical value from the t-distribution with \\(n-1\\) degrees of freedom.\n\n\n\n\n\nThe interpretation requires care: a 95% confidence interval means that if we repeated this procedure many times, 95% of the resulting intervals would contain the true parameter. Any particular interval either does or does not contain the true value—we just don’t know which.\n\n\nCode\n# Calculate a confidence interval\nset.seed(42)\nsample_data &lt;- rnorm(30, mean = 100, sd = 15)\n\nsample_mean &lt;- mean(sample_data)\nsample_se &lt;- sd(sample_data) / sqrt(length(sample_data))\nt_crit &lt;- qt(0.975, df = length(sample_data) - 1)\n\nlower &lt;- sample_mean - t_crit * sample_se\nupper &lt;- sample_mean + t_crit * sample_se\n\ncat(\"Sample mean:\", round(sample_mean, 2), \"\\n\")\n\n\nSample mean: 101.03 \n\n\nCode\ncat(\"95% CI: [\", round(lower, 2), \",\", round(upper, 2), \"]\\n\")\n\n\n95% CI: [ 94 , 108.06 ]\n\n\nCode\n# Or use t.test directly\nt.test(sample_data)$conf.int\n\n\n[1]  93.99927 108.05833\nattr(,\"conf.level\")\n[1] 0.95",
    "crumbs": [
      "Probability and Distributions",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Sampling and Parameter Estimation</span>"
    ]
  },
  {
    "objectID": "chapters/10-sampling-estimation.html#coefficient-of-variation",
    "href": "chapters/10-sampling-estimation.html#coefficient-of-variation",
    "title": "10  Sampling and Parameter Estimation",
    "section": "10.7 Coefficient of Variation",
    "text": "10.7 Coefficient of Variation\nWhen comparing variability across groups with different means, the standard deviation alone can be misleading. The coefficient of variation (CV) standardizes variability relative to the mean:\n\\[CV = \\frac{s}{\\bar{x}} \\times 100\\%\\]\nA CV of 10% means the standard deviation is 10% of the mean. This allows meaningful comparisons between groups or measurements on different scales.",
    "crumbs": [
      "Probability and Distributions",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Sampling and Parameter Estimation</span>"
    ]
  },
  {
    "objectID": "chapters/10-sampling-estimation.html#percentiles-and-quantiles",
    "href": "chapters/10-sampling-estimation.html#percentiles-and-quantiles",
    "title": "10  Sampling and Parameter Estimation",
    "section": "10.8 Percentiles and Quantiles",
    "text": "10.8 Percentiles and Quantiles\nPercentiles describe the relative position of values within a distribution. The \\(p\\)th percentile is the value below which \\(p\\)% of the data falls. The 50th percentile is the median, the 25th percentile is the first quartile, and the 75th percentile is the third quartile.\nQuantiles divide data into equal parts. Quartiles divide into four parts, deciles into ten parts, percentiles into one hundred parts.\n\n\nCode\n# Calculate percentiles\ndata &lt;- c(12, 15, 18, 22, 25, 28, 32, 35, 40, 45)\n\nquantile(data, probs = c(0.25, 0.5, 0.75))\n\n\n  25%   50%   75% \n19.00 26.50 34.25 \n\n\nCode\nsummary(data)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  12.00   19.00   26.50   27.20   34.25   45.00 \n\n\nQuantiles form the basis for many statistical procedures, including constructing confidence intervals and calculating p-values.",
    "crumbs": [
      "Probability and Distributions",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Sampling and Parameter Estimation</span>"
    ]
  },
  {
    "objectID": "chapters/10-sampling-estimation.html#bias-and-variability",
    "href": "chapters/10-sampling-estimation.html#bias-and-variability",
    "title": "10  Sampling and Parameter Estimation",
    "section": "10.9 Bias and Variability",
    "text": "10.9 Bias and Variability\nTwo distinct types of error affect estimates:\nBias is systematic error—the tendency for an estimator to consistently over- or underestimate the true parameter. An unbiased estimator has zero bias: its average value across all possible samples equals the true parameter.\nVariability is random error—the spread of estimates around their average value. Low variability means estimates cluster tightly together.\nThe ideal estimator has both low bias and low variability. Sometimes there is a tradeoff: a slightly biased estimator might have much lower variability, resulting in estimates that are closer to the truth on average.\nThe mean squared error (MSE) combines both sources of error:\n\\[MSE = Bias^2 + Variance\\]",
    "crumbs": [
      "Probability and Distributions",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Sampling and Parameter Estimation</span>"
    ]
  },
  {
    "objectID": "chapters/10-sampling-estimation.html#key-takeaways",
    "href": "chapters/10-sampling-estimation.html#key-takeaways",
    "title": "10  Sampling and Parameter Estimation",
    "section": "10.10 Key Takeaways",
    "text": "10.10 Key Takeaways\nUnderstanding sampling distributions and estimation is fundamental to statistical inference. Key points to remember:\n\nStatistics vary from sample to sample; this variability is quantified by the standard error\nLarger samples give more precise estimates (smaller standard errors)\nConfidence intervals quantify uncertainty about parameter estimates\nThe Central Limit Theorem explains why the normal distribution appears so frequently\nBoth bias and variability affect the quality of estimates\n\nThese concepts provide the foundation for hypothesis testing and the statistical inference methods we develop in subsequent chapters.",
    "crumbs": [
      "Probability and Distributions",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Sampling and Parameter Estimation</span>"
    ]
  },
  {
    "objectID": "chapters/11-hypothesis-testing.html",
    "href": "chapters/11-hypothesis-testing.html",
    "title": "11  Hypothesis Testing",
    "section": "",
    "text": "11.1 What is a Hypothesis?\nA hypothesis is a statement of belief about the world—a claim that can be evaluated with data. In statistics, we formalize hypothesis testing as a framework for using data to decide between competing claims.\nThe null hypothesis (\\(H_0\\)) represents a default position, typically stating that there is no effect, no difference, or no relationship. The alternative hypothesis (\\(H_A\\)) represents what we would conclude if we reject the null—typically that there is an effect, difference, or relationship.\nFor example, consider testing whether an amino acid substitution changes the catalytic rate of an enzyme:\nThe alternative hypothesis might be directional (the substitution increases the rate) or non-directional (the substitution changes the rate, in either direction). This distinction affects how we calculate p-values.",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "chapters/11-hypothesis-testing.html#what-is-a-hypothesis",
    "href": "chapters/11-hypothesis-testing.html#what-is-a-hypothesis",
    "title": "11  Hypothesis Testing",
    "section": "",
    "text": "\\(H_0\\): The substitution does not change the catalytic rate\n\\(H_A\\): The substitution does change the catalytic rate",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "chapters/11-hypothesis-testing.html#the-logic-of-hypothesis-testing",
    "href": "chapters/11-hypothesis-testing.html#the-logic-of-hypothesis-testing",
    "title": "11  Hypothesis Testing",
    "section": "11.2 The Logic of Hypothesis Testing",
    "text": "11.2 The Logic of Hypothesis Testing\nHypothesis testing follows a specific logic. We assume the null hypothesis is true and ask: how likely would we be to observe data as extreme as what we actually observed? If this probability is very small, we conclude that the null hypothesis is unlikely to be true and reject it in favor of the alternative.\nKey questions in hypothesis testing include:\n\nWhat is the probability of rejecting a true null hypothesis?\nWhat is the probability of failing to reject a false null hypothesis?\nHow do we decide when to reject the null hypothesis?\nWhat can we conclude if we fail to reject?",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "chapters/11-hypothesis-testing.html#type-i-and-type-ii-errors",
    "href": "chapters/11-hypothesis-testing.html#type-i-and-type-ii-errors",
    "title": "11  Hypothesis Testing",
    "section": "11.3 Type I and Type II Errors",
    "text": "11.3 Type I and Type II Errors\nTwo types of mistakes are possible in hypothesis testing.\n\n\n\n\n\nA Type I error occurs when we reject a true null hypothesis—concluding there is an effect when there is not. The probability of a Type I error is denoted \\(\\alpha\\) and is called the significance level. By convention, \\(\\alpha\\) is often set to 0.05, meaning we accept a 5% chance of falsely rejecting a true null.\nA Type II error occurs when we fail to reject a false null hypothesis—concluding there is no effect when there actually is one. The probability of a Type II error is denoted \\(\\beta\\).\nPower is the probability of correctly rejecting a false null hypothesis: Power = \\(1 - \\beta\\). Power depends on the effect size (how big the true effect is), sample size, significance level, and variability in the data.\n\n\n\n\n\\(H_0\\) True\n\\(H_0\\) False\n\n\n\n\nReject \\(H_0\\)\nType I Error (\\(\\alpha\\))\nCorrect Decision (Power)\n\n\nFail to Reject \\(H_0\\)\nCorrect Decision\nType II Error (\\(\\beta\\))",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "chapters/11-hypothesis-testing.html#p-values",
    "href": "chapters/11-hypothesis-testing.html#p-values",
    "title": "11  Hypothesis Testing",
    "section": "11.4 P-Values",
    "text": "11.4 P-Values\nThe p-value is the probability of observing a test statistic as extreme or more extreme than the one calculated from the data, assuming the null hypothesis is true.\nA small p-value indicates that the observed data would be unlikely if the null hypothesis were true, providing evidence against the null. A large p-value indicates that the data are consistent with the null hypothesis.\nThe p-value is NOT the probability that the null hypothesis is true. It is the probability of the data (or more extreme data) given the null hypothesis, not the probability of the null hypothesis given the data.",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "chapters/11-hypothesis-testing.html#significance-level-and-decision-rules",
    "href": "chapters/11-hypothesis-testing.html#significance-level-and-decision-rules",
    "title": "11  Hypothesis Testing",
    "section": "11.5 Significance Level and Decision Rules",
    "text": "11.5 Significance Level and Decision Rules\nThe significance level \\(\\alpha\\) is the threshold below which we reject the null hypothesis. If \\(p &lt; \\alpha\\), we reject \\(H_0\\). If \\(p \\geq \\alpha\\), we fail to reject \\(H_0\\).\n\n\n\n\n\nThe conventional choice of \\(\\alpha = 0.05\\) is arbitrary but widely used. In contexts where Type I errors are particularly costly (e.g., approving an ineffective drug), smaller \\(\\alpha\\) values may be appropriate. In exploratory research, larger \\(\\alpha\\) values might be acceptable.\nImportant: “fail to reject” is not the same as “accept.” Failing to reject the null hypothesis means the data did not provide sufficient evidence against it, not that the null hypothesis is true.",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "chapters/11-hypothesis-testing.html#test-statistics-and-statistical-distributions",
    "href": "chapters/11-hypothesis-testing.html#test-statistics-and-statistical-distributions",
    "title": "11  Hypothesis Testing",
    "section": "11.6 Test Statistics and Statistical Distributions",
    "text": "11.6 Test Statistics and Statistical Distributions\nA test statistic summarizes the data in a way that allows comparison to a known distribution under the null hypothesis. Different tests use different statistics: the t-statistic for t-tests, the F-statistic for ANOVA, the chi-squared statistic for contingency tables.\nJust like raw data, test statistics are random variables with their own sampling distributions. Under the null hypothesis, we know what distribution the test statistic should follow. We can then calculate how unusual our observed statistic is under this distribution.",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "chapters/11-hypothesis-testing.html#one-tailed-vs.-two-tailed-tests",
    "href": "chapters/11-hypothesis-testing.html#one-tailed-vs.-two-tailed-tests",
    "title": "11  Hypothesis Testing",
    "section": "11.7 One-Tailed vs. Two-Tailed Tests",
    "text": "11.7 One-Tailed vs. Two-Tailed Tests\nA two-tailed test considers extreme values in both directions. The alternative hypothesis is non-directional: \\(H_A: \\mu \\neq \\mu_0\\). Extreme values in either tail of the distribution count as evidence against the null.\nA one-tailed test considers extreme values in only one direction. The alternative hypothesis is directional: \\(H_A: \\mu &gt; \\mu_0\\) or \\(H_A: \\mu &lt; \\mu_0\\). Only extreme values in the specified direction count as evidence against the null.\n\n\n\n\n\nTwo-tailed tests are more conservative and are appropriate when you do not have a strong prior expectation about the direction of an effect. One-tailed tests have more power to detect effects in the specified direction but will miss effects in the opposite direction.",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "chapters/11-hypothesis-testing.html#multiple-testing",
    "href": "chapters/11-hypothesis-testing.html#multiple-testing",
    "title": "11  Hypothesis Testing",
    "section": "11.8 Multiple Testing",
    "text": "11.8 Multiple Testing\nWhen you perform many hypothesis tests, the probability of at least one Type I error increases. If you test 20 independent hypotheses at \\(\\alpha = 0.05\\), you expect about one false positive even when all null hypotheses are true.\nSeveral approaches address multiple testing:\nBonferroni correction divides \\(\\alpha\\) by the number of tests. For 20 tests, use \\(\\alpha = 0.05/20 = 0.0025\\). This is conservative and may miss true effects.\nFalse Discovery Rate (FDR) control allows some false positives but controls their proportion among rejected hypotheses. This is less conservative than Bonferroni and widely used in genomics and other high-throughput applications.",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "chapters/11-hypothesis-testing.html#practical-vs.-statistical-significance",
    "href": "chapters/11-hypothesis-testing.html#practical-vs.-statistical-significance",
    "title": "11  Hypothesis Testing",
    "section": "11.9 Practical vs. Statistical Significance",
    "text": "11.9 Practical vs. Statistical Significance\nStatistical significance does not imply practical importance. With a large enough sample, even trivially small effects become statistically significant. Conversely, practically important effects may not reach statistical significance with small samples.\nAlways consider effect sizes alongside p-values. Report confidence intervals, which convey both the magnitude of an effect and the uncertainty about it. A 95% confidence interval that excludes zero is equivalent to statistical significance at \\(\\alpha = 0.05\\), but also shows the range of plausible effect sizes.",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "chapters/11-hypothesis-testing.html#example-null-distribution-via-randomization",
    "href": "chapters/11-hypothesis-testing.html#example-null-distribution-via-randomization",
    "title": "11  Hypothesis Testing",
    "section": "11.10 Example: Null Distribution via Randomization",
    "text": "11.10 Example: Null Distribution via Randomization\nWe can create empirical null distributions through randomization, providing an alternative to parametric assumptions.\n\n\nCode\n# Two groups to compare\nset.seed(56)\npop_1 &lt;- rnorm(n = 50, mean = 20.1, sd = 2)\npop_2 &lt;- rnorm(n = 50, mean = 19.3, sd = 2)\n\n# Observed t-statistic\nt_obs &lt;- t.test(x = pop_1, y = pop_2, alternative = \"greater\")$statistic\n\n# Create null distribution by randomization\npops_comb &lt;- c(pop_1, pop_2)\n\nt_rand &lt;- replicate(1000, {\n  pops_shuf &lt;- sample(pops_comb)\n  t.test(x = pops_shuf[1:50], y = pops_shuf[51:100], alternative = \"greater\")$statistic\n})\n\n# Plot null distribution\nhist(t_rand, breaks = 30, main = \"Randomization Null Distribution\",\n     xlab = \"t-statistic\", col = \"lightblue\")\nabline(v = t_obs, col = \"red\", lwd = 2)\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Calculate p-value\np_value &lt;- sum(t_rand &gt;= t_obs) / 1000\ncat(\"Observed t:\", round(t_obs, 3), \"\\n\")\n\n\nObserved t: 2.211 \n\n\nCode\ncat(\"P-value:\", p_value, \"\\n\")\n\n\nP-value: 0.016",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "chapters/11-hypothesis-testing.html#summary",
    "href": "chapters/11-hypothesis-testing.html#summary",
    "title": "11  Hypothesis Testing",
    "section": "11.11 Summary",
    "text": "11.11 Summary\nHypothesis testing provides a framework for using data to evaluate claims about populations. Key concepts include:\n\nNull and alternative hypotheses formalize competing claims\nType I errors (false positives) and Type II errors (false negatives) represent the two ways we can be wrong\nP-values quantify evidence against the null hypothesis\nSignificance levels set thresholds for decision-making\nMultiple testing requires adjustment to control error rates\nStatistical significance does not imply practical importance\n\nIn the following chapters, we apply this framework to specific tests: t-tests for comparing means, chi-squared tests for categorical data, and nonparametric alternatives when assumptions are violated.",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "chapters/12-t-tests.html",
    "href": "chapters/12-t-tests.html",
    "title": "12  T-Tests",
    "section": "",
    "text": "12.1 Comparing Means\nOne of the most common questions in data analysis is whether two groups differ. Is the mean expression level different between treatment and control? Does the new material have different strength than the standard? Do patients on drug A have different outcomes than patients on drug B?\nThe t-test is the classic method for comparing means. It compares the observed difference between groups to the variability expected by chance, producing a test statistic that follows a t-distribution under the null hypothesis of no difference.",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>T-Tests</span>"
    ]
  },
  {
    "objectID": "chapters/12-t-tests.html#the-t-distribution",
    "href": "chapters/12-t-tests.html#the-t-distribution",
    "title": "12  T-Tests",
    "section": "12.2 The T-Distribution",
    "text": "12.2 The T-Distribution\nThe t-distribution, discovered by William Sealy Gosset (who published under the pseudonym “Student”), resembles the normal distribution but has heavier tails. This accounts for the extra uncertainty that comes from estimating the population standard deviation from sample data.\nThe t-distribution is characterized by its degrees of freedom (df). As df increases, the t-distribution approaches the normal distribution. For small samples, the heavier tails mean that extreme values are more likely, leading to wider confidence intervals and more conservative tests.\n\n\nCode\n# Compare t-distributions with different df\nx &lt;- seq(-4, 4, length.out = 200)\nplot(x, dnorm(x), type = \"l\", lwd = 2, col = \"black\",\n     xlab = \"x\", ylab = \"Density\",\n     main = \"T-distributions vs. Normal\")\nlines(x, dt(x, df = 3), lwd = 2, col = \"red\")\nlines(x, dt(x, df = 10), lwd = 2, col = \"blue\")\nlegend(\"topright\", \n       legend = c(\"Normal\", \"t (df=3)\", \"t (df=10)\"),\n       col = c(\"black\", \"red\", \"blue\"), lwd = 2)",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>T-Tests</span>"
    ]
  },
  {
    "objectID": "chapters/12-t-tests.html#one-sample-t-test",
    "href": "chapters/12-t-tests.html#one-sample-t-test",
    "title": "12  T-Tests",
    "section": "12.3 One-Sample T-Test",
    "text": "12.3 One-Sample T-Test\nThe one-sample t-test compares a sample mean to a hypothesized population value. The null hypothesis is that the population mean equals the specified value: \\(H_0: \\mu = \\mu_0\\).\nThe test statistic is:\n\\[t = \\frac{\\bar{x} - \\mu_0}{s / \\sqrt{n}}\\]\nThis is the difference between the sample mean and hypothesized value, divided by the standard error of the mean. Under the null hypothesis, this statistic follows a t-distribution with \\(n-1\\) degrees of freedom.\n\n\nCode\n# One-sample t-test example\n# Does this sample come from a population with mean = 100?\nset.seed(42)\nsample_data &lt;- rnorm(25, mean = 105, sd = 15)\n\nt.test(sample_data, mu = 100)\n\n\n\n    One Sample t-test\n\ndata:  sample_data\nt = 1.9936, df = 24, p-value = 0.05768\nalternative hypothesis: true mean is not equal to 100\n95 percent confidence interval:\n  99.72443 115.90166\nsample estimates:\nmean of x \n  107.813 \n\n\nThe output shows the t-statistic, degrees of freedom, p-value, confidence interval, and sample mean. The small p-value indicates evidence that the true mean differs from 100.",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>T-Tests</span>"
    ]
  },
  {
    "objectID": "chapters/12-t-tests.html#two-sample-t-test",
    "href": "chapters/12-t-tests.html#two-sample-t-test",
    "title": "12  T-Tests",
    "section": "12.4 Two-Sample T-Test",
    "text": "12.4 Two-Sample T-Test\nThe two-sample (independent samples) t-test compares means from two independent groups. The null hypothesis is that the population means are equal: \\(H_0: \\mu_1 = \\mu_2\\).\nThe test assumes: - Independence of observations within and between groups - Normally distributed populations (or large samples) - Equal variances in both groups (for the standard version)\n\n\nCode\n# Two-sample t-test example\nset.seed(518)\ntreatment &lt;- rnorm(n = 30, mean = 12, sd = 3)\ncontrol &lt;- rnorm(n = 30, mean = 10, sd = 3)\n\n# Visualize the data\npar(mfrow = c(1, 2))\nboxplot(treatment, control, names = c(\"Treatment\", \"Control\"),\n        col = c(\"lightblue\", \"lightgreen\"), main = \"Boxplot\")\n        \n# Combined histogram\nhist(treatment, col = rgb(0, 0, 1, 0.5), xlim = c(0, 20),\n     main = \"Histograms\", xlab = \"Value\")\nhist(control, col = rgb(0, 1, 0, 0.5), add = TRUE)\nlegend(\"topright\", legend = c(\"Treatment\", \"Control\"),\n       fill = c(rgb(0, 0, 1, 0.5), rgb(0, 1, 0, 0.5)))\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Perform the t-test\nt.test(treatment, control)\n\n\n\n    Welch Two Sample t-test\n\ndata:  treatment and control\nt = 1.3224, df = 57.98, p-value = 0.1912\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -0.5256045  2.5718411\nsample estimates:\nmean of x mean of y \n 11.08437  10.06125",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>T-Tests</span>"
    ]
  },
  {
    "objectID": "chapters/12-t-tests.html#welchs-t-test",
    "href": "chapters/12-t-tests.html#welchs-t-test",
    "title": "12  T-Tests",
    "section": "12.5 Welch’s T-Test",
    "text": "12.5 Welch’s T-Test\nThe classic two-sample t-test assumes equal variances. When this assumption is violated, Welch’s t-test provides a better alternative. It adjusts the degrees of freedom to account for unequal variances.\nR’s t.test() function uses Welch’s test by default. To use the equal-variance version, set var.equal = TRUE.\n\n\nCode\n# When variances are unequal\nset.seed(42)\ngroup1 &lt;- rnorm(30, mean = 50, sd = 5)\ngroup2 &lt;- rnorm(30, mean = 52, sd = 15)\n\n# Welch's test (default)\nt.test(group1, group2)\n\n\n\n    Welch Two Sample t-test\n\ndata:  group1 and group2\nt = 0.055423, df = 37.98, p-value = 0.9561\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -6.095093  6.438216\nsample estimates:\nmean of x mean of y \n 50.34293  50.17137 \n\n\nCode\n# Equal variance assumed\nt.test(group1, group2, var.equal = TRUE)\n\n\n\n    Two Sample t-test\n\ndata:  group1 and group2\nt = 0.055423, df = 58, p-value = 0.956\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -6.024786  6.367910\nsample estimates:\nmean of x mean of y \n 50.34293  50.17137",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>T-Tests</span>"
    ]
  },
  {
    "objectID": "chapters/12-t-tests.html#paired-t-test",
    "href": "chapters/12-t-tests.html#paired-t-test",
    "title": "12  T-Tests",
    "section": "12.6 Paired T-Test",
    "text": "12.6 Paired T-Test\nWhen observations in two groups are naturally paired—the same subjects measured twice, matched pairs, or before-and-after measurements—the paired t-test is more appropriate. It tests whether the mean difference within pairs is zero.\nThe paired t-test is more powerful than the two-sample test when pairs are positively correlated, because it removes between-subject variability.\n\n\nCode\n# Paired t-test example: before and after treatment\nset.seed(123)\nn &lt;- 20\nbefore &lt;- rnorm(n, mean = 100, sd = 15)\n# After measurements are correlated with before\nafter &lt;- before + rnorm(n, mean = 5, sd = 5)\n\n# Paired test (correct for this data)\nt.test(after, before, paired = TRUE)\n\n\n\n    Paired t-test\n\ndata:  after and before\nt = 5.1123, df = 19, p-value = 6.19e-05\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n 2.801598 6.685830\nsample estimates:\nmean difference \n       4.743714 \n\n\nCode\n# Compare to unpaired (less power)\nt.test(after, before, paired = FALSE)\n\n\n\n    Welch Two Sample t-test\n\ndata:  after and before\nt = 1.0209, df = 37.992, p-value = 0.3138\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -4.663231 14.150660\nsample estimates:\nmean of x mean of y \n 106.8681  102.1244 \n\n\nNotice that the paired test produces a smaller p-value because it accounts for the correlation between measurements on the same subject.",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>T-Tests</span>"
    ]
  },
  {
    "objectID": "chapters/12-t-tests.html#one-tailed-vs.-two-tailed-tests",
    "href": "chapters/12-t-tests.html#one-tailed-vs.-two-tailed-tests",
    "title": "12  T-Tests",
    "section": "12.7 One-Tailed vs. Two-Tailed Tests",
    "text": "12.7 One-Tailed vs. Two-Tailed Tests\nBy default, t.test() performs a two-tailed test. For a one-tailed test, specify the alternative hypothesis:\n\n\nCode\n# Two-tailed (default): H_A: treatment ≠ control\nt.test(treatment, control, alternative = \"two.sided\")$p.value\n\n\n[1] 0.1912327\n\n\nCode\n# One-tailed: H_A: treatment &gt; control\nt.test(treatment, control, alternative = \"greater\")$p.value\n\n\n[1] 0.09561633\n\n\nCode\n# One-tailed: H_A: treatment &lt; control\nt.test(treatment, control, alternative = \"less\")$p.value\n\n\n[1] 0.9043837\n\n\nUse one-tailed tests only when you have a strong prior reason to expect an effect in a specific direction and would not act on an effect in the opposite direction.",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>T-Tests</span>"
    ]
  },
  {
    "objectID": "chapters/12-t-tests.html#checking-assumptions",
    "href": "chapters/12-t-tests.html#checking-assumptions",
    "title": "12  T-Tests",
    "section": "12.8 Checking Assumptions",
    "text": "12.8 Checking Assumptions\nT-tests assume normally distributed data (or large samples) and, for the standard two-sample test, equal variances. Check these assumptions before interpreting results.\nNormality: Use histograms, Q-Q plots, or formal tests like Shapiro-Wilk.\n\n\nCode\n# Check normality with Q-Q plot\nqqnorm(treatment)\nqqline(treatment, col = \"red\")\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Shapiro-Wilk test for normality\nshapiro.test(treatment)\n\n\n\n    Shapiro-Wilk normality test\n\ndata:  treatment\nW = 0.9115, p-value = 0.01624\n\n\nA non-significant Shapiro-Wilk test suggests the data are consistent with normality. However, this test has low power for small samples and may reject normality for trivial deviations with large samples.\nEqual variances: Compare standard deviations or use Levene’s test.\n\n\nCode\n# Compare standard deviations\nsd(treatment)\n\n\n[1] 3.024138\n\n\nCode\nsd(control)\n\n\n[1] 2.968592\n\n\nCode\n# Levene's test (from car package)\n# car::leveneTest(c(treatment, control), \n#                 factor(rep(c(\"treatment\", \"control\"), each = 30)))",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>T-Tests</span>"
    ]
  },
  {
    "objectID": "chapters/12-t-tests.html#effect-size-cohens-d",
    "href": "chapters/12-t-tests.html#effect-size-cohens-d",
    "title": "12  T-Tests",
    "section": "12.9 Effect Size: Cohen’s d",
    "text": "12.9 Effect Size: Cohen’s d\nStatistical significance does not tell you how large an effect is. Cohen’s d measures effect size as the standardized difference between means:\n\\[d = \\frac{\\bar{x}_1 - \\bar{x}_2}{s_{pooled}}\\]\nwhere \\(s_{pooled}\\) is the pooled standard deviation.\nConventional interpretations: \\(|d| = 0.2\\) is small, \\(|d| = 0.5\\) is medium, \\(|d| = 0.8\\) is large. However, context matters—a small d might be practically important in some fields.\n\n\nCode\n# Calculate Cohen's d\nmean_diff &lt;- mean(treatment) - mean(control)\ns_pooled &lt;- sqrt((var(treatment) + var(control)) / 2)\ncohens_d &lt;- mean_diff / s_pooled\n\ncat(\"Cohen's d:\", round(cohens_d, 2), \"\\n\")\n\n\nCohen's d: 0.34",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>T-Tests</span>"
    ]
  },
  {
    "objectID": "chapters/12-t-tests.html#practical-example",
    "href": "chapters/12-t-tests.html#practical-example",
    "title": "12  T-Tests",
    "section": "12.10 Practical Example",
    "text": "12.10 Practical Example\nLet’s work through a complete analysis comparing two groups:\n\n\nCode\n# Simulated drug trial data\nset.seed(999)\ndrug &lt;- rnorm(40, mean = 75, sd = 12)\nplacebo &lt;- rnorm(40, mean = 70, sd = 12)\n\n# Step 1: Visualize\npar(mfrow = c(2, 2))\nboxplot(drug, placebo, names = c(\"Drug\", \"Placebo\"), \n        col = c(\"coral\", \"lightblue\"), main = \"Response by Group\")\n\n# Step 2: Check normality\nqqnorm(drug, main = \"Q-Q Plot: Drug\")\nqqline(drug, col = \"red\")\nqqnorm(placebo, main = \"Q-Q Plot: Placebo\")\nqqline(placebo, col = \"red\")\n\n# Combined histogram\nhist(drug, col = rgb(1, 0.5, 0.5, 0.5), xlim = c(40, 110),\n     main = \"Distribution Comparison\", xlab = \"Response\")\nhist(placebo, col = rgb(0.5, 0.5, 1, 0.5), add = TRUE)\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Step 3: Perform t-test\nresult &lt;- t.test(drug, placebo)\nprint(result)\n\n\n\n    Welch Two Sample t-test\n\ndata:  drug and placebo\nt = 1.2147, df = 75.923, p-value = 0.2282\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -1.990367  8.213525\nsample estimates:\nmean of x mean of y \n 72.26982  69.15824 \n\n\nCode\n# Step 4: Calculate effect size\ncohens_d &lt;- (mean(drug) - mean(placebo)) / \n            sqrt((var(drug) + var(placebo)) / 2)\ncat(\"\\nCohen's d:\", round(cohens_d, 2), \"\\n\")\n\n\n\nCohen's d: 0.27 \n\n\nThe t-test shows a significant difference (p &lt; 0.05), and Cohen’s d indicates a medium effect size. We can conclude that the drug group shows higher response than the placebo group, with the mean difference being about 0.4 standard deviations.",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>T-Tests</span>"
    ]
  },
  {
    "objectID": "chapters/13-nonparametric-tests.html",
    "href": "chapters/13-nonparametric-tests.html",
    "title": "13  Nonparametric Tests",
    "section": "",
    "text": "13.1 When Assumptions Fail\nParametric tests like the t-test make assumptions about the underlying data distribution—typically that data are normally distributed with equal variances across groups. When these assumptions are violated, the tests may give misleading results. Nonparametric tests provide alternatives that make fewer assumptions about the data.\nNonparametric methods are sometimes called distribution-free methods because they do not assume a specific probability distribution. Instead, they typically work with ranks or signs of data rather than the raw values. This makes them robust to outliers and applicable to ordinal data where parametric methods would be inappropriate.",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Nonparametric Tests</span>"
    ]
  },
  {
    "objectID": "chapters/13-nonparametric-tests.html#the-mann-whitney-u-test",
    "href": "chapters/13-nonparametric-tests.html#the-mann-whitney-u-test",
    "title": "13  Nonparametric Tests",
    "section": "13.2 The Mann-Whitney U Test",
    "text": "13.2 The Mann-Whitney U Test\nThe Mann-Whitney U test (also called the Wilcoxon rank-sum test) is the nonparametric equivalent of the two-sample t-test. It tests whether two independent groups tend to have different values, based on comparing the ranks of observations rather than the observations themselves.\nThe null hypothesis is that the distributions of the two groups are identical. The alternative is that one group tends to have larger values than the other.\n\n\nCode\n# Generate data with non-normal distributions\nset.seed(518)\ngroup1 &lt;- sample(rnorm(n = 10000, mean = 2, sd = 0.5), size = 100)\ngroup2 &lt;- sample(rnorm(n = 10000, mean = 5, sd = 1.5), size = 100)\n\n# Mann-Whitney U test\nwilcox.test(group1, group2)\n\n\n\n    Wilcoxon rank sum test with continuity correction\n\ndata:  group1 and group2\nW = 440, p-value &lt; 2.2e-16\nalternative hypothesis: true location shift is not equal to 0\n\n\nThe test works by combining all observations, ranking them, and comparing the sum of ranks in each group. If one group tends to have higher values, its rank sum will be larger than expected by chance.",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Nonparametric Tests</span>"
    ]
  },
  {
    "objectID": "chapters/13-nonparametric-tests.html#wilcoxon-signed-rank-test",
    "href": "chapters/13-nonparametric-tests.html#wilcoxon-signed-rank-test",
    "title": "13  Nonparametric Tests",
    "section": "13.3 Wilcoxon Signed-Rank Test",
    "text": "13.3 Wilcoxon Signed-Rank Test\nFor paired data, the Wilcoxon signed-rank test is the nonparametric alternative to the paired t-test. It tests whether the median difference between pairs is zero.\n\n\nCode\n# Paired data example\nset.seed(123)\nbefore &lt;- rnorm(20, mean = 100, sd = 15)\nafter &lt;- before + rexp(20, rate = 0.2)  # Skewed improvement\n\nwilcox.test(after, before, paired = TRUE)\n\n\n\n    Wilcoxon signed rank exact test\n\ndata:  after and before\nV = 210, p-value = 1.907e-06\nalternative hypothesis: true location shift is not equal to 0\n\n\nThe test calculates the differences between pairs, ranks their absolute values, and considers the signs of the differences. Under the null hypothesis, positive and negative differences should be equally likely and of similar magnitude.",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Nonparametric Tests</span>"
    ]
  },
  {
    "objectID": "chapters/13-nonparametric-tests.html#kruskal-wallis-test",
    "href": "chapters/13-nonparametric-tests.html#kruskal-wallis-test",
    "title": "13  Nonparametric Tests",
    "section": "13.4 Kruskal-Wallis Test",
    "text": "13.4 Kruskal-Wallis Test\nThe Kruskal-Wallis test extends the Mann-Whitney U test to more than two groups, serving as a nonparametric alternative to one-way ANOVA. It tests whether at least one group tends to have different values from the others.\n\n\nCode\n# Example with three groups\nset.seed(42)\ndata &lt;- data.frame(\n  value = c(rexp(30, 0.1), rexp(30, 0.15), rexp(30, 0.2)),\n  group = factor(rep(c(\"A\", \"B\", \"C\"), each = 30))\n)\n\nkruskal.test(value ~ group, data = data)\n\n\n\n    Kruskal-Wallis rank sum test\n\ndata:  value by group\nKruskal-Wallis chi-squared = 9.3507, df = 2, p-value = 0.009322\n\n\nLike ANOVA, a significant Kruskal-Wallis test tells you that groups differ but not which specific groups differ from which others. Post-hoc pairwise comparisons can follow up on a significant result.",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Nonparametric Tests</span>"
    ]
  },
  {
    "objectID": "chapters/13-nonparametric-tests.html#advantages-and-limitations",
    "href": "chapters/13-nonparametric-tests.html#advantages-and-limitations",
    "title": "13  Nonparametric Tests",
    "section": "13.5 Advantages and Limitations",
    "text": "13.5 Advantages and Limitations\nAdvantages of nonparametric tests:\nNonparametric tests do not require normally distributed data. They are robust to outliers since they work with ranks rather than raw values. They can be applied to ordinal data where the assumption of interval-level measurement would be violated. They often have good power relative to parametric tests even when parametric assumptions are met.\nLimitations:\nWhen parametric assumptions are met, nonparametric tests are slightly less powerful than their parametric counterparts. They test hypotheses about distributions or medians rather than means, which may not always align with research questions. They can be more difficult to extend to complex designs with multiple factors or covariates.",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Nonparametric Tests</span>"
    ]
  },
  {
    "objectID": "chapters/13-nonparametric-tests.html#choosing-between-parametric-and-nonparametric",
    "href": "chapters/13-nonparametric-tests.html#choosing-between-parametric-and-nonparametric",
    "title": "13  Nonparametric Tests",
    "section": "13.6 Choosing Between Parametric and Nonparametric",
    "text": "13.6 Choosing Between Parametric and Nonparametric\nThe choice depends on your data and research question. If your data are reasonably normal (or your sample is large enough for the Central Limit Theorem to apply) and you care about means, parametric tests are appropriate and efficient. If your data are severely non-normal, contain outliers, or are ordinal in nature, nonparametric tests provide a safer alternative.\nWith large samples, the Central Limit Theorem ensures that parametric tests are robust to non-normality, so the choice matters less. With small samples, checking assumptions becomes more important.",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Nonparametric Tests</span>"
    ]
  },
  {
    "objectID": "chapters/14-bootstrapping.html",
    "href": "chapters/14-bootstrapping.html",
    "title": "14  Bootstrapping",
    "section": "",
    "text": "14.1 The Bootstrap Idea\nFor the sample mean, we have elegant formulas for standard errors and confidence intervals derived from probability theory. But what about other statistics—the median, a correlation coefficient, the ratio of two means? For many estimators, no convenient formula exists.\nThe bootstrap, invented by Bradley Efron in 1979, provides a general solution. The key insight is that we can learn about the sampling distribution of a statistic by resampling from our data. If our sample is representative of the population, then samples drawn from our sample (with replacement) mimic what we would get from repeated sampling from the population.",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Bootstrapping</span>"
    ]
  },
  {
    "objectID": "chapters/14-bootstrapping.html#why-the-bootstrap-works",
    "href": "chapters/14-bootstrapping.html#why-the-bootstrap-works",
    "title": "14  Bootstrapping",
    "section": "14.2 Why the Bootstrap Works",
    "text": "14.2 Why the Bootstrap Works\nThe bootstrap treats the observed sample as if it were the population. By drawing many samples with replacement from this “population,” we create a distribution of the statistic of interest. This bootstrap distribution approximates the true sampling distribution.\nThe bootstrap standard error is the standard deviation of the bootstrap distribution. Bootstrap confidence intervals can be constructed from the percentiles of the bootstrap distribution—the 2.5th and 97.5th percentiles give an approximate 95% confidence interval.",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Bootstrapping</span>"
    ]
  },
  {
    "objectID": "chapters/14-bootstrapping.html#bootstrap-procedure",
    "href": "chapters/14-bootstrapping.html#bootstrap-procedure",
    "title": "14  Bootstrapping",
    "section": "14.3 Bootstrap Procedure",
    "text": "14.3 Bootstrap Procedure\nThe basic algorithm is straightforward:\n\nDraw a random sample of size n from your data with replacement (the bootstrap sample)\nCalculate the statistic of interest from this bootstrap sample\nRepeat steps 1 and 2 many times (1000 or more)\nUse the distribution of bootstrap statistics to estimate standard error or confidence intervals\n\n\n\nCode\n# Bootstrap example: estimating standard error of median\nset.seed(42)\noriginal_data &lt;- rexp(50, rate = 0.1)  # Skewed distribution\n\n# Observed median\nobserved_median &lt;- median(original_data)\n\n# Bootstrap\nn_boot &lt;- 1000\nboot_medians &lt;- replicate(n_boot, {\n  boot_sample &lt;- sample(original_data, replace = TRUE)\n  median(boot_sample)\n})\n\n# Bootstrap standard error\nboot_se &lt;- sd(boot_medians)\n\n# Bootstrap confidence interval (percentile method)\nboot_ci &lt;- quantile(boot_medians, c(0.025, 0.975))\n\ncat(\"Observed median:\", round(observed_median, 2), \"\\n\")\n\n\nObserved median: 6.59 \n\n\nCode\ncat(\"Bootstrap SE:\", round(boot_se, 2), \"\\n\")\n\n\nBootstrap SE: 1.46 \n\n\nCode\ncat(\"95% CI:\", round(boot_ci, 2), \"\\n\")\n\n\n95% CI: 4.39 11.92 \n\n\nCode\nhist(boot_medians, breaks = 30, main = \"Bootstrap Distribution of Median\",\n     xlab = \"Median\", col = \"lightblue\")\nabline(v = observed_median, col = \"red\", lwd = 2)\nabline(v = boot_ci, col = \"blue\", lwd = 2, lty = 2)",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Bootstrapping</span>"
    ]
  },
  {
    "objectID": "chapters/14-bootstrapping.html#advantages-of-the-bootstrap",
    "href": "chapters/14-bootstrapping.html#advantages-of-the-bootstrap",
    "title": "14  Bootstrapping",
    "section": "14.4 Advantages of the Bootstrap",
    "text": "14.4 Advantages of the Bootstrap\nThe bootstrap is remarkably versatile. It can be applied to almost any statistic—means, medians, correlations, regression coefficients, eigenvalues, and more. It works when no formula for standard errors exists. It is nonparametric, making no assumptions about the underlying distribution. It handles complex sampling designs and calculations that would be intractable analytically.\nThe bootstrap is widely used for assessing confidence in phylogenetic trees, where the complexity of tree-building algorithms makes analytical approaches impractical. In machine learning, bootstrap aggregating (bagging) improves prediction accuracy by combining models trained on bootstrap samples.",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Bootstrapping</span>"
    ]
  },
  {
    "objectID": "chapters/14-bootstrapping.html#when-the-bootstrap-fails",
    "href": "chapters/14-bootstrapping.html#when-the-bootstrap-fails",
    "title": "14  Bootstrapping",
    "section": "14.5 When the Bootstrap Fails",
    "text": "14.5 When the Bootstrap Fails\nThe bootstrap is not a magic solution to all problems. It requires that the original sample be representative of the population—a biased sample produces biased bootstrap estimates. It can struggle with very small samples where the original data may not adequately represent the population.\nCertain statistics, like the maximum of a sample, are poorly estimated by the bootstrap because the bootstrap distribution is bounded by the observed data. The bootstrap also assumes that observations are independent; for dependent data (like time series), specialized bootstrap methods are needed.",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Bootstrapping</span>"
    ]
  },
  {
    "objectID": "chapters/14-bootstrapping.html#bootstrap-confidence-intervals",
    "href": "chapters/14-bootstrapping.html#bootstrap-confidence-intervals",
    "title": "14  Bootstrapping",
    "section": "14.6 Bootstrap Confidence Intervals",
    "text": "14.6 Bootstrap Confidence Intervals\nSeveral methods exist for constructing bootstrap confidence intervals. The percentile method uses the quantiles of the bootstrap distribution directly. The basic bootstrap method reflects the bootstrap distribution around the observed estimate. The BCa (bias-corrected and accelerated) method adjusts for bias and skewness in the bootstrap distribution.\n\n\nCode\n# Different bootstrap CI methods\nlibrary(boot)\n\n# Define statistic function\nmedian_fun &lt;- function(data, indices) {\n  median(data[indices])\n}\n\n# Run bootstrap\nboot_result &lt;- boot(original_data, median_fun, R = 1000)\n\n# Different CI methods\nboot.ci(boot_result, type = c(\"perc\", \"basic\", \"bca\"))\n\n\nBOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS\nBased on 1000 bootstrap replicates\n\nCALL : \nboot.ci(boot.out = boot_result, type = c(\"perc\", \"basic\", \"bca\"))\n\nIntervals : \nLevel      Basic              Percentile            BCa          \n95%   ( 1.256,  8.841 )   ( 4.331, 11.916 )   ( 4.244, 11.582 )  \nCalculations and Intervals on Original Scale\n\n\nThe BCa method is generally preferred when computationally feasible, as it provides better coverage in many situations.",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Bootstrapping</span>"
    ]
  },
  {
    "objectID": "chapters/14-bootstrapping.html#practical-recommendations",
    "href": "chapters/14-bootstrapping.html#practical-recommendations",
    "title": "14  Bootstrapping",
    "section": "14.7 Practical Recommendations",
    "text": "14.7 Practical Recommendations\nFor most applications, 1000 bootstrap replications provide adequate precision for standard errors. For confidence intervals, especially when using the BCa method, 10,000 replications may be preferable. Always set a random seed for reproducibility.\nRemember that the bootstrap estimates sampling variability—it cannot fix problems with biased samples or invalid measurements. Use it as a tool for understanding uncertainty, not as a cure for poor data quality.",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Bootstrapping</span>"
    ]
  },
  {
    "objectID": "chapters/15-correlation.html",
    "href": "chapters/15-correlation.html",
    "title": "15  Correlation",
    "section": "",
    "text": "15.1 Measuring Association\nWhen two variables vary together, we say they are correlated. Understanding whether and how variables are related is fundamental to science—it helps us identify potential causal relationships, make predictions, and understand systems.\nCorrelation quantifies the strength and direction of the linear relationship between two variables. A positive correlation means that high values of one variable tend to occur with high values of the other. A negative correlation means that high values of one variable tend to occur with low values of the other.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Correlation</span>"
    ]
  },
  {
    "objectID": "chapters/15-correlation.html#covariance",
    "href": "chapters/15-correlation.html#covariance",
    "title": "15  Correlation",
    "section": "15.2 Covariance",
    "text": "15.2 Covariance\nThe covariance measures how two variables vary together. If X and Y tend to be above their means at the same time (and below their means at the same time), the covariance is positive. If one tends to be above its mean when the other is below, the covariance is negative.\n\\[Cov(X, Y) = \\frac{\\sum_{i=1}^{n}(x_i - \\bar{x})(y_i - \\bar{y})}{n-1}\\]\nThe problem with covariance is that its magnitude depends on the scales of X and Y, making it hard to interpret. Is a covariance of 100 strong or weak? It depends entirely on the units of measurement.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Correlation</span>"
    ]
  },
  {
    "objectID": "chapters/15-correlation.html#pearsons-correlation-coefficient",
    "href": "chapters/15-correlation.html#pearsons-correlation-coefficient",
    "title": "15  Correlation",
    "section": "15.3 Pearson’s Correlation Coefficient",
    "text": "15.3 Pearson’s Correlation Coefficient\nThe Pearson correlation coefficient standardizes covariance by dividing by the product of the standard deviations:\n\\[r = \\frac{Cov(X, Y)}{s_X s_Y} = \\frac{\\sum_{i=1}^{n}(x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum(x_i - \\bar{x})^2 \\sum(y_i - \\bar{y})^2}}\\]\nThis produces a value between -1 and +1:\n\n\\(r = 1\\): perfect positive linear relationship\n\\(r = -1\\): perfect negative linear relationship\n\n\\(r = 0\\): no linear relationship\n\n\n\n\n\n\n\n\nCode\n# Examples of different correlations\nset.seed(42)\nn &lt;- 100\n\npar(mfrow = c(2, 2))\n\n# Strong positive\nx1 &lt;- rnorm(n)\ny1 &lt;- 0.9 * x1 + rnorm(n, sd = 0.4)\nplot(x1, y1, main = paste(\"r =\", round(cor(x1, y1), 2)), pch = 19, col = \"blue\")\n\n# Moderate negative\ny2 &lt;- -0.6 * x1 + rnorm(n, sd = 0.8)\nplot(x1, y2, main = paste(\"r =\", round(cor(x1, y2), 2)), pch = 19, col = \"red\")\n\n# No correlation\ny3 &lt;- rnorm(n)\nplot(x1, y3, main = paste(\"r =\", round(cor(x1, y3), 2)), pch = 19, col = \"gray\")\n\n# Non-linear relationship (correlation misleading)\nx4 &lt;- runif(n, -3, 3)\ny4 &lt;- x4^2 + rnorm(n, sd = 0.5)\nplot(x4, y4, main = paste(\"r =\", round(cor(x4, y4), 2), \"(non-linear!)\"), \n     pch = 19, col = \"purple\")",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Correlation</span>"
    ]
  },
  {
    "objectID": "chapters/15-correlation.html#anscombes-quartet",
    "href": "chapters/15-correlation.html#anscombes-quartet",
    "title": "15  Correlation",
    "section": "15.4 Anscombe’s Quartet",
    "text": "15.4 Anscombe’s Quartet\nFrancis Anscombe created a famous set of four datasets that all have nearly identical statistical properties—same means, variances, correlations, and regression lines—yet look completely different when plotted. This demonstrates why visualization is essential.\n\n\n\n\n\nAlways plot your data before calculating correlations. The correlation coefficient captures only linear relationships and can be misleading for non-linear patterns.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Correlation</span>"
    ]
  },
  {
    "objectID": "chapters/15-correlation.html#testing-correlation",
    "href": "chapters/15-correlation.html#testing-correlation",
    "title": "15  Correlation",
    "section": "15.5 Testing Correlation",
    "text": "15.5 Testing Correlation\nThe cor.test() function tests whether a correlation is significantly different from zero:\n\n\nCode\n# Example: zebrafish length and weight\nset.seed(123)\nlength &lt;- rnorm(50, mean = 2.5, sd = 0.5)\nweight &lt;- 10 * length^2 + rnorm(50, sd = 5)\n\ncor.test(length, weight)\n\n\n\n    Pearson's product-moment correlation\n\ndata:  length and weight\nt = 29.857, df = 48, p-value &lt; 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.9546054 0.9853063\nsample estimates:\n     cor \n0.974118 \n\n\nThe null hypothesis is that the population correlation is zero (\\(H_0: \\rho = 0\\)). A small p-value indicates evidence of a non-zero correlation.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Correlation</span>"
    ]
  },
  {
    "objectID": "chapters/15-correlation.html#parametric-assumptions",
    "href": "chapters/15-correlation.html#parametric-assumptions",
    "title": "15  Correlation",
    "section": "15.6 Parametric Assumptions",
    "text": "15.6 Parametric Assumptions\nPearson’s correlation assumes that both variables are normally distributed (or at least that the relationship is linear and homoscedastic). When these assumptions are violated, nonparametric alternatives may be more appropriate.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Correlation</span>"
    ]
  },
  {
    "objectID": "chapters/15-correlation.html#nonparametric-correlation",
    "href": "chapters/15-correlation.html#nonparametric-correlation",
    "title": "15  Correlation",
    "section": "15.7 Nonparametric Correlation",
    "text": "15.7 Nonparametric Correlation\nSpearman’s rank correlation replaces values with their ranks before calculating correlation. It measures monotonic (consistently increasing or decreasing) rather than strictly linear relationships and is robust to outliers.\nKendall’s tau is another rank-based measure that counts concordant and discordant pairs. It is particularly appropriate for small samples or data with many ties.\n\n\nCode\n# Compare methods on non-normal data\nset.seed(42)\nx &lt;- rexp(30, rate = 0.1)\ny &lt;- x + rexp(30, rate = 0.2)\n\ncat(\"Pearson:\", round(cor(x, y, method = \"pearson\"), 3), \"\\n\")\n\n\nPearson: 0.764 \n\n\nCode\ncat(\"Spearman:\", round(cor(x, y, method = \"spearman\"), 3), \"\\n\")\n\n\nSpearman: 0.808 \n\n\nCode\ncat(\"Kendall:\", round(cor(x, y, method = \"kendall\"), 3), \"\\n\")\n\n\nKendall: 0.623",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Correlation</span>"
    ]
  },
  {
    "objectID": "chapters/15-correlation.html#correlation-is-not-causation",
    "href": "chapters/15-correlation.html#correlation-is-not-causation",
    "title": "15  Correlation",
    "section": "15.8 Correlation Is Not Causation",
    "text": "15.8 Correlation Is Not Causation\nA correlation between X and Y might arise because X causes Y, because Y causes X, because a third variable Z causes both, or simply by chance. Correlation alone cannot distinguish these possibilities.\nTo establish causation, you need experimental manipulation (changing X and observing Y), temporal precedence (X occurs before Y), and ruling out confounding variables. Observational correlations are valuable for generating hypotheses but insufficient for establishing causal relationships.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Correlation</span>"
    ]
  },
  {
    "objectID": "chapters/16-simple-linear-regression.html",
    "href": "chapters/16-simple-linear-regression.html",
    "title": "16  Simple Linear Regression",
    "section": "",
    "text": "16.1 From Correlation to Prediction\nCorrelation tells us that two variables are related, but it does not allow us to predict one from the other or to describe the nature of that relationship. Linear regression goes further—it models the relationship between variables, allowing us to make predictions and to quantify how changes in one variable are associated with changes in another.\nIn simple linear regression, we model a response variable Y as a linear function of a predictor variable X:\n\\[y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i\\]\nHere \\(\\beta_0\\) is the intercept (the expected value of Y when X equals zero), \\(\\beta_1\\) is the slope (how much Y changes for a one-unit change in X), and \\(\\epsilon_i\\) represents the random error—the part of Y not explained by X.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "chapters/16-simple-linear-regression.html#origins-of-the-term-regression",
    "href": "chapters/16-simple-linear-regression.html#origins-of-the-term-regression",
    "title": "16  Simple Linear Regression",
    "section": "16.2 Origins of the Term “Regression”",
    "text": "16.2 Origins of the Term “Regression”\nThe term “regression” comes from Francis Galton’s studies of heredity in the 1880s. He observed that tall parents tended to have children who were tall, but not as extremely tall as the parents—children’s heights “regressed” toward the population mean. This phenomenon, now called regression to the mean, is a statistical artifact that appears whenever two variables are imperfectly correlated.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "chapters/16-simple-linear-regression.html#fitting-the-model-ordinary-least-squares",
    "href": "chapters/16-simple-linear-regression.html#fitting-the-model-ordinary-least-squares",
    "title": "16  Simple Linear Regression",
    "section": "16.3 Fitting the Model: Ordinary Least Squares",
    "text": "16.3 Fitting the Model: Ordinary Least Squares\nThe most common method for fitting a regression line is ordinary least squares (OLS). OLS finds the line that minimizes the sum of squared residuals—the squared vertical distances between observed points and the fitted line.\n\n\n\n\n\nThe OLS estimates for the slope and intercept are:\n\\[\\hat{\\beta}_1 = \\frac{\\sum(x_i - \\bar{x})(y_i - \\bar{y})}{\\sum(x_i - \\bar{x})^2} = r \\frac{s_y}{s_x}\\]\n\\[\\hat{\\beta}_0 = \\bar{y} - \\hat{\\beta}_1 \\bar{x}\\]\nNotice that the slope equals the correlation coefficient times the ratio of standard deviations. This makes clear the connection between correlation and regression.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "chapters/16-simple-linear-regression.html#linear-regression-in-r",
    "href": "chapters/16-simple-linear-regression.html#linear-regression-in-r",
    "title": "16  Simple Linear Regression",
    "section": "16.4 Linear Regression in R",
    "text": "16.4 Linear Regression in R\n\n\nCode\n# Example: zebrafish size data\nset.seed(42)\nn &lt;- 100\nlength_cm &lt;- runif(n, 0.5, 3.5)\nweight_mg &lt;- 15 * length_cm^2 + rnorm(n, sd = 10)\n\n# Fit the model\nfish_lm &lt;- lm(weight_mg ~ length_cm)\nsummary(fish_lm)\n\n\n\nCall:\nlm(formula = weight_mg ~ length_cm)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-39.277  -9.033  -0.432   9.998  29.934 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  -51.429      3.579  -14.37   &lt;2e-16 ***\nlength_cm     61.660      1.583   38.95   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 14.27 on 98 degrees of freedom\nMultiple R-squared:  0.9393,    Adjusted R-squared:  0.9387 \nF-statistic:  1517 on 1 and 98 DF,  p-value: &lt; 2.2e-16\n\n\nThe output shows the estimated coefficients with their standard errors, t-statistics, and p-values. The Multiple R-squared indicates how much of the variance in Y is explained by X.\n\n\nCode\n# Visualize the fit\nplot(length_cm, weight_mg, pch = 19, col = \"blue\",\n     xlab = \"Length (cm)\", ylab = \"Weight (mg)\",\n     main = \"Linear Regression: Weight vs Length\")\nabline(fish_lm, col = \"red\", lwd = 2)",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "chapters/16-simple-linear-regression.html#interpretation-of-coefficients",
    "href": "chapters/16-simple-linear-regression.html#interpretation-of-coefficients",
    "title": "16  Simple Linear Regression",
    "section": "16.5 Interpretation of Coefficients",
    "text": "16.5 Interpretation of Coefficients\nThe intercept \\(\\hat{\\beta}_0\\) is the predicted value of Y when X equals zero. This may or may not be meaningful depending on whether X = 0 makes sense in your context.\nThe slope \\(\\hat{\\beta}_1\\) is the predicted change in Y for a one-unit increase in X. If the slope is 15, then each additional unit of X is associated with 15 more units of Y on average.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "chapters/16-simple-linear-regression.html#hypothesis-testing-in-regression",
    "href": "chapters/16-simple-linear-regression.html#hypothesis-testing-in-regression",
    "title": "16  Simple Linear Regression",
    "section": "16.6 Hypothesis Testing in Regression",
    "text": "16.6 Hypothesis Testing in Regression\nThe hypothesis test for the slope asks whether there is evidence of a relationship between X and Y:\n\\[H_0: \\beta_1 = 0 \\quad \\text{(no relationship)}\\] \\[H_A: \\beta_1 \\neq 0 \\quad \\text{(relationship exists)}\\]\nThe test uses a t-statistic, comparing the estimated slope to its standard error. The p-value indicates the probability of observing a slope this far from zero if the true slope were zero.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "chapters/16-simple-linear-regression.html#r-squared-measuring-model-fit",
    "href": "chapters/16-simple-linear-regression.html#r-squared-measuring-model-fit",
    "title": "16  Simple Linear Regression",
    "section": "16.7 R-Squared: Measuring Model Fit",
    "text": "16.7 R-Squared: Measuring Model Fit\nR-squared (\\(R^2\\)) measures the proportion of variance in Y explained by the model:\n\\[R^2 = 1 - \\frac{SS_{error}}{SS_{total}} = \\frac{SS_{model}}{SS_{total}}\\]\nIn simple linear regression, \\(R^2\\) equals the square of the correlation coefficient. An \\(R^2\\) of 0.7 means the model explains 70% of the variance in Y; the remaining 30% is unexplained.\nBe cautious with \\(R^2\\)—it always increases when you add predictors, even useless ones. Adjusted \\(R^2\\) penalizes for model complexity.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "chapters/16-simple-linear-regression.html#making-predictions",
    "href": "chapters/16-simple-linear-regression.html#making-predictions",
    "title": "16  Simple Linear Regression",
    "section": "16.8 Making Predictions",
    "text": "16.8 Making Predictions\nOnce you have a fitted model, you can predict Y for new values of X:\n\n\nCode\n# Predict weight for new lengths\nnew_lengths &lt;- data.frame(length_cm = c(1.0, 2.0, 3.0))\npredict(fish_lm, newdata = new_lengths, interval = \"confidence\")\n\n\n        fit        lwr       upr\n1  10.23108   5.828292  14.63387\n2  71.89135  69.050838  74.73186\n3 133.55162 129.491293 137.61194\n\n\nThe confidence interval indicates uncertainty about the mean Y at each X value. A prediction interval (using interval = \"prediction\") would be wider, accounting for individual variability around that mean.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "chapters/16-simple-linear-regression.html#model-assumptions",
    "href": "chapters/16-simple-linear-regression.html#model-assumptions",
    "title": "16  Simple Linear Regression",
    "section": "16.9 Model Assumptions",
    "text": "16.9 Model Assumptions\nLinear regression assumptions include:\n\nLinearity: The relationship between X and Y is linear\nIndependence: Observations are independent of each other\nNormality: Residuals are normally distributed\nHomoscedasticity: Residuals have constant variance across X\n\nThese assumptions should be checked through residual analysis, which we cover in the next chapter.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "chapters/17-residual-analysis.html",
    "href": "chapters/17-residual-analysis.html",
    "title": "17  Residual Analysis",
    "section": "",
    "text": "17.1 What Are Residuals?\nResiduals are the differences between observed values and values predicted by the model:\n\\[e_i = y_i - \\hat{y}_i\\]\nThey represent the part of the data not explained by the model—the “leftover” variation. Analyzing residuals helps us check whether the assumptions of our model are met and identify potential problems.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Residual Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/17-residual-analysis.html#why-residual-analysis-matters",
    "href": "chapters/17-residual-analysis.html#why-residual-analysis-matters",
    "title": "17  Residual Analysis",
    "section": "17.2 Why Residual Analysis Matters",
    "text": "17.2 Why Residual Analysis Matters\nA regression model might fit the data well according to R-squared while still being inappropriate. The model might capture the wrong pattern, miss non-linear relationships, or be unduly influenced by outliers. Residual analysis reveals these problems.\nRemember Anscombe’s Quartet—four datasets with identical regression lines but very different patterns. Looking only at the regression output would miss these differences entirely.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Residual Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/17-residual-analysis.html#checking-assumptions",
    "href": "chapters/17-residual-analysis.html#checking-assumptions",
    "title": "17  Residual Analysis",
    "section": "17.3 Checking Assumptions",
    "text": "17.3 Checking Assumptions\n\n17.3.1 Linearity\nIf the relationship is truly linear, residuals should show no systematic pattern when plotted against fitted values or the predictor variable. A curved pattern suggests non-linearity.\n\n\nCode\n# Create data with non-linear relationship\nset.seed(42)\nx &lt;- seq(0, 10, length.out = 100)\ny &lt;- 2 + 0.5 * x + 0.1 * x^2 + rnorm(100, sd = 1)\n\nmodel &lt;- lm(y ~ x)\n\npar(mfrow = c(1, 2))\nplot(x, y, main = \"Data with Quadratic Pattern\")\nabline(model, col = \"red\")\n\nplot(fitted(model), residuals(model), \n     main = \"Residuals vs Fitted\",\n     xlab = \"Fitted values\", ylab = \"Residuals\")\nabline(h = 0, col = \"red\", lty = 2)\n\n\n\n\n\n\n\n\n\nThe curved pattern in the residual plot reveals that a linear model is inadequate.\n\n\n17.3.2 Normality\nResiduals should be approximately normally distributed. Check with a histogram or Q-Q plot:\n\n\nCode\n# Good model for comparison\nx2 &lt;- rnorm(100)\ny2 &lt;- 2 + 3 * x2 + rnorm(100)\ngood_model &lt;- lm(y2 ~ x2)\n\npar(mfrow = c(1, 2))\nhist(residuals(good_model), breaks = 20, main = \"Histogram of Residuals\",\n     xlab = \"Residuals\", col = \"lightblue\")\nqqnorm(residuals(good_model))\nqqline(residuals(good_model), col = \"red\")\n\n\n\n\n\n\n\n\n\nPoints on the Q-Q plot should fall approximately along the diagonal line. Systematic departures indicate non-normality.\n\n\n17.3.3 Homoscedasticity\nResiduals should have constant variance across the range of fitted values. A fan or cone shape indicates heteroscedasticity (unequal variance).\n\n\n\n\n\n\n\n17.3.4 Independence\nResiduals should be independent of each other. This is hard to check visually but is violated when observations are related (e.g., repeated measurements on the same subjects, or time series data).",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Residual Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/17-residual-analysis.html#diagnostic-plots-in-r",
    "href": "chapters/17-residual-analysis.html#diagnostic-plots-in-r",
    "title": "17  Residual Analysis",
    "section": "17.4 Diagnostic Plots in R",
    "text": "17.4 Diagnostic Plots in R\nR provides built-in diagnostic plots for linear models:\n\n\nCode\n# Standard diagnostic plots\npar(mfrow = c(2, 2))\nplot(good_model)\n\n\n\n\n\n\n\n\n\nThese four plots show: 1. Residuals vs Fitted: Check for linearity and homoscedasticity 2. Q-Q Plot: Check for normality 3. Scale-Location: Check for homoscedasticity 4. Residuals vs Leverage: Identify influential points",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Residual Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/17-residual-analysis.html#leverage-and-influence",
    "href": "chapters/17-residual-analysis.html#leverage-and-influence",
    "title": "17  Residual Analysis",
    "section": "17.5 Leverage and Influence",
    "text": "17.5 Leverage and Influence\nNot all observations affect the regression equally. Leverage measures how unusual an observation’s X value is—points with extreme X values have more potential to influence the fitted line.\nCook’s Distance measures how much the regression would change if an observation were removed. High Cook’s D values indicate influential points that merit closer examination.\n\n\n\n\n\n\n\nCode\n# Check for influential points\ninfluence.measures(good_model)$is.inf[1:5,]  # First 5 observations\n\n\n  dfb.1_ dfb.x2 dffit cov.r cook.d   hat\n1  FALSE  FALSE FALSE FALSE  FALSE FALSE\n2  FALSE  FALSE FALSE FALSE  FALSE FALSE\n3  FALSE  FALSE FALSE FALSE  FALSE FALSE\n4  FALSE  FALSE  TRUE FALSE  FALSE FALSE\n5  FALSE  FALSE FALSE FALSE  FALSE FALSE",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Residual Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/17-residual-analysis.html#handling-violations",
    "href": "chapters/17-residual-analysis.html#handling-violations",
    "title": "17  Residual Analysis",
    "section": "17.6 Handling Violations",
    "text": "17.6 Handling Violations\nWhen assumptions are violated, several approaches may help:\nTransform the data: Log, square root, or other transformations can stabilize variance and improve linearity.\nUse robust regression: Methods like rlm() from the MASS package down-weight influential observations.\nTry a different model: Non-linear regression, generalized linear models, or generalized additive models may be more appropriate.\nRemove outliers: Only if you have substantive reasons—never simply to improve fit.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Residual Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/17-residual-analysis.html#residual-analysis-workflow",
    "href": "chapters/17-residual-analysis.html#residual-analysis-workflow",
    "title": "17  Residual Analysis",
    "section": "17.7 Residual Analysis Workflow",
    "text": "17.7 Residual Analysis Workflow\nA systematic approach to residual analysis:\n\nFit the model\nGenerate diagnostic plots\nCheck for patterns in residuals vs. fitted values\nExamine the Q-Q plot for normality\nLook for influential points\nIf problems exist, consider transformations or alternative models\nRe-check diagnostics after any changes\n\nResidual analysis is not optional—it is an essential part of any regression analysis. Models that look good on paper may tell misleading stories if their assumptions are violated.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Residual Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/18-statistical-power.html",
    "href": "chapters/18-statistical-power.html",
    "title": "18  Statistical Power",
    "section": "",
    "text": "18.1 What is Statistical Power?\nPower is the probability of correctly rejecting a false null hypothesis—the probability of detecting an effect when one truly exists. If the true effect size is non-zero, power tells us how likely our study is to find it.\nPower = 1 - \\(\\beta\\), where \\(\\beta\\) is the probability of a Type II error (failing to reject a false null hypothesis). We typically aim for power of at least 80%, meaning we accept a 20% chance of missing a true effect.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Statistical Power</span>"
    ]
  },
  {
    "objectID": "chapters/18-statistical-power.html#why-power-matters",
    "href": "chapters/18-statistical-power.html#why-power-matters",
    "title": "18  Statistical Power",
    "section": "18.2 Why Power Matters",
    "text": "18.2 Why Power Matters\nA study with low power has poor chances of detecting true effects. Even if an effect exists, the study may fail to find statistical significance. Worse, significant results from underpowered studies tend to overestimate effect sizes—a phenomenon called the “winner’s curse.”\nUnderstanding power helps us interpret results appropriately. If we fail to reject the null hypothesis, was it because no effect exists, or because our study lacked the power to detect it?",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Statistical Power</span>"
    ]
  },
  {
    "objectID": "chapters/18-statistical-power.html#determinants-of-power",
    "href": "chapters/18-statistical-power.html#determinants-of-power",
    "title": "18  Statistical Power",
    "section": "18.3 Determinants of Power",
    "text": "18.3 Determinants of Power\nPower depends on four factors that are mathematically related:\n\\[\\text{Power} \\propto \\frac{(\\text{Effect Size}) \\times (\\alpha) \\times (\\sqrt{n})}{\\sigma}\\]\nEffect Size: Larger effects are easier to detect. Effect size can be measured in original units or standardized (like Cohen’s d).\nSample Size (n): Larger samples provide more information and higher power.\nSignificance Level (\\(\\alpha\\)): Using a more lenient alpha (e.g., 0.10 instead of 0.05) increases power but also increases Type I error risk.\nVariability (\\(\\sigma\\)): Less variable data makes effects easier to detect.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Statistical Power</span>"
    ]
  },
  {
    "objectID": "chapters/18-statistical-power.html#cohens-d-standardized-effect-size",
    "href": "chapters/18-statistical-power.html#cohens-d-standardized-effect-size",
    "title": "18  Statistical Power",
    "section": "18.4 Cohen’s d: Standardized Effect Size",
    "text": "18.4 Cohen’s d: Standardized Effect Size\nCohen’s d expresses the difference between means in standard deviation units:\n\\[d = \\frac{\\mu_1 - \\mu_2}{s_{pooled}}\\]\nConventional benchmarks (Cohen, 1988): - d = 0.2: small effect - d = 0.5: medium effect - d = 0.8: large effect\nThese benchmarks are only guidelines—what counts as “small” depends on the research context.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Statistical Power</span>"
    ]
  },
  {
    "objectID": "chapters/18-statistical-power.html#a-priori-power-analysis",
    "href": "chapters/18-statistical-power.html#a-priori-power-analysis",
    "title": "18  Statistical Power",
    "section": "18.5 A Priori Power Analysis",
    "text": "18.5 A Priori Power Analysis\nBefore collecting data, power analysis helps determine the sample size needed to detect effects of interest. This requires specifying:\n\nThe expected effect size\nThe desired power (typically 0.80)\nThe significance level (typically 0.05)\nThe statistical test to be used\n\n\n\nCode\n# How many subjects needed to detect d = 0.5 with 80% power?\npwr.t.test(d = 0.5, sig.level = 0.05, power = 0.80, type = \"two.sample\")\n\n\n\n     Two-sample t test power calculation \n\n              n = 63.76561\n              d = 0.5\n      sig.level = 0.05\n          power = 0.8\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n\n\nAbout 64 subjects per group are needed to detect a medium effect with 80% power using a two-sample t-test.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Statistical Power</span>"
    ]
  },
  {
    "objectID": "chapters/18-statistical-power.html#power-curves",
    "href": "chapters/18-statistical-power.html#power-curves",
    "title": "18  Statistical Power",
    "section": "18.6 Power Curves",
    "text": "18.6 Power Curves\nPower curves show how power changes with sample size or effect size:\n\n\nCode\n# Power curve for different effect sizes\nsample_sizes &lt;- seq(10, 200, by = 5)\neffect_sizes &lt;- c(0.2, 0.5, 0.8)\n\npower_data &lt;- expand.grid(n = sample_sizes, d = effect_sizes)\npower_data$power &lt;- mapply(function(n, d) {\n  pwr.t.test(n = n, d = d, sig.level = 0.05, type = \"two.sample\")$power\n}, power_data$n, power_data$d)\n\nggplot(power_data, aes(x = n, y = power, color = factor(d))) +\n  geom_line(size = 1.2) +\n  geom_hline(yintercept = 0.8, linetype = \"dashed\") +\n  labs(title = \"Power Curves for Two-Sample t-Test\",\n       x = \"Sample Size per Group\",\n       y = \"Power\",\n       color = \"Effect Size (d)\") +\n  theme_minimal()",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Statistical Power</span>"
    ]
  },
  {
    "objectID": "chapters/18-statistical-power.html#power-for-anova",
    "href": "chapters/18-statistical-power.html#power-for-anova",
    "title": "18  Statistical Power",
    "section": "18.7 Power for ANOVA",
    "text": "18.7 Power for ANOVA\nFor ANOVA, effect size is measured by Cohen’s f:\n\\[f = \\frac{\\sigma_{between}}{\\sigma_{within}}\\]\nBenchmarks: f = 0.10 (small), f = 0.25 (medium), f = 0.40 (large).\n\n\nCode\n# Sample size for ANOVA with 3 groups\npwr.anova.test(k = 3, f = 0.25, sig.level = 0.05, power = 0.80)\n\n\n\n     Balanced one-way analysis of variance power calculation \n\n              k = 3\n              n = 52.3966\n              f = 0.25\n      sig.level = 0.05\n          power = 0.8\n\nNOTE: n is number in each group",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Statistical Power</span>"
    ]
  },
  {
    "objectID": "chapters/18-statistical-power.html#simulation-based-power-analysis",
    "href": "chapters/18-statistical-power.html#simulation-based-power-analysis",
    "title": "18  Statistical Power",
    "section": "18.8 Simulation-Based Power Analysis",
    "text": "18.8 Simulation-Based Power Analysis\nFor complex designs, simulation provides a flexible approach:\n\n\nCode\n# Simulation-based power for comparing two Poisson distributions\nset.seed(42)\n\npower_sim &lt;- function(n, lambda1, lambda2, n_sims = 1000) {\n  significant &lt;- replicate(n_sims, {\n    x1 &lt;- rpois(n, lambda1)\n    x2 &lt;- rpois(n, lambda2)\n    t.test(x1, x2)$p.value &lt; 0.05\n  })\n  mean(significant)\n}\n\n# Power for different sample sizes\nsample_sizes &lt;- seq(10, 100, by = 10)\npowers &lt;- sapply(sample_sizes, power_sim, lambda1 = 10, lambda2 = 12)\n\nplot(sample_sizes, powers, type = \"b\", pch = 19,\n     xlab = \"Sample Size per Group\", ylab = \"Power\",\n     main = \"Simulated Power (λ1=10 vs λ2=12)\")\nabline(h = 0.8, lty = 2, col = \"red\")",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Statistical Power</span>"
    ]
  },
  {
    "objectID": "chapters/18-statistical-power.html#post-hoc-power-analysis",
    "href": "chapters/18-statistical-power.html#post-hoc-power-analysis",
    "title": "18  Statistical Power",
    "section": "18.9 Post Hoc Power Analysis",
    "text": "18.9 Post Hoc Power Analysis\nCalculating power after a study is completed is controversial. Post hoc power calculated from observed effect sizes is mathematically determined by the p-value and adds no new information. It cannot tell you whether a non-significant result reflects a true null or insufficient power.\nIf you want to understand what your study could detect, specify effect sizes based on scientific considerations, not observed results.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Statistical Power</span>"
    ]
  },
  {
    "objectID": "chapters/18-statistical-power.html#practical-recommendations",
    "href": "chapters/18-statistical-power.html#practical-recommendations",
    "title": "18  Statistical Power",
    "section": "18.10 Practical Recommendations",
    "text": "18.10 Practical Recommendations\nAlways conduct power analysis before data collection. Use realistic effect size estimates based on pilot data or previous literature. Consider what effect size would be practically meaningful, not just what you think exists.\nBe conservative—effects are often smaller than expected. Plan for some attrition or missing data. When in doubt, collect more data if feasible.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Statistical Power</span>"
    ]
  },
  {
    "objectID": "chapters/19-anova.html",
    "href": "chapters/19-anova.html",
    "title": "19  Analysis of Variance",
    "section": "",
    "text": "19.1 Beyond Two Groups\nThe t-test compares two groups, but many experiments involve more than two. We might compare three drug treatments, five temperature conditions, or four genetic strains. Running multiple t-tests creates problems: with many comparisons, false positives become likely even when no true differences exist.\nAnalysis of Variance (ANOVA) provides a solution. It tests whether any of the group means differ from the others in a single test, controlling the overall Type I error rate.",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Analysis of Variance</span>"
    ]
  },
  {
    "objectID": "chapters/19-anova.html#the-anova-framework",
    "href": "chapters/19-anova.html#the-anova-framework",
    "title": "19  Analysis of Variance",
    "section": "19.2 The ANOVA Framework",
    "text": "19.2 The ANOVA Framework\nANOVA partitions the total variation in the data into components: variation between groups (due to treatment effects) and variation within groups (due to random error).\n\n\n\n\n\nThe key insight is that if groups have equal means, the between-group variation should be similar to the within-group variation. If the between-group variation is much larger, the group means probably differ.",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Analysis of Variance</span>"
    ]
  },
  {
    "objectID": "chapters/19-anova.html#the-f-test",
    "href": "chapters/19-anova.html#the-f-test",
    "title": "19  Analysis of Variance",
    "section": "19.3 The F-Test",
    "text": "19.3 The F-Test\nANOVA uses the F-statistic:\n\\[F = \\frac{MS_{between}}{MS_{within}} = \\frac{\\text{Variance between groups}}{\\text{Variance within groups}}\\]\nUnder the null hypothesis (all group means equal), F follows an F-distribution. Large F values indicate that group means differ more than expected by chance.",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Analysis of Variance</span>"
    ]
  },
  {
    "objectID": "chapters/19-anova.html#one-way-anova-in-r",
    "href": "chapters/19-anova.html#one-way-anova-in-r",
    "title": "19  Analysis of Variance",
    "section": "19.4 One-Way ANOVA in R",
    "text": "19.4 One-Way ANOVA in R\n\n\nCode\n# Example using iris data\niris_aov &lt;- aov(Sepal.Length ~ Species, data = iris)\nsummary(iris_aov)\n\n\n             Df Sum Sq Mean Sq F value Pr(&gt;F)    \nSpecies       2  63.21  31.606   119.3 &lt;2e-16 ***\nResiduals   147  38.96   0.265                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe significant p-value tells us that sepal length differs among species, but not which species differ from which.",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Analysis of Variance</span>"
    ]
  },
  {
    "objectID": "chapters/19-anova.html#anova-assumptions",
    "href": "chapters/19-anova.html#anova-assumptions",
    "title": "19  Analysis of Variance",
    "section": "19.5 ANOVA Assumptions",
    "text": "19.5 ANOVA Assumptions\nLike the t-test, ANOVA assumes:\n\nNormality: Observations within each group are normally distributed\nHomogeneity of variance: Groups have equal variances\nIndependence: Observations are independent\n\nANOVA is robust to mild violations of normality, especially with balanced designs and large samples. Serious violations of homogeneity of variance are more problematic but can be addressed with Welch’s ANOVA or transformations.",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Analysis of Variance</span>"
    ]
  },
  {
    "objectID": "chapters/19-anova.html#post-hoc-comparisons",
    "href": "chapters/19-anova.html#post-hoc-comparisons",
    "title": "19  Analysis of Variance",
    "section": "19.6 Post-Hoc Comparisons",
    "text": "19.6 Post-Hoc Comparisons\nA significant ANOVA tells us groups differ but not how. Post-hoc tests compare specific pairs of groups while controlling for multiple comparisons.\nTukey’s HSD (Honestly Significant Difference) compares all pairs:\n\n\nCode\nTukeyHSD(iris_aov)\n\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = Sepal.Length ~ Species, data = iris)\n\n$Species\n                      diff       lwr       upr p adj\nversicolor-setosa    0.930 0.6862273 1.1737727     0\nvirginica-setosa     1.582 1.3382273 1.8257727     0\nvirginica-versicolor 0.652 0.4082273 0.8957727     0\n\n\nEach pairwise comparison includes the difference in means, confidence interval, and adjusted p-value.",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Analysis of Variance</span>"
    ]
  },
  {
    "objectID": "chapters/19-anova.html#planned-contrasts",
    "href": "chapters/19-anova.html#planned-contrasts",
    "title": "19  Analysis of Variance",
    "section": "19.7 Planned Contrasts",
    "text": "19.7 Planned Contrasts\nIf you have specific hypotheses about which groups should differ (decided before seeing the data), planned contrasts are more powerful than post-hoc tests. They focus statistical power on the comparisons you care about.\n\n\nCode\n# Example: Compare setosa to the average of the other two species\ncontrasts(iris$Species) &lt;- cbind(\n  setosa_vs_others = c(2, -1, -1)\n)\nsummary.lm(aov(Sepal.Length ~ Species, data = iris))\n\n\n\nCall:\naov(formula = Sepal.Length ~ Species, data = iris)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.6880 -0.3285 -0.0060  0.3120  1.3120 \n\nCoefficients:\n                        Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)              5.84333    0.04203 139.020  &lt; 2e-16 ***\nSpeciessetosa_vs_others -0.41867    0.02972 -14.086  &lt; 2e-16 ***\nSpecies                  0.46103    0.07280   6.333 2.77e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5148 on 147 degrees of freedom\nMultiple R-squared:  0.6187,    Adjusted R-squared:  0.6135 \nF-statistic: 119.3 on 2 and 147 DF,  p-value: &lt; 2.2e-16",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Analysis of Variance</span>"
    ]
  },
  {
    "objectID": "chapters/19-anova.html#fixed-vs.-random-effects",
    "href": "chapters/19-anova.html#fixed-vs.-random-effects",
    "title": "19  Analysis of Variance",
    "section": "19.8 Fixed vs. Random Effects",
    "text": "19.8 Fixed vs. Random Effects\nFixed effects are specific treatments of interest that would be the same if the study were replicated—drug A, drug B, drug C. Conclusions apply only to these specific treatments.\nRandom effects are levels sampled from a larger population—particular subjects, batches, or locations. The goal is to generalize to the population of possible levels, not just those observed.\nThe distinction matters because it affects how F-ratios are calculated and what conclusions can be drawn.",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Analysis of Variance</span>"
    ]
  },
  {
    "objectID": "chapters/19-anova.html#two-way-anova",
    "href": "chapters/19-anova.html#two-way-anova",
    "title": "19  Analysis of Variance",
    "section": "19.9 Two-Way ANOVA",
    "text": "19.9 Two-Way ANOVA\nWhen experiments have two factors, two-way ANOVA examines main effects of each factor and their interaction.\n\n\nCode\n# Simulated factorial design\nset.seed(42)\nn &lt;- 20\ndata_factorial &lt;- data.frame(\n  response = c(rnorm(n, 10, 2), rnorm(n, 12, 2), \n               rnorm(n, 11, 2), rnorm(n, 18, 2)),\n  factor_A = rep(c(\"A1\", \"A1\", \"A2\", \"A2\"), each = n),\n  factor_B = rep(c(\"B1\", \"B2\", \"B1\", \"B2\"), each = n)\n)\n\ntwo_way &lt;- aov(response ~ factor_A * factor_B, data = data_factorial)\nsummary(two_way)\n\n\n                  Df Sum Sq Mean Sq F value   Pr(&gt;F)    \nfactor_A           1  279.6   279.6   59.95 3.41e-11 ***\nfactor_B           1  352.6   352.6   75.61 5.12e-13 ***\nfactor_A:factor_B  1  195.2   195.2   41.87 8.57e-09 ***\nResiduals         76  354.4     4.7                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nAn interaction means the effect of one factor depends on the level of the other. Significant interactions often require examining simple effects rather than main effects.",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Analysis of Variance</span>"
    ]
  },
  {
    "objectID": "chapters/19-anova.html#interaction-plots",
    "href": "chapters/19-anova.html#interaction-plots",
    "title": "19  Analysis of Variance",
    "section": "19.10 Interaction Plots",
    "text": "19.10 Interaction Plots\n\n\nCode\n# Visualize interaction\ninteraction.plot(data_factorial$factor_A, data_factorial$factor_B, \n                 data_factorial$response,\n                 col = c(\"blue\", \"red\"), lwd = 2,\n                 xlab = \"Factor A\", ylab = \"Response\",\n                 trace.label = \"Factor B\")\n\n\n\n\n\n\n\n\n\nNon-parallel lines suggest an interaction. Parallel lines suggest additive (non-interacting) effects.",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Analysis of Variance</span>"
    ]
  },
  {
    "objectID": "chapters/19-anova.html#nested-designs",
    "href": "chapters/19-anova.html#nested-designs",
    "title": "19  Analysis of Variance",
    "section": "19.11 Nested Designs",
    "text": "19.11 Nested Designs\nIn nested designs, levels of one factor exist only within levels of another. For example, technicians might be nested within labs—each technician works in only one lab.\nNested designs have no interaction term because not all combinations of factor levels exist. They are common when sampling is hierarchical.",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Analysis of Variance</span>"
    ]
  },
  {
    "objectID": "chapters/19-anova.html#practical-considerations",
    "href": "chapters/19-anova.html#practical-considerations",
    "title": "19  Analysis of Variance",
    "section": "19.12 Practical Considerations",
    "text": "19.12 Practical Considerations\nReport effect sizes (like \\(\\eta^2\\)) alongside p-values. A significant ANOVA with tiny effect size may not be practically meaningful.\nCheck assumptions with residual plots. Consider transformations or nonparametric alternatives (Kruskal-Wallis) when assumptions are violated.\nPlan your sample size using power analysis before collecting data, specifying the minimum effect size you want to detect.",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Analysis of Variance</span>"
    ]
  },
  {
    "objectID": "chapters/20-multiple-regression.html",
    "href": "chapters/20-multiple-regression.html",
    "title": "20  Multiple Regression",
    "section": "",
    "text": "20.1 Beyond One Predictor\nSimple linear regression uses a single predictor. But the response variable often depends on multiple factors. A patient’s blood pressure might depend on age, weight, sodium intake, and medication. Gene expression might depend on temperature, time, and treatment condition.\nMultiple regression extends linear regression to multiple predictors:\n\\[y_i = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\ldots + \\beta_p x_p + \\epsilon_i\\]",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Multiple Regression</span>"
    ]
  },
  {
    "objectID": "chapters/20-multiple-regression.html#goals-of-multiple-regression",
    "href": "chapters/20-multiple-regression.html#goals-of-multiple-regression",
    "title": "20  Multiple Regression",
    "section": "20.2 Goals of Multiple Regression",
    "text": "20.2 Goals of Multiple Regression\nMultiple regression serves two main purposes. First, it often improves prediction by incorporating multiple sources of information. Second, it allows us to investigate the effect of each predictor while controlling for the others—the effect of X1 “holding X2 constant.”\nThis second goal is powerful but requires caution. In observational data, controlling for variables statistically is not the same as controlling them experimentally. Confounding variables you do not measure cannot be controlled.",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Multiple Regression</span>"
    ]
  },
  {
    "objectID": "chapters/20-multiple-regression.html#additive-vs.-multiplicative-models",
    "href": "chapters/20-multiple-regression.html#additive-vs.-multiplicative-models",
    "title": "20  Multiple Regression",
    "section": "20.3 Additive vs. Multiplicative Models",
    "text": "20.3 Additive vs. Multiplicative Models\nAn additive model assumes predictors contribute independently:\n\\[y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\epsilon\\]\nA multiplicative model includes interactions—the effect of one predictor depends on the value of another:\n\\[y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_1 x_2 + \\epsilon\\]\n\n\nCode\n# Example with two predictors\nset.seed(42)\nn &lt;- 100\nx1 &lt;- rnorm(n)\nx2 &lt;- rnorm(n)\ny &lt;- 2 + 3*x1 + 2*x2 + 1.5*x1*x2 + rnorm(n)\n\n# Additive model\nadd_model &lt;- lm(y ~ x1 + x2)\n\n# Model with interaction\nint_model &lt;- lm(y ~ x1 * x2)\n\nsummary(int_model)\n\n\n\nCall:\nlm(formula = y ~ x1 * x2)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.55125 -0.69885 -0.03771  0.56441  2.42157 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  2.00219    0.10108   19.81   &lt;2e-16 ***\nx1           2.84494    0.09734   29.23   &lt;2e-16 ***\nx2           2.04126    0.11512   17.73   &lt;2e-16 ***\nx1:x2        1.35163    0.09228   14.65   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.005 on 96 degrees of freedom\nMultiple R-squared:  0.9289,    Adjusted R-squared:  0.9267 \nF-statistic:   418 on 3 and 96 DF,  p-value: &lt; 2.2e-16",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Multiple Regression</span>"
    ]
  },
  {
    "objectID": "chapters/20-multiple-regression.html#interpretation-of-coefficients",
    "href": "chapters/20-multiple-regression.html#interpretation-of-coefficients",
    "title": "20  Multiple Regression",
    "section": "20.4 Interpretation of Coefficients",
    "text": "20.4 Interpretation of Coefficients\nIn multiple regression, each coefficient represents the expected change in Y for a one-unit change in that predictor, holding all other predictors constant.\nThis “holding constant” interpretation makes the coefficients different from what you would get from separate simple regressions. The coefficient for X1 in multiple regression represents the unique contribution of X1 after accounting for X2.",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Multiple Regression</span>"
    ]
  },
  {
    "objectID": "chapters/20-multiple-regression.html#multicollinearity",
    "href": "chapters/20-multiple-regression.html#multicollinearity",
    "title": "20  Multiple Regression",
    "section": "20.5 Multicollinearity",
    "text": "20.5 Multicollinearity\nWhen predictors are correlated with each other, interpreting individual coefficients becomes problematic. This multicollinearity inflates standard errors and can make coefficients unstable.\n\n\nCode\n# Check for multicollinearity visually\nlibrary(car)\npairs(~ x1 + x2, main = \"Scatterplot Matrix\")\n\n\n\n\n\n\n\n\n\nThe Variance Inflation Factor (VIF) quantifies multicollinearity. VIF &gt; 10 suggests serious problems; VIF &gt; 5 warrants attention.\n\n\nCode\nvif(int_model)\n\n\n      x1       x2    x1:x2 \n1.006276 1.061022 1.066455",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Multiple Regression</span>"
    ]
  },
  {
    "objectID": "chapters/20-multiple-regression.html#model-selection",
    "href": "chapters/20-multiple-regression.html#model-selection",
    "title": "20  Multiple Regression",
    "section": "20.6 Model Selection",
    "text": "20.6 Model Selection\nWith many potential predictors, how do we choose which to include? Adding variables always improves fit to the training data but may hurt prediction on new data through overfitting.\nSeveral criteria balance fit and complexity:\nAdjusted R² penalizes for the number of predictors.\nAIC (Akaike Information Criterion) estimates prediction error, penalizing complexity. Lower is better.\nBIC (Bayesian Information Criterion) similar to AIC but penalizes complexity more heavily.\n\n\nCode\n# Compare models\nAIC(add_model, int_model)\n\n\n          df      AIC\nadd_model  4 406.1808\nint_model  5 290.7926\n\n\nCode\nBIC(add_model, int_model)\n\n\n          df      BIC\nadd_model  4 416.6015\nint_model  5 303.8185",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Multiple Regression</span>"
    ]
  },
  {
    "objectID": "chapters/20-multiple-regression.html#model-selection-strategies",
    "href": "chapters/20-multiple-regression.html#model-selection-strategies",
    "title": "20  Multiple Regression",
    "section": "20.7 Model Selection Strategies",
    "text": "20.7 Model Selection Strategies\nForward selection starts with no predictors and adds them one at a time based on statistical criteria.\nBackward elimination starts with all predictors and removes them one at a time.\nAll subsets examines all possible combinations and selects the best.\nNo strategy is universally best. Automated selection can lead to overfitting and unstable models. Theory-driven model building—starting with predictors you have scientific reasons to include—is often preferable.",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Multiple Regression</span>"
    ]
  },
  {
    "objectID": "chapters/20-multiple-regression.html#polynomial-regression",
    "href": "chapters/20-multiple-regression.html#polynomial-regression",
    "title": "20  Multiple Regression",
    "section": "20.8 Polynomial Regression",
    "text": "20.8 Polynomial Regression\nPolynomial terms can capture non-linear relationships while still using the linear regression framework:\n\n\nCode\n# Non-linear relationship\nx &lt;- seq(0, 10, length.out = 100)\ny &lt;- 2 + 0.5*x - 0.1*x^2 + rnorm(100, sd = 0.5)\n\n# Fit polynomial models\nmodel1 &lt;- lm(y ~ poly(x, 1))  # Linear\nmodel2 &lt;- lm(y ~ poly(x, 2))  # Quadratic\nmodel3 &lt;- lm(y ~ poly(x, 5))  # Degree 5\n\n# Compare\nAIC(model1, model2, model3)\n\n\n       df      AIC\nmodel1  3 250.3022\nmodel2  4 121.8041\nmodel3  7 126.6399\n\n\nHigher-degree polynomials fit better but risk overfitting. The principle of parsimony suggests using the simplest adequate model.",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Multiple Regression</span>"
    ]
  },
  {
    "objectID": "chapters/20-multiple-regression.html#assumptions",
    "href": "chapters/20-multiple-regression.html#assumptions",
    "title": "20  Multiple Regression",
    "section": "20.9 Assumptions",
    "text": "20.9 Assumptions\nMultiple regression shares assumptions with simple regression: linearity (in each predictor), independence, normality of residuals, and constant variance. Additionally, predictors should not be perfectly correlated (no perfect multicollinearity).\nCheck assumptions with residual plots. Partial regression plots can help diagnose problems with individual predictors.",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Multiple Regression</span>"
    ]
  },
  {
    "objectID": "chapters/20-multiple-regression.html#practical-guidelines",
    "href": "chapters/20-multiple-regression.html#practical-guidelines",
    "title": "20  Multiple Regression",
    "section": "20.10 Practical Guidelines",
    "text": "20.10 Practical Guidelines\nStart with a theoretically motivated model rather than throwing in all available predictors. Check for multicollinearity before interpreting coefficients. Use cross-validation to assess prediction performance. Report standardized coefficients when comparing the relative importance of predictors on different scales.\nBe humble about causation. Multiple regression describes associations; experimental manipulation is needed to establish causation.",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Multiple Regression</span>"
    ]
  },
  {
    "objectID": "chapters/21-glm.html",
    "href": "chapters/21-glm.html",
    "title": "21  Generalized Linear Models",
    "section": "",
    "text": "21.1 Beyond Normal Distributions\nStandard linear regression assumes that the response variable is continuous and normally distributed. But many important response variables violate these assumptions. Binary outcomes (success/failure, alive/dead) follow binomial distributions. Count data (number of events, cells, species) often follow Poisson distributions.\nGeneralized Linear Models (GLMs) extend linear regression to handle these situations. They provide a unified framework for modeling responses that follow different distributions from the exponential family.",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Generalized Linear Models</span>"
    ]
  },
  {
    "objectID": "chapters/21-glm.html#components-of-a-glm",
    "href": "chapters/21-glm.html#components-of-a-glm",
    "title": "21  Generalized Linear Models",
    "section": "21.2 Components of a GLM",
    "text": "21.2 Components of a GLM\nGLMs have three components:\nRandom component: Specifies the probability distribution of the response variable (e.g., binomial, Poisson, normal).\nSystematic component: The linear predictor, a linear combination of explanatory variables: \\[\\eta = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\ldots\\]\nLink function: Connects the random and systematic components, transforming the expected value of the response to the scale of the linear predictor.",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Generalized Linear Models</span>"
    ]
  },
  {
    "objectID": "chapters/21-glm.html#the-link-function",
    "href": "chapters/21-glm.html#the-link-function",
    "title": "21  Generalized Linear Models",
    "section": "21.3 The Link Function",
    "text": "21.3 The Link Function\nDifferent distributions use different link functions:\n\n\n\nDistribution\nTypical Link\nLink Function\n\n\n\n\nNormal\nIdentity\n\\(\\eta = \\mu\\)\n\n\nBinomial\nLogit\n\\(\\eta = \\log(\\frac{\\mu}{1-\\mu})\\)\n\n\nPoisson\nLog\n\\(\\eta = \\log(\\mu)\\)",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Generalized Linear Models</span>"
    ]
  },
  {
    "objectID": "chapters/21-glm.html#logistic-regression",
    "href": "chapters/21-glm.html#logistic-regression",
    "title": "21  Generalized Linear Models",
    "section": "21.4 Logistic Regression",
    "text": "21.4 Logistic Regression\nLogistic regression models binary outcomes. The response is 0 or 1 (failure or success), and we model the probability of success as a function of predictors.\nThe logistic function maps the linear predictor to probabilities:\n\\[P(Y = 1 | X) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 X)}}\\]\nEquivalently, we model the log-odds:\n\\[\\log\\left(\\frac{P}{1-P}\\right) = \\beta_0 + \\beta_1 X\\]\n\n\nCode\n# The logistic function\ncurve(1 / (1 + exp(-x)), from = -6, to = 6, \n      xlab = \"Linear Predictor\", ylab = \"Probability\",\n      main = \"The Logistic Function\", lwd = 2, col = \"blue\")",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Generalized Linear Models</span>"
    ]
  },
  {
    "objectID": "chapters/21-glm.html#fitting-logistic-regression",
    "href": "chapters/21-glm.html#fitting-logistic-regression",
    "title": "21  Generalized Linear Models",
    "section": "21.5 Fitting Logistic Regression",
    "text": "21.5 Fitting Logistic Regression\n\n\nCode\n# Example: predicting transmission type from mpg\ndata(mtcars)\nmtcars$am &lt;- factor(mtcars$am, labels = c(\"automatic\", \"manual\"))\n\nlogit_model &lt;- glm(am ~ mpg, data = mtcars, family = binomial)\nsummary(logit_model)\n\n\n\nCall:\nglm(formula = am ~ mpg, family = binomial, data = mtcars)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)   \n(Intercept)  -6.6035     2.3514  -2.808  0.00498 **\nmpg           0.3070     0.1148   2.673  0.00751 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 43.230  on 31  degrees of freedom\nResidual deviance: 29.675  on 30  degrees of freedom\nAIC: 33.675\n\nNumber of Fisher Scoring iterations: 5",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Generalized Linear Models</span>"
    ]
  },
  {
    "objectID": "chapters/21-glm.html#interpreting-logistic-coefficients",
    "href": "chapters/21-glm.html#interpreting-logistic-coefficients",
    "title": "21  Generalized Linear Models",
    "section": "21.6 Interpreting Logistic Coefficients",
    "text": "21.6 Interpreting Logistic Coefficients\nCoefficients are on the log-odds scale. To interpret them:\nExponentiate to get odds ratios:\n\n\nCode\nexp(coef(logit_model))\n\n\n(Intercept)         mpg \n0.001355579 1.359379288 \n\n\nThe odds ratio for mpg (1.36) means that each additional mpg is associated with 36% higher odds of having a manual transmission.",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Generalized Linear Models</span>"
    ]
  },
  {
    "objectID": "chapters/21-glm.html#making-predictions",
    "href": "chapters/21-glm.html#making-predictions",
    "title": "21  Generalized Linear Models",
    "section": "21.7 Making Predictions",
    "text": "21.7 Making Predictions\n\n\nCode\n# Predict probability for specific mpg values\nnew_data &lt;- data.frame(mpg = c(15, 20, 25, 30))\npredict(logit_model, newdata = new_data, type = \"response\")\n\n\n        1         2         3         4 \n0.1194021 0.3862832 0.7450109 0.9313311 \n\n\nThe type = \"response\" argument returns probabilities rather than log-odds.",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Generalized Linear Models</span>"
    ]
  },
  {
    "objectID": "chapters/21-glm.html#poisson-regression",
    "href": "chapters/21-glm.html#poisson-regression",
    "title": "21  Generalized Linear Models",
    "section": "21.8 Poisson Regression",
    "text": "21.8 Poisson Regression\nPoisson regression models count data—the number of events in a fixed period or area. The response must be non-negative integers, and we assume events occur independently at a constant rate.\n\\[\\log(\\mu) = \\beta_0 + \\beta_1 X\\]\n\n\nCode\n# Example: modeling count data\nset.seed(42)\nexposure &lt;- runif(100, 1, 10)\ncounts &lt;- rpois(100, lambda = exp(0.5 + 0.3 * exposure))\n\npois_model &lt;- glm(counts ~ exposure, family = poisson)\nsummary(pois_model)\n\n\n\nCall:\nglm(formula = counts ~ exposure, family = poisson)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  0.42499    0.10624    4.00 6.32e-05 ***\nexposure     0.30714    0.01345   22.84  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 744.110  on 99  degrees of freedom\nResidual deviance:  97.826  on 98  degrees of freedom\nAIC: 498.52\n\nNumber of Fisher Scoring iterations: 4",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Generalized Linear Models</span>"
    ]
  },
  {
    "objectID": "chapters/21-glm.html#overdispersion",
    "href": "chapters/21-glm.html#overdispersion",
    "title": "21  Generalized Linear Models",
    "section": "21.9 Overdispersion",
    "text": "21.9 Overdispersion\nA key assumption of Poisson regression is that the mean equals the variance. When variance exceeds the mean (overdispersion), standard errors are underestimated and p-values too small.\nSolutions include: - Quasi-Poisson (estimates dispersion from data) - Negative binomial regression - Zero-inflated models (for excess zeros)\n\n\nCode\n# Check for overdispersion\n# Ratio of residual deviance to df should be near 1\npois_model$deviance / pois_model$df.residual\n\n\n[1] 0.9982259",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Generalized Linear Models</span>"
    ]
  },
  {
    "objectID": "chapters/21-glm.html#model-assessment",
    "href": "chapters/21-glm.html#model-assessment",
    "title": "21  Generalized Linear Models",
    "section": "21.10 Model Assessment",
    "text": "21.10 Model Assessment\nGLMs use deviance rather than R² to assess fit. Deviance compares the fitted model to a saturated model (one parameter per observation).\nNull deviance: Deviance with only the intercept Residual deviance: Deviance of the fitted model\nA large drop from null to residual deviance indicates the predictors explain substantial variation.\n\n\nCode\n# Compare deviances\nwith(logit_model, null.deviance - deviance)\n\n\n[1] 13.55457\n\n\nCode\n# Chi-square test for improvement\nwith(logit_model, pchisq(null.deviance - deviance, \n                         df.null - df.residual, \n                         lower.tail = FALSE))\n\n\n[1] 0.0002317271",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Generalized Linear Models</span>"
    ]
  },
  {
    "objectID": "chapters/21-glm.html#model-comparison-with-aic",
    "href": "chapters/21-glm.html#model-comparison-with-aic",
    "title": "21  Generalized Linear Models",
    "section": "21.11 Model Comparison with AIC",
    "text": "21.11 Model Comparison with AIC\nAs with linear models, AIC helps compare GLMs:\n\n\nCode\n# Compare models\nmodel1 &lt;- glm(am ~ mpg, data = mtcars, family = binomial)\nmodel2 &lt;- glm(am ~ mpg + wt, data = mtcars, family = binomial)\nmodel3 &lt;- glm(am ~ mpg * wt, data = mtcars, family = binomial)\n\nAIC(model1, model2, model3)\n\n\n       df      AIC\nmodel1  2 33.67517\nmodel2  3 23.18426\nmodel3  4 24.49947",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Generalized Linear Models</span>"
    ]
  },
  {
    "objectID": "chapters/21-glm.html#assumptions-and-diagnostics",
    "href": "chapters/21-glm.html#assumptions-and-diagnostics",
    "title": "21  Generalized Linear Models",
    "section": "21.12 Assumptions and Diagnostics",
    "text": "21.12 Assumptions and Diagnostics\nGLM assumptions include: - Correct specification of the distribution - Correct link function - Independence of observations - No extreme multicollinearity\nDiagnostic tools include: - Residual plots (deviance or Pearson residuals) - Influence measures - Goodness-of-fit tests\n\n\nCode\npar(mfrow = c(1, 2))\nplot(logit_model, which = c(1, 2))",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Generalized Linear Models</span>"
    ]
  },
  {
    "objectID": "chapters/21-glm.html#summary",
    "href": "chapters/21-glm.html#summary",
    "title": "21  Generalized Linear Models",
    "section": "21.13 Summary",
    "text": "21.13 Summary\nGLMs provide a flexible framework for modeling non-normal response variables while maintaining the interpretability of linear models. Logistic regression for binary outcomes and Poisson regression for counts are the most common applications, but the framework extends to other distributions as needed.",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Generalized Linear Models</span>"
    ]
  },
  {
    "objectID": "chapters/22-statistical-learning.html",
    "href": "chapters/22-statistical-learning.html",
    "title": "22  Statistical Learning",
    "section": "",
    "text": "22.1 From Inference to Prediction\nTraditional statistics emphasizes inference—understanding relationships, testing hypotheses, and quantifying uncertainty. Statistical learning (or machine learning) shifts focus toward prediction—building models that accurately predict outcomes for new data.\nBoth approaches use similar mathematical tools, but the goals differ. In inference, we want to understand the true relationship between variables. In prediction, we want accurate predictions, even if the model does not perfectly capture the underlying mechanism.",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Statistical Learning</span>"
    ]
  },
  {
    "objectID": "chapters/22-statistical-learning.html#the-overfitting-problem",
    "href": "chapters/22-statistical-learning.html#the-overfitting-problem",
    "title": "22  Statistical Learning",
    "section": "22.2 The Overfitting Problem",
    "text": "22.2 The Overfitting Problem\nModels are built to fit training data as closely as possible. A linear regression minimizes squared errors; a logistic regression maximizes likelihood. But models that fit training data too well often predict poorly on new data.\nOverfitting occurs when a model captures noise specific to the training data rather than the true underlying pattern. Complex models with many parameters are especially susceptible.\nThe solution is to evaluate models on data they have not seen—held-out test data or through cross-validation.",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Statistical Learning</span>"
    ]
  },
  {
    "objectID": "chapters/22-statistical-learning.html#cross-validation",
    "href": "chapters/22-statistical-learning.html#cross-validation",
    "title": "22  Statistical Learning",
    "section": "22.3 Cross-Validation",
    "text": "22.3 Cross-Validation\nCross-validation estimates how well a model will generalize to new data.\nK-fold cross-validation: 1. Split data into k roughly equal parts (folds) 2. For each fold: train on k-1 folds, test on the held-out fold 3. Average performance across all folds\n\n\nCode\n# Simple CV example with linear regression\nlibrary(boot)\n\n# Generate data\nset.seed(42)\nx &lt;- rnorm(100)\ny &lt;- 2 + 3*x + rnorm(100)\ndata &lt;- data.frame(x, y)\n\n# Fit model and perform CV\nmodel &lt;- glm(y ~ x, data = data)\n\n# 10-fold cross-validation\ncv_result &lt;- cv.glm(data, model, K = 10)\ncat(\"CV estimate of prediction error:\", round(cv_result$delta[1], 3), \"\\n\")\n\n\nCV estimate of prediction error: 0.846 \n\n\nLeave-one-out cross-validation (LOOCV) is k-fold with k = n: each observation is held out once. More computationally expensive but lower variance.",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Statistical Learning</span>"
    ]
  },
  {
    "objectID": "chapters/22-statistical-learning.html#bias-variance-tradeoff",
    "href": "chapters/22-statistical-learning.html#bias-variance-tradeoff",
    "title": "22  Statistical Learning",
    "section": "22.4 Bias-Variance Tradeoff",
    "text": "22.4 Bias-Variance Tradeoff\nPrediction error has two components:\nBias: Error from approximating a complex reality with a simpler model. Simple models have high bias—they may miss important patterns.\nVariance: Error from sensitivity to training data. Complex models have high variance—they change substantially with different training samples.\nThe best predictions come from models that balance bias and variance. As model complexity increases, bias decreases but variance increases. The optimal complexity minimizes total prediction error.",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Statistical Learning</span>"
    ]
  },
  {
    "objectID": "chapters/22-statistical-learning.html#loess-flexible-non-parametric-smoothing",
    "href": "chapters/22-statistical-learning.html#loess-flexible-non-parametric-smoothing",
    "title": "22  Statistical Learning",
    "section": "22.5 LOESS: Flexible Non-Parametric Smoothing",
    "text": "22.5 LOESS: Flexible Non-Parametric Smoothing\nLOESS (Locally Estimated Scatterplot Smoothing) fits local regressions to subsets of data, weighted by distance from each point.\n\n\nCode\n# Compare linear regression and LOESS\nset.seed(123)\nx &lt;- seq(0, 4*pi, length.out = 100)\ny &lt;- sin(x) + rnorm(100, sd = 0.3)\n\nplot(x, y, pch = 16, col = \"gray60\", main = \"Linear vs LOESS\")\nabline(lm(y ~ x), col = \"red\", lwd = 2)\nlines(x, predict(loess(y ~ x, span = 0.3)), col = \"blue\", lwd = 2)\nlegend(\"topright\", c(\"Linear\", \"LOESS\"), col = c(\"red\", \"blue\"), lwd = 2)\n\n\n\n\n\n\n\n\n\nThe span parameter controls smoothness: smaller values fit more locally (more flexible), larger values average more broadly (smoother).",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Statistical Learning</span>"
    ]
  },
  {
    "objectID": "chapters/22-statistical-learning.html#classification",
    "href": "chapters/22-statistical-learning.html#classification",
    "title": "22  Statistical Learning",
    "section": "22.6 Classification",
    "text": "22.6 Classification\nWhen the response is categorical, we have a classification problem rather than regression. The goal is to predict which category an observation belongs to.\nLogistic regression produces probabilities that can be converted to class predictions.\nDecision trees recursively partition the feature space based on simple rules.\nRandom forests combine many decision trees for more robust predictions.",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Statistical Learning</span>"
    ]
  },
  {
    "objectID": "chapters/22-statistical-learning.html#confusion-matrices",
    "href": "chapters/22-statistical-learning.html#confusion-matrices",
    "title": "22  Statistical Learning",
    "section": "22.7 Confusion Matrices",
    "text": "22.7 Confusion Matrices\nClassification performance is evaluated with a confusion matrix:\n\n\n\n\nPredicted Positive\nPredicted Negative\n\n\n\n\nActual Positive\nTrue Positive (TP)\nFalse Negative (FN)\n\n\nActual Negative\nFalse Positive (FP)\nTrue Negative (TN)\n\n\n\nKey metrics: - Accuracy: (TP + TN) / Total - Sensitivity (Recall): TP / (TP + FN) — how many positives were caught - Specificity: TN / (TN + FP) — how many negatives were correctly identified - Precision: TP / (TP + FP) — among positive predictions, how many were correct",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Statistical Learning</span>"
    ]
  },
  {
    "objectID": "chapters/22-statistical-learning.html#decision-trees",
    "href": "chapters/22-statistical-learning.html#decision-trees",
    "title": "22  Statistical Learning",
    "section": "22.8 Decision Trees",
    "text": "22.8 Decision Trees\nDecision trees make predictions by asking a series of yes/no questions about the features:\n\n\nCode\nlibrary(rpart)\nlibrary(rpart.plot)\n\n# Build a simple decision tree\ndata(iris)\ntree_model &lt;- rpart(Species ~ Sepal.Length + Sepal.Width, data = iris)\nrpart.plot(tree_model)\n\n\n\n\n\n\n\n\n\nTrees are interpretable but prone to overfitting. Pruning (removing branches) or using ensembles helps.",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Statistical Learning</span>"
    ]
  },
  {
    "objectID": "chapters/22-statistical-learning.html#random-forests",
    "href": "chapters/22-statistical-learning.html#random-forests",
    "title": "22  Statistical Learning",
    "section": "22.9 Random Forests",
    "text": "22.9 Random Forests\nRandom forests improve on single trees by: 1. Building many trees on bootstrap samples (bagging) 2. Using a random subset of features at each split 3. Averaging predictions across all trees\n\n\nCode\nlibrary(randomForest)\n\nrf_model &lt;- randomForest(Species ~ ., data = iris, ntree = 100)\nrf_model\n\n\n\nCall:\n randomForest(formula = Species ~ ., data = iris, ntree = 100) \n               Type of random forest: classification\n                     Number of trees: 100\nNo. of variables tried at each split: 2\n\n        OOB estimate of  error rate: 6%\nConfusion matrix:\n           setosa versicolor virginica class.error\nsetosa         50          0         0        0.00\nversicolor      0         47         3        0.06\nvirginica       0          6        44        0.12\n\n\nCode\n# Variable importance\nvarImpPlot(rf_model)",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Statistical Learning</span>"
    ]
  },
  {
    "objectID": "chapters/22-statistical-learning.html#practical-workflow",
    "href": "chapters/22-statistical-learning.html#practical-workflow",
    "title": "22  Statistical Learning",
    "section": "22.10 Practical Workflow",
    "text": "22.10 Practical Workflow\nA typical statistical learning workflow:\n\nSplit data into training and test sets\nExplore the training data\nBuild candidate models with different algorithms or parameters\nEvaluate using cross-validation on training data\nSelect the best model\nFinal evaluation on held-out test data\nReport honest estimates of performance\n\nNever use test data for model building or selection—that defeats the purpose of holding it out.",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Statistical Learning</span>"
    ]
  },
  {
    "objectID": "chapters/22-statistical-learning.html#when-to-use-statistical-learning",
    "href": "chapters/22-statistical-learning.html#when-to-use-statistical-learning",
    "title": "22  Statistical Learning",
    "section": "22.11 When to Use Statistical Learning",
    "text": "22.11 When to Use Statistical Learning\nStatistical learning excels when: - Prediction is the primary goal - Relationships are complex or non-linear - You have substantial data - Interpretability is less critical\nTraditional statistical methods may be preferable when: - Understanding relationships matters more than prediction - Sample sizes are small - You need confidence intervals and hypothesis tests - Interpretability is essential",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Statistical Learning</span>"
    ]
  },
  {
    "objectID": "chapters/23-high-performance-computing.html",
    "href": "chapters/23-high-performance-computing.html",
    "title": "23  High Performance Computing",
    "section": "",
    "text": "23.1 Why High Performance Computing?\nAs datasets grow and analyses become more complex, your laptop may not be enough. Genomic datasets can be terabytes in size. Simulations might require millions of iterations. Machine learning models may need to be trained on billions of data points. High Performance Computing (HPC) provides the resources to tackle problems that exceed what personal computers can handle.\nHPC systems come in different forms. Computing clusters—collections of interconnected computers working together—are common at universities and research institutions. Cloud computing services from Amazon (AWS), Google, and Microsoft (Azure) provide on-demand access to computing resources. GPUs (Graphics Processing Units) accelerate certain types of parallel computations.",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>High Performance Computing</span>"
    ]
  },
  {
    "objectID": "chapters/23-high-performance-computing.html#computing-clusters",
    "href": "chapters/23-high-performance-computing.html#computing-clusters",
    "title": "23  High Performance Computing",
    "section": "23.2 Computing Clusters",
    "text": "23.2 Computing Clusters\nA typical university computing cluster consists of a head node (login node) where you submit jobs, and many compute nodes where jobs actually run. The head node manages the queue of waiting jobs and allocates resources.\nAt the University of Oregon, the Talapas cluster provides researchers with access to thousands of CPU cores and specialized hardware including GPUs. Access requires an account, which graduate students can request through their research groups.",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>High Performance Computing</span>"
    ]
  },
  {
    "objectID": "chapters/23-high-performance-computing.html#connecting-to-remote-systems",
    "href": "chapters/23-high-performance-computing.html#connecting-to-remote-systems",
    "title": "23  High Performance Computing",
    "section": "23.3 Connecting to Remote Systems",
    "text": "23.3 Connecting to Remote Systems\nYou access remote systems through SSH (Secure Shell):\nssh username@talapas-login.uoregon.edu\nAfter authenticating, you are in a terminal on the remote system, working in a Unix environment just as you would locally. File transfer between your computer and the cluster uses scp or rsync:\n# Copy file to cluster\nscp data.csv username@talapas-login.uoregon.edu:~/project/\n\n# Copy file from cluster\nscp username@talapas-login.uoregon.edu:~/project/results.csv ./",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>High Performance Computing</span>"
    ]
  },
  {
    "objectID": "chapters/23-high-performance-computing.html#job-schedulers",
    "href": "chapters/23-high-performance-computing.html#job-schedulers",
    "title": "23  High Performance Computing",
    "section": "23.4 Job Schedulers",
    "text": "23.4 Job Schedulers\nYou do not run computationally intensive jobs directly on the login node. Instead, you submit them to a job scheduler (like SLURM on Talapas) that queues jobs and runs them when resources become available.\nA basic SLURM submission script:\n#!/bin/bash\n#SBATCH --job-name=my_analysis\n#SBATCH --account=your_account\n#SBATCH --partition=short\n#SBATCH --time=2:00:00\n#SBATCH --nodes=1\n#SBATCH --ntasks=1\n#SBATCH --cpus-per-task=4\n#SBATCH --mem=16G\n\n# Load required software\nmodule load R/4.2.1\n\n# Run your script\nRscript my_analysis.R\nSubmit with sbatch script.sh. Check job status with squeue -u username. Cancel jobs with scancel job_id.",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>High Performance Computing</span>"
    ]
  },
  {
    "objectID": "chapters/23-high-performance-computing.html#resource-requests",
    "href": "chapters/23-high-performance-computing.html#resource-requests",
    "title": "23  High Performance Computing",
    "section": "23.5 Resource Requests",
    "text": "23.5 Resource Requests\nJobs must request resources: time, memory, and CPUs. Request enough to complete your job but not so much that it waits unnecessarily in the queue. Start with conservative estimates and adjust based on actual usage.\nCommon SLURM directives: - --time: Maximum runtime (job is killed if exceeded) - --mem: Memory per node - --cpus-per-task: Number of CPU cores - --array: For running many similar jobs",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>High Performance Computing</span>"
    ]
  },
  {
    "objectID": "chapters/23-high-performance-computing.html#environment-modules",
    "href": "chapters/23-high-performance-computing.html#environment-modules",
    "title": "23  High Performance Computing",
    "section": "23.6 Environment Modules",
    "text": "23.6 Environment Modules\nHPC systems use environment modules to manage software. Instead of installing software yourself, you load pre-installed modules:\nmodule avail              # List available software\nmodule load R/4.2.1       # Load R\nmodule load python/3.10   # Load Python\nmodule list               # Show loaded modules\nmodule purge              # Unload all modules",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>High Performance Computing</span>"
    ]
  },
  {
    "objectID": "chapters/23-high-performance-computing.html#running-r-on-a-cluster",
    "href": "chapters/23-high-performance-computing.html#running-r-on-a-cluster",
    "title": "23  High Performance Computing",
    "section": "23.7 Running R on a Cluster",
    "text": "23.7 Running R on a Cluster\nR scripts run non-interactively on clusters. Instead of using RStudio, you write your analysis as a script and run it with Rscript:\n# my_analysis.R\nlibrary(tidyverse)\n\n# Read data\ndata &lt;- read.csv(\"large_dataset.csv\")\n\n# Perform analysis\nresults &lt;- data |&gt;\n  group_by(category) |&gt;\n  summarize(mean_value = mean(value))\n\n# Save results\nwrite.csv(results, \"output.csv\")",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>High Performance Computing</span>"
    ]
  },
  {
    "objectID": "chapters/23-high-performance-computing.html#parallelization-in-r",
    "href": "chapters/23-high-performance-computing.html#parallelization-in-r",
    "title": "23  High Performance Computing",
    "section": "23.8 Parallelization in R",
    "text": "23.8 Parallelization in R\nR can use multiple CPU cores to speed up computations. The parallel package provides tools for parallel processing:\nlibrary(parallel)\n\n# Detect number of cores\nn_cores &lt;- detectCores()\n\n# Create a cluster\ncl &lt;- makeCluster(n_cores - 1)\n\n# Parallel apply\nresults &lt;- parLapply(cl, data_list, analysis_function)\n\n# Stop the cluster\nstopCluster(cl)\nThe future and furrr packages provide more user-friendly parallelization.",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>High Performance Computing</span>"
    ]
  },
  {
    "objectID": "chapters/23-high-performance-computing.html#cloud-computing",
    "href": "chapters/23-high-performance-computing.html#cloud-computing",
    "title": "23  High Performance Computing",
    "section": "23.9 Cloud Computing",
    "text": "23.9 Cloud Computing\nCloud platforms (AWS, Google Cloud, Azure) offer computing resources on demand. You pay for what you use rather than having fixed resources.\nAdvantages: - Scale up quickly when needed - No hardware maintenance - Access to specialized hardware (GPUs, large memory instances)\nDisadvantages: - Costs can accumulate quickly - Requires learning platform-specific tools - Data transfer can be slow and expensive",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>High Performance Computing</span>"
    ]
  },
  {
    "objectID": "chapters/23-high-performance-computing.html#best-practices",
    "href": "chapters/23-high-performance-computing.html#best-practices",
    "title": "23  High Performance Computing",
    "section": "23.10 Best Practices",
    "text": "23.10 Best Practices\nStart small: Test your code on a small subset before running on full data.\nUse version control: Keep your scripts in Git for reproducibility.\nDocument everything: Future you (and others) need to understand what you did.\nSave intermediate results: If a job fails, you do not want to start from scratch.\nMonitor resource usage: Check how much time and memory your jobs actually use.\nClean up: Delete unnecessary files; storage is shared.",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>High Performance Computing</span>"
    ]
  },
  {
    "objectID": "chapters/23-high-performance-computing.html#getting-help",
    "href": "chapters/23-high-performance-computing.html#getting-help",
    "title": "23  High Performance Computing",
    "section": "23.11 Getting Help",
    "text": "23.11 Getting Help\nMost HPC systems have documentation and support staff. At UO, Research Advanced Computing Services (RACS) provides Talapas documentation and consultations. Reading the documentation before asking questions will make your interactions more productive.\nLearning to use HPC effectively takes time, but the ability to run large-scale analyses is essential for modern bioengineering research.",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>High Performance Computing</span>"
    ]
  },
  {
    "objectID": "chapters/A1-eugenics-history.html",
    "href": "chapters/A1-eugenics-history.html",
    "title": "24  The Eugenics History of Statistics",
    "section": "",
    "text": "24.1 Why This History Matters\nMany of the statistical methods we use today—correlation, regression, ANOVA, and others—were developed by scientists whose primary motivation was eugenics. Understanding this history is important not to discredit the methods themselves, which remain mathematically valid and useful, but to understand the context in which science develops and to remain vigilant about how scientific tools can be misused.\nScience does not exist in a vacuum. The questions scientists ask, the data they collect, and the interpretations they favor are shaped by the social contexts in which they work. The history of statistics and eugenics offers a stark example of this principle.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>The Eugenics History of Statistics</span>"
    ]
  },
  {
    "objectID": "chapters/A1-eugenics-history.html#francis-galton-founder-of-eugenics",
    "href": "chapters/A1-eugenics-history.html#francis-galton-founder-of-eugenics",
    "title": "24  The Eugenics History of Statistics",
    "section": "24.2 Francis Galton: Founder of Eugenics",
    "text": "24.2 Francis Galton: Founder of Eugenics\n\n\n\n\n\nFrancis Galton (1822–1911) was Charles Darwin’s half-cousin and a polymath who made genuine contributions to statistics, meteorology, and other fields. He invented correlation, pioneered the use of questionnaires, and conducted early twin studies. He is also the person who coined the term “eugenics” and devoted much of his career to promoting it.\nGalton believed that human intelligence was hereditary and that the “improvement” of the human race could be achieved through selective breeding. He studied prominent academics and concluded that talent ran in families, attributing this entirely to heredity while ignoring the advantages of wealth, education, and social connections.\nHis statistical innovations—correlation, regression to the mean—were developed specifically to analyze human inheritance and support eugenic arguments. The concept of “regression to the mean” came from his observation that tall parents had children who were tall but not as extremely tall as the parents, which he interpreted through a lens of hereditary “quality.”",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>The Eugenics History of Statistics</span>"
    ]
  },
  {
    "objectID": "chapters/A1-eugenics-history.html#eugenics-in-america",
    "href": "chapters/A1-eugenics-history.html#eugenics-in-america",
    "title": "24  The Eugenics History of Statistics",
    "section": "24.3 Eugenics in America",
    "text": "24.3 Eugenics in America\nEugenic ideas found fertile ground in the United States. The Eugenics Record Office (ERO) was founded at Cold Spring Harbor in 1910, conducting research aimed at identifying “unfit” individuals who should be prevented from reproducing.\n\n\n\n\n\nState fairs featured “fitter family” contests. Educational materials promoted eugenic thinking. Thirty states passed laws allowing forced sterilization of people deemed “unfit”—a category that disproportionately targeted poor people, immigrants, people with disabilities, and people of color.\nBetween 1907 and 1963, over 64,000 people were forcibly sterilized in the United States under eugenic legislation. California led the nation in forced sterilizations.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>The Eugenics History of Statistics</span>"
    ]
  },
  {
    "objectID": "chapters/A1-eugenics-history.html#r.a.-fisher-and-eugenics",
    "href": "chapters/A1-eugenics-history.html#r.a.-fisher-and-eugenics",
    "title": "24  The Eugenics History of Statistics",
    "section": "24.4 R.A. Fisher and Eugenics",
    "text": "24.4 R.A. Fisher and Eugenics\n\n\n\n\n\nRonald A. Fisher (1890–1962) is one of the most influential statisticians in history. He developed analysis of variance (ANOVA), the concept of statistical significance, maximum likelihood estimation, and experimental design principles that remain standard today.\nFisher was also deeply committed to eugenics throughout his career. He was the founding chairman of the Cambridge University Eugenics Society. Approximately one-third of his landmark book “The Genetical Theory of Natural Selection” (1930) addresses eugenics, arguing that the fall of civilizations was caused by differential fertility between social classes.\nFisher used his statistical methods to analyze human variation in ways meant to support racial hierarchies. His scientific work and his eugenic advocacy were not separate activities—they were intertwined.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>The Eugenics History of Statistics</span>"
    ]
  },
  {
    "objectID": "chapters/A1-eugenics-history.html#connections-to-nazi-germany",
    "href": "chapters/A1-eugenics-history.html#connections-to-nazi-germany",
    "title": "24  The Eugenics History of Statistics",
    "section": "24.5 Connections to Nazi Germany",
    "text": "24.5 Connections to Nazi Germany\nAmerican eugenics directly influenced Nazi Germany. The Nazi sterilization program, which forcibly sterilized hundreds of thousands of people, was explicitly modeled on American laws, particularly California’s.\n\n\n\n\n\nThe Holocaust itself was the most extreme expression of eugenic ideology—the belief that human populations could and should be “improved” through preventing certain people from reproducing, taken to its murderous conclusion.\nAfter World War II, the horrors of Nazi eugenics discredited the movement publicly, but eugenic practices continued. Forced sterilizations continued in the United States into the 1970s. California did not pass legislation explicitly banning sterilization of prison inmates until 2014.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>The Eugenics History of Statistics</span>"
    ]
  },
  {
    "objectID": "chapters/A1-eugenics-history.html#what-do-we-do-with-this-history",
    "href": "chapters/A1-eugenics-history.html#what-do-we-do-with-this-history",
    "title": "24  The Eugenics History of Statistics",
    "section": "24.6 What Do We Do With This History?",
    "text": "24.6 What Do We Do With This History?\nThe statistical methods developed by Galton, Fisher, and others are mathematically sound. A t-test does not care about the motives of the person who developed it. These tools have been used to improve medicine, agriculture, and countless other fields.\nBut acknowledging this history serves several purposes:\nIt reminds us that science is not neutral. The questions scientists ask are shaped by their values and social context. Eugenic statistics were developed to answer eugenic questions.\nIt encourages vigilance. Similar misuses of science can occur today. Claims about genetic differences between groups, about who deserves resources or rights, should be scrutinized carefully.\nIt honors the victims. Tens of thousands of people were forcibly sterilized, and millions were murdered, under policies justified by scientific authority. Acknowledging this history recognizes their suffering.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>The Eugenics History of Statistics</span>"
    ]
  },
  {
    "objectID": "chapters/A1-eugenics-history.html#moving-forward",
    "href": "chapters/A1-eugenics-history.html#moving-forward",
    "title": "24  The Eugenics History of Statistics",
    "section": "24.7 Moving Forward",
    "text": "24.7 Moving Forward\nUnderstanding this history does not mean abandoning statistics—it means using these tools thoughtfully and ethically. It means being skeptical when scientific claims align too conveniently with existing prejudices. It means recognizing that data and analysis can be weaponized.\nScience has the potential to improve human welfare, but only when practiced with awareness of its history and constant attention to its ethics.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>The Eugenics History of Statistics</span>"
    ]
  },
  {
    "objectID": "chapters/A1-eugenics-history.html#further-reading",
    "href": "chapters/A1-eugenics-history.html#further-reading",
    "title": "24  The Eugenics History of Statistics",
    "section": "24.8 Further Reading",
    "text": "24.8 Further Reading\nFor those interested in exploring this history further:\n\n“Superior: The Return of Race Science” by Angela Saini\n“The Gene: An Intimate History” by Siddhartha Mukherjee\n\n“Control: The Dark History and Troubling Present of Eugenics” by Adam Rutherford",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>The Eugenics History of Statistics</span>"
    ]
  },
  {
    "objectID": "chapters/A2-r-reference.html",
    "href": "chapters/A2-r-reference.html",
    "title": "25  R Quick Reference",
    "section": "",
    "text": "25.1 Basic Operations\nThis appendix provides a quick reference for commonly used R functions and syntax covered in this book.\nCode\n# Assignment\nx &lt;- 5\ny = 10  # also works but &lt;- is preferred\n\n# Arithmetic\nx + y   # addition\nx - y   # subtraction\nx * y   # multiplication\nx / y   # division\nx^2     # exponentiation\nx %% y  # modulo (remainder)\n\n# Comparison\nx == y  # equal\nx != y  # not equal\nx &lt; y   # less than\nx &gt; y   # greater than\nx &lt;= y  # less than or equal\nx &gt;= y  # greater than or equal",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>R Quick Reference</span>"
    ]
  },
  {
    "objectID": "chapters/A2-r-reference.html#vectors",
    "href": "chapters/A2-r-reference.html#vectors",
    "title": "25  R Quick Reference",
    "section": "25.2 Vectors",
    "text": "25.2 Vectors\n\n\nCode\n# Create vectors\nc(1, 2, 3, 4, 5)\n1:10\nseq(0, 100, by = 10)\nrep(1, times = 5)\n\n# Access elements\nx[1]          # first element\nx[c(1, 3, 5)] # multiple elements\nx[-1]         # all except first\nx[x &gt; 5]      # conditional selection\n\n# Vector operations\nlength(x)\nsum(x)\nmean(x)\nsd(x)\nvar(x)\nmin(x)\nmax(x)\nrange(x)\nsort(x)\nrev(x)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>R Quick Reference</span>"
    ]
  },
  {
    "objectID": "chapters/A2-r-reference.html#data-frames",
    "href": "chapters/A2-r-reference.html#data-frames",
    "title": "25  R Quick Reference",
    "section": "25.3 Data Frames",
    "text": "25.3 Data Frames\n\n\nCode\n# Create data frame\ndata.frame(x = 1:5, y = c(\"a\", \"b\", \"c\", \"d\", \"e\"))\n\n# Access columns\ndf$column_name\ndf[, \"column_name\"]\ndf[, 1]\n\n# Access rows\ndf[1, ]\ndf[1:5, ]\ndf[df$x &gt; 2, ]\n\n# Dimensions\nnrow(df)\nncol(df)\ndim(df)\nnames(df)\nhead(df)\ntail(df)\nstr(df)\nsummary(df)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>R Quick Reference</span>"
    ]
  },
  {
    "objectID": "chapters/A2-r-reference.html#reading-and-writing-data",
    "href": "chapters/A2-r-reference.html#reading-and-writing-data",
    "title": "25  R Quick Reference",
    "section": "25.4 Reading and Writing Data",
    "text": "25.4 Reading and Writing Data\n\n\nCode\n# Read data\nread.csv(\"file.csv\")\nread.table(\"file.txt\", header = TRUE, sep = \"\\t\")\nreadRDS(\"file.rds\")\n\n# With tidyverse\nread_csv(\"file.csv\")\nread_tsv(\"file.tsv\")\n\n# Write data\nwrite.csv(df, \"file.csv\", row.names = FALSE)\nwrite.table(df, \"file.txt\", sep = \"\\t\", row.names = FALSE)\nsaveRDS(df, \"file.rds\")",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>R Quick Reference</span>"
    ]
  },
  {
    "objectID": "chapters/A2-r-reference.html#dplyr-verbs",
    "href": "chapters/A2-r-reference.html#dplyr-verbs",
    "title": "25  R Quick Reference",
    "section": "25.5 dplyr Verbs",
    "text": "25.5 dplyr Verbs\n\n\nCode\nlibrary(dplyr)\n\n# Filter rows\nfilter(df, condition)\n\n# Select columns\nselect(df, col1, col2)\n\n# Arrange rows\narrange(df, column)\narrange(df, desc(column))\n\n# Create new columns\nmutate(df, new_col = expression)\n\n# Summarize\nsummarize(df, mean_val = mean(x))\n\n# Group operations\ngroup_by(df, category) |&gt;\n  summarize(mean = mean(value))\n\n# Pipe operator\ndf |&gt;\n  filter(x &gt; 0) |&gt;\n  mutate(y = x^2) |&gt;\n  summarize(mean_y = mean(y))",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>R Quick Reference</span>"
    ]
  },
  {
    "objectID": "chapters/A2-r-reference.html#ggplot2-graphics",
    "href": "chapters/A2-r-reference.html#ggplot2-graphics",
    "title": "25  R Quick Reference",
    "section": "25.6 ggplot2 Graphics",
    "text": "25.6 ggplot2 Graphics\n\n\nCode\nlibrary(ggplot2)\n\n# Basic structure\nggplot(data, aes(x = var1, y = var2)) +\n  geom_point()\n\n# Common geoms\ngeom_point()      # scatter plot\ngeom_line()       # line plot\ngeom_bar()        # bar chart\ngeom_histogram()  # histogram\ngeom_boxplot()    # box plot\ngeom_smooth()     # smoothed line\n\n# Aesthetics\naes(x, y, color, fill, size, shape, alpha)\n\n# Faceting\nfacet_wrap(~ variable)\nfacet_grid(row ~ col)\n\n# Labels\nlabs(title = \"Title\", x = \"X axis\", y = \"Y axis\")\n\n# Themes\ntheme_minimal()\ntheme_bw()\ntheme_classic()",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>R Quick Reference</span>"
    ]
  },
  {
    "objectID": "chapters/A2-r-reference.html#statistical-tests",
    "href": "chapters/A2-r-reference.html#statistical-tests",
    "title": "25  R Quick Reference",
    "section": "25.7 Statistical Tests",
    "text": "25.7 Statistical Tests\n\n\nCode\n# T-tests\nt.test(x, mu = 0)                    # one-sample\nt.test(x, y)                         # two-sample (Welch)\nt.test(x, y, var.equal = TRUE)       # two-sample (equal var)\nt.test(x, y, paired = TRUE)          # paired\n\n# Nonparametric tests\nwilcox.test(x, y)                    # Mann-Whitney\nwilcox.test(x, y, paired = TRUE)     # Wilcoxon signed-rank\nkruskal.test(value ~ group, data)    # Kruskal-Wallis\n\n# Correlation\ncor(x, y)\ncor.test(x, y)\ncor.test(x, y, method = \"spearman\")\n\n# Linear models\nlm(y ~ x, data)\nlm(y ~ x1 + x2, data)\nlm(y ~ x1 * x2, data)\n\n# ANOVA\naov(y ~ group, data)\nanova(model)\nTukeyHSD(aov_result)\n\n# Model summary\nsummary(model)\nconfint(model)\npredict(model, newdata)\nresiduals(model)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>R Quick Reference</span>"
    ]
  },
  {
    "objectID": "chapters/A2-r-reference.html#probability-distributions",
    "href": "chapters/A2-r-reference.html#probability-distributions",
    "title": "25  R Quick Reference",
    "section": "25.8 Probability Distributions",
    "text": "25.8 Probability Distributions\n\n\nCode\n# Pattern: d = density, p = cumulative, q = quantile, r = random\n\n# Normal\ndnorm(x, mean, sd)\npnorm(q, mean, sd)\nqnorm(p, mean, sd)\nrnorm(n, mean, sd)\n\n# Binomial\ndbinom(x, size, prob)\npbinom(q, size, prob)\nrbinom(n, size, prob)\n\n# Poisson\ndpois(x, lambda)\nppois(q, lambda)\nrpois(n, lambda)\n\n# t-distribution\ndt(x, df)\npt(q, df)\nqt(p, df)\nrt(n, df)\n\n# F-distribution\ndf(x, df1, df2)\npf(q, df1, df2)\nrf(n, df1, df2)\n\n# Chi-squared\ndchisq(x, df)\npchisq(q, df)\nrchisq(n, df)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>R Quick Reference</span>"
    ]
  },
  {
    "objectID": "chapters/A2-r-reference.html#programming-constructs",
    "href": "chapters/A2-r-reference.html#programming-constructs",
    "title": "25  R Quick Reference",
    "section": "25.9 Programming Constructs",
    "text": "25.9 Programming Constructs\n\n\nCode\n# If-else\nif (condition) {\n  # code if true\n} else {\n  # code if false\n}\n\n# For loop\nfor (i in 1:10) {\n  print(i)\n}\n\n# While loop\nwhile (condition) {\n  # code\n}\n\n# Functions\nmy_function &lt;- function(arg1, arg2 = default) {\n  result &lt;- arg1 + arg2\n  return(result)\n}\n\n# Apply functions\napply(matrix, 1, mean)   # apply to rows\napply(matrix, 2, mean)   # apply to columns\nsapply(list, function)   # simplify result\nlapply(list, function)   # return list\nreplicate(n, expression) # repeat n times",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>R Quick Reference</span>"
    ]
  },
  {
    "objectID": "chapters/A2-r-reference.html#useful-utilities",
    "href": "chapters/A2-r-reference.html#useful-utilities",
    "title": "25  R Quick Reference",
    "section": "25.10 Useful Utilities",
    "text": "25.10 Useful Utilities\n\n\nCode\n# Help\n?function_name\nhelp(function_name)\nexample(function_name)\n\n# Working directory\ngetwd()\nsetwd(\"path\")\n\n# Packages\ninstall.packages(\"package_name\")\nlibrary(package_name)\n\n# Objects\nls()           # list objects\nrm(x)          # remove object\nrm(list=ls())  # remove all\n\n# Set seed for reproducibility\nset.seed(42)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>R Quick Reference</span>"
    ]
  }
]