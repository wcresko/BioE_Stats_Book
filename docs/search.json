[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Statistics for Bioengineering",
    "section": "",
    "text": "Preface\nWelcome to Statistics for Bioengineering, a comprehensive guide designed for graduate students at the University of Oregon Knight Campus. This book provides the foundational skills needed for a successful scientific career in bioengineering and related fields, combining rigorous statistical theory with practical computational implementation in R.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#why-this-book",
    "href": "index.html#why-this-book",
    "title": "Statistics for Bioengineering",
    "section": "Why This Book?",
    "text": "Why This Book?\nModern bioengineering research generates vast amounts of data—from RNA sequencing experiments to biomaterial characterization studies, from neural recordings to clinical trial outcomes. Making sense of this data requires more than just running statistical tests; it demands a deep understanding of the principles underlying those tests and the computational skills to implement them properly.\nThis course takes a practical, hands-on approach to learning statistics. Rather than deriving every formula from first principles, we focus on understanding when and why to apply particular methods, how to implement them in R, and how to interpret and communicate results. Throughout the book, you’ll work with real biological data and develop the skills to analyze your own research questions.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#what-you-will-learn",
    "href": "index.html#what-you-will-learn",
    "title": "Statistics for Bioengineering",
    "section": "What You Will Learn",
    "text": "What You Will Learn\nThe material spans several interconnected domains:\nComputational Foundations. You will develop proficiency in Unix command-line operations and R programming, including the tidyverse ecosystem for data manipulation and visualization. These tools form the backbone of modern reproducible research practices.\nStatistical Theory. The book covers probability distributions, parameter estimation, hypothesis testing, and the logic of statistical inference. Understanding these concepts allows you to choose appropriate methods and interpret results correctly.\nPractical Analysis Methods. From t-tests to linear regression to analysis of variance, you’ll learn to implement a wide range of statistical techniques. Each method is presented with clear guidance on assumptions, implementation in R, and interpretation of output.\nReproducible Research. Using Markdown, Git, and GitHub, you’ll learn to document your analyses in ways that others (including your future self) can understand and reproduce.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#prerequisites",
    "href": "index.html#prerequisites",
    "title": "Statistics for Bioengineering",
    "section": "Prerequisites",
    "text": "Prerequisites\nThis book assumes no prior programming experience, though familiarity with basic algebra and an introductory statistics course will be helpful. You should have access to a computer running Windows, MacOS, or Linux with R and RStudio installed. Instructions for software setup are provided in the opening chapters.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#course-materials",
    "href": "index.html#course-materials",
    "title": "Statistics for Bioengineering",
    "section": "Course Materials",
    "text": "Course Materials\n\nRequired Texts (Free Online)\nThe following textbooks are available freely online and provide excellent supplementary material:\n\nR for Data Science (RDS). 2025. Wickham, Çetinkaya-Rundel, and Grolemund. O’Reilly Press.\nModern Statistics with R (MSR). 2025. Måns Thulin, CRC Press.\nAn Introduction to Statistical Learning (ISLR). 2023. James, Witten, Hastie, Tibshirani. Springer.\n\n\n\nAdditional Resources\n\nModern Statistics for Modern Biology. 2019. Holmes and Huber. Cambridge University Press.\nggPlot2: Elegant Graphics for Data Analysis, 3rd Edition. Wickham, Navarro, Pedersen. Springer.\nThe Visual Display of Quantitative Information. Tufte, E.R. Graphics Press.\n\n\n\nSoftware Requirements\n\nLatest version of R (install here)\nLatest version of RStudio (install here)\nA terminal with SSH capabilities for connecting to computing clusters\nGit installed and a GitHub account\nLaTeX for document preparation",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#how-to-use-this-book",
    "href": "index.html#how-to-use-this-book",
    "title": "Statistics for Bioengineering",
    "section": "How to Use This Book",
    "text": "How to Use This Book\nEach chapter builds on previous material, so working through the book sequentially is recommended for beginners. However, the modular organization also allows readers with some background to jump to specific topics of interest.\nCode examples are provided throughout the text, and you are strongly encouraged to type them yourself rather than copying and pasting. The act of typing reinforces learning and helps you notice details that might otherwise slip by. When you encounter errors—and you will—treat them as learning opportunities.\nThe exercises at the end of each chapter progress from straightforward applications of the material to more challenging problems requiring synthesis across topics. Attempting these exercises, even when difficult, is essential for developing genuine competence.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#acknowledgments",
    "href": "index.html#acknowledgments",
    "title": "Statistics for Bioengineering",
    "section": "Acknowledgments",
    "text": "Acknowledgments\nThis book grew out of many years of teaching biostatistics at the University of Oregon. I am grateful to the many students whose questions and struggles have shaped how I present this material, and to colleagues who have shared their insights on effective teaching of statistics.\n\nBill Cresko\nKnight Campus for Accelerating Scientific Impact\nUniversity of Oregon",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "chapters/01-introduction.html",
    "href": "chapters/01-introduction.html",
    "title": "1  Introduction",
    "section": "",
    "text": "1.1 The Role of Statistics in Bioengineering\nBioengineering sits at the intersection of biology, engineering, and medicine, a field where understanding complex systems requires both precise measurement and rigorous analysis. Whether you are developing new biomaterials, engineering tissues, designing medical devices, or analyzing genomic data, you will encounter situations where you need to draw conclusions from imperfect data. Statistics provides the framework for doing this responsibly.\nAt its core, statistics addresses a fundamental problem: we almost never know the world perfectly, yet we still need to make decisions and draw conclusions. When you measure the mechanical properties of a hydrogel, characterize the response of neurons to a stimulus, or quantify gene expression in different treatment groups, you obtain samples from larger populations. Statistics gives us the tools to estimate underlying parameters from these samples and to quantify our uncertainty about those estimates.",
    "crumbs": [
      "Foundations of Data Science",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapters/01-introduction.html#why-statistics-matters-for-your-career",
    "href": "chapters/01-introduction.html#why-statistics-matters-for-your-career",
    "title": "1  Introduction",
    "section": "1.2 Why Statistics Matters for Your Career",
    "text": "1.2 Why Statistics Matters for Your Career\nThe importance of statistical literacy for bioengineers cannot be overstated. Experimental design, data analysis, and the interpretation of results form the backbone of scientific research. Understanding statistics allows you to design experiments that can actually answer your questions, analyze data appropriately, and communicate your findings clearly and honestly.\nBeyond research, statistical thinking is increasingly important in industry applications. Quality control in biomanufacturing relies on statistical process control. Clinical trials require sophisticated statistical designs. Machine learning algorithms that power diagnostic tools and drug discovery pipelines are fundamentally statistical methods. Familiarity with these concepts will serve you throughout your career.",
    "crumbs": [
      "Foundations of Data Science",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapters/01-introduction.html#coding-and-scripting-for-data-analysis",
    "href": "chapters/01-introduction.html#coding-and-scripting-for-data-analysis",
    "title": "1  Introduction",
    "section": "1.3 Coding and Scripting for Data Analysis",
    "text": "1.3 Coding and Scripting for Data Analysis\nThis course emphasizes computational approaches to statistics. While it is possible to perform many statistical calculations by hand or using spreadsheet software, modern data analysis almost always involves programming. The ability to write code opens up enormous possibilities.\nProgramming is incredibly fast and powerful, particularly for repeated actions. A single command can accomplish what would require thousands of mouse clicks. You gain the ability to analyze large datasets that spreadsheet software cannot handle efficiently. You have access to thousands of free programs created by and for scientists. Your analyses become reproducible—you can document exactly what you did and share that documentation with others.\n\n\n\n\n\nWe distinguish between coding and scripting, though the line between them has blurred considerably. Coding generally refers to programming in compiled languages like C++ or Fortran, where source code is translated into machine code before execution. Scripting typically involves interpreted languages like Python, R, or Julia, where commands are executed on the fly without a separate compilation step. Compiled code tends to run faster but is less flexible during development; scripting languages offer more interactivity at some cost in execution speed. Modern analytical pipelines typically combine both approaches, using scripting languages for data manipulation and visualization while calling compiled code for computationally intensive operations.",
    "crumbs": [
      "Foundations of Data Science",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapters/01-introduction.html#what-you-will-learn",
    "href": "chapters/01-introduction.html#what-you-will-learn",
    "title": "1  Introduction",
    "section": "1.4 What You Will Learn",
    "text": "1.4 What You Will Learn\nThis course provides broad coverage of the core components of modern statistics while giving you the computational tools necessary to carry out your work. By the end, you will be able to read and write code in Unix and R, implement reproducible research practices through Markdown, GitHub, and cloud computing platforms, perform exploratory data analysis and visualization, understand probability in the context of distributions and sampling, and conduct a wide range of statistical analyses from t-tests to linear models to machine learning methods.\nThe course is organized around progressive skill building. We start with the computational foundations—Unix, R, and tools for reproducible research. We then develop the probability theory needed to understand statistical inference. With these foundations in place, we cover classical hypothesis testing and parametric methods before moving to more advanced topics like linear models and statistical learning.",
    "crumbs": [
      "Foundations of Data Science",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapters/01-introduction.html#course-philosophy",
    "href": "chapters/01-introduction.html#course-philosophy",
    "title": "1  Introduction",
    "section": "1.5 Course Philosophy",
    "text": "1.5 Course Philosophy\nThis is a practical course, and we will learn by doing. Class time will be devoted primarily to hands-on coding practice rather than traditional lecturing. You will work through exercises, debug code, and analyze real data. This active approach to learning is more challenging than passive note-taking, but it produces much deeper understanding.\nExpect to struggle at times. Programming is frustrating, especially when you are learning. Error messages will seem cryptic. Code that should work will not work. Problems that seem simple will prove difficult. This is normal, and working through these challenges is how you develop genuine competence. The goal is not to avoid mistakes but to develop the skills to diagnose and fix them.\nThroughout the course, we emphasize reproducibility and transparency. Your analyses should be documented in ways that allow others to understand and verify what you did. This is not just good practice for collaboration; it also helps you when you return to your own work months or years later.",
    "crumbs": [
      "Foundations of Data Science",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapters/01-introduction.html#statistical-thinking",
    "href": "chapters/01-introduction.html#statistical-thinking",
    "title": "1  Introduction",
    "section": "1.6 Statistical Thinking",
    "text": "1.6 Statistical Thinking\nStatistics ultimately aims to turn data into conclusions about the world. We want to make point estimates and construct confidence intervals that quantify our uncertainty. We design experiments that can distinguish between competing hypotheses. We test those hypotheses using data. When dealing with high-dimensional data, we need methods to reduce complexity while preserving important information.\nAll of this requires a firm understanding of probability, sampling, and distributions. Probability provides the mathematical framework for reasoning about uncertainty. Understanding how samples relate to populations allows us to make inferences about things we cannot directly observe. Knowledge of common probability distributions tells us what to expect under various conditions and helps us identify when data deviate from expectations.\nWe will explore two major approaches to statistical inference. Frequentist statistics, the classical approach taught in most introductory courses, interprets probabilities as long-run frequencies and uses null hypothesis testing as its primary framework. Hierarchical probabilistic modeling, including maximum likelihood estimation and Bayesian methods, provides complementary tools that are increasingly important in modern statistical practice. Both perspectives have their uses, and understanding both will make you a more versatile analyst.",
    "crumbs": [
      "Foundations of Data Science",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapters/01-introduction.html#getting-started",
    "href": "chapters/01-introduction.html#getting-started",
    "title": "1  Introduction",
    "section": "1.7 Getting Started",
    "text": "1.7 Getting Started\nThe remainder of this chapter covers the practical matters of getting your computational environment set up. In subsequent chapters, we will dive into the material itself, beginning with Unix and the command line before moving to R and RStudio. With these tools in place, we will begin our exploration of probability, inference, and statistical modeling.\nThe journey ahead requires effort, but the skills you develop will serve you throughout your career. Let’s begin.",
    "crumbs": [
      "Foundations of Data Science",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapters/02-unix-command-line.html",
    "href": "chapters/02-unix-command-line.html",
    "title": "2  Unix and the Command Line",
    "section": "",
    "text": "2.1 What is Unix?\nUnix is a family of operating systems that originated at Bell Labs in 1969 and was released publicly in 1973. Its design philosophy emphasizes modularity—small programs that do one thing well and can be combined to accomplish complex tasks. This approach has proven remarkably durable, and Unix-based systems remain dominant in scientific computing, web servers, and high-performance computing environments.\nLinux is an open-source implementation of Unix that runs on everything from embedded devices to the world’s fastest supercomputers. MacOS is built on a Unix foundation, which means Mac users have native access to Unix commands. Windows historically used a different approach, but recent versions include the Windows Subsystem for Linux (WSL), allowing Windows users to run Linux environments alongside their Windows applications.\nUnderstanding Unix is essential for modern data science. You will need it to access remote computing resources like supercomputer clusters, to run bioinformatics software that is only available through the command line, and to automate repetitive tasks. The skills you develop here will transfer across platforms and remain relevant throughout your career.",
    "crumbs": [
      "Foundations of Data Science",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Unix and the Command Line</span>"
    ]
  },
  {
    "objectID": "chapters/02-unix-command-line.html#the-shell-and-terminal",
    "href": "chapters/02-unix-command-line.html#the-shell-and-terminal",
    "title": "2  Unix and the Command Line",
    "section": "2.2 The Shell and Terminal",
    "text": "2.2 The Shell and Terminal\nThe shell is a program that interprets your commands and communicates with the operating system. When you type a command, the shell parses it, figures out what you want to do, and tells the operating system to do it. The results are then displayed back to you.\nBash (Bourne Again SHell) is the most common shell on Linux systems and was the default on MacOS until recently (MacOS now defaults to zsh, which is very similar). The shell runs inside a terminal application, which provides the window where you type commands and see output.\n\n\n\n\n\nOn Mac, you can access the terminal by opening the Terminal app or a third-party alternative like iTerm2. On Linux, look for a Terminal application in your system menus. Windows users should install the Windows Subsystem for Linux following Microsoft’s documentation, then access it through the Ubuntu app or similar.\nRStudio also includes a terminal pane, which can be convenient when you want shell access without leaving your R development environment.",
    "crumbs": [
      "Foundations of Data Science",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Unix and the Command Line</span>"
    ]
  },
  {
    "objectID": "chapters/02-unix-command-line.html#anatomy-of-a-shell-command",
    "href": "chapters/02-unix-command-line.html#anatomy-of-a-shell-command",
    "title": "2  Unix and the Command Line",
    "section": "2.3 Anatomy of a Shell Command",
    "text": "2.3 Anatomy of a Shell Command\nShell commands follow a consistent structure. You type a command name, possibly followed by options that modify its behavior, and arguments that specify what the command should operate on. The shell waits at a prompt—typically $ for regular users or # for administrators—indicating it is ready to accept input.\n\n\n\n\n\nConsider the command ls -l Documents. Here, ls is the command (list directory contents), -l is an option (use long format), and Documents is the argument (the directory to list). Options usually begin with a dash and can often be combined: ls -la combines the -l (long format) and -a (show hidden files) options.",
    "crumbs": [
      "Foundations of Data Science",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Unix and the Command Line</span>"
    ]
  },
  {
    "objectID": "chapters/02-unix-command-line.html#file-system-organization",
    "href": "chapters/02-unix-command-line.html#file-system-organization",
    "title": "2  Unix and the Command Line",
    "section": "2.4 File System Organization",
    "text": "2.4 File System Organization\nUnix organizes files in a hierarchical structure of directories (folders) and files. The root directory, represented by a single forward slash /, sits at the top of this hierarchy and contains all other directories.\n\n\n\n\n\nYour home directory is your personal workspace, typically located at /Users/yourusername on Mac or /home/yourusername on Linux. The tilde character ~ serves as a shorthand for your home directory, so ~/Documents refers to the Documents folder in your home directory.\nEvery file and directory has a path—a specification of its location in the file system. Absolute paths start from the root directory and give the complete location, like /Users/wcresko/Documents/data.csv. Relative paths specify location relative to your current directory, so if you are in your home directory, Documents/data.csv refers to the same file.",
    "crumbs": [
      "Foundations of Data Science",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Unix and the Command Line</span>"
    ]
  },
  {
    "objectID": "chapters/02-unix-command-line.html#navigation-commands",
    "href": "chapters/02-unix-command-line.html#navigation-commands",
    "title": "2  Unix and the Command Line",
    "section": "2.5 Navigation Commands",
    "text": "2.5 Navigation Commands\nThe most fundamental navigation command is pwd (print working directory), which tells you where you currently are in the file system. This is often the first thing you type when opening a terminal to orient yourself.\n\n\n\n\n\npwd\nThe ls command lists the contents of a directory. Without arguments, it lists the current directory. With a path argument, it lists that location.\nls                  # list current directory\nls Documents        # list the Documents folder\nls -l               # long format with details\nls -a               # include hidden files (starting with .)\nls -la              # combine long format and hidden files\nls -lS              # long format, sorted by size\n\n\n\n\n\nThe cd command (change directory) moves you to a different location.\ncd Documents        # move into Documents\ncd ..               # move up one level (parent directory)\ncd ~                # move to home directory\ncd /                # move to root directory\ncd -                # move to previous location",
    "crumbs": [
      "Foundations of Data Science",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Unix and the Command Line</span>"
    ]
  },
  {
    "objectID": "chapters/02-unix-command-line.html#working-with-files-and-directories",
    "href": "chapters/02-unix-command-line.html#working-with-files-and-directories",
    "title": "2  Unix and the Command Line",
    "section": "2.6 Working with Files and Directories",
    "text": "2.6 Working with Files and Directories\nCreating new directories uses the mkdir command.\nmkdir project_data\nmkdir -p analysis/results/figures  # create nested directories\nThe -p flag tells mkdir to create parent directories as needed, which is useful for creating nested folder structures in one command.\nTo remove an empty directory, use rmdir.\nrmdir empty_folder                 # remove an empty directory\nNote that rmdir only works on empty directories. For directories with contents, you need rm -r (discussed below).\nCreating a new, empty file uses the touch command.\ntouch newfile.txt                  # create an empty file\ntouch notes.md data.csv            # create multiple files\nThe touch command is also useful for updating the modification timestamp of existing files without changing their contents.\nMoving and renaming files uses the mv command.\nmv old_name.txt new_name.txt       # rename a file\nmv file.txt Documents/             # move file to Documents\nmv file.txt Documents/newname.txt  # move and rename\nCopying files uses cp.\ncp original.txt copy.txt           # copy a file\ncp -r folder/ backup/              # copy a directory recursively\nRemoving files uses rm. Be careful with this command—there is no trash can or undo in the shell.\nrm unwanted_file.txt               # remove a file\nrm -r unwanted_folder/             # remove a directory and contents\nrm -i file.txt                     # ask for confirmation before removing\n\n\n\n\n\n\nInterrupting Commands\n\n\n\nIf you need to stop a running command—perhaps you started a process that is taking too long or realize you made a mistake—press Ctrl-C. This sends an interrupt signal that terminates most running processes and returns you to the command prompt.",
    "crumbs": [
      "Foundations of Data Science",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Unix and the Command Line</span>"
    ]
  },
  {
    "objectID": "chapters/02-unix-command-line.html#viewing-file-contents",
    "href": "chapters/02-unix-command-line.html#viewing-file-contents",
    "title": "2  Unix and the Command Line",
    "section": "2.7 Viewing File Contents",
    "text": "2.7 Viewing File Contents\nSeveral commands let you examine file contents without opening them in an editor.\nThe cat command displays the entire contents of a file.\ncat data.txt\nFor longer files, head and tail show the beginning and end.\nhead data.csv          # first 10 lines\nhead -n 20 data.csv    # first 20 lines\ntail data.csv          # last 10 lines\ntail -f logfile.txt    # follow a file as it grows\nThe less command opens an interactive viewer that lets you scroll through large files.\nless large_data.txt\nInside less, use arrow keys to scroll, / to search, and q to quit.\nThe wc command counts lines, words, and characters.\nwc data.txt            # lines, words, characters\nwc -l data.txt         # just lines",
    "crumbs": [
      "Foundations of Data Science",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Unix and the Command Line</span>"
    ]
  },
  {
    "objectID": "chapters/02-unix-command-line.html#getting-help",
    "href": "chapters/02-unix-command-line.html#getting-help",
    "title": "2  Unix and the Command Line",
    "section": "2.8 Getting Help",
    "text": "2.8 Getting Help\nUnix provides documentation through manual pages, accessible with the man command.\nman ls                 # manual page for ls command\nManual pages can be dense, but they are comprehensive. Use the spacebar to page through, / to search, and q to exit. Many commands also accept a --help flag that provides a shorter summary.\nls --help\nOf course, the internet provides extensive resources. When you encounter an unfamiliar command or error message, searching online often leads to helpful explanations and examples.",
    "crumbs": [
      "Foundations of Data Science",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Unix and the Command Line</span>"
    ]
  },
  {
    "objectID": "chapters/02-unix-command-line.html#pipes-and-redirection",
    "href": "chapters/02-unix-command-line.html#pipes-and-redirection",
    "title": "2  Unix and the Command Line",
    "section": "2.9 Pipes and Redirection",
    "text": "2.9 Pipes and Redirection\nOne of Unix’s most powerful features is the ability to combine simple commands into complex pipelines. The pipe operator | sends the output of one command to another command as input.\nls -l | head -n 5           # list files, show only first 5\ncat data.txt | wc -l        # count lines in file\nRedirection operators send output to files instead of the screen.\nls -l &gt; file_list.txt       # write output to file (overwrite)\nls -l &gt;&gt; file_list.txt      # append output to file\nThese features enable powerful text processing. Combined with tools like grep (search for patterns), sort, and cut (extract columns), you can accomplish sophisticated data manipulation with compact commands.\ngrep \"gene\" data.txt                    # find lines containing \"gene\"\ngrep -c \"gene\" data.txt                 # count matching lines\nsort data.txt                           # sort lines alphabetically\nsort -n numbers.txt                     # sort numerically\ncut -f1,3 data.tsv                      # extract columns 1 and 3 from tab-separated file",
    "crumbs": [
      "Foundations of Data Science",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Unix and the Command Line</span>"
    ]
  },
  {
    "objectID": "chapters/02-unix-command-line.html#advanced-text-processing",
    "href": "chapters/02-unix-command-line.html#advanced-text-processing",
    "title": "2  Unix and the Command Line",
    "section": "2.10 Advanced Text Processing",
    "text": "2.10 Advanced Text Processing\nThe basic commands above are just the beginning. Unix provides powerful tools for searching, manipulating, and transforming text files—skills that are invaluable when working with biological data.\n\n2.10.1 Pattern Matching with grep\nThe grep command becomes even more powerful when you use special characters to define patterns. These patterns, called regular expressions, allow you to search for complex text structures.\nCommon special characters in grep patterns:\n\n^ matches the beginning of a line\n$ matches the end of a line\n. matches any single character (except newline)\n* matches zero or more of the preceding character\n\\s matches any whitespace character\n\ngrep \"^embryo\" data.tsv          # lines starting with \"embryo\"\ngrep \"gene$\" data.tsv            # lines ending with \"gene\"\ngrep \"sample.*control\" data.tsv  # lines with \"sample\" followed by anything then \"control\"\ngrep \"^embryo_10\\s\" data.tsv     # lines starting with \"embryo_10\" followed by whitespace\nUseful grep flags include:\n\n-c counts matching lines instead of displaying them\n-v returns lines that do NOT match the pattern (inverse match)\n-n includes line numbers in the output\n-i performs case-insensitive matching\n\ngrep -v \"^#\" data.tsv            # exclude comment lines starting with #\ngrep -n \"error\" logfile.txt      # show line numbers for matches\ngrep -c \"ATCG\" sequences.fasta   # count lines containing this sequence\n\n\n2.10.2 Search and Replace with sed\nThe sed (stream editor) command is commonly used for search-and-replace operations. The basic syntax uses slashes to separate the pattern, replacement, and flags:\nsed 's/old/new/' file.txt        # replace first occurrence on each line\nsed 's/old/new/g' file.txt       # replace all occurrences (global)\nsed 's/\\t/,/g' file.tsv          # convert tabs to commas\nsed 's/^/prefix_/' file.txt      # add prefix to beginning of each line\nBy default, sed prints the modified text to the terminal. To modify a file in place, use the -i flag (use with caution):\nsed -i 's/old/new/g' file.txt    # modify file in place\nA safer approach is to redirect output to a new file:\nsed 's/old/new/g' input.txt &gt; output.txt\n\n\n2.10.3 Column Operations with cut and join\nThe cut command extracts specific columns from delimited files. By default, it assumes tab-delimited data.\ncut -f1,2 data.tsv               # extract columns 1 and 2 (tab-delimited)\ncut -f1,3 -d\",\" data.csv         # extract columns 1 and 3 (comma-delimited)\ncut -f2-5 data.tsv               # extract columns 2 through 5\nThe join command combines two files based on a common field, similar to a database join. Both files should be sorted on the join field.\njoin file1.txt file2.txt         # join on first field\njoin -1 2 -2 1 file1.txt file2.txt  # join file1's column 2 with file2's column 1\n\n\n2.10.4 Sorting with Advanced Options\nThe sort command has many options for controlling how data is sorted.\nsort -n data.txt                 # sort numerically\nsort -r data.txt                 # sort in reverse order\nsort -k2,2 data.tsv              # sort by second column\nsort -k2,2 -n data.tsv           # sort by second column numerically\nsort -k2,2 -nr data.tsv          # sort by second column, numerically, in reverse\nsort -u data.txt                 # sort and remove duplicate lines\nsort -t\",\" -k3,3 data.csv        # sort comma-separated file by third column\n\n\n2.10.5 Flexible Text Processing with awk\nThe awk command is an extremely powerful tool for processing structured text. It treats each line as a record and each column as a field, making it ideal for tabular data. Fields are referenced using $1, $2, etc., where $0 represents the entire line.\nawk '{print $1}' data.tsv                    # print first column\nawk '{print $1, $3}' data.tsv                # print columns 1 and 3\nawk -F\",\" '{print $1, $2}' data.csv          # specify comma as delimiter\nawk '{print NR, $0}' data.txt                # print line numbers with each line\nOne of awk’s strengths is its ability to filter rows based on conditions:\nawk '$3 &gt; 100 {print $1, $3}' data.tsv       # print columns 1 and 3 where column 3 &gt; 100\nawk '$2 == \"control\" {print $0}' data.tsv    # print lines where column 2 is \"control\"\nawk 'NR &gt; 1 {print $0}' data.tsv             # skip header (print from line 2 onward)\nawk '$4 &gt;= 0.05 {print $1}' results.tsv      # extract IDs where p-value &gt;= 0.05\nYou can also perform calculations:\nawk '{sum += $2} END {print sum}' data.tsv           # sum of column 2\nawk '{sum += $2} END {print sum/NR}' data.tsv        # average of column 2\nawk '{print $1, $2 * 1000}' data.tsv                 # multiply column 2 by 1000\n\n\n2.10.6 Combining Commands in Pipelines\nThe real power of Unix text processing comes from combining these tools. Here are some examples relevant to biological data analysis:\n# Count unique gene names in column 1 (skipping header)\ntail -n +2 data.tsv | cut -f1 | sort | uniq | wc -l\n\n# Extract rows with significant p-values and sort by effect size\nawk '$5 &lt; 0.05' results.tsv | sort -k3,3 -nr | head -20\n\n# Convert a file from comma to tab-delimited and extract specific columns\nsed 's/,/\\t/g' data.csv | cut -f1,3,5 &gt; subset.tsv\n\n# Find all unique values in column 2 and count occurrences\ncut -f2 data.tsv | sort | uniq -c | sort -nr\n\n# Process a FASTA file to count sequences per chromosome\ngrep \"^&gt;\" sequences.fasta | cut -d\":\" -f1 | sort | uniq -c\n\n\n\n\n\n\nLearning More\n\n\n\nThese tools have many more capabilities than we can cover here. The man pages provide comprehensive documentation, and online resources like the GNU Awk User’s Guide offer in-depth tutorials. With practice, you will develop intuition for which tool to use for different tasks.",
    "crumbs": [
      "Foundations of Data Science",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Unix and the Command Line</span>"
    ]
  },
  {
    "objectID": "chapters/02-unix-command-line.html#wildcards-and-pattern-matching",
    "href": "chapters/02-unix-command-line.html#wildcards-and-pattern-matching",
    "title": "2  Unix and the Command Line",
    "section": "2.11 Wildcards and Pattern Matching",
    "text": "2.11 Wildcards and Pattern Matching\nOne of Unix’s most powerful features is wildcards—special characters that match multiple files at once. Instead of typing each filename individually, you can specify patterns that match many files simultaneously.\nThe asterisk * matches any number of any characters (including zero characters):\nls *.csv              # all CSV files\nls data*              # all files starting with \"data\"\nls *.txt *.md         # all text and markdown files\nrm temp*              # remove all files starting with \"temp\"\nThe question mark ? matches exactly one character:\nls sample?.txt        # sample1.txt, sample2.txt, etc. (but not sample10.txt)\nls data_??.csv        # data_01.csv, data_AB.csv, etc.\nSquare brackets [] match any single character from a set:\nls sample[123].txt    # sample1.txt, sample2.txt, or sample3.txt\nls data_[0-9].csv     # data_0.csv through data_9.csv\nls file[A-Z].txt      # fileA.txt through fileZ.txt\n\n\n\n\n\n\nWildcard Safety\n\n\n\nCombining rm with wildcards can be dangerous. The command rm * deletes everything in the current directory without confirmation. Always use ls first to see what a wildcard pattern matches before using it with rm. Consider using rm -i for interactive confirmation when removing files with wildcards.\n\n\nWildcards are expanded by the shell before the command runs, so they work with any command:\n# Count lines in all CSV files\nwc -l *.csv\n\n# Copy all R scripts to a backup folder\ncp *.R backup/\n\n# Search for \"gene\" in all text files\ngrep \"gene\" *.txt",
    "crumbs": [
      "Foundations of Data Science",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Unix and the Command Line</span>"
    ]
  },
  {
    "objectID": "chapters/02-unix-command-line.html#environment-variables",
    "href": "chapters/02-unix-command-line.html#environment-variables",
    "title": "2  Unix and the Command Line",
    "section": "2.12 Environment Variables",
    "text": "2.12 Environment Variables\nUnix maintains settings called environment variables that affect how the shell and programs behave. These variables store information about your user session, system configuration, and program preferences. Environment variables are distinguished by a $ prefix when you reference them.\nSeveral important environment variables are set automatically:\n# Your home directory\necho $HOME\n/home/wcresko\n\n# Your username\necho $USER\nwcresko\n\n# Your current shell program\necho $SHELL\n/bin/bash\n\n# Where Unix looks for executable programs\necho $PATH\n/usr/local/bin:/usr/bin:/bin:/home/wcresko/bin\n\n# Your current working directory\necho $PWD\n/home/wcresko/projects\nThe PATH variable is particularly important—it contains a colon-separated list of directories where Unix searches for programs. When you type a command like ls or python, Unix looks through each directory in your PATH until it finds a matching executable. You can find where a program is located using which:\nwhich python\n/usr/bin/python\n\nwhich R\n/usr/local/bin/R\nYou can set your own environment variables and use them in commands:\n# Set a variable\nPROJECT_DIR=~/projects/analysis\n\n# Use it (note the $)\ncd $PROJECT_DIR\nls $PROJECT_DIR/data\nTo make environment variables available to subprocesses (programs you launch), use export:\nexport PROJECT_DIR=~/projects/analysis\nTo see all environment variables currently set, use env or printenv:\nenv | head -20       # Show first 20 environment variables\n\n2.12.1 Shell Configuration Files\nYour shell can be customized through configuration files that run when you open a terminal. For Bash, the main configuration files are ~/.bashrc (for interactive shells) and ~/.bash_profile (for login shells). For zsh (the default on modern macOS), use ~/.zshrc.\nCommon customizations include:\n# Add a directory to your PATH\nexport PATH=\"$HOME/bin:$PATH\"\n\n# Create an alias (shortcut) for a common command\nalias ll='ls -la'\nalias rm='rm -i'    # Always ask before deleting\n\n# Set default options for programs\nexport R_LIBS_USER=\"$HOME/R/library\"\nAfter editing configuration files, apply the changes by either starting a new terminal or running:\nsource ~/.bashrc",
    "crumbs": [
      "Foundations of Data Science",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Unix and the Command Line</span>"
    ]
  },
  {
    "objectID": "chapters/02-unix-command-line.html#file-permissions",
    "href": "chapters/02-unix-command-line.html#file-permissions",
    "title": "2  Unix and the Command Line",
    "section": "2.13 File Permissions",
    "text": "2.13 File Permissions\nUnix is a multi-user system, and every file has permissions controlling who can read, write, or execute it. Understanding permissions is essential for security and for running scripts on shared computing clusters.\nWhen you run ls -l, you see permission strings at the beginning of each line:\nls -l\n-rwxr-xr-x 1 wcresko staff 2048 Jan 15 10:30 analysis.sh\ndrwxr-xr-x 3 wcresko staff   96 Jan 15 09:00 data\nThe permission string -rwxr-xr-x encodes three sets of permissions:\n\n\n\nPosition\nMeaning\n\n\n\n\n1st character\nType: - (file), d (directory), l (link)\n\n\nCharacters 2-4\nOwner permissions\n\n\nCharacters 5-7\nGroup permissions\n\n\nCharacters 8-10\nOthers (everyone else) permissions\n\n\n\nWithin each set, the three characters represent:\n\nr (read): View file contents or list directory contents\nw (write): Modify file or add/remove files in directory\nx (execute): Run file as program or enter directory\n- (dash): Permission denied for that operation\n\nSo -rwxr-xr-x means: this is a regular file; the owner can read, write, and execute; the group can read and execute; others can read and execute.\n\n2.13.1 Changing Permissions with chmod\nThe chmod command changes file permissions. You can use symbolic notation or numeric (octal) notation.\nSymbolic notation uses letters and operators:\n# Add execute permission for the owner\nchmod u+x script.sh\n\n# Remove write permission for group and others\nchmod go-w data.csv\n\n# Set exact permissions\nchmod u=rwx,g=rx,o=rx script.sh\nThe letters are: u (user/owner), g (group), o (others), a (all). The operators are: + (add), - (remove), = (set exactly).\nOctal notation uses numbers where each permission has a value:\n\n\n\nPermission\nValue\n\n\n\n\nread (r)\n4\n\n\nwrite (w)\n2\n\n\nexecute (x)\n1\n\n\n\nAdd the values for each set. For example, rwx = 4+2+1 = 7, and r-x = 4+0+1 = 5.\nchmod 755 script.sh    # rwxr-xr-x (executable script)\nchmod 644 data.csv     # rw-r--r-- (readable data file)\nchmod 700 private/     # rwx------ (private directory)\nCommon permission patterns:\n\n755: Executable scripts (owner can modify, everyone can run)\n644: Data files (owner can modify, everyone can read)\n700: Private directories (only owner has access)\n600: Private files (only owner can read/write)\n\n\n\n2.13.2 Making Scripts Executable\nWhen you write a shell script, you need to make it executable before you can run it directly:\n# Create a simple script\necho '#!/bin/bash' &gt; myscript.sh\necho 'echo \"Hello, World!\"' &gt;&gt; myscript.sh\n\n# Try to run it (will fail)\n./myscript.sh\n# bash: ./myscript.sh: Permission denied\n\n# Make it executable\nchmod +x myscript.sh\n\n# Now it works\n./myscript.sh\n# Hello, World!\nThe #!/bin/bash line at the top of the script (called a “shebang”) tells Unix which program should interpret the script.",
    "crumbs": [
      "Foundations of Data Science",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Unix and the Command Line</span>"
    ]
  },
  {
    "objectID": "chapters/02-unix-command-line.html#connecting-to-remote-systems",
    "href": "chapters/02-unix-command-line.html#connecting-to-remote-systems",
    "title": "2  Unix and the Command Line",
    "section": "2.14 Connecting to Remote Systems",
    "text": "2.14 Connecting to Remote Systems\nThe ssh command (secure shell) lets you connect to remote computers.\nssh username@server.university.edu\nYou will use this to connect to computing clusters like Talapas for computationally intensive work. Once connected, you work in a shell environment on the remote system just as you would locally.\nThe scp command copies files between your computer and remote systems.\nscp local_file.txt username@server.edu:~/destination/\nscp username@server.edu:~/remote_file.txt ./local_copy.txt",
    "crumbs": [
      "Foundations of Data Science",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Unix and the Command Line</span>"
    ]
  },
  {
    "objectID": "chapters/02-unix-command-line.html#data-file-best-practices",
    "href": "chapters/02-unix-command-line.html#data-file-best-practices",
    "title": "2  Unix and the Command Line",
    "section": "2.15 Data File Best Practices",
    "text": "2.15 Data File Best Practices\nWhen working with data files, following consistent practices will save you time and prevent errors. These guidelines apply whether you are using Unix tools, R, or any other analysis software.\n\n2.15.1 Do\n\nStore data in plain text formats such as tab-separated (.tsv) or comma-separated (.csv) files. These nonproprietary formats can be read by any software and will remain accessible for years to come.\nKeep an unedited copy of original data files. Even when your analysis requires modifications, preserve the raw data separately.\nUse descriptive, consistent names for files and variables. A name like experiment1_control_measurements.tsv is far more useful than data2.txt.\nMaintain metadata that documents what each variable means, how data were collected, and any processing steps applied.\nAdd new observations as rows and new variables as columns to maintain a consistent rectangular structure.\n\n\n\n2.15.2 Don’t\n\nDon’t mix data types within a column. If a column contains numbers, every entry should be a number (or explicitly missing).\nDon’t use special characters in file or directory names. Stick to letters, numbers, underscores, and hyphens. Avoid spaces, which can cause problems with command-line tools.\nDon’t use delimiter characters in data values. If your file is comma-delimited, don’t use commas within data entries. For example, use 2024-03-08 rather than March 8, 2024 for dates.\nDon’t copy data from formatted documents like Microsoft Word directly into data files. Hidden formatting characters can corrupt your data.\nDon’t edit data files in spreadsheet programs that might silently convert values (for example, Excel’s tendency to convert gene names like SEPT1 to dates).\n\n\n\n\n\n\n\nPreserving Raw Data\n\n\n\nPerhaps the most important principle is to never modify your original raw data files. Keep them in a separate directory with restricted write permissions if possible. All data cleaning and transformation should be done programmatically (in scripts that can be re-run), with outputs saved to new files.",
    "crumbs": [
      "Foundations of Data Science",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Unix and the Command Line</span>"
    ]
  },
  {
    "objectID": "chapters/02-unix-command-line.html#practice-exercises",
    "href": "chapters/02-unix-command-line.html#practice-exercises",
    "title": "2  Unix and the Command Line",
    "section": "2.16 Practice Exercises",
    "text": "2.16 Practice Exercises\nThe best way to learn command-line skills is through practice. Create a project folder structure for organizing your course work. Navigate through the file system and examine the contents of various directories. Create some text files and practice viewing, copying, moving, and removing them.\nTry combining commands with pipes. For example, you might list all files in a directory, filter for those with a particular extension, and count how many there are. Start simple and gradually build more complex pipelines as you become comfortable with the individual commands.",
    "crumbs": [
      "Foundations of Data Science",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Unix and the Command Line</span>"
    ]
  },
  {
    "objectID": "chapters/02-unix-command-line.html#practice-exercises-1",
    "href": "chapters/02-unix-command-line.html#practice-exercises-1",
    "title": "2  Unix and the Command Line",
    "section": "2.17 Practice Exercises",
    "text": "2.17 Practice Exercises\nFor hands-on practice with the concepts covered in this chapter, see Section 35.1 in the Practice Exercises appendix. The exercises include:\n\nBasic navigation and file operations\nWorking with data files using grep, cut, sort, and awk\nBuilding command pipelines\nFile permissions and shell scripts",
    "crumbs": [
      "Foundations of Data Science",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Unix and the Command Line</span>"
    ]
  },
  {
    "objectID": "chapters/02-unix-command-line.html#additional-resources",
    "href": "chapters/02-unix-command-line.html#additional-resources",
    "title": "2  Unix and the Command Line",
    "section": "2.18 Additional Resources",
    "text": "2.18 Additional Resources\n\nUnix/Linux Command Reference - A comprehensive cheat sheet of common commands\nUnix and Perl Primer for Biologists - An outstanding tutorial by Keith Bradnam and Ian Korf, specifically designed for life scientists\nIntroduction to Shell for Data Science - DataCamp’s interactive tutorial\nThe GNU Awk User’s Guide - Comprehensive documentation for mastering awk\nSoftware Carpentry Shell Lessons - Excellent tutorials designed for researchers",
    "crumbs": [
      "Foundations of Data Science",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Unix and the Command Line</span>"
    ]
  },
  {
    "objectID": "chapters/03-r-rstudio.html",
    "href": "chapters/03-r-rstudio.html",
    "title": "3  R and RStudio",
    "section": "",
    "text": "3.1 What is R?\nR is a computer programming language and environment especially useful for graphic visualization and statistical analysis of data. It is an offshoot of a language developed in 1976 at Bell Laboratories called S. R is an interpreted language, meaning that every time code is run it must be translated to machine language by the R interpreter, as opposed to being compiled prior to running. R is the premier computational platform for statistical analysis thanks to its GNU open-source status and countless packages contributed by diverse members of the scientific community.",
    "crumbs": [
      "Foundations of Data Science",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>R and RStudio</span>"
    ]
  },
  {
    "objectID": "chapters/03-r-rstudio.html#why-r",
    "href": "chapters/03-r-rstudio.html#why-r",
    "title": "3  R and RStudio",
    "section": "3.2 Why R?",
    "text": "3.2 Why R?\nR is a programming language designed specifically for statistical computing and graphics. Created in the early 1990s as an open-source implementation of the S language, R has become the lingua franca of statistical analysis in academia and is widely used in industry as well.\nSeveral features make R particularly well-suited for data analysis. It provides an extensive collection of statistical and graphical techniques built into the language. It is powerful, flexible, and completely free. It runs on Windows, Mac, and Linux, so your code will work across platforms. New capabilities are constantly being added through packages contributed by the community, with thousands of packages available for specialized analyses.\nR excels at reproducibility. You can keep your scripts to document exactly what analyses you performed. Unlike point-and-click software where actions leave no trace, R code provides a complete record of your analytical workflow. This record can be shared with collaborators, included in publications, and revisited years later when you need to remember how you produced a particular result.\nYou can write your own functions in R, extending the language to meet your specific needs. Extensive online help and active user communities mean that answers to most questions are a web search away. The RStudio integrated development environment makes working with R much more pleasant, especially for newcomers. And with tools like R Markdown and Quarto, you can embed your analyses in polished documents, presentations, websites, and books—this book itself was created with these tools.",
    "crumbs": [
      "Foundations of Data Science",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>R and RStudio</span>"
    ]
  },
  {
    "objectID": "chapters/03-r-rstudio.html#installing-r-and-rstudio",
    "href": "chapters/03-r-rstudio.html#installing-r-and-rstudio",
    "title": "3  R and RStudio",
    "section": "3.3 Installing R and RStudio",
    "text": "3.3 Installing R and RStudio\nR must be installed before RStudio. Download R from https://www.r-project.org, selecting the version appropriate for your operating system. Follow the installation instructions for your platform.\nRStudio is an integrated development environment (IDE) that makes working with R much easier. Download the free RStudio Desktop from https://www.rstudio.com. RStudio provides a console for running R commands, an editor for writing scripts, tools for viewing plots and data, and integration with version control systems.\nAfter installing both programs, launch RStudio. You will see a window divided into panes, each serving a different purpose. The console pane is where R commands are executed. The source pane is where you edit scripts and documents. The environment pane shows what objects currently exist in your R session. The files/plots/packages/help pane provides access to various utilities.",
    "crumbs": [
      "Foundations of Data Science",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>R and RStudio</span>"
    ]
  },
  {
    "objectID": "chapters/03-r-rstudio.html#r-basics",
    "href": "chapters/03-r-rstudio.html#r-basics",
    "title": "3  R and RStudio",
    "section": "3.4 R Basics",
    "text": "3.4 R Basics\nR evaluates expressions and returns results. You can use it as a calculator by typing arithmetic expressions at the console.\n\n\nCode\n4 * 4\n\n\n[1] 16\n\n\nCode\n(4 + 3 * 2^2)\n\n\n[1] 16\n\n\nNotice that R follows standard mathematical order of operations: exponentiation before multiplication and division, which come before addition and subtraction. Parentheses can override this ordering.",
    "crumbs": [
      "Foundations of Data Science",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>R and RStudio</span>"
    ]
  },
  {
    "objectID": "chapters/03-r-rstudio.html#variables-and-assignment",
    "href": "chapters/03-r-rstudio.html#variables-and-assignment",
    "title": "3  R and RStudio",
    "section": "3.5 Variables and Assignment",
    "text": "3.5 Variables and Assignment\nMore useful than evaluating isolated expressions is storing values in variables for later use. Variables are assigned using the &lt;- operator (a less-than sign followed by a hyphen).\n\n\nCode\nx &lt;- 2\nx * 3\n\n\n[1] 6\n\n\nCode\ny &lt;- x * 3\ny - 2\n\n\n[1] 4\n\n\nVariable names must begin with a letter but can contain letters, numbers, periods, and underscores after the first character. R is case-sensitive, so myVariable, MyVariable, and myvariable are three different names. Choose descriptive names that make your code readable. It is good practice to avoid periods in variable names, as they have other functionality in related programming languages like Python.\n\n\n\n\n\n\nInvalid Variable Names\n\n\n\nVariable names cannot begin with numbers or contain operators. The following will produce errors:\n\n\nCode\n3y &lt;- 3    # cannot start with a number\n3*y &lt;- 3   # cannot include operators\n\n\n\n\n\n3.5.1 Reserved Words\nR has reserved words that cannot be used as variable names because they have special meaning in the language:\n\n\n\nReserved Words\nPurpose\n\n\n\n\nif, else\nConditional statements\n\n\nfor, while, repeat\nLoops\n\n\nfunction\nFunction definition\n\n\nin, next, break\nLoop control\n\n\nTRUE, FALSE\nLogical constants\n\n\nNULL, NA, NaN, Inf\nSpecial values\n\n\n\nR also has semi-reserved names—built-in functions and constants that you can technically overwrite but should avoid:\n\n\nCode\n# These work but are dangerous:\nT &lt;- 5       # Overwrites TRUE abbreviation\nc &lt;- \"text\"  # Shadows the c() function\nmean &lt;- 42   # Shadows mean()\n\n# If you accidentally overwrite something, remove it:\nrm(c)        # Restores access to c()\n\n\n\n\n\n\n\n\nAvoid Common Name Collisions\n\n\n\nNever name variables T, F (abbreviations for TRUE/FALSE), c, t, mean, sum, data, or df. These are commonly used R functions, and shadowing them leads to confusing errors.\n\n\nNote that when you assign a value to a variable, R does not print anything. To see a variable’s value, type its name alone or use the print() function.\n\n\nCode\nz &lt;- 100\nz\n\n\n[1] 100\n\n\nCode\nprint(z)\n\n\n[1] 100",
    "crumbs": [
      "Foundations of Data Science",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>R and RStudio</span>"
    ]
  },
  {
    "objectID": "chapters/03-r-rstudio.html#understanding-r-objects",
    "href": "chapters/03-r-rstudio.html#understanding-r-objects",
    "title": "3  R and RStudio",
    "section": "3.6 Understanding R Objects",
    "text": "3.6 Understanding R Objects\nA fundamental principle of R is that everything is an object. Numbers, text, datasets, functions—all are stored as objects with specific properties. Understanding this helps you debug problems and write better code.\nEvery object has a class (which determines how functions treat it) and a type (its underlying storage mode). Use class() and typeof() to examine objects:\n\n\nCode\n# Numbers are objects\nx &lt;- 42\nclass(x)\n\n\n[1] \"numeric\"\n\n\nCode\ntypeof(x)\n\n\n[1] \"double\"\n\n\nCode\n# Text strings are objects\nname &lt;- \"Gene Expression\"\nclass(name)\n\n\n[1] \"character\"\n\n\nCode\n# Even functions are objects!\nclass(mean)\n\n\n[1] \"function\"\n\n\nThe str() function (structure) provides a compact display of any object’s structure—it is one of the most useful diagnostic tools in R:\n\n\nCode\n# Examine a vector\nstr(c(1, 2, 3, 4, 5))\n\n\n num [1:5] 1 2 3 4 5\n\n\nCode\n# Examine a data frame\nstr(head(iris))\n\n\n'data.frame':   6 obs. of  5 variables:\n $ Sepal.Length: num  5.1 4.9 4.7 4.6 5 5.4\n $ Sepal.Width : num  3.5 3 3.2 3.1 3.6 3.9\n $ Petal.Length: num  1.4 1.4 1.3 1.5 1.4 1.7\n $ Petal.Width : num  0.2 0.2 0.2 0.2 0.2 0.4\n $ Species     : Factor w/ 3 levels \"setosa\",\"versicolor\",..: 1 1 1 1 1 1\n\n\nWhen functions produce errors or unexpected results, checking the class of your objects is often the first step toward understanding what went wrong.",
    "crumbs": [
      "Foundations of Data Science",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>R and RStudio</span>"
    ]
  },
  {
    "objectID": "chapters/03-r-rstudio.html#functions",
    "href": "chapters/03-r-rstudio.html#functions",
    "title": "3  R and RStudio",
    "section": "3.7 Functions",
    "text": "3.7 Functions\nFunctions are the workhorses of R. A function takes inputs (called arguments), performs some operation, and returns an output. R has many built-in functions, and packages provide thousands more.\n\n\nCode\nlog(10)\n\n\n[1] 2.302585\n\n\nCode\nsqrt(16)\n\n\n[1] 4\n\n\nCode\nexp(1)\n\n\n[1] 2.718282\n\n\nFunctions are called by typing their name followed by parentheses containing their arguments. Many functions accept multiple arguments, separated by commas. Arguments can be specified by position or by name.\n\n\nCode\nround(3.14159, digits = 2)\n\n\n[1] 3.14\n\n\nCode\nround(3.14159, 2)  # same result, argument specified by position\n\n\n[1] 3.14\n\n\nTo learn about a function, use the help system. Type ?functionname or help(functionname) to open the documentation.\n\n\nCode\n?round\nhelp(sqrt)",
    "crumbs": [
      "Foundations of Data Science",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>R and RStudio</span>"
    ]
  },
  {
    "objectID": "chapters/03-r-rstudio.html#vectors",
    "href": "chapters/03-r-rstudio.html#vectors",
    "title": "3  R and RStudio",
    "section": "3.8 Vectors",
    "text": "3.8 Vectors\nThe fundamental data structure in R is the vector, an ordered collection of values of the same type. You create vectors using the c() function (for concatenate or combine).\n\n\nCode\nnumbers &lt;- c(1, 2, 3, 4, 5)\nnumbers\n\n\n[1] 1 2 3 4 5\n\n\nCode\nnames &lt;- c(\"Alice\", \"Bob\", \"Carol\")\nnames\n\n\n[1] \"Alice\" \"Bob\"   \"Carol\"\n\n\nMany operations in R are vectorized, meaning they operate on entire vectors at once rather than requiring you to loop through elements.\n\n\nCode\nnumbers * 2\n\n\n[1]  2  4  6  8 10\n\n\nCode\nnumbers + 10\n\n\n[1] 11 12 13 14 15\n\n\nCode\nnumbers^2\n\n\n[1]  1  4  9 16 25\n\n\nYou can access individual elements using square brackets with an index (R uses 1-based indexing, so the first element is at position 1).\n\n\nCode\nnumbers[1]\n\n\n[1] 1\n\n\nCode\nnumbers[3]\n\n\n[1] 3\n\n\nCode\nnumbers[c(1, 3, 5)]\n\n\n[1] 1 3 5",
    "crumbs": [
      "Foundations of Data Science",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>R and RStudio</span>"
    ]
  },
  {
    "objectID": "chapters/03-r-rstudio.html#creating-sequences",
    "href": "chapters/03-r-rstudio.html#creating-sequences",
    "title": "3  R and RStudio",
    "section": "3.9 Creating Sequences",
    "text": "3.9 Creating Sequences\nR provides convenient functions for creating regular sequences.\n\n\nCode\n1:10\n\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\n\nCode\nseq(0, 10, by = 2)\n\n\n[1]  0  2  4  6  8 10\n\n\nCode\nseq(0, 1, length.out = 5)\n\n\n[1] 0.00 0.25 0.50 0.75 1.00\n\n\nCode\nrep(1, times = 5)\n\n\n[1] 1 1 1 1 1\n\n\nCode\nrep(c(1, 2), times = 3)\n\n\n[1] 1 2 1 2 1 2",
    "crumbs": [
      "Foundations of Data Science",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>R and RStudio</span>"
    ]
  },
  {
    "objectID": "chapters/03-r-rstudio.html#generating-random-numbers",
    "href": "chapters/03-r-rstudio.html#generating-random-numbers",
    "title": "3  R and RStudio",
    "section": "3.10 Generating Random Numbers",
    "text": "3.10 Generating Random Numbers\nR can generate random numbers from various probability distributions, which is invaluable for simulation and understanding statistical concepts.\n\n\nCode\n# Draw 1000 values from a normal distribution with mean 0 and SD 10\nx &lt;- rnorm(1000, mean = 0, sd = 10)\nhist(x)\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Draw from a binomial distribution: 1000 experiments, 20 trials each, p=0.5\nheads &lt;- rbinom(n = 1000, size = 20, prob = 0.5)\nhist(heads)\n\n\n\n\n\n\n\n\n\nThe set.seed() function allows you to make random simulations reproducible by initializing the random number generator to a known state.\n\n\nCode\nset.seed(42)\nrnorm(5)\n\n\n[1]  1.3709584 -0.5646982  0.3631284  0.6328626  0.4042683\n\n\nCode\nset.seed(42)  # same seed produces same \"random\" numbers\nrnorm(5)\n\n\n[1]  1.3709584 -0.5646982  0.3631284  0.6328626  0.4042683",
    "crumbs": [
      "Foundations of Data Science",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>R and RStudio</span>"
    ]
  },
  {
    "objectID": "chapters/03-r-rstudio.html#data-frames",
    "href": "chapters/03-r-rstudio.html#data-frames",
    "title": "3  R and RStudio",
    "section": "3.11 Data Frames",
    "text": "3.11 Data Frames\nData frames are R’s structure for tabular data—rows of observations and columns of variables. Each column can contain a different type of data (numeric, character, logical), but all values within a column must be the same type.\n\n\nCode\n# Create a data frame from vectors\nhydrogel_concentration &lt;- factor(c(\"low\", \"high\", \"high\", \"high\", \n                                    \"medium\", \"medium\", \"medium\", \"low\"))\ncompression &lt;- c(3.4, 3.4, 8.4, 3, 5.6, 8.1, 8.3, 4.5)\nconductivity &lt;- c(0, 9.2, 3.8, 5, 5.6, 4.1, 7.1, 5.3)\n\nmydata &lt;- data.frame(hydrogel_concentration, compression, conductivity)\nmydata\n\n\n  hydrogel_concentration compression conductivity\n1                    low         3.4          0.0\n2                   high         3.4          9.2\n3                   high         8.4          3.8\n4                   high         3.0          5.0\n5                 medium         5.6          5.6\n6                 medium         8.1          4.1\n7                 medium         8.3          7.1\n8                    low         4.5          5.3\n\n\nAccess columns using the $ operator or square brackets.\n\n\nCode\nmydata$compression\n\n\n[1] 3.4 3.4 8.4 3.0 5.6 8.1 8.3 4.5\n\n\nCode\nmydata[, 2]  # second column\n\n\n[1] 3.4 3.4 8.4 3.0 5.6 8.1 8.3 4.5\n\n\nCode\nmydata[1, ]  # first row\n\n\n  hydrogel_concentration compression conductivity\n1                    low         3.4            0\n\n\nCode\nmydata[1, 2] # first row, second column\n\n\n[1] 3.4",
    "crumbs": [
      "Foundations of Data Science",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>R and RStudio</span>"
    ]
  },
  {
    "objectID": "chapters/03-r-rstudio.html#reading-and-writing-data",
    "href": "chapters/03-r-rstudio.html#reading-and-writing-data",
    "title": "3  R and RStudio",
    "section": "3.12 Reading and Writing Data",
    "text": "3.12 Reading and Writing Data\nReal analyses typically begin by reading data from external files. R provides functions for various file formats.\n\n\nCode\n# Read comma-separated values\ndata &lt;- read.csv(\"mydata.csv\")\n\n# Read tab-separated values\ndata &lt;- read.table(\"mydata.txt\", header = TRUE, sep = \"\\t\")\n\n# Read Excel files (requires readxl package)\nlibrary(readxl)\ndata &lt;- read_excel(\"mydata.xlsx\")\n\n\nSimilarly, you can write data to files.\n\n\nCode\nwrite.csv(mydata, \"output.csv\", row.names = FALSE)\nwrite.table(mydata, \"output.txt\", sep = \"\\t\", row.names = FALSE)",
    "crumbs": [
      "Foundations of Data Science",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>R and RStudio</span>"
    ]
  },
  {
    "objectID": "chapters/03-r-rstudio.html#basic-plotting",
    "href": "chapters/03-r-rstudio.html#basic-plotting",
    "title": "3  R and RStudio",
    "section": "3.13 Basic Plotting",
    "text": "3.13 Basic Plotting\nR has extensive graphics capabilities. The base plot() function creates scatterplots and other basic visualizations.\n\n\nCode\nx &lt;- 1:10\ny &lt;- x^2\nplot(x, y, \n     xlab = \"X values\", \n     ylab = \"Y squared\",\n     main = \"A Simple Plot\",\n     col = \"blue\",\n     pch = 19)\n\n\n\n\n\n\n\n\n\nHistograms visualize the distribution of a single variable.\n\n\nCode\ndata &lt;- rnorm(1000)\nhist(data, breaks = 30, col = \"lightblue\", main = \"Normal Distribution\")\n\n\n\n\n\n\n\n\n\nBoxplots compare distributions across groups.\n\n\nCode\nboxplot(compression ~ hydrogel_concentration, data = mydata,\n        xlab = \"Concentration\", ylab = \"Compression\")\n\n\n\n\n\n\n\n\n\nWe will explore the more sophisticated ggplot2 package for graphics in a later chapter.",
    "crumbs": [
      "Foundations of Data Science",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>R and RStudio</span>"
    ]
  },
  {
    "objectID": "chapters/03-r-rstudio.html#scripts-and-reproducibility",
    "href": "chapters/03-r-rstudio.html#scripts-and-reproducibility",
    "title": "3  R and RStudio",
    "section": "3.14 Scripts and Reproducibility",
    "text": "3.14 Scripts and Reproducibility\nWhile you can type commands directly at the console, for anything beyond simple explorations you should write scripts—text files containing R commands that can be saved, edited, and rerun.\nIn RStudio, create a new script with File &gt; New File &gt; R Script. Type your commands in the script editor, and run them by placing your cursor on a line and pressing Ctrl+Enter (Cmd+Enter on Mac) or by selecting code and clicking Run.\nScripts should be self-contained, including all the commands needed to reproduce your analysis from start to finish. Begin scripts by loading required packages, then reading data, then performing analyses. Add comments (lines beginning with #) to explain what your code does and why.\n\n\nCode\n# Analysis of hydrogel mechanical properties\n# Author: Your Name\n# Date: 2025-04-01\n\n# Load required packages\nlibrary(tidyverse)\n\n# Read data\ndata &lt;- read.csv(\"hydrogel_data.csv\")\n\n# Calculate summary statistics\nsummary(data)\n\n# Create visualization\nggplot(data, aes(x = concentration, y = compression)) +\n  geom_boxplot()",
    "crumbs": [
      "Foundations of Data Science",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>R and RStudio</span>"
    ]
  },
  {
    "objectID": "chapters/03-r-rstudio.html#getting-help",
    "href": "chapters/03-r-rstudio.html#getting-help",
    "title": "3  R and RStudio",
    "section": "3.15 Getting Help",
    "text": "3.15 Getting Help\nWhen you encounter problems, R provides several resources. The ? operator opens documentation for functions. The help.search() function searches the help system for topics. The example() function runs examples from a function’s documentation.\n\n\nCode\n?mean\nhelp.search(\"regression\")\nexample(plot)\n\n\nBeyond R’s built-in help, the internet offers vast resources. Stack Overflow has answers to almost any R question you can imagine. Package vignettes provide tutorials for specific packages. The RStudio community forums are welcoming to beginners.\nWhen asking for help online, provide a minimal reproducible example—the smallest piece of code that demonstrates your problem, including sample data. This makes it much easier for others to understand and solve your issue.",
    "crumbs": [
      "Foundations of Data Science",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>R and RStudio</span>"
    ]
  },
  {
    "objectID": "chapters/03-r-rstudio.html#data-types-in-r",
    "href": "chapters/03-r-rstudio.html#data-types-in-r",
    "title": "3  R and RStudio",
    "section": "3.16 Data Types in R",
    "text": "3.16 Data Types in R\nR has several fundamental data types that you will work with frequently.\n\n3.16.1 Character Strings\nAssignments and operations can be performed on characters as well as numbers. Characters need to be set off by quotation marks to differentiate them from numeric objects or variable names.\n\n\nCode\nx &lt;- \"I Love\"\nprint(x)\n\n\n[1] \"I Love\"\n\n\nCode\ny &lt;- \"Biostatistics\"\nprint(y)\n\n\n[1] \"Biostatistics\"\n\n\nCode\n# Combine strings using c()\nz &lt;- c(x, y)\nprint(z)\n\n\n[1] \"I Love\"        \"Biostatistics\"\n\n\nThe variable z is now a vector of character objects. Note that we are overwriting our previous numeric assignments—a good general rule is to use descriptive, unique names for each variable.\n\n\n3.16.2 Factors\nSometimes we would like to treat character objects as if they were categorical units for subsequent calculations. These are called factors, and we can convert a character vector to factor class.\n\n\nCode\nz_factor &lt;- as.factor(z)\nprint(z_factor)\n\n\n[1] I Love        Biostatistics\nLevels: Biostatistics I Love\n\n\nCode\nclass(z_factor)\n\n\n[1] \"factor\"\n\n\nNote that factor levels are reported alphabetically. The class() function tells us what type of object we are working with—it is one of the most important diagnostic tools in R. Often you can debug your code simply by checking and changing the class of an object.\nFactors are especially important for statistical analyses where we might want to calculate the mean or variance for different experimental treatments. In that case, the treatments would be coded as different levels of a factor.\n\n\n3.16.3 Missing Values (NA)\nR uses special values to represent missing or undefined data. The most common is NA, which stands for “Not Available.”\n\n\nCode\nclass(NA)\n\n\n[1] \"logical\"\n\n\nNA is a logical data type and is distinct from the character string “NA”, the numeric 0, or an empty string. It is also a reserved word and cannot be used as a variable name.\nAny instance of a blank entry in your data file will be read into R as NA. Many functions in R will not work by default if passed any NA values:\n\n\nCode\nnum &lt;- c(0, 1, 2, NA, 4)\nmean(num)\n\n\n[1] NA\n\n\nCode\n# Use na.rm = TRUE to ignore missing values\nmean(num, na.rm = TRUE)\n\n\n[1] 1.75\n\n\nCode\n# Check for missing values\nis.na(num)\n\n\n[1] FALSE FALSE FALSE  TRUE FALSE\n\n\n\n\n3.16.4 Floating-Point Precision\nA common source of confusion involves floating-point arithmetic. Computers represent decimal numbers with limited precision, which can lead to unexpected results:\n\n\nCode\n# This seems wrong, but is due to how computers store decimals\n0.1 + 0.2 == 0.3\n\n\n[1] FALSE\n\n\nCode\n# The actual values differ slightly\nprint(0.1 + 0.2, digits = 20)\n\n\n[1] 0.30000000000000004441\n\n\nCode\nprint(0.3, digits = 20)\n\n\n[1] 0.2999999999999999889\n\n\nNever use == to compare floating-point numbers directly. Instead, use all.equal() which checks if values are “nearly equal” within a small tolerance:\n\n\nCode\n# Safe comparison for floating-point numbers\nall.equal(0.1 + 0.2, 0.3)\n\n\n[1] TRUE\n\n\nCode\n# Use isTRUE() if you need a logical result\nisTRUE(all.equal(0.1 + 0.2, 0.3))\n\n\n[1] TRUE\n\n\nThe tidyverse provides dplyr::near() as a convenient alternative, especially when filtering data frames:\n\n\nCode\n# Works well in filter operations\nlibrary(dplyr)\ndata |&gt; filter(near(value, target_value))\n\n\n\n\n\n\n\n\nFloating-Point Comparisons\n\n\n\nAlways use all.equal() or near() instead of == when comparing decimal calculations. This is a common source of bugs in data analysis code.",
    "crumbs": [
      "Foundations of Data Science",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>R and RStudio</span>"
    ]
  },
  {
    "objectID": "chapters/03-r-rstudio.html#more-on-vectors",
    "href": "chapters/03-r-rstudio.html#more-on-vectors",
    "title": "3  R and RStudio",
    "section": "3.17 More on Vectors",
    "text": "3.17 More on Vectors\n\n3.17.1 Indexing Vectors\nIsolating specific elements from vectors is called indexing. R uses 1-based indexing with square brackets [].\n\n\nCode\nx &lt;- c(10, 20, 30, 40, 50, 100, 200)\n\n# First element\nx[1]\n\n\n[1] 10\n\n\nCode\n# Third element\nx[3]\n\n\n[1] 30\n\n\nCode\n# Series of consecutive elements\nx[1:4]\n\n\n[1] 10 20 30 40\n\n\nCode\n# Last four elements\nx[4:7]\n\n\n[1]  40  50 100 200\n\n\nCode\n# Non-consecutive elements using c()\nx[c(1:3, 5)]\n\n\n[1] 10 20 30 50\n\n\nCode\n# All elements EXCEPT the first two\nx[-c(1:2)]\n\n\n[1]  30  40  50 100 200\n\n\n\n\n3.17.2 Useful Functions for Vectors\nFunctions that provide information about vectors:\n\nhead(): returns the first elements of an object\ntail(): returns the last elements of an object\nlength(): returns the number of elements in a vector\nclass(): returns the class of elements in a vector\n\nFunctions that modify or generate vectors:\n\nsort(): returns a sorted vector\nseq(): creates a sequence of values\nrep(): repeats values\n\n\n\nCode\nrep(1, 5)\n\n\n[1] 1 1 1 1 1\n\n\nCode\nrep(\"treatment\", 5)\n\n\n[1] \"treatment\" \"treatment\" \"treatment\" \"treatment\" \"treatment\"\n\n\nFunctions for random sampling:\n\nsample(): randomly selects elements from a vector\nrnorm(): draws values from a normal distribution\nrbinom(): draws values from a binomial distribution\nset.seed(): sets the random number generator seed for reproducibility\n\nFunctions to change data types:\n\nas.numeric(): converts to numeric class\nas.factor(): converts to factor class\nas.character(): converts to character class",
    "crumbs": [
      "Foundations of Data Science",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>R and RStudio</span>"
    ]
  },
  {
    "objectID": "chapters/03-r-rstudio.html#lists",
    "href": "chapters/03-r-rstudio.html#lists",
    "title": "3  R and RStudio",
    "section": "3.18 Lists",
    "text": "3.18 Lists\nLists in R are aggregates of different objects that can be mixed types and different lengths.\n\n\nCode\nvec1 &lt;- c(10, 20, 30, 40, 50, 100, 200)\nvec2 &lt;- c(\"happy\", \"sad\", \"grumpy\")\nvec3 &lt;- factor(c(\"high\", \"low\"))\n\nmylist &lt;- list(vec1, vec2, vec3)\nprint(mylist)\n\n\n[[1]]\n[1]  10  20  30  40  50 100 200\n\n[[2]]\n[1] \"happy\"  \"sad\"    \"grumpy\"\n\n[[3]]\n[1] high low \nLevels: high low\n\n\nCode\nclass(mylist)\n\n\n[1] \"list\"\n\n\nCode\nstr(mylist)\n\n\nList of 3\n $ : num [1:7] 10 20 30 40 50 100 200\n $ : chr [1:3] \"happy\" \"sad\" \"grumpy\"\n $ : Factor w/ 2 levels \"high\",\"low\": 1 2\n\n\nElements of lists are indexed with double square brackets [[]]. To access the second element of mylist:\n\n\nCode\nmylist[[2]]\n\n\n[1] \"happy\"  \"sad\"    \"grumpy\"\n\n\nCode\n# The second item of the second element\nmylist[[2]][2]\n\n\n[1] \"sad\"\n\n\nThe str() function (for “structure”) is extremely useful for understanding complex R objects.",
    "crumbs": [
      "Foundations of Data Science",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>R and RStudio</span>"
    ]
  },
  {
    "objectID": "chapters/03-r-rstudio.html#matrices",
    "href": "chapters/03-r-rstudio.html#matrices",
    "title": "3  R and RStudio",
    "section": "3.19 Matrices",
    "text": "3.19 Matrices\nMatrices in R are two-dimensional arrays where all elements must be the same type. They are indexed by [row, column].\n\n\nCode\n# Create a 3x3 matrix\nmatrix(1:9, nrow = 3, ncol = 3)\n\n\n     [,1] [,2] [,3]\n[1,]    1    4    7\n[2,]    2    5    8\n[3,]    3    6    9\n\n\nUseful matrix functions include:\n\ndim(): returns the dimensions (rows and columns)\nt(): transposes a matrix (swaps rows and columns)\ncbind(): combines columns\nrbind(): combines rows",
    "crumbs": [
      "Foundations of Data Science",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>R and RStudio</span>"
    ]
  },
  {
    "objectID": "chapters/03-r-rstudio.html#installing-and-using-packages",
    "href": "chapters/03-r-rstudio.html#installing-and-using-packages",
    "title": "3  R and RStudio",
    "section": "3.20 Installing and Using Packages",
    "text": "3.20 Installing and Using Packages\nBase R includes many useful functions, but the real power comes from packages—collections of functions contributed by the community. Packages are distributed via the Comprehensive R Archive Network (CRAN).\n\n\nCode\n# Install a package (only need to do once)\ninstall.packages(\"name_of_package\")\n\n# Check if package is installed\ninstalled.packages(\"name_of_package\")\n\n# Load package for use (needed each session)\nlibrary(name_of_package)\n\n\nNote that install.packages() requires the package name in quotation marks, while library() does not.\n\n3.20.1 Namespace Conflicts\nWhen you load multiple packages, function names can collide. If two packages define a function with the same name, the most recently loaded package “wins,” and its version masks the earlier one. R warns you when this happens:\n\n\nCode\nlibrary(dplyr)\n# Attaching package: 'dplyr'\n# The following objects are masked from 'package:stats':\n#     filter, lag\n\n\nThis message indicates that dplyr’s filter() and lag() functions are now masking the base R functions with those names. If you need the masked version, use the package prefix:\n\n\nCode\n# Use dplyr's filter (now the default after loading dplyr)\ndata |&gt; filter(x &gt; 5)\n\n# Explicitly use base R's filter\nstats::filter(x, method = \"convolution\")\n\n# You can use the prefix even without loading a package\nstringr::str_detect(text, \"pattern\")\n\n\nCommon conflicts occur between:\n\ndplyr::filter() and stats::filter()\ndplyr::lag() and stats::lag()\ndplyr::select() and MASS::select()\n\n\n\n\n\n\n\nAvoiding Conflicts\n\n\n\nThe :: notation explicitly specifies which package’s function to use. When writing scripts, it is good practice to use package::function() for functions that commonly conflict, making your code’s behavior explicit and predictable.",
    "crumbs": [
      "Foundations of Data Science",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>R and RStudio</span>"
    ]
  },
  {
    "objectID": "chapters/03-r-rstudio.html#the-split-apply-combine-approach",
    "href": "chapters/03-r-rstudio.html#the-split-apply-combine-approach",
    "title": "3  R and RStudio",
    "section": "3.21 The Split-Apply-Combine Approach",
    "text": "3.21 The Split-Apply-Combine Approach\nA common pattern in data analysis is to split data by groups, apply a function to each group, and combine the results. R provides several functions for this workflow.\n\n3.21.1 The replicate() Function\nRepeats an expression multiple times and collects the results:\n\n\nCode\n# Shuffle integers 1-10 five times\nreplicate(5, sample(1:10, size = 10, replace = FALSE))\n\n\n      [,1] [,2] [,3] [,4] [,5]\n [1,]    3    9    9    3    5\n [2,]    1    2   10    8    4\n [3,]    8    3    3    6    9\n [4,]    9    6    4    9    1\n [5,]   10    5    2    4   10\n [6,]    7    4    1    7    7\n [7,]    4    1    5    5    6\n [8,]    5   10    8   10    2\n [9,]    6    8    6    2    8\n[10,]    2    7    7    1    3\n\n\n\n\n3.21.2 The apply() Family\nThe apply() function applies a function to rows or columns of a matrix or data frame:\n\n\nCode\n# Create sample matrix\nm &lt;- matrix(1:12, nrow = 3, ncol = 4)\nm\n\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    4    7   10\n[2,]    2    5    8   11\n[3,]    3    6    9   12\n\n\nCode\n# Sum across rows (MARGIN = 1)\napply(m, 1, sum)\n\n\n[1] 22 26 30\n\n\nCode\n# Sum across columns (MARGIN = 2)\napply(m, 2, sum)\n\n\n[1]  6 15 24 33\n\n\n\n\n3.21.3 The tapply() Function\nApplies a function to subsets of a vector, grouped by a factor:\n\n\nCode\n# Find maximum petal length for each species\ntapply(iris$Petal.Length, iris$Species, max)\n\n\n    setosa versicolor  virginica \n       1.9        5.1        6.9 \n\n\n\n\n3.21.4 The aggregate() Function\nSummarizes multiple variables by groups:\n\n\nCode\n# Mean of each variable by species\naggregate(iris[, 1:4], by = list(Species = iris$Species), FUN = mean)\n\n\n     Species Sepal.Length Sepal.Width Petal.Length Petal.Width\n1     setosa        5.006       3.428        1.462       0.246\n2 versicolor        5.936       2.770        4.260       1.326\n3  virginica        6.588       2.974        5.552       2.026",
    "crumbs": [
      "Foundations of Data Science",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>R and RStudio</span>"
    ]
  },
  {
    "objectID": "chapters/03-r-rstudio.html#conditional-statements-with-ifelse",
    "href": "chapters/03-r-rstudio.html#conditional-statements-with-ifelse",
    "title": "3  R and RStudio",
    "section": "3.22 Conditional Statements with ifelse()",
    "text": "3.22 Conditional Statements with ifelse()\nThe ifelse() function provides vectorized conditional logic. The first argument is a logical test, the second is the value if TRUE, and the third is the value if FALSE.\n\n\nCode\n# Create a character vector\ntreatment &lt;- c(rep(\"treatment\", 5), rep(\"control\", 3),\n               rep(\"treatment\", 4), rep(\"control\", 6))\n\n# Assign colors based on treatment\ncolors &lt;- ifelse(treatment == \"treatment\", \"red\", \"blue\")\nprint(colors)\n\n\n [1] \"red\"  \"red\"  \"red\"  \"red\"  \"red\"  \"blue\" \"blue\" \"blue\" \"red\"  \"red\" \n[11] \"red\"  \"red\"  \"blue\" \"blue\" \"blue\" \"blue\" \"blue\" \"blue\"",
    "crumbs": [
      "Foundations of Data Science",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>R and RStudio</span>"
    ]
  },
  {
    "objectID": "chapters/03-r-rstudio.html#for-loops",
    "href": "chapters/03-r-rstudio.html#for-loops",
    "title": "3  R and RStudio",
    "section": "3.23 For Loops",
    "text": "3.23 For Loops\nFor loops iterate through a sequence, executing code for each value. However, R is vectorized, so many operations that would require loops in other languages can be done more efficiently without them.\nWhen loops are necessary, pre-allocate output objects for better performance:\n\n\nCode\n# Pre-allocate a numeric vector\nresults &lt;- numeric(5)\n\nfor (i in 1:5) {\n  results[i] &lt;- i^2\n}\nresults\n\n\n[1]  1  4  9 16 25\n\n\n\n\n\n\n\n\nAvoiding Loops\n\n\n\nBefore writing a loop, consider whether the task can be accomplished with vectorized operations or the apply family of functions. These approaches are often faster and more readable.",
    "crumbs": [
      "Foundations of Data Science",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>R and RStudio</span>"
    ]
  },
  {
    "objectID": "chapters/03-r-rstudio.html#more-on-plotting",
    "href": "chapters/03-r-rstudio.html#more-on-plotting",
    "title": "3  R and RStudio",
    "section": "3.24 More on Plotting",
    "text": "3.24 More on Plotting\n\n3.24.1 Customizing Plots with par()\nMany plotting parameters are controlled by the par() function. Understanding par() dramatically increases your plotting capabilities.\n\n\nCode\n# Create multiple panels\npar(mfrow = c(1, 2))  # 1 row, 2 columns\n\nseq_1 &lt;- seq(0, 10, by = 0.1)\nseq_2 &lt;- seq(10, 0, by = -0.1)\n\nplot(seq_1, xlab = \"Index\", ylab = \"Value\", type = \"p\", col = \"red\",\n     main = \"Increasing Sequence\")\nplot(seq_2, xlab = \"Index\", ylab = \"Value\", type = \"l\", col = \"blue\",\n     main = \"Decreasing Sequence\")\n\n\n\n\n\n\n\n\n\n\n\n3.24.2 Vectorized Graphical Parameters\nGraphical parameters like col, pch (point character), and cex (character expansion) are vectorized:\n\n\nCode\nseq_1 &lt;- seq(0, 10, by = 0.1)\nseq_2 &lt;- seq(10, 0, by = -0.1)\n\n# First 10 points blue, rest red\ncolors &lt;- c(rep(\"blue\", 10), rep(\"red\", 91))\n\nplot(seq_1, seq_2, xlab = \"Sequence 1\", ylab = \"Sequence 2\",\n     col = colors, pch = 19,\n     main = \"Two-Color Scatterplot\")\n\n\n\n\n\n\n\n\n\n\n\n3.24.3 Useful Plotting Arguments\nKey arguments for plot() and related functions:\n\nmain: plot title\nxlab, ylab: axis labels\nxlim, ylim: axis limits\ncol: color\npch: point character (0-25)\ncex: character/point size multiplier\nlwd: line width\ntype: “p” for points, “l” for lines, “b” for both",
    "crumbs": [
      "Foundations of Data Science",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>R and RStudio</span>"
    ]
  },
  {
    "objectID": "chapters/03-r-rstudio.html#introduction-to-r-markdown",
    "href": "chapters/03-r-rstudio.html#introduction-to-r-markdown",
    "title": "3  R and RStudio",
    "section": "3.25 Introduction to R Markdown",
    "text": "3.25 Introduction to R Markdown\nR Markdown combines R code with formatted text to create reproducible documents. Files have the .Rmd extension and can be rendered (“knitted”) to HTML, PDF, or Word.\n\n3.25.1 Getting Started\nInstall the rmarkdown package, then in RStudio: File → New File → R Markdown.\n\n\n3.25.2 Basic Formatting\n## Section Header\n### Subsection Header\n\nText can be *italicized* or **bolded** or ***both***.\n\nLinks: [Link Text](https://example.com)\n\n\n3.25.3 Code Chunks\nR code is placed in code chunks delimited by three backticks:\n```{r}\nseq(1, 10, 1)\n```\nChunk options control whether code is evaluated (eval), displayed (echo), and more:\n```{r, eval = TRUE, echo = TRUE}\nseq(1, 10, 1)\n```\n\n\n3.25.4 Knitting\nClick the “Knit” button in RStudio to render your document. Start with HTML output, which has the fewest dependencies.\n\n\n\n\n\n\nLearning More\n\n\n\nFor comprehensive R Markdown documentation, see the R Markdown introduction and R Markdown cheat sheet.",
    "crumbs": [
      "Foundations of Data Science",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>R and RStudio</span>"
    ]
  },
  {
    "objectID": "chapters/03-r-rstudio.html#practice-exercises",
    "href": "chapters/03-r-rstudio.html#practice-exercises",
    "title": "3  R and RStudio",
    "section": "3.26 Practice Exercises",
    "text": "3.26 Practice Exercises\nFor hands-on practice with the concepts covered in this chapter, see Section 35.2 in the Practice Exercises appendix. The exercises include:\n\nExploring the RStudio environment\nBasic mathematics and variable assignment\nWorking with vectors, factors, and data frames\nCreating visualizations\nData import and export",
    "crumbs": [
      "Foundations of Data Science",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>R and RStudio</span>"
    ]
  },
  {
    "objectID": "chapters/03-r-rstudio.html#additional-resources",
    "href": "chapters/03-r-rstudio.html#additional-resources",
    "title": "3  R and RStudio",
    "section": "3.27 Additional Resources",
    "text": "3.27 Additional Resources\n\nLogan (2010) - A comprehensive introduction to R for statistical analysis\nA Primer for Computational Biology - Free online textbook by S.T. O’Neil\nR Colors Reference - Visual guide to R colors\nIntroduction to Colors in R - Tutorial on using colors effectively\n\n\n\n\n\n\n\nLogan, Murray. 2010. Biostatistical Design and Analysis Using r. Wiley-Blackwell.",
    "crumbs": [
      "Foundations of Data Science",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>R and RStudio</span>"
    ]
  },
  {
    "objectID": "chapters/04-markdown-latex.html",
    "href": "chapters/04-markdown-latex.html",
    "title": "4  Markdown and LaTeX",
    "section": "",
    "text": "4.1 The Power of Plain Text\nScientific communication requires more than just words—we need formatted text, mathematical equations, code, figures, and tables. Traditionally, researchers used word processors for this purpose, but word processors have significant limitations for technical writing. They obscure the structure of documents behind visual formatting. They make collaboration difficult because different versions become hard to reconcile. They separate code from the documents that describe it, making it easy for analyses and their descriptions to get out of sync.\nMarkup languages offer a different approach. You write in plain text, adding simple annotations that specify how the document should be formatted. A processor then converts this annotated text into beautifully formatted output. Because the source is plain text, it can be version-controlled, compared across versions, and edited with any text editor.",
    "crumbs": [
      "Foundations of Data Science",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Markdown and LaTeX</span>"
    ]
  },
  {
    "objectID": "chapters/04-markdown-latex.html#what-is-markdown",
    "href": "chapters/04-markdown-latex.html#what-is-markdown",
    "title": "4  Markdown and LaTeX",
    "section": "4.2 What is Markdown?",
    "text": "4.2 What is Markdown?\nMarkdown is a lightweight markup language designed to be easy to read and write. Created by John Gruber in 2004, it uses intuitive syntax that looks reasonable even before processing. A document written in Markdown can be rendered into HTML, PDF, Word documents, presentations, websites, and more.\nThe basic philosophy is that common formatting should be quick to type and not interrupt the flow of writing. Bold text is wrapped in double asterisks. Italics use single asterisks. Headers are indicated by hash marks at the start of a line.\n\n4.2.1 Text Formatting\nTo make text italic, wrap it in single asterisks or underscores:\n*italic text* or _italic text_\nFor bold text, use double asterisks or underscores:\n**bold text** or __bold text__\nYou can combine them for bold italic:\n***bold and italic***\n\n\n4.2.2 Headers\nHeaders structure your document into sections. Use hash marks at the start of a line, with more hashes indicating lower-level headers:\n# First-Level Header\n## Second-Level Header\n### Third-Level Header\n\n\n4.2.3 Lists\nCreate bullet lists by starting lines with dashes, asterisks, or plus signs:\n- First item\n- Second item\n    - Sub-item (indent with spaces or tabs)\n    - Another sub-item\n- Third item\nFor numbered lists, use numbers followed by periods:\n1. First step\n2. Second step\n3. Third step\n\n\n4.2.4 Block Quotes\nBlock quotes are useful for highlighting important passages or attributing quotes:\n&gt; \"You know the greatest danger facing us is ourselves, \n&gt; an irrational fear of the unknown. But there's no such \n&gt; thing as the unknown — only things temporarily hidden, \n&gt; temporarily not understood.\"\n&gt;\n&gt; --- Captain James T. Kirk\nThis renders as:\n\n“You know the greatest danger facing us is ourselves, an irrational fear of the unknown. But there’s no such thing as the unknown — only things temporarily hidden, temporarily not understood.”\n— Captain James T. Kirk\n\n\n\n4.2.5 Links and Images\nLinks use square brackets for the text and parentheses for the URL:\n[Link text](https://www.example.com)\nImages use the same syntax with an exclamation mark prefix:\n![Image caption](path/to/image.png)\n\n\n4.2.6 Code\nInline code is wrapped in single backticks: `code`. Code blocks use triple backticks, optionally specifying the language for syntax highlighting:\n```r\nx &lt;- rnorm(100)\nmean(x)\n```",
    "crumbs": [
      "Foundations of Data Science",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Markdown and LaTeX</span>"
    ]
  },
  {
    "objectID": "chapters/04-markdown-latex.html#what-is-latex",
    "href": "chapters/04-markdown-latex.html#what-is-latex",
    "title": "4  Markdown and LaTeX",
    "section": "4.3 What is LaTeX?",
    "text": "4.3 What is LaTeX?\nLaTeX (pronounced “LAH-tek” or “LAY-tek”) is a document preparation system for high-quality typesetting, particularly of technical and scientific documents. Originally created by Leslie Lamport in the 1980s, LaTeX builds on the TeX typesetting system developed by Donald Knuth.\nLaTeX excels at mathematical notation. Complex equations that would be tedious or impossible to create in a word processor can be expressed elegantly in LaTeX. The system handles numbering, cross-references, bibliographies, and other scholarly apparatus automatically.\nMost importantly for our purposes, LaTeX can be embedded directly in Markdown documents. This gives us the best of both worlds: the simplicity of Markdown for prose and the power of LaTeX for mathematics.",
    "crumbs": [
      "Foundations of Data Science",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Markdown and LaTeX</span>"
    ]
  },
  {
    "objectID": "chapters/04-markdown-latex.html#mathematical-notation-with-latex",
    "href": "chapters/04-markdown-latex.html#mathematical-notation-with-latex",
    "title": "4  Markdown and LaTeX",
    "section": "4.4 Mathematical Notation with LaTeX",
    "text": "4.4 Mathematical Notation with LaTeX\n\n4.4.1 Inline and Display Math\nMathematical expressions can appear inline with text or as centered display equations. Inline math is wrapped in single dollar signs: $x^2$ produces \\(x^2\\). Display equations use double dollar signs:\n$$E = mc^2$$\nProduces:\n\\[E = mc^2\\]\n\n\n4.4.2 Greek Letters and Symbols\nGreek letters are typed as their names preceded by a backslash:\n$$\\alpha, \\beta, \\gamma, \\delta, \\epsilon$$\n$$\\mu, \\sigma, \\pi, \\theta, \\lambda$$\n\\[\\alpha, \\beta, \\gamma, \\delta, \\epsilon\\] \\[\\mu, \\sigma, \\pi, \\theta, \\lambda\\]\nCommon mathematical symbols:\n$$\\neq, \\approx, \\leq, \\geq, \\pm, \\times, \\div$$\n\\[\\neq, \\approx, \\leq, \\geq, \\pm, \\times, \\div\\]\n\n\n4.4.3 Superscripts and Subscripts\nSuperscripts use the caret ^ and subscripts use the underscore _:\n$$x^2, x_i, x_i^2, x_{ij}$$\n\\[x^2, x_i, x_i^2, x_{ij}\\]\nFor multi-character superscripts or subscripts, use curly braces:\n$$x^{n+1}, x_{i,j}$$\n\\[x^{n+1}, x_{i,j}\\]\n\n\n4.4.4 Fractions\nFractions use the \\frac{numerator}{denominator} command:\n$$\\frac{a}{b}, \\frac{x^2 + 1}{x - 1}$$\n\\[\\frac{a}{b}, \\frac{x^2 + 1}{x - 1}\\]\n\n\n4.4.5 Square Roots\nSquare roots and nth roots:\n$$\\sqrt{x}, \\sqrt[3]{x}, \\sqrt[n]{x}$$\n\\[\\sqrt{x}, \\sqrt[3]{x}, \\sqrt[n]{x}\\]\n\n\n4.4.6 Summation and Products\nSums and products with limits:\n$$\\sum_{i=1}^{n} x_i, \\prod_{i=1}^{n} x_i$$\n\\[\\sum_{i=1}^{n} x_i, \\prod_{i=1}^{n} x_i\\]\n\n\n4.4.7 Integrals\nIntegrals with limits:\n$$\\int_{a}^{b} f(x) \\, dx$$\n$$\\iint f(x,y) \\, dx \\, dy$$\n\\[\\int_{a}^{b} f(x) \\, dx\\] \\[\\iint f(x,y) \\, dx \\, dy\\]\n\n\n4.4.8 Statistical Formulas\nHere are some common statistical formulas in LaTeX:\nThe sample mean:\n$$\\bar{x} = \\frac{1}{n}\\sum_{i=1}^{n} x_i$$\n\\[\\bar{x} = \\frac{1}{n}\\sum_{i=1}^{n} x_i\\]\nThe sample variance:\n$$s^2 = \\frac{1}{n-1}\\sum_{i=1}^{n} (x_i - \\bar{x})^2$$\n\\[s^2 = \\frac{1}{n-1}\\sum_{i=1}^{n} (x_i - \\bar{x})^2\\]\nThe binomial probability formula:\n$$P(X=k) = \\binom{n}{k} p^{k} (1-p)^{n-k}$$\n\\[P(X=k) = \\binom{n}{k} p^{k} (1-p)^{n-k}\\]\nThe Poisson probability formula:\n$$P(Y=r) = \\frac{e^{-\\mu}\\mu^r}{r!}$$\n\\[P(Y=r) = \\frac{e^{-\\mu}\\mu^r}{r!}\\]\nThe normal distribution density:\n$$f(x) = \\frac{1}{\\sigma\\sqrt{2\\pi}} e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}$$\n\\[f(x) = \\frac{1}{\\sigma\\sqrt{2\\pi}} e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}\\]\n\n\n4.4.9 Matrices\nMatrices are created with the matrix environment:\n$$\\begin{matrix}\na & b \\\\\nc & d\n\\end{matrix}$$\n\\[\\begin{matrix}\na & b \\\\\nc & d\n\\end{matrix}\\]\nFor brackets around the matrix, use pmatrix (parentheses) or bmatrix (square brackets):\n$$\\begin{pmatrix}\na & b \\\\\nc & d\n\\end{pmatrix}$$\n\\[\\begin{pmatrix}\na & b \\\\\nc & d\n\\end{pmatrix}\\]",
    "crumbs": [
      "Foundations of Data Science",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Markdown and LaTeX</span>"
    ]
  },
  {
    "objectID": "chapters/04-markdown-latex.html#quarto-and-r-markdown",
    "href": "chapters/04-markdown-latex.html#quarto-and-r-markdown",
    "title": "4  Markdown and LaTeX",
    "section": "4.5 Quarto and R Markdown",
    "text": "4.5 Quarto and R Markdown\nQuarto and R Markdown extend Markdown by allowing you to embed executable code chunks. When the document is rendered, the code runs and its output—whether text, tables, or figures—is automatically included in the final document.\n\n4.5.1 Document Structure\nA Quarto document has three main parts:\n\nYAML Header: Document metadata and settings (enclosed by ---)\nMarkdown Content: Text and formatting\nCode Chunks: Executable code blocks\n\n\n\n4.5.2 YAML Header\nThe YAML header appears at the top of the document and configures how it will be rendered:\n---\ntitle: \"My Analysis Report\"\nauthor: \"Your Name\"\ndate: today\nformat: html\n---\nCommon YAML options include:\n\n\n\nOption\nDescription\n\n\n\n\ntitle\nDocument title\n\n\nauthor\nAuthor name(s)\n\n\ndate\nPublication date (today for current date)\n\n\nformat\nOutput format: html, pdf, docx\n\n\ntoc\nInclude table of contents (true/false)\n\n\ncode-fold\nCollapse code by default\n\n\nexecute\nGlobal code execution options\n\n\n\nA more complete header might look like:\n---\ntitle: \"Genomic Analysis Report\"\nauthor: \"Jane Scientist\"\ndate: today\nformat:\n  html:\n    theme: cosmo\n    toc: true\n    code-fold: true\nexecute:\n  echo: true\n  warning: false\n---\n\n\n4.5.3 Code Chunks\nA code chunk in Quarto looks like this:\n```{r}\nx &lt;- rnorm(100)\nmean(x)\n```\nWhen rendered, this shows both the code and its output:\n\n\nCode\nx &lt;- rnorm(100)\nmean(x)\n\n\n[1] -0.03479557\n\n\n\n\n4.5.4 Chunk Options\nChunk options control how code is displayed and executed. In Quarto, options are specified with #| comments at the start of the chunk:\n```{r}\n#| label: my-analysis\n#| echo: false\n#| fig-width: 6\n#| fig-height: 4\nhist(rnorm(1000))\n```\nEssential chunk options:\n\n\n\nOption\nDescription\nDefault\n\n\n\n\necho\nShow code in output\ntrue\n\n\neval\nExecute the code\ntrue\n\n\ninclude\nInclude chunk in output\ntrue\n\n\nwarning\nShow warnings\ntrue\n\n\nmessage\nShow messages\ntrue\n\n\nfig-cap\nFigure caption\nnone\n\n\nfig-width\nFigure width (inches)\n7\n\n\nfig-height\nFigure height (inches)\n5\n\n\n\n\n\n4.5.5 Callout Blocks\nQuarto provides special callout blocks for highlighting information:\n::: {.callout-note}\nThis is a note with additional information.\n:::\n\n::: {.callout-warning}\nThis warns readers about potential issues.\n:::\n\n::: {.callout-tip}\nThis provides helpful tips.\n:::\nThese render as visually distinct boxes that draw attention to important content.\n\n\n\n\n\n\nNote\n\n\n\nThis is an example of a callout note—useful for supplementary information.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nThis is a warning callout—use for potential pitfalls or important cautions.\n\n\n\n\n4.5.6 Cross-References\nQuarto supports cross-references to figures, tables, and sections. Label items and reference them with @:\n```{r}\n#| label: fig-scatter\n#| fig-cap: \"Relationship between x and y\"\nplot(x, y)\n```\n\nSee @fig-scatter for the scatter plot.\nFor sections, add a label after the header:\n## Methods {#sec-methods}\n\nAs described in @sec-methods, we used...\n\n\n4.5.7 Rendering Documents\nTo render a Quarto document:\n\nIn RStudio: Click the “Render” button or press Ctrl+Shift+K (Windows/Linux) or Cmd+Shift+K (Mac)\nCommand line: Run quarto render document.qmd\n\nYou can render to multiple formats:\nquarto render document.qmd --to html\nquarto render document.qmd --to pdf\nquarto render document.qmd --to docx",
    "crumbs": [
      "Foundations of Data Science",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Markdown and LaTeX</span>"
    ]
  },
  {
    "objectID": "chapters/04-markdown-latex.html#why-this-matters",
    "href": "chapters/04-markdown-latex.html#why-this-matters",
    "title": "4  Markdown and LaTeX",
    "section": "4.6 Why This Matters",
    "text": "4.6 Why This Matters\nThe combination of Markdown, LaTeX, and executable code enables truly reproducible research. Your analysis code lives in the same document as your prose. When data change, you re-render the document and everything updates automatically. Collaborators can see exactly what you did. Future you can understand past you’s work.\nThese tools have become standard in data science. Learning them now will pay dividends throughout your career, whether you are writing homework assignments, thesis chapters, journal articles, or technical reports.",
    "crumbs": [
      "Foundations of Data Science",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Markdown and LaTeX</span>"
    ]
  },
  {
    "objectID": "chapters/04-markdown-latex.html#practice-exercises",
    "href": "chapters/04-markdown-latex.html#practice-exercises",
    "title": "4  Markdown and LaTeX",
    "section": "4.7 Practice Exercises",
    "text": "4.7 Practice Exercises\nFor hands-on practice with Markdown and LaTeX, see Section 35.3 in the Practice Exercises appendix. The exercises include:\n\nCreating and formatting R Markdown documents\nWorking with code chunk options\nWriting mathematical equations in LaTeX\nCreating tables and formatted output",
    "crumbs": [
      "Foundations of Data Science",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Markdown and LaTeX</span>"
    ]
  },
  {
    "objectID": "chapters/05-tidy-data.html",
    "href": "chapters/05-tidy-data.html",
    "title": "5  Tidy Data and Data Wrangling",
    "section": "",
    "text": "5.1 What is Tidy Data?\nData comes in many shapes, and not all shapes are equally convenient for analysis. The concept of “tidy data” provides a standard way to organize data that works well with R and makes many analyses straightforward. In tidy data, each variable forms a column, each observation forms a row, and each type of observational unit forms a table.\nThis structure might seem obvious, but real-world data rarely arrives in tidy form. Spreadsheets often encode information in column names, spread a single variable across multiple columns, or combine multiple variables in a single column. Data wrangling is the process of transforming messy data into tidy data.",
    "crumbs": [
      "Foundations of Data Science",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Tidy Data and Data Wrangling</span>"
    ]
  },
  {
    "objectID": "chapters/05-tidy-data.html#rules-of-thumb-for-data-organization",
    "href": "chapters/05-tidy-data.html#rules-of-thumb-for-data-organization",
    "title": "5  Tidy Data and Data Wrangling",
    "section": "5.2 Rules of Thumb for Data Organization",
    "text": "5.2 Rules of Thumb for Data Organization\nWhether you are creating a new dataset or cleaning an existing one, following these principles will save you time and frustration.\nStore a copy of your data in nonproprietary formats like plain text CSV files. Proprietary formats can become unreadable as software changes. Keep an uncorrected copy of your original data separate from any cleaned or processed versions. Use descriptive names for files and variables that convey meaning without requiring external documentation. Include a header row with variable names. Maintain metadata—a data dictionary explaining what each variable means, how it was measured, and what units it uses.\nWhen you add new observations, add rows. When you add new variables, add columns. A column should contain only one data type—don’t mix numbers and text in the same column. Dates should be in a consistent format. Missing values should be represented consistently, typically as empty cells or NA in R.",
    "crumbs": [
      "Foundations of Data Science",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Tidy Data and Data Wrangling</span>"
    ]
  },
  {
    "objectID": "chapters/05-tidy-data.html#types-of-data",
    "href": "chapters/05-tidy-data.html#types-of-data",
    "title": "5  Tidy Data and Data Wrangling",
    "section": "5.3 Types of Data",
    "text": "5.3 Types of Data\nUnderstanding the types of data you are working with guides how you analyze them.\nCategorical data classify observations into groups. Nominal categorical data have no inherent order—for example, species names, treatment groups, or colors. Ordinal categorical data have a meaningful order—ratings like “low,” “medium,” and “high,” or educational levels. In R, categorical data are often represented as factors, which store both the values and the set of possible levels.\nQuantitative data are numerical measurements. Interval data have meaningful differences between values but no true zero point—temperature in Celsius, where 0° does not mean “no temperature.” Ratio data have a true zero and meaningful ratios—mass, length, counts, where zero means “none” and twice as much is twice as much.\n\n\n\n\n\n\n\n\n\nCategorical\n\nQuantitative\n\n\n\n\n\nOrdinal\nNominal\nRatio\nInterval\n\n\nsmall, medium, large\napples, oranges\nkilograms, dollars\ntemperature, calendar year\n\n\nordered character\ncharacter\nnumeric\ninteger",
    "crumbs": [
      "Foundations of Data Science",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Tidy Data and Data Wrangling</span>"
    ]
  },
  {
    "objectID": "chapters/05-tidy-data.html#the-tidyverse",
    "href": "chapters/05-tidy-data.html#the-tidyverse",
    "title": "5  Tidy Data and Data Wrangling",
    "section": "5.4 The Tidyverse",
    "text": "5.4 The Tidyverse\nThe tidyverse is a collection of R packages designed for data science. These packages share a common philosophy and are designed to work together seamlessly. The core tidyverse packages include ggplot2 for visualization, dplyr for data manipulation, tidyr for reshaping data, readr for reading data files, and several others.\n\n\nCode\nlibrary(tidyverse)\n\n\nLoading the tidyverse loads all core packages at once. The message shows which packages are attached and notes any functions that conflict with base R or other packages.",
    "crumbs": [
      "Foundations of Data Science",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Tidy Data and Data Wrangling</span>"
    ]
  },
  {
    "objectID": "chapters/05-tidy-data.html#tibbles",
    "href": "chapters/05-tidy-data.html#tibbles",
    "title": "5  Tidy Data and Data Wrangling",
    "section": "5.5 Tibbles",
    "text": "5.5 Tibbles\nTibbles are the tidyverse’s enhanced data frames. They print more informatively, showing only the first few rows and as many columns as fit on screen, along with the dimensions and column types.\n\n\n\n\n\n\n\nCode\n# Create a tibble\nmy_tibble &lt;- tibble(\n  name = c(\"Alice\", \"Bob\", \"Carol\"),\n  score = c(85, 92, 78),\n  passed = c(TRUE, TRUE, TRUE)\n)\nmy_tibble\n\n\n# A tibble: 3 × 3\n  name  score passed\n  &lt;chr&gt; &lt;dbl&gt; &lt;lgl&gt; \n1 Alice    85 TRUE  \n2 Bob      92 TRUE  \n3 Carol    78 TRUE  \n\n\nThe column types are shown below the column names: &lt;chr&gt; for character, &lt;dbl&gt; for double (numeric), and &lt;lgl&gt; for logical.",
    "crumbs": [
      "Foundations of Data Science",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Tidy Data and Data Wrangling</span>"
    ]
  },
  {
    "objectID": "chapters/05-tidy-data.html#key-dplyr-verbs",
    "href": "chapters/05-tidy-data.html#key-dplyr-verbs",
    "title": "5  Tidy Data and Data Wrangling",
    "section": "5.6 Key dplyr Verbs",
    "text": "5.6 Key dplyr Verbs\nThe dplyr package provides a grammar for data manipulation. Five key verbs handle most data manipulation tasks.\n\n5.6.1 filter(): Subset Rows\nfilter() selects rows that meet specified conditions.\n\n\nCode\n# Flights in November or December\nfilter(flights, month == 11 | month == 12)\n\n# Flights with arrival delay greater than 2 hours\nfilter(flights, arr_delay &gt; 120)\n\n\nConditions use comparison operators: == (equals), != (not equals), &lt;, &gt;, &lt;=, &gt;=. Combine conditions with & (and) and | (or). The %in% operator checks membership in a set.\n\n\n5.6.2 select(): Choose Columns\nselect() picks columns by name.\n\n\nCode\n# Select specific columns\nselect(flights, year, month, day)\n\n# Select a range of columns\nselect(flights, year:day)\n\n# Drop columns\nselect(flights, -year, -month)\n\n\n\n\n5.6.3 arrange(): Sort Rows\narrange() reorders rows by column values.\n\n\nCode\n# Sort by year, then month, then day\narrange(flights, year, month, day)\n\n# Sort in descending order\narrange(flights, desc(dep_delay))\n\n\n\n\n5.6.4 mutate(): Create New Columns\nmutate() adds new columns that are functions of existing columns.\n\n\nCode\nmutate(flights,\n  gain = arr_delay - dep_delay,\n  hours = air_time / 60,\n  gain_per_hour = gain / hours\n)\n\n\n\n\n5.6.5 summarize(): Aggregate Data\nsummarize() collapses multiple rows into summary values.\n\n\nCode\nsummarize(flights, \n  mean_delay = mean(dep_delay, na.rm = TRUE),\n  n = n()\n)\n\n\nThe na.rm = TRUE argument tells mean() to ignore missing values. The n() function counts rows.",
    "crumbs": [
      "Foundations of Data Science",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Tidy Data and Data Wrangling</span>"
    ]
  },
  {
    "objectID": "chapters/05-tidy-data.html#grouping-with-group_by",
    "href": "chapters/05-tidy-data.html#grouping-with-group_by",
    "title": "5  Tidy Data and Data Wrangling",
    "section": "5.7 Grouping with group_by()",
    "text": "5.7 Grouping with group_by()\nThe real power of summarize() emerges when combined with group_by(), which splits data into groups for separate analysis.\n\n\nCode\n# Group by destination, then summarize\nby_dest &lt;- group_by(flights, dest)\nsummarize(by_dest, \n  count = n(),\n  mean_delay = mean(arr_delay, na.rm = TRUE)\n)\n\n\nThis calculates summary statistics separately for each destination.",
    "crumbs": [
      "Foundations of Data Science",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Tidy Data and Data Wrangling</span>"
    ]
  },
  {
    "objectID": "chapters/05-tidy-data.html#the-pipe-operator",
    "href": "chapters/05-tidy-data.html#the-pipe-operator",
    "title": "5  Tidy Data and Data Wrangling",
    "section": "5.8 The Pipe Operator",
    "text": "5.8 The Pipe Operator\nChaining multiple operations together can become unwieldy with nested function calls. The pipe operator |&gt; (or the tidyverse’s %&gt;%) passes the result of one operation as the first argument of the next, allowing you to read operations left-to-right, top-to-bottom.\n\n\nCode\n# Without pipe: nested and hard to read\nsummarize(group_by(filter(flights, !is.na(arr_delay)), dest), \n          mean_delay = mean(arr_delay))\n\n# With pipe: clear sequence of operations\nflights |&gt;\n  filter(!is.na(arr_delay)) |&gt;\n  group_by(dest) |&gt;\n  summarize(mean_delay = mean(arr_delay))\n\n\nRead the pipe as “then”—take flights, then filter, then group, then summarize.",
    "crumbs": [
      "Foundations of Data Science",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Tidy Data and Data Wrangling</span>"
    ]
  },
  {
    "objectID": "chapters/05-tidy-data.html#handling-missing-values",
    "href": "chapters/05-tidy-data.html#handling-missing-values",
    "title": "5  Tidy Data and Data Wrangling",
    "section": "5.9 Handling Missing Values",
    "text": "5.9 Handling Missing Values\nMissing values are a fact of life in real data. In R, missing values are represented as NA. Most operations involving NA return NA, which can cause problems if you are not careful.\n\n\nCode\nx &lt;- c(1, 2, NA, 4)\nmean(x)\n\n\n[1] NA\n\n\nCode\nmean(x, na.rm = TRUE)\n\n\n[1] 2.333333\n\n\nCheck for missing values with is.na():\n\n\nCode\nis.na(x)\n\n\n[1] FALSE FALSE  TRUE FALSE\n\n\nCode\nsum(is.na(x))  # count missing values\n\n\n[1] 1\n\n\nFilter out missing values:\n\n\nCode\nx[!is.na(x)]\n\n\n[1] 1 2 4\n\n\nOr use tidyr functions:\n\n\nCode\n# Remove rows with any missing values\ndrop_na(data)\n\n# Remove rows with missing values in specific columns\ndrop_na(data, column_name)",
    "crumbs": [
      "Foundations of Data Science",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Tidy Data and Data Wrangling</span>"
    ]
  },
  {
    "objectID": "chapters/05-tidy-data.html#reshaping-data",
    "href": "chapters/05-tidy-data.html#reshaping-data",
    "title": "5  Tidy Data and Data Wrangling",
    "section": "5.10 Reshaping Data",
    "text": "5.10 Reshaping Data\nSometimes data is not in the right shape for your analysis. The tidyr package provides functions to reshape data.\npivot_longer() takes wide data (variables spread across columns) and makes it long (variables stacked in rows). pivot_wider() does the reverse.\n\n\nCode\n# Example: wide data\nwide_data &lt;- tibble(\n  sample = c(\"A\", \"B\", \"C\"),\n  treatment_1 = c(10, 15, 12),\n  treatment_2 = c(8, 14, 11)\n)\nwide_data\n\n\n# A tibble: 3 × 3\n  sample treatment_1 treatment_2\n  &lt;chr&gt;        &lt;dbl&gt;       &lt;dbl&gt;\n1 A               10           8\n2 B               15          14\n3 C               12          11\n\n\nCode\n# Convert to long format\nlong_data &lt;- wide_data |&gt;\n  pivot_longer(\n    cols = starts_with(\"treatment\"),\n    names_to = \"treatment\",\n    values_to = \"response\"\n  )\nlong_data\n\n\n# A tibble: 6 × 3\n  sample treatment   response\n  &lt;chr&gt;  &lt;chr&gt;          &lt;dbl&gt;\n1 A      treatment_1       10\n2 A      treatment_2        8\n3 B      treatment_1       15\n4 B      treatment_2       14\n5 C      treatment_1       12\n6 C      treatment_2       11",
    "crumbs": [
      "Foundations of Data Science",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Tidy Data and Data Wrangling</span>"
    ]
  },
  {
    "objectID": "chapters/05-tidy-data.html#joining-data",
    "href": "chapters/05-tidy-data.html#joining-data",
    "title": "5  Tidy Data and Data Wrangling",
    "section": "5.11 Joining Data",
    "text": "5.11 Joining Data\nOften data comes in multiple tables that need to be combined. Join operations merge tables based on matching values in key columns.\n\n\nCode\n# Example tables\nsamples &lt;- tibble(\n  sample_id = c(\"S1\", \"S2\", \"S3\"),\n  concentration = c(0.1, 0.5, 1.0)\n)\n\nmeasurements &lt;- tibble(\n  sample_id = c(\"S1\", \"S1\", \"S2\", \"S2\", \"S3\", \"S3\"),\n  replicate = c(1, 2, 1, 2, 1, 2),\n  value = c(2.3, 2.1, 5.4, 5.6, 10.2, 10.8)\n)\n\n# Join tables\nleft_join(measurements, samples, by = \"sample_id\")\n\n\n# A tibble: 6 × 4\n  sample_id replicate value concentration\n  &lt;chr&gt;         &lt;dbl&gt; &lt;dbl&gt;         &lt;dbl&gt;\n1 S1                1   2.3           0.1\n2 S1                2   2.1           0.1\n3 S2                1   5.4           0.5\n4 S2                2   5.6           0.5\n5 S3                1  10.2           1  \n6 S3                2  10.8           1  \n\n\n\n5.11.1 Types of Joins\nDifferent joins handle non-matching rows differently. Understanding when to use each type is important for correct data analysis:\n\n\n\n\n\n\n\nJoin Type\nResult\n\n\n\n\nleft_join()\nKeep all rows from left table, add matching data from right\n\n\nright_join()\nKeep all rows from right table, add matching data from left\n\n\ninner_join()\nKeep only rows with matches in both tables\n\n\nfull_join()\nKeep all rows from both tables\n\n\nsemi_join()\nKeep rows from left table that have matches in right\n\n\nanti_join()\nKeep rows from left table with NO matches in right\n\n\n\nThe anti_join() is particularly useful for finding data quality issues—rows that should have matches but don’t:\n\n\nCode\n# Which measurements have no sample information?\nanti_join(measurements, samples, by = \"sample_id\")",
    "crumbs": [
      "Foundations of Data Science",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Tidy Data and Data Wrangling</span>"
    ]
  },
  {
    "objectID": "chapters/05-tidy-data.html#additional-dplyr-functions",
    "href": "chapters/05-tidy-data.html#additional-dplyr-functions",
    "title": "5  Tidy Data and Data Wrangling",
    "section": "5.12 Additional dplyr Functions",
    "text": "5.12 Additional dplyr Functions\nBeyond the five core verbs, dplyr provides many useful functions for common data manipulation tasks.\n\n5.12.1 Conditional Logic with case_when()\nThe case_when() function is a vectorized if-else that handles multiple conditions:\n\n\nCode\n# Create sample data\nexpression_data &lt;- tibble(\n  gene = c(\"gene_A\", \"gene_B\", \"gene_C\", \"gene_D\"),\n  fold_change = c(0.5, 1.2, 3.5, -2.1)\n)\n\nexpression_data |&gt;\n  mutate(\n    regulation = case_when(\n      fold_change &gt; 2 ~ \"strongly up\",\n      fold_change &gt; 1 ~ \"up\",\n      fold_change &lt; -1 ~ \"down\",\n      TRUE ~ \"unchanged\"  # default case\n    )\n  )\n\n\n# A tibble: 4 × 3\n  gene   fold_change regulation \n  &lt;chr&gt;        &lt;dbl&gt; &lt;chr&gt;      \n1 gene_A         0.5 unchanged  \n2 gene_B         1.2 up         \n3 gene_C         3.5 strongly up\n4 gene_D        -2.1 down       \n\n\n\n\n5.12.2 Counting with count() and n_distinct()\nThe count() function is a shortcut for grouping and counting:\n\n\nCode\n# Equivalent to: group_by(x) |&gt; summarize(n = n())\ndata |&gt; count(treatment)\n\n# Sort by count\ndata |&gt; count(treatment, sort = TRUE)\n\n\nUse n_distinct() inside summarize() to count unique values:\n\n\nCode\n# Count unique samples per treatment group\ndata |&gt;\n  group_by(treatment) |&gt;\n  summarize(n_samples = n_distinct(sample_id))\n\n\n\n\n5.12.3 Selecting Rows by Position with slice()\nWhile filter() selects rows by condition, slice() selects by position:\n\n\nCode\n# First 3 rows\nhead(iris, 3)\n\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1          5.1         3.5          1.4         0.2  setosa\n2          4.9         3.0          1.4         0.2  setosa\n3          4.7         3.2          1.3         0.2  setosa\n\n\nCode\n# Using slice variants\niris |&gt; slice_head(n = 3)    # First 3 rows\n\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1          5.1         3.5          1.4         0.2  setosa\n2          4.9         3.0          1.4         0.2  setosa\n3          4.7         3.2          1.3         0.2  setosa\n\n\nCode\niris |&gt; slice_tail(n = 3)    # Last 3 rows\n\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width   Species\n1          6.5         3.0          5.2         2.0 virginica\n2          6.2         3.4          5.4         2.3 virginica\n3          5.9         3.0          5.1         1.8 virginica\n\n\nCode\niris |&gt; slice_sample(n = 5)  # Random 5 rows\n\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width    Species\n1          5.8         2.7          5.1         1.9  virginica\n2          4.6         3.1          1.5         0.2     setosa\n3          4.6         3.4          1.4         0.3     setosa\n4          5.4         3.4          1.7         0.2     setosa\n5          5.7         2.8          4.1         1.3 versicolor\n\n\nCode\niris |&gt; slice_max(Sepal.Length, n = 3)  # Top 3 by value\n\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width   Species\n1          7.9         3.8          6.4         2.0 virginica\n2          7.7         3.8          6.7         2.2 virginica\n3          7.7         2.6          6.9         2.3 virginica\n4          7.7         2.8          6.7         2.0 virginica\n5          7.7         3.0          6.1         2.3 virginica\n\n\n\n\n5.12.4 Extracting Columns with pull()\nTo extract a single column as a vector (rather than a data frame), use pull():\n\n\nCode\n# Returns a vector, not a data frame\niris |&gt;\n  filter(Species == \"setosa\") |&gt;\n  pull(Sepal.Length) |&gt;\n  mean()\n\n\n[1] 5.006\n\n\n\n\n5.12.5 Distinct Values with distinct()\nRemove duplicate rows based on specified columns:\n\n\nCode\n# Unique values in one column\ndata |&gt; distinct(treatment)\n\n# Unique combinations of multiple columns\ndata |&gt; distinct(treatment, time_point)",
    "crumbs": [
      "Foundations of Data Science",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Tidy Data and Data Wrangling</span>"
    ]
  },
  {
    "objectID": "chapters/05-tidy-data.html#working-with-factors",
    "href": "chapters/05-tidy-data.html#working-with-factors",
    "title": "5  Tidy Data and Data Wrangling",
    "section": "5.13 Working with Factors",
    "text": "5.13 Working with Factors\nFactors are R’s way of representing categorical data with a fixed set of possible values (called levels). The forcats package (part of tidyverse) provides tools for working with factors.\n\n5.13.1 Reordering Factor Levels\nFor visualization, you often want factor levels ordered by a value rather than alphabetically:\n\n\nCode\n# Sample data\ngene_data &lt;- tibble(\n  gene = c(\"BRCA1\", \"TP53\", \"EGFR\", \"KRAS\", \"MYC\"),\n  expression = c(5.2, 8.1, 3.4, 6.7, 9.2)\n)\n\n# Default: alphabetical order\nggplot(gene_data, aes(x = gene, y = expression)) +\n  geom_col() +\n  labs(title = \"Default (Alphabetical) Order\")\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Reorder by expression value\nlibrary(forcats)\ngene_data |&gt;\n  mutate(gene = fct_reorder(gene, expression)) |&gt;\n  ggplot(aes(x = gene, y = expression)) +\n  geom_col() +\n  labs(title = \"Ordered by Expression\")\n\n\n\n\n\n\n\n\n\n\n\n5.13.2 Recoding Factor Levels\nUse fct_recode() to change level names:\n\n\nCode\n# Original factor\nstatus &lt;- factor(c(\"WT\", \"WT\", \"KO\", \"HET\", \"KO\"))\n\n# Recode to more descriptive names\nfct_recode(status,\n  \"Wild Type\" = \"WT\",\n  \"Knockout\" = \"KO\",\n  \"Heterozygous\" = \"HET\"\n)\n\n\n[1] Wild Type    Wild Type    Knockout     Heterozygous Knockout    \nLevels: Heterozygous Knockout Wild Type\n\n\n\n\n5.13.3 Collapsing Rare Levels\nUse fct_lump_n() to combine infrequent categories into “Other”:\n\n\nCode\n# Sample with many categories\nmany_categories &lt;- factor(rep(c(\"A\", \"B\", \"C\", \"D\", \"E\", \"F\"),\n                               c(50, 30, 10, 5, 3, 2)))\n\n# Keep only the top 3 most common\nfct_lump_n(many_categories, n = 3)\n\n\n  [1] A     A     A     A     A     A     A     A     A     A     A     A    \n [13] A     A     A     A     A     A     A     A     A     A     A     A    \n [25] A     A     A     A     A     A     A     A     A     A     A     A    \n [37] A     A     A     A     A     A     A     A     A     A     A     A    \n [49] A     A     B     B     B     B     B     B     B     B     B     B    \n [61] B     B     B     B     B     B     B     B     B     B     B     B    \n [73] B     B     B     B     B     B     B     B     C     C     C     C    \n [85] C     C     C     C     C     C     Other Other Other Other Other Other\n [97] Other Other Other Other\nLevels: A B C Other",
    "crumbs": [
      "Foundations of Data Science",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Tidy Data and Data Wrangling</span>"
    ]
  },
  {
    "objectID": "chapters/05-tidy-data.html#additional-tidyr-functions",
    "href": "chapters/05-tidy-data.html#additional-tidyr-functions",
    "title": "5  Tidy Data and Data Wrangling",
    "section": "5.14 Additional tidyr Functions",
    "text": "5.14 Additional tidyr Functions\n\n5.14.1 Separating and Combining Columns\nseparate() splits one column into multiple columns; unite() combines columns:\n\n\nCode\n# Data with combined values\ncombined_data &lt;- tibble(\n  sample = c(\"control_rep1\", \"control_rep2\", \"treatment_rep1\"),\n  value = c(10, 12, 25)\n)\n\n# Separate into treatment and replicate\ncombined_data |&gt;\n  separate(sample, into = c(\"treatment\", \"replicate\"), sep = \"_\")\n\n\n# A tibble: 3 × 3\n  treatment replicate value\n  &lt;chr&gt;     &lt;chr&gt;     &lt;dbl&gt;\n1 control   rep1         10\n2 control   rep2         12\n3 treatment rep1         25\n\n\n\n\nCode\n# Combine columns\ntibble(year = 2024, month = 3, day = 15) |&gt;\n  unite(date, year, month, day, sep = \"-\")\n\n\n# A tibble: 1 × 1\n  date     \n  &lt;chr&gt;    \n1 2024-3-15\n\n\n\n\n5.14.2 Handling Implicit Missing Values with complete()\nSometimes data has implicit missing values—combinations that should exist but don’t appear. complete() makes them explicit:\n\n\nCode\n# Implicit missing: no observation for site B in year 2021\nobservations &lt;- tibble(\n  site = c(\"A\", \"A\", \"B\"),\n  year = c(2020, 2021, 2020),\n  count = c(10, 15, 8)\n)\n\n# Make all combinations explicit\nobservations |&gt;\n  complete(site, year, fill = list(count = 0))\n\n\n# A tibble: 4 × 3\n  site   year count\n  &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 A      2020    10\n2 A      2021    15\n3 B      2020     8\n4 B      2021     0",
    "crumbs": [
      "Foundations of Data Science",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Tidy Data and Data Wrangling</span>"
    ]
  },
  {
    "objectID": "chapters/05-tidy-data.html#practice-exercises",
    "href": "chapters/05-tidy-data.html#practice-exercises",
    "title": "5  Tidy Data and Data Wrangling",
    "section": "5.15 Practice Exercises",
    "text": "5.15 Practice Exercises\nHere is a workflow to practice these concepts:\n\nRead a dataset into R\nConvert it to a tibble with as_tibble()\nSelect the columns you need\nFilter to the observations of interest\nCreate new variables with mutate\nGroup by categorical variables and summarize\nVisualize the results\n\nWorking through this process with your own data will cement these concepts better than any number of examples.\nFor additional hands-on practice with data wrangling, see Section 35.4 in the Practice Exercises appendix. The exercises include:\n\nPractice with the five core dplyr verbs\nGrouping and summarizing data\nBuilding data wrangling pipelines\nReshaping data with pivot operations\nJoining multiple data tables",
    "crumbs": [
      "Foundations of Data Science",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Tidy Data and Data Wrangling</span>"
    ]
  },
  {
    "objectID": "chapters/06-data-visualization.html",
    "href": "chapters/06-data-visualization.html",
    "title": "6  Data Visualization",
    "section": "",
    "text": "6.1 Why Visualize Data?\nBefore diving into the mechanics of creating plots, consider why visualization matters. The human visual system excels at detecting patterns, spotting outliers, and perceiving relationships—abilities that summary statistics cannot replace.\nConsider Anscombe’s Quartet—four datasets with nearly identical summary statistics (same mean, variance, and correlation) but completely different patterns:\nCode\n# Reshape Anscombe's built-in dataset\nanscombe_long &lt;- anscombe |&gt;\n  pivot_longer(everything(),\n               names_to = c(\".value\", \"set\"),\n               names_pattern = \"(.)(.)\")\n\nggplot(anscombe_long, aes(x = x, y = y)) +\n  geom_point(size = 2, color = \"steelblue\") +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"coral\") +\n  facet_wrap(~set, ncol = 2) +\n  labs(title = \"Same Mean, Variance, and Correlation—Different Stories\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nFigure 6.1: Anscombe’s Quartet: Four datasets with identical summary statistics but very different patterns\nAll four datasets have nearly identical statistical summaries, yet they represent fundamentally different phenomena: a linear relationship, a curved relationship, an outlier-driven relationship, and a vertical cluster with one outlier. Summary statistics alone would suggest these datasets are equivalent—only visualization reveals the truth.",
    "crumbs": [
      "Foundations of Data Science",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "chapters/06-data-visualization.html#why-visualize-data",
    "href": "chapters/06-data-visualization.html#why-visualize-data",
    "title": "6  Data Visualization",
    "section": "",
    "text": "Always Visualize Your Data\n\n\n\nNever trust summary statistics alone. Before running statistical tests, visualize your data to check assumptions, identify outliers, and understand the underlying patterns.",
    "crumbs": [
      "Foundations of Data Science",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "chapters/06-data-visualization.html#choosing-the-right-chart-type",
    "href": "chapters/06-data-visualization.html#choosing-the-right-chart-type",
    "title": "6  Data Visualization",
    "section": "6.2 Choosing the Right Chart Type",
    "text": "6.2 Choosing the Right Chart Type\nDifferent questions call for different visualizations. Matching your question to the right chart type is the first step toward effective communication:\n\n\n\n\n\n\n\n\nQuestion\nChart Type\nWhy\n\n\n\n\nHow are values distributed?\nHistogram, density plot\nShows shape, center, spread\n\n\nHow do groups compare?\nBox plot, bar chart\nSide-by-side comparison\n\n\nHow do two variables relate?\nScatter plot\nShows correlation, patterns\n\n\nHow does a value change over time?\nLine plot\nConnects sequential observations\n\n\nWhat is the composition?\nStacked bar chart\nShows parts of a whole\n\n\n\n\n\n\n\n\n\nStart with the Question\n\n\n\nBefore creating any visualization, ask yourself: “What question am I trying to answer?” The chart type should emerge from the question, not the other way around.",
    "crumbs": [
      "Foundations of Data Science",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "chapters/06-data-visualization.html#the-grammar-of-graphics",
    "href": "chapters/06-data-visualization.html#the-grammar-of-graphics",
    "title": "6  Data Visualization",
    "section": "6.3 The Grammar of Graphics",
    "text": "6.3 The Grammar of Graphics\nData visualization is both an art and a science. A well-designed graphic can reveal patterns, communicate findings, and guide analysis in ways that tables of numbers cannot. The ggplot2 package implements a coherent system for creating graphics based on Leland Wilkinson’s “Grammar of Graphics”—a framework that describes the fundamental components from which all statistical graphics can be built.\nJust as grammar provides rules for constructing sentences from words, the grammar of graphics provides rules for constructing visualizations from components. Every graphic is composed of data, aesthetic mappings that connect variables to visual properties, and geometric objects that represent data points. Additional components like scales, statistical transformations, coordinate systems, and facets allow for sophisticated customizations.",
    "crumbs": [
      "Foundations of Data Science",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "chapters/06-data-visualization.html#building-plots-with-ggplot2",
    "href": "chapters/06-data-visualization.html#building-plots-with-ggplot2",
    "title": "6  Data Visualization",
    "section": "6.4 Building Plots with ggplot2",
    "text": "6.4 Building Plots with ggplot2\nThe basic structure of a ggplot2 call begins with the ggplot() function, which creates a coordinate system. You add layers to this foundation using the + operator.\n\n\nCode\nggplot(data = mpg, mapping = aes(x = displ, y = hwy)) +\n  geom_point()\n\n\n\n\n\n\n\n\n\nThis creates a scatterplot of highway fuel efficiency against engine displacement using the built-in mpg dataset. The aes() function establishes the aesthetic mapping—which variables map to which visual properties. Here, displ maps to the x-axis and hwy to the y-axis. The geom_point() function adds a layer of points.",
    "crumbs": [
      "Foundations of Data Science",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "chapters/06-data-visualization.html#aesthetic-mappings",
    "href": "chapters/06-data-visualization.html#aesthetic-mappings",
    "title": "6  Data Visualization",
    "section": "6.5 Aesthetic Mappings",
    "text": "6.5 Aesthetic Mappings\nAesthetics are visual properties of the plot. Beyond position (x and y), common aesthetics include color, size, shape, and transparency (alpha). You can map variables to these aesthetics to encode additional information.\n\n\nCode\nggplot(mpg, aes(x = displ, y = hwy, color = class)) + \n  geom_point(size = 3, alpha = 0.7)\n\n\n\n\n\n\n\n\n\nNow the color of each point indicates the vehicle class. The legend is created automatically. Note that aesthetics defined inside aes() are mapped to variables, while those defined outside (like size = 3) apply uniformly to all points.",
    "crumbs": [
      "Foundations of Data Science",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "chapters/06-data-visualization.html#geometric-objects",
    "href": "chapters/06-data-visualization.html#geometric-objects",
    "title": "6  Data Visualization",
    "section": "6.6 Geometric Objects",
    "text": "6.6 Geometric Objects\nGeometric objects, or geoms, determine what type of plot you create. Different geoms represent data in different ways.\n\n6.6.1 Scatterplots with geom_point()\nPoints are good for showing the relationship between two continuous variables:\n\n\nCode\nggplot(mpg, aes(x = displ, y = hwy)) +\n  geom_point()\n\n\n\n\n\n\n\n\n\n\n\n6.6.2 Line Plots with geom_line() and geom_smooth()\nLines connect points in order, useful for time series or showing trends:\n\n\nCode\nggplot(mpg, aes(x = displ, y = hwy)) +\n  geom_point() +\n  geom_smooth()\n\n\n\n\n\n\n\n\n\nThe geom_smooth() function adds a smoothed conditional mean with confidence interval.\n\n\n6.6.3 Bar Charts with geom_bar()\nBar charts show counts or summaries of categorical data:\n\n\nCode\nggplot(diamonds, aes(x = cut)) +\n  geom_bar()\n\n\n\n\n\n\n\n\n\nUse fill to color bars by another variable:\n\n\nCode\nggplot(diamonds, aes(x = cut, fill = clarity)) +\n  geom_bar(position = \"dodge\")\n\n\n\n\n\n\n\n\n\n\n\n6.6.4 Histograms with geom_histogram()\nHistograms show the distribution of a continuous variable:\n\n\nCode\nggplot(diamonds, aes(x = carat)) +\n  geom_histogram(binwidth = 0.1, fill = \"steelblue\", color = \"white\")\n\n\n\n\n\n\n\n\n\n\n\n6.6.5 Boxplots with geom_boxplot()\nBoxplots summarize distributions and highlight outliers:\n\n\nCode\nggplot(mpg, aes(x = class, y = hwy)) +\n  geom_boxplot()",
    "crumbs": [
      "Foundations of Data Science",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "chapters/06-data-visualization.html#combining-multiple-geoms",
    "href": "chapters/06-data-visualization.html#combining-multiple-geoms",
    "title": "6  Data Visualization",
    "section": "6.7 Combining Multiple Geoms",
    "text": "6.7 Combining Multiple Geoms\nYou can layer multiple geoms to create richer visualizations:\n\n\nCode\nggplot(mpg, aes(x = displ, y = hwy)) +\n  geom_point(aes(color = class)) +\n  geom_smooth(se = FALSE, color = \"black\")",
    "crumbs": [
      "Foundations of Data Science",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "chapters/06-data-visualization.html#faceting",
    "href": "chapters/06-data-visualization.html#faceting",
    "title": "6  Data Visualization",
    "section": "6.8 Faceting",
    "text": "6.8 Faceting\nFaceting creates small multiples—separate panels for subsets of the data. This is powerful for comparing patterns across groups.\n\n\nCode\nggplot(mpg, aes(x = displ, y = hwy)) +\n  geom_point() +\n  facet_wrap(~ class, nrow = 2)\n\n\n\n\n\n\n\n\n\nUse facet_grid() for two-variable faceting:\n\n\nCode\nggplot(mpg, aes(x = displ, y = hwy)) +\n  geom_point() +\n  facet_grid(drv ~ cyl)",
    "crumbs": [
      "Foundations of Data Science",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "chapters/06-data-visualization.html#labels-and-titles",
    "href": "chapters/06-data-visualization.html#labels-and-titles",
    "title": "6  Data Visualization",
    "section": "6.9 Labels and Titles",
    "text": "6.9 Labels and Titles\nAdd informative labels with the labs() function:\n\n\nCode\nggplot(mpg, aes(x = displ, y = hwy, color = class)) +\n  geom_point() +\n  geom_smooth(se = FALSE) +\n  labs(\n    title = \"Fuel Efficiency Decreases with Engine Size\",\n    subtitle = \"Data from EPA fuel economy tests\",\n    caption = \"Source: fueleconomy.gov\",\n    x = \"Engine Displacement (liters)\",\n    y = \"Highway Fuel Efficiency (mpg)\",\n    color = \"Vehicle Class\"\n  )",
    "crumbs": [
      "Foundations of Data Science",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "chapters/06-data-visualization.html#themes",
    "href": "chapters/06-data-visualization.html#themes",
    "title": "6  Data Visualization",
    "section": "6.10 Themes",
    "text": "6.10 Themes\nThemes control the non-data aspects of the plot—background, grid lines, fonts, etc. ggplot2 includes several built-in themes:\n\n\nCode\nggplot(mpg, aes(x = displ, y = hwy)) +\n  geom_point() +\n  theme_classic()\n\n\n\n\n\n\n\n\n\nOther built-in themes include theme_minimal(), theme_bw(), theme_light(), and theme_dark(). The ggthemes package provides many additional themes.",
    "crumbs": [
      "Foundations of Data Science",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "chapters/06-data-visualization.html#choosing-the-right-plot",
    "href": "chapters/06-data-visualization.html#choosing-the-right-plot",
    "title": "6  Data Visualization",
    "section": "6.11 Choosing the Right Plot",
    "text": "6.11 Choosing the Right Plot\nChoosing an appropriate visualization depends on the types of variables you want to display and the message you want to convey.\n\n\n\n\n\nFor one categorical variable, use bar charts. For one continuous variable, use histograms or density plots. For two continuous variables, use scatterplots. For one continuous and one categorical, use boxplots or violin plots. For two categorical variables, use stacked or grouped bar charts or heat maps.",
    "crumbs": [
      "Foundations of Data Science",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "chapters/06-data-visualization.html#principles-of-effective-visualization",
    "href": "chapters/06-data-visualization.html#principles-of-effective-visualization",
    "title": "6  Data Visualization",
    "section": "6.12 Principles of Effective Visualization",
    "text": "6.12 Principles of Effective Visualization\nEdward Tufte articulated principles of graphical excellence that remain influential: “Graphical excellence is that which gives to the viewer the greatest number of ideas in the shortest time with the least ink in the smallest space.”\nKey principles include:\nShow the data. Above all else, make the data visible. Avoid chart junk that obscures what you are trying to communicate.\nEncourage comparison. Design graphics to facilitate comparison of different groups or conditions.\nRepresent magnitudes honestly. The visual representation should be proportional to the numerical quantities being represented. Avoid truncated axes that exaggerate differences.\nMinimize clutter. Remove unnecessary grid lines, borders, and decorations. Every element should serve a purpose.\nMake displays easy to interpret. Use clear labels, appropriate colors, and logical organization.\n\n6.12.1 Order Categories Meaningfully\nBy default, R orders categorical variables alphabetically, which is rarely the most informative arrangement. Use reorder() to order categories by a meaningful value:\n\n\nCode\n# Create sample data\nsample_data &lt;- tibble(\n  treatment = c(\"Control\", \"Low Dose\", \"Medium Dose\", \"High Dose\"),\n  response = c(12, 18, 25, 31)\n)\n\n# Default alphabetical order (not ideal)\nggplot(sample_data, aes(x = treatment, y = response)) +\n  geom_col(fill = \"steelblue\") +\n  labs(title = \"Default Order (Alphabetical)\",\n       x = \"Treatment\", y = \"Response\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Order by response value (more meaningful)\nsample_data |&gt;\n  mutate(treatment = reorder(treatment, response)) |&gt;\n  ggplot(aes(x = treatment, y = response)) +\n  geom_col(fill = \"steelblue\") +\n  labs(title = \"Ordered by Value (More Meaningful)\",\n       x = \"Treatment\", y = \"Response\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nThe reorder() function takes a categorical variable and a numeric variable, reordering the categories by the numeric values. For horizontal bar charts (which are often easier to read), add coord_flip():\n\n\nCode\nsample_data |&gt;\n  mutate(treatment = reorder(treatment, response)) |&gt;\n  ggplot(aes(x = treatment, y = response)) +\n  geom_col(fill = \"steelblue\") +\n  coord_flip() +\n  labs(title = \"Horizontal Bars (Good for Long Labels)\",\n       x = NULL, y = \"Response\") +\n  theme_minimal()",
    "crumbs": [
      "Foundations of Data Science",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "chapters/06-data-visualization.html#visual-encoding-how-we-perceive-data",
    "href": "chapters/06-data-visualization.html#visual-encoding-how-we-perceive-data",
    "title": "6  Data Visualization",
    "section": "6.13 Visual Encoding: How We Perceive Data",
    "text": "6.13 Visual Encoding: How We Perceive Data\nEffective visualization depends on understanding how humans perceive visual information. We encode data using visual cues—properties like position, length, color, and shape. But not all visual cues are equally effective.\n\n6.13.1 The Hierarchy of Visual Encoding\nResearch by Cleveland and McGill established that we perceive some visual encodings more accurately than others. From most to least accurate:\n\nPosition along a common scale (scatterplots, dot plots)\nPosition along non-aligned scales (small multiples)\nLength (bar charts)\nAngle/slope (some line charts)\nArea (bubble charts, treemaps)\nVolume (3D charts—generally avoid)\nColor saturation/hue (choropleth maps, heatmaps)\n\n\n\n\n\n\n\n\n\n\nThis hierarchy explains why bar charts work better than pie charts for comparing quantities—we judge lengths more accurately than angles or areas.\n\n\n6.13.2 Position Is Most Powerful\nWhen possible, encode your most important data using position. Scatterplots, line graphs, and dot plots all use position effectively:\n\n\nCode\n# Compare a pie chart vs. bar chart for the same data\nlibrary(patchwork)\n\ncategory_data &lt;- data.frame(\n  category = c(\"Engineering\", \"Medicine\", \"Natural Sciences\", \"Social Sciences\"),\n  funding = c(35, 28, 22, 15)\n)\n\n# Bar chart - easy to compare\np_bar &lt;- ggplot(category_data, aes(x = reorder(category, funding), y = funding)) +\n  geom_col(fill = \"steelblue\") +\n  coord_flip() +\n  labs(title = \"Bar Chart: Easy Comparison\", x = \"\", y = \"Funding (%)\") +\n  theme_minimal()\n\n# Pie chart - harder to compare\np_pie &lt;- ggplot(category_data, aes(x = \"\", y = funding, fill = category)) +\n  geom_col(width = 1) +\n  coord_polar(\"y\") +\n  labs(title = \"Pie Chart: Harder to Compare\", fill = \"Category\") +\n  theme_void() +\n  theme(legend.position = \"bottom\")\n\np_bar + p_pie\n\n\n\n\n\n\n\n\n\nThe bar chart makes it immediately obvious that Engineering has the most funding. With the pie chart, you must work harder to compare the slice sizes.",
    "crumbs": [
      "Foundations of Data Science",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "chapters/06-data-visualization.html#when-should-zero-be-included",
    "href": "chapters/06-data-visualization.html#when-should-zero-be-included",
    "title": "6  Data Visualization",
    "section": "6.14 When Should Zero Be Included?",
    "text": "6.14 When Should Zero Be Included?\nA contentious issue in data visualization is whether the y-axis should always start at zero. The answer depends on the type of chart and what you’re trying to show.\n\n6.14.1 Bar Charts: Always Include Zero\nFor bar charts, the length of the bar represents the magnitude of the value. If the axis doesn’t start at zero, the visual representation misrepresents the data:\n\n\nCode\n# Demonstration of misleading truncated axis\ngdp_data &lt;- data.frame(\n  country = c(\"A\", \"B\", \"C\"),\n  gdp = c(45000, 47000, 49000)\n)\n\np_trunc &lt;- ggplot(gdp_data, aes(x = country, y = gdp)) +\n  geom_col(fill = \"steelblue\") +\n  coord_cartesian(ylim = c(44000, 50000)) +\n  labs(title = \"Truncated Axis: Misleading\",\n       subtitle = \"Differences appear huge\",\n       y = \"GDP per capita\") +\n  theme_minimal()\n\np_full &lt;- ggplot(gdp_data, aes(x = country, y = gdp)) +\n  geom_col(fill = \"steelblue\") +\n  labs(title = \"Full Axis: Honest\",\n       subtitle = \"Differences in proper context\",\n       y = \"GDP per capita\") +\n  theme_minimal()\n\np_trunc + p_full\n\n\n\n\n\n\n\n\n\n\n\n6.14.2 Scatterplots and Line Charts: Context Matters\nFor position-based encodings like scatterplots and line charts, zero doesn’t need to be included if it would waste space and obscure meaningful variation:\n\n\nCode\n# Temperature data - zero would be meaningless\nset.seed(42)\ntemp_data &lt;- data.frame(\n  day = 1:30,\n  temp = rnorm(30, mean = 72, sd = 5)\n)\n\np_zero &lt;- ggplot(temp_data, aes(x = day, y = temp)) +\n  geom_line(color = \"firebrick\") +\n  ylim(0, 100) +\n  labs(title = \"Including Zero: Wastes Space\", y = \"Temperature (°F)\") +\n  theme_minimal()\n\np_auto &lt;- ggplot(temp_data, aes(x = day, y = temp)) +\n  geom_line(color = \"firebrick\") +\n  labs(title = \"Natural Range: Shows Variation\", y = \"Temperature (°F)\") +\n  theme_minimal()\n\np_zero + p_auto\n\n\n\n\n\n\n\n\n\nThe key question is: what would zero mean for this variable? For temperature in Fahrenheit, zero has no special significance for daily weather data. For proportions or counts, zero is meaningful and often should be included.\n\n\n\n\n\n\nThe Zero Rule\n\n\n\nBar charts: Always include zero—the bar length represents magnitude. Line charts and scatterplots: Include zero if it’s meaningful; otherwise, show the natural range of the data. When in doubt: Ask whether excluding zero could mislead readers about the magnitude of differences.",
    "crumbs": [
      "Foundations of Data Science",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "chapters/06-data-visualization.html#data-transformations-for-visualization",
    "href": "chapters/06-data-visualization.html#data-transformations-for-visualization",
    "title": "6  Data Visualization",
    "section": "6.15 Data Transformations for Visualization",
    "text": "6.15 Data Transformations for Visualization\nSometimes the raw data doesn’t visualize well. Transformations can reveal patterns that are hidden in the original scale.\n\n6.15.1 Log Transformations for Skewed Data\nMany biological variables—gene expression, population sizes, concentrations—follow approximately log-normal distributions with long right tails. Log transformation can make patterns visible:\n\n\nCode\n# Simulated gene expression data\nset.seed(123)\nexpression_data &lt;- data.frame(\n  gene_a = rlnorm(200, meanlog = 2, sdlog = 1.5),\n  gene_b = rlnorm(200, meanlog = 3, sdlog = 1.2)\n)\n\np_raw &lt;- ggplot(expression_data, aes(x = gene_a, y = gene_b)) +\n  geom_point(alpha = 0.5) +\n  labs(title = \"Original Scale\",\n       subtitle = \"Pattern obscured by outliers\",\n       x = \"Gene A Expression\", y = \"Gene B Expression\") +\n  theme_minimal()\n\np_log &lt;- ggplot(expression_data, aes(x = gene_a, y = gene_b)) +\n  geom_point(alpha = 0.5) +\n  scale_x_log10() +\n  scale_y_log10() +\n  labs(title = \"Log Scale\",\n       subtitle = \"Relationship visible\",\n       x = \"Gene A Expression (log)\", y = \"Gene B Expression (log)\") +\n  theme_minimal()\n\np_raw + p_log\n\n\n\n\n\n\n\n\n\n\n\n6.15.2 When to Use Log Scales\nConsider log transformation when:\n\nData span several orders of magnitude\nRelationships are multiplicative rather than additive\nDistribution is strongly right-skewed\nYou’re comparing fold changes or ratios\n\nBe sure to label axes clearly when using transformed scales, and remember that zero cannot be log-transformed.",
    "crumbs": [
      "Foundations of Data Science",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "chapters/06-data-visualization.html#color-in-data-visualization",
    "href": "chapters/06-data-visualization.html#color-in-data-visualization",
    "title": "6  Data Visualization",
    "section": "6.16 Color in Data Visualization",
    "text": "6.16 Color in Data Visualization\nColor is a powerful but often misused encoding. Effective use of color requires understanding perception and accessibility.\n\n6.16.1 Types of Color Scales\nSequential: For ordered data from low to high. Use a single hue varying in lightness.\n\n\n\n\n\n\n\n\n\nDiverging: For data with a meaningful midpoint. Two hues diverge from a neutral center.\n\n\n\n\n\n\n\n\n\nQualitative: For categorical data with no inherent order. Use distinct hues.\n\n\n6.16.2 Color Accessibility\nApproximately 8% of men and 0.5% of women have some form of color vision deficiency. Design for accessibility:\n\nAvoid red-green as the only distinguishing feature\nUse the viridis color scales, designed for perceptual uniformity and colorblind accessibility\nSupplement color with shape or pattern when possible\n\n\n\nCode\n# Good practice: color + shape\nggplot(mpg, aes(x = displ, y = hwy, color = drv, shape = drv)) +\n  geom_point(size = 3) +\n  scale_color_viridis_d() +\n  labs(title = \"Color + Shape: Accessible Design\",\n       x = \"Engine Displacement (L)\", y = \"Highway MPG\",\n       color = \"Drive Type\", shape = \"Drive Type\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAvoid Rainbow Color Scales\n\n\n\nRainbow color scales (like the default “jet” colormap in MATLAB) have serious problems:\n\nThey’re not perceptually uniform—yellow appears brighter than blue\nThey create false boundaries where colors change dramatically\nThey’re particularly problematic for colorblind viewers\n\nUse viridis, plasma, or other perceptually uniform scales instead.",
    "crumbs": [
      "Foundations of Data Science",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "chapters/06-data-visualization.html#the-power-of-small-multiples",
    "href": "chapters/06-data-visualization.html#the-power-of-small-multiples",
    "title": "6  Data Visualization",
    "section": "6.17 The Power of Small Multiples",
    "text": "6.17 The Power of Small Multiples\nSmall multiples—the same chart repeated for different subsets of the data—are remarkably effective for comparison. Edward Tufte called them “the best design solution for a wide range of problems in data presentation.”\n\n\nCode\n# Small multiples example\nggplot(gapminder::gapminder %&gt;%\n         filter(continent != \"Oceania\"),\n       aes(x = gdpPercap, y = lifeExp)) +\n  geom_point(alpha = 0.3, size = 0.8) +\n  geom_smooth(method = \"loess\", se = FALSE, color = \"firebrick\") +\n  scale_x_log10(labels = scales::comma) +\n  facet_grid(continent ~ cut(year, breaks = c(1950, 1970, 1990, 2010),\n                              labels = c(\"1952-1970\", \"1971-1990\", \"1991-2007\"))) +\n  labs(title = \"Life Expectancy vs. GDP Over Time by Continent\",\n       x = \"GDP per Capita (log scale)\",\n       y = \"Life Expectancy (years)\") +\n  theme_minimal() +\n  theme(strip.text = element_text(size = 9))\n\n\n\n\n\n\n\n\n\nSmall multiples work because:\n\nThe eye can quickly scan and compare panels\nEach panel has identical axes, making comparison fair\nPatterns and outliers become visible through repetition",
    "crumbs": [
      "Foundations of Data Science",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "chapters/06-data-visualization.html#common-visualization-mistakes",
    "href": "chapters/06-data-visualization.html#common-visualization-mistakes",
    "title": "6  Data Visualization",
    "section": "6.18 Common Visualization Mistakes",
    "text": "6.18 Common Visualization Mistakes\nBeyond the principles discussed, watch out for these common errors:\nOverplotting: Too many points obscure patterns. Use transparency, jittering, or density plots.\n\n\nCode\n# Overplotting solution\nset.seed(42)\noverplot_data &lt;- data.frame(\n  x = rnorm(5000),\n  y = rnorm(5000)\n)\n\np_over &lt;- ggplot(overplot_data, aes(x, y)) +\n  geom_point() +\n  labs(title = \"Overplotting: Points Hidden\") +\n  theme_minimal()\n\np_alpha &lt;- ggplot(overplot_data, aes(x, y)) +\n  geom_point(alpha = 0.1) +\n  labs(title = \"Transparency: Density Visible\") +\n  theme_minimal()\n\np_over + p_alpha\n\n\n\n\n\n\n\n\n\nDual y-axes: These are almost always misleading. The relationship between the two scales is arbitrary and can be manipulated to show any desired pattern.\n3D effects: Three-dimensional bar charts, pie charts, and similar decorations distort perception without adding information. Avoid them.\nExcessive decoration: Gridlines, borders, backgrounds, and other “chart junk” should be minimized. Focus attention on the data.",
    "crumbs": [
      "Foundations of Data Science",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "chapters/06-data-visualization.html#examples-of-poor-graphics",
    "href": "chapters/06-data-visualization.html#examples-of-poor-graphics",
    "title": "6  Data Visualization",
    "section": "6.19 Examples of Poor Graphics",
    "text": "6.19 Examples of Poor Graphics\nRecognizing bad graphics helps you avoid making them.\n\n\n\n\n\nTicker-tape style displays make it hard to see patterns. Lines connecting unrelated points mislead. Pie charts make comparisons difficult because humans are poor at judging angles. Three-dimensional effects distort perception without adding information.",
    "crumbs": [
      "Foundations of Data Science",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "chapters/06-data-visualization.html#a-famous-good-example",
    "href": "chapters/06-data-visualization.html#a-famous-good-example",
    "title": "6  Data Visualization",
    "section": "6.20 A Famous Good Example",
    "text": "6.20 A Famous Good Example\nCharles Minard’s 1869 map of Napoleon’s Russian campaign is often cited as one of the best statistical graphics ever made. It displays six variables: the size of the army, its location (latitude and longitude), direction of movement, temperature, and date—all in a single coherent image.\n\n\n\n\n\nThe graphic tells a story. You can see the army shrink as it advances, the devastating losses during the retreat, and the correlation with plummeting temperatures. No legend is needed; the meaning is immediately apparent.",
    "crumbs": [
      "Foundations of Data Science",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "chapters/06-data-visualization.html#saving-plots",
    "href": "chapters/06-data-visualization.html#saving-plots",
    "title": "6  Data Visualization",
    "section": "6.21 Saving Plots",
    "text": "6.21 Saving Plots\nSave plots with ggsave():\n\n\nCode\n# Create and save a plot\np &lt;- ggplot(mpg, aes(x = displ, y = hwy)) +\n  geom_point()\n\nggsave(\"my_plot.png\", p, width = 8, height = 6, dpi = 300)\nggsave(\"my_plot.pdf\", p, width = 8, height = 6)\n\n\nThe function infers the format from the file extension. Specify dimensions and resolution for publication-quality output.",
    "crumbs": [
      "Foundations of Data Science",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "chapters/06-data-visualization.html#practice-exercises",
    "href": "chapters/06-data-visualization.html#practice-exercises",
    "title": "6  Data Visualization",
    "section": "6.22 Practice Exercises",
    "text": "6.22 Practice Exercises\nThe best way to learn ggplot2 is to use it. Take a dataset you care about and try different visualizations. Experiment with aesthetics, geoms, and facets. Read error messages carefully—they often point directly to the problem. The ggplot2 documentation and the R Graph Gallery (r-graph-gallery.com) provide extensive examples to learn from.\nFor structured practice with visualization concepts, see Section 35.5 in the Practice Exercises appendix. The exercises include:\n\nCreating basic ggplot2 visualizations\nWorking with different geometric objects\nMapping variables to aesthetics\nCreating faceted plots (small multiples)\nCombining layers for complex visualizations\nCreating publication-quality figures",
    "crumbs": [
      "Foundations of Data Science",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "chapters/07-probability-foundations.html",
    "href": "chapters/07-probability-foundations.html",
    "title": "7  Foundations of Probability",
    "section": "",
    "text": "7.1 Why Probability Matters\nIn games of chance, probability has a very intuitive definition. We know what it means that the chance of a pair of dice coming up seven is 1 in 6. However, probability is used much more broadly today, with the word commonly appearing in everyday language. Google’s auto-complete of “What are the chances of” gives us: “having twins”, “rain today”, “getting struck by lightning”, and “getting cancer”. One goal of this chapter is to help us understand how probability is useful to understand and describe real-world events when performing data analysis.\nBecause knowing how to compute probabilities gives you an edge in games of chance, throughout history many smart individuals—including famous mathematicians such as Cardano, Fermat, and Pascal—spent time and energy thinking through the math of these games. As a result, Probability Theory was born. Probability continues to be highly useful in modern games of chance. For example, in poker, we can compute the probability of winning a hand based on the cards on the table. Casinos rely on probability theory to develop games that almost certainly guarantee a profit.\nProbability theory is useful in many other contexts and, in particular, in areas that depend on data affected by chance in some way. All of the other chapters in this part build upon probability theory. Knowledge of probability is therefore indispensable for data science.",
    "crumbs": [
      "Probability and Distributions",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Foundations of Probability</span>"
    ]
  },
  {
    "objectID": "chapters/07-probability-foundations.html#two-interpretations-of-probability",
    "href": "chapters/07-probability-foundations.html#two-interpretations-of-probability",
    "title": "7  Foundations of Probability",
    "section": "7.2 Two Interpretations of Probability",
    "text": "7.2 Two Interpretations of Probability\nThere are two main ways to think about what probability means.\nThe frequentist interpretation views probability as mathematically convenient approximations to long-run relative frequencies. If we say the probability of heads when flipping a fair coin is 0.5, we mean that if we flipped the coin many, many times, about half the flips would come up heads. This interpretation grounds probability in observable, repeatable phenomena.\nThe subjective (Bayesian) interpretation views probability as a measure of belief or uncertainty. A probability statement expresses the opinion of some individual regarding how certain an event is to occur, given their current information. This interpretation allows us to assign probabilities to one-time events and to update beliefs as we gather evidence.\nBoth interpretations have their uses, and modern statistics draws on both perspectives. For now, the frequentist interpretation provides good intuition for the concepts we will develop.",
    "crumbs": [
      "Probability and Distributions",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Foundations of Probability</span>"
    ]
  },
  {
    "objectID": "chapters/07-probability-foundations.html#random-variables-and-sample-spaces",
    "href": "chapters/07-probability-foundations.html#random-variables-and-sample-spaces",
    "title": "7  Foundations of Probability",
    "section": "7.3 Random Variables and Sample Spaces",
    "text": "7.3 Random Variables and Sample Spaces\nA random variable is a quantity that can take on different values with different probabilities. The outcome of a coin flip, the number of bacterial colonies on a plate, and the expression level of a gene are all random variables.\nThe sample space of a random variable is the set of all possible values it can take. For a coin flip, the sample space is {Heads, Tails}. For a die roll, it is {1, 2, 3, 4, 5, 6}. For the concentration of a protein, it might be any non-negative real number.\nA probability distribution describes how likely each value in the sample space is:\n\nFor discrete random variables (those that take distinct values), we use a probability mass function that gives the probability of each possible value\nFor continuous random variables (those that can take any value in a range), we use a probability density function from which probabilities are calculated by integration\n\nOne fundamental rule: the probabilities across the entire sample space must sum (or integrate) to 1. Something from the sample space must happen.",
    "crumbs": [
      "Probability and Distributions",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Foundations of Probability</span>"
    ]
  },
  {
    "objectID": "chapters/07-probability-foundations.html#discrete-probability",
    "href": "chapters/07-probability-foundations.html#discrete-probability",
    "title": "7  Foundations of Probability",
    "section": "7.4 Discrete Probability",
    "text": "7.4 Discrete Probability\nWe start by covering some basic principles related to categorical data. This subset of probability is referred to as discrete probability. It will help us understand the probability theory we will later introduce for numeric and continuous data, which is much more common in data science applications.\n\n7.4.1 Relative Frequency\nA precise definition of probability can be given by noting all possible outcomes and counting how many satisfy the condition for our event. For example, if we have 2 red beads and 3 blue beads inside an urn and we pick one at random, what is the probability of picking a red one?\nOur intuition tells us that the answer is 2/5 or 40%. There are five possible outcomes, of which two satisfy the condition necessary for the event “pick a red bead”. Since each of the five outcomes has the same chance of occurring, we conclude that the probability is 0.4 for red and 0.6 for blue.\nA more tangible way to think about the probability of an event is as the proportion of times the event occurs when we repeat the experiment an infinite number of times, independently, and under the same conditions.\n\n\n7.4.2 Notation\nWe use the notation \\(\\mbox{Pr}(A)\\) to denote the probability of event \\(A\\) happening. We use the very general term event to refer to things that can happen when something occurs by chance. In our previous example, the event was “picking a red bead”. In a political poll in which we call 100 likely voters at random, an example of an event is “calling 48 Democrats and 52 Republicans”.\nIn data science applications, we will often deal with continuous variables. These events will often be things like “is this person taller than 6 feet”. In this case, we write events in a more mathematical form: \\(X \\geq 6\\).\n\n\n7.4.3 Probability Distributions for Categorical Data\nIf we know the relative frequency of the different categories, defining a distribution for categorical outcomes is straightforward. We simply assign a probability to each category. In cases that can be thought of as beads in an urn, for each bead type, their proportion defines the distribution.\nIf we are randomly calling likely voters from a population that is 44% Democrat, 44% Republican, 10% undecided, and 2% Green Party, these proportions define the probability for each group:\n\n\n\nGroup\nProbability\n\n\n\n\nRepublican\n0.44\n\n\nDemocrat\n0.44\n\n\nUndecided\n0.10\n\n\nGreen\n0.02",
    "crumbs": [
      "Probability and Distributions",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Foundations of Probability</span>"
    ]
  },
  {
    "objectID": "chapters/07-probability-foundations.html#distribution-moments-and-parameters",
    "href": "chapters/07-probability-foundations.html#distribution-moments-and-parameters",
    "title": "7  Foundations of Probability",
    "section": "7.5 Distribution Moments and Parameters",
    "text": "7.5 Distribution Moments and Parameters\nProbability distributions can be characterized by their moments—metrics that describe the shape of the distribution. The first four moments correspond to important properties:\n\nMean (\\(\\mu\\)) - the center or expected value\nVariance (\\(\\sigma^2\\)) - the spread or dispersion\nSkewness - the asymmetry of the distribution\nKurtosis - the “tailedness” or peakedness\n\nFor a discrete random variable X, the expected value (mean) is:\n\\[E[X] = \\sum_{\\text{all } x} x \\cdot P(X = x) = \\mu\\]\nThe variance measures dispersion around the mean:\n\\[\\text{Var}(X) = E[(X - \\mu)^2] = \\sigma^2\\]\nThese parameters are crucial because they describe real properties of the systems we study. In biology, for example, the mean height of a population tells us about the typical value, while the variance tells us about the diversity of heights among individuals.",
    "crumbs": [
      "Probability and Distributions",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Foundations of Probability</span>"
    ]
  },
  {
    "objectID": "chapters/07-probability-foundations.html#monte-carlo-simulations",
    "href": "chapters/07-probability-foundations.html#monte-carlo-simulations",
    "title": "7  Foundations of Probability",
    "section": "7.6 Monte Carlo Simulations",
    "text": "7.6 Monte Carlo Simulations\nComputers provide a way to actually perform random experiments. Random number generators permit us to mimic the process of picking at random. An example is the sample function in R.\nFirst, we use the function rep to generate the urn:\n\n\nCode\nbeads &lt;- rep(c(\"red\", \"blue\"), times = c(2, 3))\nbeads\n\n\n[1] \"red\"  \"red\"  \"blue\" \"blue\" \"blue\"\n\n\nThen use sample to pick a bead at random:\n\n\nCode\nsample(beads, 1)\n\n\n[1] \"red\"\n\n\nThis line of code produces one random outcome. We want to repeat this experiment a large enough number of times to make the results practically equivalent to repeating forever. This is an example of a Monte Carlo simulation.\nTo perform our first Monte Carlo simulation, we use the replicate function, which permits us to repeat the same task any number of times. Here, we repeat the random event B = 10,000 times:\n\n\nCode\nset.seed(1986)  # For reproducibility\nB &lt;- 10000\nevents &lt;- replicate(B, sample(beads, 1))\n\n\nWe can now see if our definition actually agrees with this Monte Carlo simulation approximation:\n\n\nCode\ntab &lt;- table(events)\nprop.table(tab)\n\n\nevents\n  blue    red \n0.6014 0.3986 \n\n\nThe numbers above are the estimated probabilities provided by this Monte Carlo simulation. Statistical theory tells us that as \\(B\\) gets larger, the estimates get closer to 3/5 = 0.6 and 2/5 = 0.4.\n\n7.6.1 With and Without Replacement\nThe function sample has an argument that permits us to pick more than one element from the urn. By default, this selection occurs without replacement: after a bead is selected, it is not put back in the bag.\n\n\nCode\nsample(beads, 5)\n\n\n[1] \"blue\" \"red\"  \"blue\" \"red\"  \"blue\"\n\n\nThis results in rearrangements that always have three blue and two red beads because we can’t select more beads than exist.\nHowever, we can sample with replacement: return the bead back to the urn after selecting it:\n\n\nCode\nevents &lt;- sample(beads, B, replace = TRUE)\nprop.table(table(events))\n\n\nevents\n blue   red \n0.601 0.399 \n\n\nNot surprisingly, we get results very similar to those previously obtained with replicate.",
    "crumbs": [
      "Probability and Distributions",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Foundations of Probability</span>"
    ]
  },
  {
    "objectID": "chapters/07-probability-foundations.html#the-bernoulli-distribution",
    "href": "chapters/07-probability-foundations.html#the-bernoulli-distribution",
    "title": "7  Foundations of Probability",
    "section": "7.7 The Bernoulli Distribution",
    "text": "7.7 The Bernoulli Distribution\nThe simplest probability distribution describes a single event with two possible outcomes—success or failure, yes or no, heads or tails. This is the Bernoulli distribution.\nConsider flipping a fair coin once:\n\\[P(X = \\text{Head}) = \\frac{1}{2} = 0.5 = p\\]\nAnd the probability of tails is:\n\\[P(X = \\text{Tail}) = \\frac{1}{2} = 0.5 = 1 - p = q\\]\nIf the coin is not fair, \\(p\\) might differ from 0.5, but the probabilities still sum to 1:\n\\[p + (1-p) = 1\\]\nThis same framework applies to any binary outcome: whether a patient responds to treatment, whether an allele is inherited from a parent, or whether a product passes quality control.\n\n\nCode\n# Flip a coin 1000 times\nset.seed(42)\nflips &lt;- rbinom(1000, 1, 0.5)\n\nbarplot(table(flips) / 1000,\n        names.arg = c(\"Tails\", \"Heads\"),\n        ylab = \"Probability\",\n        ylim = c(0, 0.75),\n        col = \"steelblue\",\n        main = \"Estimated Bernoulli Distribution\")",
    "crumbs": [
      "Probability and Distributions",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Foundations of Probability</span>"
    ]
  },
  {
    "objectID": "chapters/07-probability-foundations.html#probability-rules",
    "href": "chapters/07-probability-foundations.html#probability-rules",
    "title": "7  Foundations of Probability",
    "section": "7.8 Probability Rules",
    "text": "7.8 Probability Rules\nTwo fundamental rules govern how probabilities combine. Most probability distributions can be built up from these simple rules.\n\n7.8.1 The AND Rule (Multiplication)\nThe probability that two independent events both occur is the product of their individual probabilities. If you flip a coin twice:\n\\[P(\\text{First = Head AND Second = Head}) = p \\times p = p^2\\]\nMore generally, for independent events A and B:\n\\[P(A \\text{ and } B) = P(A) \\times P(B)\\]\nFor a fair coin with \\(p = 0.5\\):\n\n\\(P(\\text{HH}) = 0.5 \\times 0.5 = 0.25\\)\n\\(P(\\text{HT}) = 0.5 \\times 0.5 = 0.25\\)\n\\(P(\\text{TH}) = 0.5 \\times 0.5 = 0.25\\)\n\\(P(\\text{TT}) = 0.5 \\times 0.5 = 0.25\\)\n\n\n\n7.8.2 The OR Rule (Addition)\nThe probability that at least one of two mutually exclusive events occurs is the sum of their probabilities:\n\\[P(A \\text{ or } B) = P(A) + P(B) - P(A \\text{ and } B)\\]\nFor mutually exclusive events (those that cannot both occur), the intersection is empty:\n\\[P(A \\text{ or } B) = P(A) + P(B)\\]\nThe probability of getting exactly one head in two flips (either HT or TH):\n\\[P(\\text{one head}) = P(\\text{HT}) + P(\\text{TH}) = 0.25 + 0.25 = 0.5\\]\n\n\n7.8.3 Multiplication Rule Under Independence\nWhen events are independent, the multiplication rule simplifies:\n\\[P(A \\text{ and } B \\text{ and } C) = P(A) \\times P(B) \\times P(C)\\]\nBut we must be careful—assuming independence can result in very different and incorrect probability calculations when we don’t actually have independence.\n\n\n\n\n\n\nIndependence Matters\n\n\n\nImagine a court case where the suspect was described as having a mustache and a beard. The defendant has both, and the prosecution brings in an “expert” who testifies that 1/10 men have beards and 1/5 have mustaches, concluding that only \\(1/10 \\times 1/5 = 0.02\\) have both.\nBut to multiply like this we need to assume independence! If the conditional probability of a man having a mustache given that he has a beard is 0.95, then the correct probability is much higher: \\(1/10 \\times 0.95 = 0.095\\).",
    "crumbs": [
      "Probability and Distributions",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Foundations of Probability</span>"
    ]
  },
  {
    "objectID": "chapters/07-probability-foundations.html#independence",
    "href": "chapters/07-probability-foundations.html#independence",
    "title": "7  Foundations of Probability",
    "section": "7.9 Independence",
    "text": "7.9 Independence\nWe say two events are independent if the outcome of one does not affect the other. The classic example is coin tosses. Every time we toss a fair coin, the probability of seeing heads is 1/2 regardless of what previous tosses have revealed. The same is true when we pick beads from an urn with replacement.\nMany examples of events that are not independent come from card games. When we deal the first card, the probability of getting a King is 1/13 since there are thirteen possibilities. Now if we deal a King for the first card and don’t replace it into the deck, the probability of a second card being a King is only 3/51. These events are not independent: the first outcome affected the next one.\n\n\nCode\n# Demonstrate non-independence with sequential draws\nset.seed(1)\nx &lt;- sample(beads, 5)\nx[1:4]  # First four draws\n\n\n[1] \"red\"  \"blue\" \"blue\" \"blue\"\n\n\nCode\nx[5]    # If first four are blue, the fifth must be...\n\n\n[1] \"red\"\n\n\nIf you have to guess the color of the first bead, you would predict blue since blue has a 60% chance. But if you know the first four were blue, the probability of the fifth being red is now 100%, not 40%. The events are not independent, so the probabilities change.",
    "crumbs": [
      "Probability and Distributions",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Foundations of Probability</span>"
    ]
  },
  {
    "objectID": "chapters/07-probability-foundations.html#conditional-probabilities",
    "href": "chapters/07-probability-foundations.html#conditional-probabilities",
    "title": "7  Foundations of Probability",
    "section": "7.10 Conditional Probabilities",
    "text": "7.10 Conditional Probabilities\nWhen events are not independent, conditional probabilities are useful. The conditional probability \\(P(B|A)\\) is the probability of B given that A has occurred:\n\\[P(\\text{Card 2 is a King} \\mid \\text{Card 1 is a King}) = \\frac{3}{51}\\]\nWe use the \\(\\mid\\) as shorthand for “given that” or “conditional on”.\nWhen two events A and B are independent:\n\\[P(A \\mid B) = P(A)\\]\nThis is the mathematical definition of independence: the fact that B happened does not affect the probability of A happening.\nThe general multiplication rule relates joint and conditional probability:\n\\[P(A \\text{ and } B) = P(A) \\times P(B \\mid A)\\]\nThis can be extended to more events:\n\\[P(A \\text{ and } B \\text{ and } C) = P(A) \\times P(B \\mid A) \\times P(C \\mid A \\text{ and } B)\\]",
    "crumbs": [
      "Probability and Distributions",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Foundations of Probability</span>"
    ]
  },
  {
    "objectID": "chapters/07-probability-foundations.html#bayes-theorem",
    "href": "chapters/07-probability-foundations.html#bayes-theorem",
    "title": "7  Foundations of Probability",
    "section": "7.11 Bayes’ Theorem",
    "text": "7.11 Bayes’ Theorem\nRearranging the multiplication rule yields Bayes’ theorem, a cornerstone of probabilistic reasoning:\n\\[P(A|B) = \\frac{P(B|A) \\times P(A)}{P(B)}\\]\nBayes’ theorem tells us how to update our beliefs about A after observing B. In Bayesian statistics, this is written as:\n\\[P(\\theta|d) = \\frac{P(d|\\theta) \\times P(\\theta)}{P(d)}\\]\nwhere:\n\n\\(P(\\theta|d)\\) = posterior probability distribution\n\\(P(d|\\theta)\\) = likelihood function for \\(\\theta\\)\n\\(P(\\theta)\\) = prior probability distribution\n\\(P(d)\\) = marginal likelihood (normalizing constant)",
    "crumbs": [
      "Probability and Distributions",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Foundations of Probability</span>"
    ]
  },
  {
    "objectID": "chapters/07-probability-foundations.html#likelihood-vs.-probability",
    "href": "chapters/07-probability-foundations.html#likelihood-vs.-probability",
    "title": "7  Foundations of Probability",
    "section": "7.12 Likelihood vs. Probability",
    "text": "7.12 Likelihood vs. Probability\nA subtle but important distinction exists between probability and likelihood.\nProbability is the chance of observing particular data given a model or parameter value. If we know a coin has \\(p = 0.5\\), what is the probability of observing 7 heads in 10 flips?\nLikelihood is how well a parameter value explains observed data. Given that we observed 7 heads in 10 flips, how likely is it that the true probability is \\(p = 0.5\\) versus \\(p = 0.7\\)?\nMathematically, the likelihood function uses the same formula as probability, but we think of it differently:\n\\[L(\\text{parameter} | \\text{data}) = P(\\text{data} | \\text{parameter})\\]\nMaximum likelihood estimation finds the parameter value that makes the observed data most probable—the value that maximizes the likelihood function.",
    "crumbs": [
      "Probability and Distributions",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Foundations of Probability</span>"
    ]
  },
  {
    "objectID": "chapters/07-probability-foundations.html#combinations-and-permutations",
    "href": "chapters/07-probability-foundations.html#combinations-and-permutations",
    "title": "7  Foundations of Probability",
    "section": "7.13 Combinations and Permutations",
    "text": "7.13 Combinations and Permutations\nFor more complicated probability calculations, we need to count possibilities systematically. The gtools package provides useful functions.\n\n7.13.1 Permutations (Order Matters)\nA permutation is an arrangement where order matters. For any list of size n, the permutations function computes all different arrangements when selecting r items:\n\n\nCode\n# All ways to arrange 2 items from {1, 2, 3}\npermutations(3, 2)\n\n\n     [,1] [,2]\n[1,]    1    2\n[2,]    1    3\n[3,]    2    1\n[4,]    2    3\n[5,]    3    1\n[6,]    3    2\n\n\nNotice that order matters: 3,1 is different than 1,3. Also, (1,1), (2,2), and (3,3) do not appear because once we pick a number, it can’t appear again.\n\n\n7.13.2 Combinations (Order Doesn’t Matter)\nA combination is a selection where order doesn’t matter:\n\n\nCode\ncombinations(3, 2)\n\n\n     [,1] [,2]\n[1,]    1    2\n[2,]    1    3\n[3,]    2    3\n\n\nThe outcome (2,1) doesn’t appear because (1,2) already represents the same combination.\n\n\n7.13.3 Example: Blackjack\nLet’s compute the probability of getting a “Natural 21” in Blackjack—an Ace and a face card in the first two cards:\n\n\nCode\n# Build a deck\nsuits &lt;- c(\"Diamonds\", \"Clubs\", \"Hearts\", \"Spades\")\nnumbers &lt;- c(\"Ace\", \"Deuce\", \"Three\", \"Four\", \"Five\", \"Six\", \"Seven\",\n             \"Eight\", \"Nine\", \"Ten\", \"Jack\", \"Queen\", \"King\")\ndeck &lt;- expand.grid(number = numbers, suit = suits)\ndeck &lt;- paste(deck$number, deck$suit)\n\n# Define aces and face cards\naces &lt;- paste(\"Ace\", suits)\nfacecard &lt;- c(\"King\", \"Queen\", \"Jack\", \"Ten\")\nfacecard &lt;- expand.grid(number = facecard, suit = suits)\nfacecard &lt;- paste(facecard$number, facecard$suit)\n\n# All possible two-card hands (order doesn't matter)\nhands &lt;- combinations(52, 2, v = deck)\n\n# Probability of Natural 21\nmean((hands[,1] %in% aces & hands[,2] %in% facecard) |\n     (hands[,2] %in% aces & hands[,1] %in% facecard))\n\n\n[1] 0.04826546",
    "crumbs": [
      "Probability and Distributions",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Foundations of Probability</span>"
    ]
  },
  {
    "objectID": "chapters/07-probability-foundations.html#classic-examples",
    "href": "chapters/07-probability-foundations.html#classic-examples",
    "title": "7  Foundations of Probability",
    "section": "7.14 Classic Examples",
    "text": "7.14 Classic Examples\n\n7.14.1 The Monty Hall Problem\nIn the game show “Let’s Make a Deal,” contestants pick one of three doors. Behind one door is a car; behind the others are goats. After you pick a door, Monty Hall opens one of the remaining doors to reveal a goat. Then he asks: “Do you want to switch doors?”\nIntuition suggests it shouldn’t matter—you’re choosing between two doors, so shouldn’t the probability be 50-50? Let’s use Monte Carlo simulation:\n\n\nCode\nB &lt;- 10000\nmonty_hall &lt;- function(strategy) {\n  doors &lt;- as.character(1:3)\n  prize &lt;- sample(c(\"car\", \"goat\", \"goat\"))\n  prize_door &lt;- doors[prize == \"car\"]\n  my_pick &lt;- sample(doors, 1)\n  show &lt;- sample(doors[!doors %in% c(my_pick, prize_door)], 1)\n\n  if (strategy == \"stick\") {\n    choice &lt;- my_pick\n  } else {\n    choice &lt;- doors[!doors %in% c(my_pick, show)]\n  }\n  choice == prize_door\n}\n\nstick_wins &lt;- replicate(B, monty_hall(\"stick\"))\nswitch_wins &lt;- replicate(B, monty_hall(\"switch\"))\n\ncat(\"Probability of winning when sticking:\", mean(stick_wins), \"\\n\")\n\n\nProbability of winning when sticking: 0.3365 \n\n\nCode\ncat(\"Probability of winning when switching:\", mean(switch_wins), \"\\n\")\n\n\nProbability of winning when switching: 0.6643 \n\n\nSwitching doubles your chances! The key insight: when you first pick, you have a 1/3 chance of being right. Monty’s reveal doesn’t change that. Since the probability the car is behind one of the other doors was 2/3, and Monty showed you which one doesn’t have it, switching gives you that 2/3 probability.\n\n\n7.14.2 The Birthday Problem\nIn a room with 50 people, what’s the probability that at least two share a birthday?\n\n\nCode\n# Monte Carlo simulation\nB &lt;- 10000\nsame_birthday &lt;- function(n) {\n  bdays &lt;- sample(1:365, n, replace = TRUE)\n  any(duplicated(bdays))\n}\n\nresults &lt;- replicate(B, same_birthday(50))\ncat(\"Probability with 50 people:\", mean(results), \"\\n\")\n\n\nProbability with 50 people: 0.9701 \n\n\nCode\n# How does this change with group size?\ncompute_prob &lt;- function(n, B = 10000) {\n  results &lt;- replicate(B, same_birthday(n))\n  mean(results)\n}\n\nn &lt;- seq(1, 60)\nprob &lt;- sapply(n, compute_prob)\nqplot(n, prob, geom = \"line\") +\n  geom_hline(yintercept = 0.5, linetype = \"dashed\", color = \"red\") +\n  labs(x = \"Group Size\", y = \"Probability of Shared Birthday\",\n       title = \"The Birthday Problem\")\n\n\n\n\n\n\n\n\n\nWith just 23 people, there’s already a 50% chance of a shared birthday! People tend to underestimate these probabilities because they think about the probability that someone shares their birthday, not the probability that any two people share a birthday.\nWe can also compute this exactly using the multiplication rule:\n\n\nCode\n# Probability that all n people have UNIQUE birthdays\nexact_prob &lt;- function(n) {\n  prob_unique &lt;- seq(365, 365 - n + 1) / 365\n  1 - prod(prob_unique)\n}\n\neprob &lt;- sapply(n, exact_prob)\ncat(\"Exact probability with 50 people:\", exact_prob(50), \"\\n\")\n\n\nExact probability with 50 people: 0.9703736",
    "crumbs": [
      "Probability and Distributions",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Foundations of Probability</span>"
    ]
  },
  {
    "objectID": "chapters/07-probability-foundations.html#covariance-and-correlation",
    "href": "chapters/07-probability-foundations.html#covariance-and-correlation",
    "title": "7  Foundations of Probability",
    "section": "7.15 Covariance and Correlation",
    "text": "7.15 Covariance and Correlation\nWhen two variables are not independent, they share information—knowing one tells you something about the other. This shared information is quantified by covariance, a measure of how two variables vary together.\nIf high values of X tend to occur with high values of Y (and low with low), the covariance is positive. If high values of X tend to occur with low values of Y, the covariance is negative. If there is no relationship, the covariance is near zero.\nCorrelation is covariance standardized to fall between -1 and 1, making it easier to interpret. A correlation of 1 means perfect positive linear relationship; -1 means perfect negative linear relationship; 0 means no linear relationship.\nThese concepts will become central when we discuss regression and other methods for relating variables to each other.",
    "crumbs": [
      "Probability and Distributions",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Foundations of Probability</span>"
    ]
  },
  {
    "objectID": "chapters/07-probability-foundations.html#how-large-is-large-enough-for-monte-carlo",
    "href": "chapters/07-probability-foundations.html#how-large-is-large-enough-for-monte-carlo",
    "title": "7  Foundations of Probability",
    "section": "7.16 How Large is “Large Enough” for Monte Carlo?",
    "text": "7.16 How Large is “Large Enough” for Monte Carlo?\nThe theory described here requires repeating experiments over and over forever. In practice, we can’t do this. In the examples above, we used \\(B = 10,000\\) Monte Carlo experiments and it turned out to provide accurate estimates.\nOne practical approach is to check for the stability of the estimate:\n\n\nCode\nB_values &lt;- 10^seq(1, 5, len = 100)\ncompute_prob_B &lt;- function(B, n = 25) {\n  same_day &lt;- replicate(B, same_birthday(n))\n  mean(same_day)\n}\n\nprob &lt;- sapply(B_values, compute_prob_B)\nqplot(log10(B_values), prob, geom = \"line\") +\n  geom_hline(yintercept = exact_prob(25), color = \"red\", linetype = \"dashed\") +\n  labs(x = \"log10(Number of Simulations)\", y = \"Estimated Probability\",\n       title = \"Monte Carlo Convergence\")\n\n\n\n\n\n\n\n\n\nThe values start to stabilize (vary less than 0.01) around 1000 simulations. The exact probability is shown in red.",
    "crumbs": [
      "Probability and Distributions",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Foundations of Probability</span>"
    ]
  },
  {
    "objectID": "chapters/07-probability-foundations.html#summary",
    "href": "chapters/07-probability-foundations.html#summary",
    "title": "7  Foundations of Probability",
    "section": "7.17 Summary",
    "text": "7.17 Summary\nThis chapter introduced the language of probability:\n\nRandom variables can take different values with different probabilities\nThe sample space contains all possible outcomes\nProbability distributions describe how likely each outcome is\nThe AND rule (multiply) and OR rule (add) combine probabilities\nIndependence means one event doesn’t affect another’s probability\nConditional probability describes probability given other information\nBayes’ theorem updates beliefs based on new evidence\nMonte Carlo simulations estimate probabilities through repeated random sampling\nClassic problems like Monty Hall and birthdays reveal counterintuitive probability results\n\nUnderstanding these foundations is essential for all statistical inference that follows.",
    "crumbs": [
      "Probability and Distributions",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Foundations of Probability</span>"
    ]
  },
  {
    "objectID": "chapters/07-probability-foundations.html#practice-exercises",
    "href": "chapters/07-probability-foundations.html#practice-exercises",
    "title": "7  Foundations of Probability",
    "section": "7.18 Practice Exercises",
    "text": "7.18 Practice Exercises\nFor hands-on practice with probability concepts, see Section 35.6 in the Practice Exercises appendix. The exercises include:\n\nSimulating coin flips and exploring the law of large numbers\nWorking with the binomial distribution\nExploring the birthday problem through Monte Carlo simulation\nUnderstanding conditional probability with card simulations\nSimulating the Monty Hall problem",
    "crumbs": [
      "Probability and Distributions",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Foundations of Probability</span>"
    ]
  },
  {
    "objectID": "chapters/07-probability-foundations.html#additional-resources",
    "href": "chapters/07-probability-foundations.html#additional-resources",
    "title": "7  Foundations of Probability",
    "section": "7.19 Additional Resources",
    "text": "7.19 Additional Resources\n\nIrizarry (2019) - A gitbook by a statistician with excellent introductions to key topics in statistical inference\nLogan (2010) - A comprehensive introduction to R for statistical analysis\nFor a detailed reference of common probability distributions, see Chapter 29\n\n\n\n\n\n\n\nIrizarry, Rafael A. 2019. Introduction to Data Science. Self-published. https://rafalab.github.io/dsbook/.\n\n\nLogan, Murray. 2010. Biostatistical Design and Analysis Using r. Wiley-Blackwell.",
    "crumbs": [
      "Probability and Distributions",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Foundations of Probability</span>"
    ]
  },
  {
    "objectID": "chapters/08-discrete-distributions.html",
    "href": "chapters/08-discrete-distributions.html",
    "title": "8  Discrete Probability Distributions",
    "section": "",
    "text": "8.1 What Are Discrete Distributions?\nDiscrete probability distributions describe random variables that take on distinct, countable values. The number of heads in ten coin flips, the count of bacterial colonies on a plate, and the number of defective items in a batch are all discrete random variables. Understanding these distributions allows you to model count data, calculate probabilities of specific outcomes, and perform statistical tests.",
    "crumbs": [
      "Probability and Distributions",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Discrete Probability Distributions</span>"
    ]
  },
  {
    "objectID": "chapters/08-discrete-distributions.html#the-binomial-distribution",
    "href": "chapters/08-discrete-distributions.html#the-binomial-distribution",
    "title": "8  Discrete Probability Distributions",
    "section": "8.2 The Binomial Distribution",
    "text": "8.2 The Binomial Distribution\nThe binomial distribution arises when you perform a fixed number of independent trials, each with the same probability of success. It answers questions like: If I flip a coin 20 times, what is the probability of getting exactly 12 heads?\nThe probability of observing exactly \\(k\\) successes in \\(n\\) trials, when each trial has success probability \\(p\\), is:\n\\[P(X = k) = \\binom{n}{k} p^k (1-p)^{n-k}\\]\nThe binomial coefficient \\(\\binom{n}{k}\\) counts the number of ways to arrange \\(k\\) successes among \\(n\\) trials.\nThe mean of a binomial distribution is \\(\\mu = np\\) and the variance is \\(\\sigma^2 = np(1-p)\\).\n\n\nCode\n# Simulate 1000 experiments of 20 coin flips each\nset.seed(42)\nheads &lt;- rbinom(n = 1000, size = 20, prob = 0.5)\n\nhist(heads, breaks = 0:20, col = \"steelblue\", \n     main = \"Distribution of Heads in 20 Coin Flips\",\n     xlab = \"Number of Heads\")\n\n\n\n\n\n\n\n\n\nWith a fair coin (\\(p = 0.5\\)) and 20 flips, we expect about 10 heads on average. The distribution is symmetric and centered at 10.\nIn R, functions for the binomial distribution include:\n\ndbinom(k, n, p) - probability of exactly k successes\npbinom(k, n, p) - probability of k or fewer successes (cumulative)\nqbinom(q, n, p) - quantile function (inverse of cumulative)\nrbinom(n, size, p) - generate random samples\n\n\n\nCode\n# Probability of exactly 10 heads in 20 flips\ndbinom(10, size = 20, prob = 0.5)\n\n\n[1] 0.1761971\n\n\nCode\n# Probability of 10 or fewer heads\npbinom(10, size = 20, prob = 0.5)\n\n\n[1] 0.5880985",
    "crumbs": [
      "Probability and Distributions",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Discrete Probability Distributions</span>"
    ]
  },
  {
    "objectID": "chapters/08-discrete-distributions.html#the-poisson-distribution",
    "href": "chapters/08-discrete-distributions.html#the-poisson-distribution",
    "title": "8  Discrete Probability Distributions",
    "section": "8.3 The Poisson Distribution",
    "text": "8.3 The Poisson Distribution\nThe Poisson distribution models the number of events occurring in a fixed interval of time or space, when events occur independently at a constant average rate. It is appropriate for count data like the number of mutations in a DNA sequence, phone calls received per hour, or organisms per quadrat in an ecological survey.\nThe probability of observing exactly \\(r\\) events when the average rate is \\(\\lambda\\) is:\n\\[P(Y = r) = \\frac{e^{-\\lambda} \\lambda^r}{r!}\\]\nA remarkable property of the Poisson distribution is that the mean and variance are both equal to \\(\\lambda\\). This provides a simple check: if your count data has variance much larger than its mean, a simple Poisson model may not be appropriate (a situation called overdispersion, common in biological data).\n\n\n\n\n\n\n\nCode\n# Show Poisson distributions with different lambda values\npar(mfrow = c(2, 2))\nfor (lambda in c(1, 3, 5, 10)) {\n  x &lt;- 0:20\n  plot(x, dpois(x, lambda), type = \"h\", lwd = 3, col = \"steelblue\",\n       main = paste(\"Poisson, λ =\", lambda),\n       xlab = \"Count\", ylab = \"Probability\")\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAs \\(\\lambda\\) increases, the Poisson distribution becomes more symmetric and approaches a normal distribution.\n\n8.3.1 A Historical Example: Horse Kick Deaths in the Prussian Army\nOne of the earliest applications of the Poisson distribution was in 1898, when it was used to model the number of soldier deaths from horse kicks in 14 different corps of the Prussian army. As shown in Figure 8.1, the Poisson distribution does a remarkable job modeling these unfortunate events.\n\n\nCode\n# Data from Ladislaus Bortkiewicz (1898)\nobserved &lt;- c(109, 65, 22, 3, 1)  # Deaths: 0, 1, 2, 3, 4\nexpected &lt;- dpois(0:4, lambda = 0.7) * 200  # 200 corps-years, estimated lambda\n\ndeaths &lt;- 0:4\nbarplot(rbind(observed, expected), beside = TRUE,\n        col = c(\"steelblue\", \"coral\"),\n        names.arg = deaths,\n        ylim = c(0, 120),\n        ylab = \"Frequency\",\n        xlab = \"Deaths per Year\",\n        main = \"Horse Kick Deaths: Observed vs. Poisson Predicted\")\nlegend(\"topright\", fill = c(\"steelblue\", \"coral\"),\n       legend = c(\"Observed\", \"Predicted from Poisson\"), bty = \"n\")\n\n\n\n\n\n\n\n\nFigure 8.1: Distribution of horse kick deaths per corps per year in the Prussian army (1875-1894). The Poisson distribution closely matches the observed data.\n\n\n\n\n\nThe Poisson distribution is particularly effective at modeling the distribution of rare, independent events like this.\n\n\nCode\n# Probability of exactly 2 events when lambda = 1\ndpois(x = 2, lambda = 1)\n\n\n[1] 0.1839397\n\n\nCode\n# Plot Poisson probabilities\nplot(dpois(x = 0:10, lambda = 3), type = \"h\", lwd = 3,\n     xlab = \"Count\", ylab = \"Probability\",\n     main = \"Poisson Distribution (λ = 3)\")",
    "crumbs": [
      "Probability and Distributions",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Discrete Probability Distributions</span>"
    ]
  },
  {
    "objectID": "chapters/08-discrete-distributions.html#the-geometric-distribution",
    "href": "chapters/08-discrete-distributions.html#the-geometric-distribution",
    "title": "8  Discrete Probability Distributions",
    "section": "8.4 The Geometric Distribution",
    "text": "8.4 The Geometric Distribution\nThe geometric distribution describes the number of trials needed to achieve the first success. If each trial has success probability \\(p\\), the probability that the first success occurs on trial \\(k\\) is:\n\\[P(X = k) = (1-p)^{k-1} p\\]\nThe mean is \\(1/p\\) and the variance is \\((1-p)/p^2\\).\nFor example, if the probability of a cell successfully transfecting is 0.1, the geometric distribution tells us how many cells we need to attempt before getting our first successful transfection.\n\n\nCode\n# Probability of first success on each trial\np &lt;- 0.1\ntrials &lt;- 1:30\nprobs &lt;- dgeom(trials - 1, prob = p)  # dgeom counts failures before first success\n\nplot(trials, probs, type = \"h\", lwd = 2, col = \"steelblue\",\n     xlab = \"Trial Number of First Success\",\n     ylab = \"Probability\",\n     main = \"Geometric Distribution (p = 0.1)\")",
    "crumbs": [
      "Probability and Distributions",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Discrete Probability Distributions</span>"
    ]
  },
  {
    "objectID": "chapters/08-discrete-distributions.html#the-negative-binomial-distribution",
    "href": "chapters/08-discrete-distributions.html#the-negative-binomial-distribution",
    "title": "8  Discrete Probability Distributions",
    "section": "8.5 The Negative Binomial Distribution",
    "text": "8.5 The Negative Binomial Distribution\nThe negative binomial distribution generalizes the geometric distribution. It describes the number of trials needed to achieve \\(r\\) successes. If each trial has success probability \\(p\\), the probability that the \\(r\\)th success occurs on trial \\(k\\) is:\n\\[P(X = k) = \\binom{k-1}{r-1} p^r (1-p)^{k-r}\\]\nThe mean is \\(r/p\\) and the variance is \\(r(1-p)/p^2\\).\nConsider a predator that must capture 10 prey to reach reproductive maturity. If the daily probability of catching prey is 0.1, the negative binomial distribution describes when the predator will be ready to reproduce.\n\n\n\n\n\nThe negative binomial is also commonly used to model overdispersed count data—counts with variance greater than their mean—which the simple Poisson cannot accommodate.",
    "crumbs": [
      "Probability and Distributions",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Discrete Probability Distributions</span>"
    ]
  },
  {
    "objectID": "chapters/08-discrete-distributions.html#common-pattern-in-r",
    "href": "chapters/08-discrete-distributions.html#common-pattern-in-r",
    "title": "8  Discrete Probability Distributions",
    "section": "8.6 Common Pattern in R",
    "text": "8.6 Common Pattern in R\nR uses a consistent naming convention for distribution functions:\n\n\n\nPrefix\nPurpose\nExample\n\n\n\n\nd\nProbability mass/density function\ndbinom(), dpois()\n\n\np\nCumulative distribution function\npbinom(), ppois()\n\n\nq\nQuantile function\nqbinom(), qpois()\n\n\nr\nRandom number generation\nrbinom(), rpois()\n\n\n\nThis pattern applies to all distributions in R:\n\n\n\nDistribution\nFunctions\n\n\n\n\nBinomial\ndbinom, pbinom, qbinom, rbinom\n\n\nPoisson\ndpois, ppois, qpois, rpois\n\n\nGeometric\ndgeom, pgeom, qgeom, rgeom\n\n\nNegative Binomial\ndnbinom, pnbinom, qnbinom, rnbinom",
    "crumbs": [
      "Probability and Distributions",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Discrete Probability Distributions</span>"
    ]
  },
  {
    "objectID": "chapters/08-discrete-distributions.html#choosing-the-right-distribution",
    "href": "chapters/08-discrete-distributions.html#choosing-the-right-distribution",
    "title": "8  Discrete Probability Distributions",
    "section": "8.7 Choosing the Right Distribution",
    "text": "8.7 Choosing the Right Distribution\nSelecting the appropriate distribution depends on the nature of your data and the process generating it.\nUse the binomial when you have a fixed number of independent trials with constant success probability and you are counting successes. Examples include the number of patients responding to treatment out of a fixed sample, the number of correct answers on a test, or the number of defective items in a batch.\nUse the Poisson when you are counting events in a fixed interval of time or space, events occur independently, and the average rate is constant. Examples include mutations per gene, radioactive decays per minute, or organisms per quadrat. Remember that for Poisson data, mean should approximately equal variance.\nUse the geometric when you are counting trials until the first success. Examples include the number of attempts until a successful measurement or the number of patients screened until finding one eligible for a trial.\nUse the negative binomial when counting trials until a specified number of successes, or when modeling overdispersed count data (variance exceeds mean).",
    "crumbs": [
      "Probability and Distributions",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Discrete Probability Distributions</span>"
    ]
  },
  {
    "objectID": "chapters/08-discrete-distributions.html#practice-with-simulations",
    "href": "chapters/08-discrete-distributions.html#practice-with-simulations",
    "title": "8  Discrete Probability Distributions",
    "section": "8.8 Practice with Simulations",
    "text": "8.8 Practice with Simulations\nUnderstanding distributions deepens through simulation. Generate data from each distribution, visualize it, and calculate summary statistics. Compare the theoretical mean and variance to what you observe in your simulated samples.\n\n\nCode\n# Compare theoretical and empirical properties\nset.seed(123)\n\n# Poisson with lambda = 5\npois_sample &lt;- rpois(10000, lambda = 5)\n\ncat(\"Poisson (λ = 5):\\n\")\n\n\nPoisson (λ = 5):\n\n\nCode\ncat(\"Theoretical mean:\", 5, \"  Observed:\", mean(pois_sample), \"\\n\")\n\n\nTheoretical mean: 5   Observed: 4.9746 \n\n\nCode\ncat(\"Theoretical var:\", 5, \"  Observed:\", var(pois_sample), \"\\n\\n\")\n\n\nTheoretical var: 5   Observed: 4.896444 \n\n\nCode\n# Binomial with n = 20, p = 0.3\nbinom_sample &lt;- rbinom(10000, size = 20, prob = 0.3)\n\ncat(\"Binomial (n = 20, p = 0.3):\\n\")\n\n\nBinomial (n = 20, p = 0.3):\n\n\nCode\ncat(\"Theoretical mean:\", 20 * 0.3, \"  Observed:\", mean(binom_sample), \"\\n\")\n\n\nTheoretical mean: 6   Observed: 5.9732 \n\n\nCode\ncat(\"Theoretical var:\", 20 * 0.3 * 0.7, \"  Observed:\", var(binom_sample), \"\\n\")\n\n\nTheoretical var: 4.2   Observed: 4.149097 \n\n\nThis kind of simulation-based exploration builds intuition that complements formal mathematical understanding.",
    "crumbs": [
      "Probability and Distributions",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Discrete Probability Distributions</span>"
    ]
  },
  {
    "objectID": "chapters/09-continuous-distributions.html",
    "href": "chapters/09-continuous-distributions.html",
    "title": "9  Continuous Probability Distributions",
    "section": "",
    "text": "9.1 From Discrete to Continuous\nMany quantities we measure—weight, concentration, time, temperature—can take any value within a range, not just discrete counts. These continuous random variables require a different mathematical treatment. Instead of probability mass functions that assign probabilities to specific values, we use probability density functions (PDFs) where probabilities come from integrating over intervals.\nFor a continuous random variable, the probability that it falls within an interval \\([a, b]\\) is:\n\\[P(a \\leq X \\leq b) = \\int_a^b f(x) \\, dx\\]\nwhere \\(f(x)\\) is the probability density function. The total area under the density curve must equal 1:\n\\[\\int_{-\\infty}^{\\infty} f(x) \\, dx = 1\\]\nNote that for continuous variables, the probability of any exact value is zero—only intervals have non-zero probability.",
    "crumbs": [
      "Probability and Distributions",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Continuous Probability Distributions</span>"
    ]
  },
  {
    "objectID": "chapters/09-continuous-distributions.html#the-uniform-distribution",
    "href": "chapters/09-continuous-distributions.html#the-uniform-distribution",
    "title": "9  Continuous Probability Distributions",
    "section": "9.2 The Uniform Distribution",
    "text": "9.2 The Uniform Distribution\nThe simplest continuous distribution is the uniform distribution, where all values in an interval are equally likely. If \\(X\\) is uniformly distributed between \\(a\\) and \\(b\\):\n\\[f(x) = \\frac{1}{b-a} \\quad \\text{for } a \\leq x \\leq b\\]\nThe mean is \\((a+b)/2\\) and the variance is \\((b-a)^2/12\\).\n\n\n\n\n\n\n\nCode\n# Uniform distribution between 0 and 10\nx &lt;- seq(0, 10, length.out = 100)\nplot(x, dunif(x, min = 0, max = 10), type = \"l\", lwd = 2,\n     xlab = \"x\", ylab = \"Density\",\n     main = \"Uniform Distribution (0, 10)\")\n\n\n\n\n\n\n\n\n\nThe uniform distribution is often used to model random number generation and situations where no outcome is favored over another within a range.",
    "crumbs": [
      "Probability and Distributions",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Continuous Probability Distributions</span>"
    ]
  },
  {
    "objectID": "chapters/09-continuous-distributions.html#the-exponential-distribution",
    "href": "chapters/09-continuous-distributions.html#the-exponential-distribution",
    "title": "9  Continuous Probability Distributions",
    "section": "9.3 The Exponential Distribution",
    "text": "9.3 The Exponential Distribution\nThe exponential distribution models waiting times between events in a Poisson process—the time until the next event when events occur randomly at a constant rate \\(\\lambda\\). Its density function is:\n\\[f(x) = \\lambda e^{-\\lambda x} \\quad \\text{for } x \\geq 0\\]\nThe mean is \\(1/\\lambda\\) and the variance is \\(1/\\lambda^2\\).\n\n\n\n\n\nIf a radioactive isotope has a decay rate of \\(\\lambda = 0.1\\) per minute, the time until the next decay follows an exponential distribution with mean 10 minutes.\n\n\nCode\n# Exponential distributions with different rates\nx &lt;- seq(0, 30, length.out = 200)\nplot(x, dexp(x, rate = 0.1), type = \"l\", lwd = 2, col = \"blue\",\n     xlab = \"Time\", ylab = \"Density\",\n     main = \"Exponential Distribution (λ = 0.1)\")\n\n\n\n\n\n\n\n\n\nA key property of the exponential distribution is memorylessness: the probability of waiting another \\(t\\) units does not depend on how long you have already waited.",
    "crumbs": [
      "Probability and Distributions",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Continuous Probability Distributions</span>"
    ]
  },
  {
    "objectID": "chapters/09-continuous-distributions.html#the-gamma-distribution",
    "href": "chapters/09-continuous-distributions.html#the-gamma-distribution",
    "title": "9  Continuous Probability Distributions",
    "section": "9.4 The Gamma Distribution",
    "text": "9.4 The Gamma Distribution\nThe gamma distribution generalizes the exponential distribution to model the waiting time until the \\(r\\)th event in a Poisson process. Its density function involves two parameters: shape \\(r\\) and rate \\(\\lambda\\):\n\\[f(x) = \\frac{\\lambda^r x^{r-1} e^{-\\lambda x}}{(r-1)!} \\quad \\text{for } x \\geq 0\\]\nThe mean is \\(r/\\lambda\\) and the variance is \\(r/\\lambda^2\\).\nWhen \\(r = 1\\), the gamma distribution reduces to the exponential. As \\(r\\) increases, the distribution becomes more symmetric and bell-shaped.",
    "crumbs": [
      "Probability and Distributions",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Continuous Probability Distributions</span>"
    ]
  },
  {
    "objectID": "chapters/09-continuous-distributions.html#the-normal-gaussian-distribution",
    "href": "chapters/09-continuous-distributions.html#the-normal-gaussian-distribution",
    "title": "9  Continuous Probability Distributions",
    "section": "9.5 The Normal (Gaussian) Distribution",
    "text": "9.5 The Normal (Gaussian) Distribution\nThe normal distribution is the most important continuous distribution in statistics. Its distinctive bell-shaped curve appears throughout nature, and the Central Limit Theorem explains why: the sum of many independent random effects tends toward normality regardless of the underlying distributions.\nThe normal distribution is characterized by two parameters: mean \\(\\mu\\) (center) and standard deviation \\(\\sigma\\) (spread):\n\\[f(x) = \\frac{1}{\\sigma\\sqrt{2\\pi}} e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}\\]\n\n\nCode\n# Normal distributions with different parameters\nx &lt;- seq(-10, 15, length.out = 200)\nplot(x, dnorm(x, mean = 0, sd = 1), type = \"l\", lwd = 2, col = \"blue\",\n     ylim = c(0, 0.5), xlab = \"x\", ylab = \"Density\",\n     main = \"Normal Distributions\")\nlines(x, dnorm(x, mean = 0, sd = 2), lwd = 2, col = \"red\")\nlines(x, dnorm(x, mean = 5, sd = 1), lwd = 2, col = \"darkgreen\")\nlegend(\"topright\", \n       legend = c(\"μ=0, σ=1\", \"μ=0, σ=2\", \"μ=5, σ=1\"),\n       col = c(\"blue\", \"red\", \"darkgreen\"), lwd = 2)\n\n\n\n\n\n\n\n\n\n\n9.5.1 Properties of the Normal Distribution\nThe normal distribution is symmetric around its mean. The mean, median, and mode are all equal. About 68% of the distribution falls within one standard deviation of the mean, 95% within two standard deviations, and 99.7% within three standard deviations (the “68-95-99.7 rule”).\n\n\n\n\n\n\nFigure 9.1: The 68-95-99.7 rule for the normal distribution.\n\n\n\n\n\n9.5.2 Estimating Normal Parameters\nThe mean of a sample provides an estimate of the population mean:\n\\[\\bar{x} = \\frac{1}{n}\\sum_{i=1}^{n}x_i\\]\nThe sample variance estimates the population variance:\n\\[s^2 = \\frac{1}{n-1}\\sum_{i=1}^{n}(x_i - \\bar{x})^2\\]\nNote the \\(n-1\\) in the denominator (called Bessel’s correction), which provides an unbiased estimate of the population variance.\n\n\n9.5.3 The Standard Normal Distribution\nWhen \\(\\mu = 0\\) and \\(\\sigma = 1\\), we have the standard normal distribution. Any normal variable can be converted to standard normal by subtracting the mean and dividing by the standard deviation:\n\\[Z = \\frac{X - \\mu}{\\sigma}\\]\nThis standardization, called computing a z-score, allows us to compare values from different normal distributions and to use tables of standard normal probabilities.\n\n\nCode\n# Probability calculations with the normal distribution\n# P(X &lt; 1.96) for standard normal\npnorm(1.96)\n\n\n[1] 0.9750021\n\n\nCode\n# P(-1.96 &lt; X &lt; 1.96)\npnorm(1.96) - pnorm(-1.96)\n\n\n[1] 0.9500042\n\n\nCode\n# What value has 97.5% of the distribution below it?\nqnorm(0.975)\n\n\n[1] 1.959964\n\n\nThe values 1.96 and -1.96 are particularly important because they bound the middle 95% of the standard normal distribution, forming the basis for 95% confidence intervals.\n\n\n9.5.4 Z-Scores\nA z-score is a standardized value that tells us how many standard deviations an observation is from the mean:\n\\[z_i = \\frac{x_i - \\bar{x}}{s}\\]\nZ-scores allow us to compare values from different normal distributions on a common scale. This is particularly useful when comparing measurements that have different units or very different magnitudes—for example, comparing the relative leg length of mice versus elephants.\n\n\n9.5.5 Why the Normal Distribution is Special in Biology\nThe normal distribution appears throughout biology because many biological traits are influenced by numerous factors, each contributing a small effect. This is particularly evident in quantitative genetics.\n\n\n\n\n\n\nFigure 9.2: The genetic model of complex traits explains why many biological measurements are normally distributed.\n\n\n\nConsider a trait influenced by multiple genes. If we have many loci, each with a small additive effect, the distribution of trait values in a population will approximate a normal distribution—even if the contribution at each locus follows a simple Mendelian pattern.\n\n\n\n\n\n\nFigure 9.3: The distribution of genotypes in an F2 cross approaches normality as the number of contributing loci increases.\n\n\n\nThis connection between many small independent effects and the normal distribution is formalized by the Central Limit Theorem, which we explore below.\n\n\n9.5.6 Checking Normality\nMany statistical methods assume normally distributed data. Before applying these methods, you should check whether the assumption is reasonable.\nVisual methods include histograms and Q-Q (quantile-quantile) plots:\n\n\nCode\n# Generate some data\nset.seed(42)\nnormal_data &lt;- rnorm(200, mean = 50, sd = 10)\nskewed_data &lt;- rexp(200, rate = 0.1)\n\npar(mfrow = c(1, 2))\n\n# Q-Q plot for normal data\nqqnorm(normal_data, main = \"Normal Data\")\nqqline(normal_data, col = \"red\")\n\n# Q-Q plot for skewed data\nqqnorm(skewed_data, main = \"Skewed Data\")\nqqline(skewed_data, col = \"red\")\n\n\n\n\n\n\n\n\n\nIn a Q-Q plot, normally distributed data should fall approximately along the diagonal line. Systematic deviations indicate non-normality.",
    "crumbs": [
      "Probability and Distributions",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Continuous Probability Distributions</span>"
    ]
  },
  {
    "objectID": "chapters/09-continuous-distributions.html#the-central-limit-theorem",
    "href": "chapters/09-continuous-distributions.html#the-central-limit-theorem",
    "title": "9  Continuous Probability Distributions",
    "section": "9.6 The Central Limit Theorem",
    "text": "9.6 The Central Limit Theorem\nThe Central Limit Theorem (CLT) states that the sampling distribution of the mean approaches normality as sample size increases, regardless of the shape of the population distribution. This is why the normal distribution appears so frequently in statistics—we often work with means or other sums of random variables.\n\n\nCode\n# Demonstrate CLT with exponential distribution\nset.seed(123)\n\n# Exponential distribution is quite skewed\npar(mfrow = c(2, 2))\n\n# Original distribution\nhist(rexp(10000, rate = 1), breaks = 50, main = \"Original: Exponential\",\n     xlab = \"x\", col = \"lightblue\")\n\n# Means of samples of size 5\nmeans_5 &lt;- replicate(10000, mean(rexp(5, rate = 1)))\nhist(means_5, breaks = 50, main = \"Means of n=5\",\n     xlab = \"Sample Mean\", col = \"lightblue\")\n\n# Means of samples of size 30\nmeans_30 &lt;- replicate(10000, mean(rexp(30, rate = 1)))\nhist(means_30, breaks = 50, main = \"Means of n=30\",\n     xlab = \"Sample Mean\", col = \"lightblue\")\n\n# Means of samples of size 100\nmeans_100 &lt;- replicate(10000, mean(rexp(100, rate = 1)))\nhist(means_100, breaks = 50, main = \"Means of n=100\",\n     xlab = \"Sample Mean\", col = \"lightblue\")\n\n\n\n\n\n\n\n\n\nEven though the exponential distribution is strongly right-skewed, the distribution of sample means becomes increasingly normal as sample size grows. This is the Central Limit Theorem in action.",
    "crumbs": [
      "Probability and Distributions",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Continuous Probability Distributions</span>"
    ]
  },
  {
    "objectID": "chapters/09-continuous-distributions.html#summary-of-distribution-functions-in-r",
    "href": "chapters/09-continuous-distributions.html#summary-of-distribution-functions-in-r",
    "title": "9  Continuous Probability Distributions",
    "section": "9.7 Summary of Distribution Functions in R",
    "text": "9.7 Summary of Distribution Functions in R\nR provides consistent functions for all distributions:\n\n\n\n\n\n\n\n\n\n\nDistribution\nd (density)\np (cumulative)\nq (quantile)\nr (random)\n\n\n\n\nUniform\ndunif\npunif\nqunif\nrunif\n\n\nExponential\ndexp\npexp\nqexp\nrexp\n\n\nNormal\ndnorm\npnorm\nqnorm\nrnorm\n\n\nGamma\ndgamma\npgamma\nqgamma\nrgamma\n\n\n\nUnderstanding these distributions and their properties prepares you for statistical inference, where we use sampling distributions to make probabilistic statements about population parameters.",
    "crumbs": [
      "Probability and Distributions",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Continuous Probability Distributions</span>"
    ]
  },
  {
    "objectID": "chapters/09-continuous-distributions.html#additional-resources",
    "href": "chapters/09-continuous-distributions.html#additional-resources",
    "title": "9  Continuous Probability Distributions",
    "section": "9.8 Additional Resources",
    "text": "9.8 Additional Resources\n\nIrizarry (2019) - Excellent chapters on probability distributions and the Central Limit Theorem\nLogan (2010) - Comprehensive treatment of distributions in the context of biological statistics\n\n\n\n\n\n\n\nIrizarry, Rafael A. 2019. Introduction to Data Science. Self-published. https://rafalab.github.io/dsbook/.\n\n\nLogan, Murray. 2010. Biostatistical Design and Analysis Using r. Wiley-Blackwell.",
    "crumbs": [
      "Probability and Distributions",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Continuous Probability Distributions</span>"
    ]
  },
  {
    "objectID": "chapters/10-sampling-estimation.html",
    "href": "chapters/10-sampling-estimation.html",
    "title": "10  Sampling and Parameter Estimation",
    "section": "",
    "text": "10.1 The Problem of Inference\nScience often works by measuring samples to learn about populations. We cannot measure every protein in a cell, every patient with a disease, or every fish in the ocean. Instead, we take samples and use statistical inference to draw conclusions about the larger populations from which they came.\nThis creates a fundamental challenge: sample statistics vary from sample to sample, even when samples come from the same population. If you take two different random samples from a population and calculate their means, you will almost certainly get two different values. How, then, can we say anything reliable about the population?\nThe answer lies in understanding the sampling distribution—the distribution of a statistic across all possible samples of a given size.",
    "crumbs": [
      "Probability and Distributions",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Sampling and Parameter Estimation</span>"
    ]
  },
  {
    "objectID": "chapters/10-sampling-estimation.html#parameters-and-statistics",
    "href": "chapters/10-sampling-estimation.html#parameters-and-statistics",
    "title": "10  Sampling and Parameter Estimation",
    "section": "10.2 Parameters and Statistics",
    "text": "10.2 Parameters and Statistics\nA parameter is a numerical characteristic of a population—the true population mean \\(\\mu\\), the true population standard deviation \\(\\sigma\\), the true proportion \\(p\\). Parameters are typically fixed but unknown.\nA statistic is a numerical characteristic of a sample—the sample mean \\(\\bar{x}\\), the sample standard deviation \\(s\\), the sample proportion \\(\\hat{p}\\). Statistics are calculated from data and vary from sample to sample.\nWe use statistics to estimate parameters. The sample mean \\(\\bar{x}\\) estimates the population mean \\(\\mu\\). The sample standard deviation \\(s\\) estimates the population standard deviation \\(\\sigma\\). These estimates will rarely equal the true parameter values exactly, but we can quantify how close they are likely to be.",
    "crumbs": [
      "Probability and Distributions",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Sampling and Parameter Estimation</span>"
    ]
  },
  {
    "objectID": "chapters/10-sampling-estimation.html#point-estimates",
    "href": "chapters/10-sampling-estimation.html#point-estimates",
    "title": "10  Sampling and Parameter Estimation",
    "section": "10.3 Point Estimates",
    "text": "10.3 Point Estimates\nA point estimate is a single number used as our best guess for a parameter. The sample mean is a natural point estimate for the population mean:\n\\[\\bar{x} = \\frac{1}{n} \\sum_{i=1}^{n} x_i\\]\nWhat makes a good estimator? Ideally, an estimator should be:\nUnbiased: On average, across many samples, the estimator equals the true parameter. The sample mean is an unbiased estimator of the population mean.\nEfficient: Among unbiased estimators, it has the smallest variance. The sample mean is the most efficient estimator of a normal mean.\nConsistent: As sample size increases, the estimator converges to the true parameter value.",
    "crumbs": [
      "Probability and Distributions",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Sampling and Parameter Estimation</span>"
    ]
  },
  {
    "objectID": "chapters/10-sampling-estimation.html#the-sampling-distribution-of-the-mean",
    "href": "chapters/10-sampling-estimation.html#the-sampling-distribution-of-the-mean",
    "title": "10  Sampling and Parameter Estimation",
    "section": "10.4 The Sampling Distribution of the Mean",
    "text": "10.4 The Sampling Distribution of the Mean\nImagine drawing all possible samples of size \\(n\\) from a population and calculating the mean of each. The distribution of these means is the sampling distribution of the mean.\nThe sampling distribution has remarkable properties:\n\nIts mean equals the population mean: \\(E[\\bar{X}] = \\mu\\)\nIts standard deviation (the standard error) equals: \\(SE = \\frac{\\sigma}{\\sqrt{n}}\\)\nFor large samples, it is approximately normal (Central Limit Theorem)\n\n\n\nCode\n# Demonstrate sampling distribution\nset.seed(32)\n\n# Create a population\ntrue_pop &lt;- rpois(n = 10000, lambda = 3)\npop_mean &lt;- mean(true_pop)\npop_sd &lt;- sd(true_pop)\n\n# Take many samples and compute their means\nsample_sizes &lt;- c(5, 20, 50, 200)\npar(mfrow = c(2, 2))\n\nfor (n in sample_sizes) {\n  sample_means &lt;- replicate(1000, mean(sample(true_pop, n)))\n  hist(sample_means, breaks = 30, main = paste(\"n =\", n),\n       xlab = \"Sample Mean\", col = \"steelblue\",\n       xlim = c(1, 5))\n  abline(v = pop_mean, col = \"red\", lwd = 2)\n}\n\n\n\n\n\n\n\n\n\nAs sample size increases, the sampling distribution becomes narrower (smaller standard error) and more normal in shape. This is why larger samples give more precise estimates.",
    "crumbs": [
      "Probability and Distributions",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Sampling and Parameter Estimation</span>"
    ]
  },
  {
    "objectID": "chapters/10-sampling-estimation.html#standard-error",
    "href": "chapters/10-sampling-estimation.html#standard-error",
    "title": "10  Sampling and Parameter Estimation",
    "section": "10.5 Standard Error",
    "text": "10.5 Standard Error\nThe standard error (SE) measures the variability of a statistic across samples. For the sample mean:\n\\[SE_{\\bar{x}} = \\frac{\\sigma}{\\sqrt{n}}\\]\nSince we usually do not know \\(\\sigma\\), we estimate the standard error using the sample standard deviation:\n\\[\\widehat{SE}_{\\bar{x}} = \\frac{s}{\\sqrt{n}}\\]\nThe standard error shrinks as sample size increases, but following a square root relationship. To halve the standard error, you need to quadruple the sample size.\n\n\nCode\n# Demonstrate how SE changes with sample size\nset.seed(32)\ntrue_pop &lt;- rpois(n = 1000, lambda = 5)\n\n# Sample size of 5\nsamps_5 &lt;- replicate(n = 50, sample(true_pop, size = 5))\nmeans_5 &lt;- apply(samps_5, 2, mean)\nse_5 &lt;- sd(means_5)\n\n# Sample size of 50\nsamps_50 &lt;- replicate(n = 50, sample(true_pop, size = 50))\nmeans_50 &lt;- apply(samps_50, 2, mean)\nse_50 &lt;- sd(means_50)\n\ncat(\"Standard error with n=5:\", round(se_5, 3), \"\\n\")\n\n\nStandard error with n=5: 0.919 \n\n\nCode\ncat(\"Standard error with n=50:\", round(se_50, 3), \"\\n\")\n\n\nStandard error with n=50: 0.305 \n\n\nCode\ncat(\"Ratio:\", round(se_5/se_50, 2), \"(theoretical: √10 =\", round(sqrt(10), 2), \")\\n\")\n\n\nRatio: 3.01 (theoretical: √10 = 3.16 )",
    "crumbs": [
      "Probability and Distributions",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Sampling and Parameter Estimation</span>"
    ]
  },
  {
    "objectID": "chapters/10-sampling-estimation.html#confidence-intervals",
    "href": "chapters/10-sampling-estimation.html#confidence-intervals",
    "title": "10  Sampling and Parameter Estimation",
    "section": "10.6 Confidence Intervals",
    "text": "10.6 Confidence Intervals\nA point estimate tells us our best guess, but not how uncertain we are. A confidence interval provides a range of plausible values for the parameter along with a measure of confidence.\nA 95% confidence interval for the population mean, when the population is normally distributed or the sample is large, is:\n\\[\\bar{x} \\pm t_{\\alpha/2} \\times \\frac{s}{\\sqrt{n}}\\]\nwhere \\(t_{\\alpha/2}\\) is the critical value from the t-distribution with \\(n-1\\) degrees of freedom.\n\n\n\n\n\nThe interpretation requires care: a 95% confidence interval means that if we repeated this procedure many times, 95% of the resulting intervals would contain the true parameter. Any particular interval either does or does not contain the true value—we just don’t know which.\n\n\nCode\n# Calculate a confidence interval\nset.seed(42)\nsample_data &lt;- rnorm(30, mean = 100, sd = 15)\n\nsample_mean &lt;- mean(sample_data)\nsample_se &lt;- sd(sample_data) / sqrt(length(sample_data))\nt_crit &lt;- qt(0.975, df = length(sample_data) - 1)\n\nlower &lt;- sample_mean - t_crit * sample_se\nupper &lt;- sample_mean + t_crit * sample_se\n\ncat(\"Sample mean:\", round(sample_mean, 2), \"\\n\")\n\n\nSample mean: 101.03 \n\n\nCode\ncat(\"95% CI: [\", round(lower, 2), \",\", round(upper, 2), \"]\\n\")\n\n\n95% CI: [ 94 , 108.06 ]\n\n\nCode\n# Or use t.test directly\nt.test(sample_data)$conf.int\n\n\n[1]  93.99927 108.05833\nattr(,\"conf.level\")\n[1] 0.95",
    "crumbs": [
      "Probability and Distributions",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Sampling and Parameter Estimation</span>"
    ]
  },
  {
    "objectID": "chapters/10-sampling-estimation.html#the-central-limit-theorem-in-practice",
    "href": "chapters/10-sampling-estimation.html#the-central-limit-theorem-in-practice",
    "title": "10  Sampling and Parameter Estimation",
    "section": "10.7 The Central Limit Theorem in Practice",
    "text": "10.7 The Central Limit Theorem in Practice\nThe Central Limit Theorem (CLT) is one of the most remarkable results in statistics. It states that the sampling distribution of the mean approaches a normal distribution as sample size increases, regardless of the shape of the underlying population distribution.\nThis is why confidence intervals work. This is why the normal distribution appears everywhere in statistics. And understanding the CLT in practice—through examples like polling and surveys—builds deep intuition for statistical inference.\n\n10.7.1 The CLT Demonstrated\nLet’s see the CLT in action with a highly non-normal distribution. We’ll sample from an exponential distribution (heavily right-skewed) and watch how the sampling distribution of the mean becomes normal:\n\n\nCode\nset.seed(42)\n\n# Exponential distribution (very skewed)\nrate &lt;- 0.5\npop_mean &lt;- 1/rate  # True mean = 2\n\npar(mfrow = c(2, 2))\n\n# Original population\nx &lt;- rexp(10000, rate = rate)\nhist(x, breaks = 50, main = \"Original Distribution (Exponential)\",\n     xlab = \"Value\", col = \"gray80\", probability = TRUE)\ncurve(dexp(x, rate = rate), add = TRUE, col = \"red\", lwd = 2)\n\n# Sample means for different n\nfor (n in c(5, 30, 100)) {\n  sample_means &lt;- replicate(5000, mean(rexp(n, rate = rate)))\n  hist(sample_means, breaks = 50, probability = TRUE,\n       main = paste(\"Sampling Distribution (n =\", n, \")\"),\n       xlab = \"Sample Mean\", col = \"steelblue\")\n\n  # Overlay normal curve with theoretical SE\n  theoretical_se &lt;- (1/rate) / sqrt(n)\n  curve(dnorm(x, mean = pop_mean, sd = theoretical_se),\n        add = TRUE, col = \"red\", lwd = 2)\n}\n\n\n\n\n\n\n\n\n\nEven with n = 30, the sampling distribution of the mean is remarkably close to normal, despite the exponential distribution being extremely skewed. By n = 100, the approximation is nearly perfect.\n\n\n10.7.2 Margin of Error: Quantifying Precision\nThe margin of error (MOE) is a practical expression of sampling uncertainty. For a confidence interval around a proportion or mean, the margin of error is the half-width of the interval:\n\\[\\text{Margin of Error} = z^* \\times SE\\]\nFor a 95% confidence interval, \\(z^* \\approx 1.96 \\approx 2\\), giving us the approximation:\n\\[\\text{95% MOE} \\approx 2 \\times SE\\]\nFor proportions, the standard error is \\(SE = \\sqrt{\\frac{p(1-p)}{n}}\\), so:\n\\[\\text{95% MOE} \\approx 2 \\times \\sqrt{\\frac{p(1-p)}{n}}\\]\nThe maximum standard error for a proportion occurs when \\(p = 0.5\\), giving the conservative formula used in sample size planning:\n\\[\\text{Maximum 95% MOE} \\approx \\frac{1}{\\sqrt{n}}\\]\n\n\n10.7.3 Polling as a Statistical Inference Problem\nPolitical polling provides an excellent practical example of sampling and the CLT. When a poll reports “48% of voters support Candidate A, with a margin of error of ±3%,” they are making a statistical inference from a sample to a population.\nLet’s simulate an election poll:\n\n\nCode\nset.seed(123)\n\n# True population proportion (unknown in real life)\ntrue_proportion &lt;- 0.52  # 52% actually support the candidate\n\n# Simulate a poll of 1000 likely voters\nn &lt;- 1000\npoll_sample &lt;- rbinom(1, size = n, prob = true_proportion) / n\n\n# Calculate margin of error\nse &lt;- sqrt(poll_sample * (1 - poll_sample) / n)\nmoe &lt;- 1.96 * se\n\ncat(\"Poll result:\", round(poll_sample * 100, 1), \"%\\n\")\n\n\nPoll result: 53.2 %\n\n\nCode\ncat(\"Margin of error: ±\", round(moe * 100, 1), \"%\\n\")\n\n\nMargin of error: ± 3.1 %\n\n\nCode\ncat(\"95% CI: [\", round((poll_sample - moe) * 100, 1), \"%, \",\n    round((poll_sample + moe) * 100, 1), \"%]\\n\")\n\n\n95% CI: [ 50.1 %,  56.3 %]\n\n\nCode\ncat(\"True proportion:\", true_proportion * 100, \"%\\n\")\n\n\nTrue proportion: 52 %\n\n\n\n\n10.7.4 How Sample Size Affects Margin of Error\nThe margin of error is inversely proportional to \\(\\sqrt{n}\\). This has important practical implications:\n\n\nCode\n# Margin of error as a function of sample size\nsample_sizes &lt;- c(100, 400, 1000, 2000, 4000, 10000)\np &lt;- 0.5  # Conservative estimate\n\nmoe_values &lt;- 1.96 * sqrt(p * (1 - p) / sample_sizes) * 100\n\nplot(sample_sizes, moe_values, type = \"b\", pch = 19,\n     xlab = \"Sample Size\", ylab = \"Margin of Error (%)\",\n     main = \"95% Margin of Error vs. Sample Size\",\n     log = \"x\", col = \"steelblue\", lwd = 2)\ngrid()\n\n# Add labels\ntext(sample_sizes, moe_values + 0.5,\n     paste0(\"±\", round(moe_values, 1), \"%\"),\n     cex = 0.8)\n\n\n\n\n\n\n\n\n\nNotice the diminishing returns: going from n = 100 to n = 400 cuts the margin of error in half (from ±10% to ±5%). But to halve it again (to ±2.5%), you need n = 1,600—four times as many respondents. This is why most national polls use samples of 1,000–2,000: it provides reasonable precision (±2-3%) at manageable cost.\n\n\n10.7.5 Interpreting Polls: Uncertainty in Action\nWhen two candidates are within the margin of error of each other, the poll results are consistent with either candidate leading. This is not a flaw of polling—it reflects genuine uncertainty.\n\n\nCode\nset.seed(456)\n\n# Simulate many polls from the same population\ntrue_p &lt;- 0.48  # Candidate A's true support\nn_poll &lt;- 1000\nn_polls &lt;- 100\n\npolls &lt;- replicate(n_polls, {\n  sample_p &lt;- rbinom(1, n_poll, true_p) / n_poll\n  se &lt;- sqrt(sample_p * (1 - sample_p) / n_poll)\n  c(estimate = sample_p, lower = sample_p - 1.96*se, upper = sample_p + 1.96*se)\n})\n\npolls_df &lt;- data.frame(\n  poll = 1:n_polls,\n  estimate = polls[\"estimate\", ],\n  lower = polls[\"lower\", ],\n  upper = polls[\"upper\", ]\n)\n\n# How many CIs contain the true value?\ncoverage &lt;- mean(polls_df$lower &lt;= true_p & polls_df$upper &gt;= true_p)\n\n# Plot first 30 polls\nggplot(polls_df[1:30, ], aes(x = poll, y = estimate)) +\n  geom_hline(yintercept = true_p, color = \"red\", linetype = \"dashed\", linewidth = 1) +\n  geom_hline(yintercept = 0.5, color = \"gray50\", linetype = \"dotted\") +\n  geom_errorbar(aes(ymin = lower, ymax = upper), width = 0.3, alpha = 0.6) +\n  geom_point(size = 2) +\n  labs(\n    title = \"95% Confidence Intervals from 30 Polls\",\n    subtitle = paste0(\"True proportion = \", true_p*100, \"% | \", round(coverage*100),\n                     \"% of all CIs contain true value\"),\n    x = \"Poll Number\",\n    y = \"Estimated Proportion\"\n  ) +\n  scale_y_continuous(labels = scales::percent_format()) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nIn repeated sampling, approximately 95% of confidence intervals contain the true value—but any single interval either does or does not contain it. This is the frequentist interpretation of confidence intervals.\n\n\n10.7.6 Correct Language for Confidence Intervals\nConfidence intervals are frequently misinterpreted. The key insight is understanding what is random before and after data collection:\n\n\n\n\n\n\nWhat Is Random?\n\n\n\nBefore data collection: The confidence interval is random—we don’t yet know where it will fall. The population parameter \\(\\mu\\) is fixed (though unknown).\nAfter data collection: The confidence interval is now fixed—it’s just two numbers like [48.2, 52.1]. It either contains the true parameter or it doesn’t. There is no probability involved anymore.\n\n\nConsider this analogy: Before flipping a coin, there’s a 50% probability of heads. After flipping, the coin shows what it shows—there’s no longer any probability, just an outcome.\nThe correct interpretation of a 95% confidence interval is:\n\n“If we repeated this sampling procedure many times, 95% of the resulting intervals would contain the true population parameter.”\n\nIncorrect interpretations (which are very common):\n\n❌ “There is a 95% probability that \\(\\mu\\) is in this interval”\n❌ “We are 95% confident that \\(\\mu\\) is between these values”\n❌ “95% of the data falls within this interval”\n\nThe first two statements incorrectly assign probability to the parameter, which is fixed. The third confuses a confidence interval for a reference range.\n\n\nCode\n# Visual demonstration: Monte Carlo simulation of CI coverage\nset.seed(2024)\n\n# True population parameters\ntrue_mean &lt;- 50\ntrue_sd &lt;- 10\nn &lt;- 25  # Sample size\nn_simulations &lt;- 100\n\n# Storage for confidence intervals\nci_data &lt;- data.frame(\n  sim = 1:n_simulations,\n  lower = numeric(n_simulations),\n  upper = numeric(n_simulations),\n  contains_true = logical(n_simulations)\n)\n\n# Generate 100 confidence intervals\nfor (i in 1:n_simulations) {\n  sample_data &lt;- rnorm(n, mean = true_mean, sd = true_sd)\n  ci &lt;- t.test(sample_data)$conf.int\n  ci_data$lower[i] &lt;- ci[1]\n  ci_data$upper[i] &lt;- ci[2]\n  ci_data$contains_true[i] &lt;- (ci[1] &lt;= true_mean) & (ci[2] &gt;= true_mean)\n}\n\n# Count how many contain the true mean\ncoverage &lt;- mean(ci_data$contains_true)\n\n# Plot\nplot(NULL, xlim = c(42, 58), ylim = c(0, n_simulations + 1),\n     xlab = \"Value\", ylab = \"Simulation Number\",\n     main = paste0(\"100 Confidence Intervals (\", round(coverage*100), \"% contain true mean)\"))\n\n# Draw intervals\nfor (i in 1:n_simulations) {\n  color &lt;- ifelse(ci_data$contains_true[i], \"steelblue\", \"red\")\n  segments(ci_data$lower[i], i, ci_data$upper[i], i, col = color, lwd = 1.5)\n}\n\n# True mean line\nabline(v = true_mean, col = \"darkgreen\", lwd = 2, lty = 2)\nlegend(\"topright\",\n       legend = c(\"Contains true mean\", \"Misses true mean\", \"True mean (μ = 50)\"),\n       col = c(\"steelblue\", \"red\", \"darkgreen\"),\n       lwd = c(2, 2, 2), lty = c(1, 1, 2))\n\n\n\n\n\n\n\n\n\nIn this simulation, each horizontal line represents a 95% confidence interval from a different random sample. The red intervals are the ~5% that failed to capture the true mean. Once calculated, each interval either contains the true value (blue) or doesn’t (red)—there’s no probability about it anymore.\n\n\n10.7.7 The Practical Value of Margin of Error\nUnderstanding margin of error helps you:\n\nInterpret reported results: A poll showing 52% vs. 48% with ±3% MOE does not clearly favor either candidate\nPlan studies: Use \\(n \\approx \\frac{1}{\\text{MOE}^2}\\) for proportions near 0.5 to achieve a desired margin of error\nCommunicate uncertainty: Always report intervals, not just point estimates\nMake decisions: Consider whether differences are within or beyond the margin of error\n\nFor sample size planning, if you want a specific margin of error:\n\n\nCode\n# Required sample size for different margins of error (95% CI)\ndesired_moe &lt;- c(0.10, 0.05, 0.03, 0.02, 0.01)\nrequired_n &lt;- ceiling((1.96 / desired_moe)^2 * 0.25)  # 0.25 = p(1-p) at p=0.5\n\ndata.frame(\n  `Margin of Error` = paste0(\"±\", desired_moe * 100, \"%\"),\n  `Required n` = required_n\n)\n\n\n  Margin.of.Error Required.n\n1            ±10%         97\n2             ±5%        385\n3             ±3%       1068\n4             ±2%       2401\n5             ±1%       9604\n\n\nThese calculations assume simple random sampling. Real-world surveys often use complex sampling designs that affect the effective sample size.",
    "crumbs": [
      "Probability and Distributions",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Sampling and Parameter Estimation</span>"
    ]
  },
  {
    "objectID": "chapters/10-sampling-estimation.html#coefficient-of-variation",
    "href": "chapters/10-sampling-estimation.html#coefficient-of-variation",
    "title": "10  Sampling and Parameter Estimation",
    "section": "10.8 Coefficient of Variation",
    "text": "10.8 Coefficient of Variation\nWhen comparing variability across groups with different means, the standard deviation alone can be misleading. The coefficient of variation (CV) standardizes variability relative to the mean:\n\\[CV = \\frac{s}{\\bar{x}} \\times 100\\%\\]\nA CV of 10% means the standard deviation is 10% of the mean. This allows meaningful comparisons between groups or measurements on different scales.",
    "crumbs": [
      "Probability and Distributions",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Sampling and Parameter Estimation</span>"
    ]
  },
  {
    "objectID": "chapters/10-sampling-estimation.html#percentiles-and-quantiles",
    "href": "chapters/10-sampling-estimation.html#percentiles-and-quantiles",
    "title": "10  Sampling and Parameter Estimation",
    "section": "10.9 Percentiles and Quantiles",
    "text": "10.9 Percentiles and Quantiles\nPercentiles describe the relative position of values within a distribution. The \\(p\\)th percentile is the value below which \\(p\\)% of the data falls. The 50th percentile is the median, the 25th percentile is the first quartile, and the 75th percentile is the third quartile.\nQuantiles divide data into equal parts. Quartiles divide into four parts, deciles into ten parts, percentiles into one hundred parts.\n\n\nCode\n# Calculate percentiles\ndata &lt;- c(12, 15, 18, 22, 25, 28, 32, 35, 40, 45)\n\nquantile(data, probs = c(0.25, 0.5, 0.75))\n\n\n  25%   50%   75% \n19.00 26.50 34.25 \n\n\nCode\nsummary(data)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  12.00   19.00   26.50   27.20   34.25   45.00 \n\n\nQuantiles form the basis for many statistical procedures, including constructing confidence intervals and calculating p-values.",
    "crumbs": [
      "Probability and Distributions",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Sampling and Parameter Estimation</span>"
    ]
  },
  {
    "objectID": "chapters/10-sampling-estimation.html#bias-and-variability",
    "href": "chapters/10-sampling-estimation.html#bias-and-variability",
    "title": "10  Sampling and Parameter Estimation",
    "section": "10.10 Bias and Variability",
    "text": "10.10 Bias and Variability\nTwo distinct types of error affect estimates:\nBias is systematic error—the tendency for an estimator to consistently over- or underestimate the true parameter. An unbiased estimator has zero bias: its average value across all possible samples equals the true parameter.\nVariability is random error—the spread of estimates around their average value. Low variability means estimates cluster tightly together.\nThe ideal estimator has both low bias and low variability. Sometimes there is a tradeoff: a slightly biased estimator might have much lower variability, resulting in estimates that are closer to the truth on average.\nThe mean squared error (MSE) combines both sources of error:\n\\[MSE = Bias^2 + Variance\\]",
    "crumbs": [
      "Probability and Distributions",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Sampling and Parameter Estimation</span>"
    ]
  },
  {
    "objectID": "chapters/10-sampling-estimation.html#the-bootstrap-resampling-for-estimation",
    "href": "chapters/10-sampling-estimation.html#the-bootstrap-resampling-for-estimation",
    "title": "10  Sampling and Parameter Estimation",
    "section": "10.11 The Bootstrap: Resampling for Estimation",
    "text": "10.11 The Bootstrap: Resampling for Estimation\nThe bootstrap, introduced by Bradley Efron in 1979, is a powerful resampling method for estimating standard errors and constructing confidence intervals when analytical formulas are unavailable or assumptions are questionable (Efron 1979).\nThe key insight is elegant: we can estimate the sampling distribution of a statistic by repeatedly resampling from our observed data. If our sample is representative of the population, then samples drawn from our sample (with replacement) should behave like samples drawn from the population.\n\n10.11.1 The Bootstrap Algorithm\n\nTake a random sample with replacement from your data (same size as original)\nCalculate the statistic of interest on this resampled data\nRepeat steps 1-2 many times (typically 1000-10000)\nThe distribution of bootstrap statistics approximates the sampling distribution\n\n\n\nCode\n# Bootstrap estimation of the sampling distribution of the mean\nset.seed(123)\n\n# Our observed sample\noriginal_sample &lt;- c(23, 31, 28, 35, 42, 29, 33, 27, 38, 31)\nn &lt;- length(original_sample)\n\n# Bootstrap: resample with replacement\nn_bootstrap &lt;- 5000\nbootstrap_means &lt;- numeric(n_bootstrap)\n\nfor (i in 1:n_bootstrap) {\n  boot_sample &lt;- sample(original_sample, size = n, replace = TRUE)\n  bootstrap_means[i] &lt;- mean(boot_sample)\n}\n\n# Compare bootstrap distribution to observed statistics\nhist(bootstrap_means, breaks = 40, col = \"steelblue\",\n     main = \"Bootstrap Distribution of the Mean\",\n     xlab = \"Sample Mean\")\nabline(v = mean(original_sample), col = \"red\", lwd = 2)\nabline(v = quantile(bootstrap_means, c(0.025, 0.975)), col = \"darkgreen\", lwd = 2, lty = 2)\n\n\n\n\n\n\n\n\n\nCode\ncat(\"Original sample mean:\", mean(original_sample), \"\\n\")\n\n\nOriginal sample mean: 31.7 \n\n\nCode\ncat(\"Bootstrap SE:\", sd(bootstrap_means), \"\\n\")\n\n\nBootstrap SE: 1.635207 \n\n\nCode\ncat(\"95% Bootstrap CI:\", quantile(bootstrap_means, c(0.025, 0.975)), \"\\n\")\n\n\n95% Bootstrap CI: 28.7 35.0025 \n\n\n\n\n10.11.2 Bootstrap Confidence Intervals\nThe bootstrap provides several methods for constructing confidence intervals:\nPercentile method: Use the 2.5th and 97.5th percentiles of the bootstrap distribution as the 95% CI bounds. This is the simplest approach shown above.\nBasic (reverse percentile) method: Reflects the percentiles around the original estimate to correct for certain types of bias.\nBCa (bias-corrected and accelerated): A more sophisticated method that adjusts for both bias and skewness in the bootstrap distribution. This is often preferred when the sampling distribution is not symmetric.\n\n\nCode\n# Using the boot package for more sophisticated bootstrap CI\nlibrary(boot)\n\n# Define statistic function\nmean_stat &lt;- function(data, indices) {\n  mean(data[indices])\n}\n\n# Run bootstrap\nboot_result &lt;- boot(original_sample, mean_stat, R = 5000)\n\n# Different CI methods\nboot.ci(boot_result, type = c(\"perc\", \"basic\", \"bca\"))\n\n\nBOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS\nBased on 5000 bootstrap replicates\n\nCALL : \nboot.ci(boot.out = boot_result, type = c(\"perc\", \"basic\", \"bca\"))\n\nIntervals : \nLevel      Basic              Percentile            BCa          \n95%   (28.3, 34.9 )   (28.5, 35.1 )   (28.5, 35.1 )  \nCalculations and Intervals on Original Scale\n\n\n\n\n10.11.3 When to Use the Bootstrap\nThe bootstrap is particularly valuable when:\n\nThe sampling distribution of your statistic is unknown or complex\nSample sizes are small and normality assumptions are questionable\nYou are estimating something other than a mean (e.g., median, correlation, regression coefficients)\nAnalytical formulas for standard errors do not exist\n\nHowever, the bootstrap has limitations. It assumes your sample is representative of the population and works poorly with very small samples (n &lt; 10-15) or when estimating extreme quantiles.",
    "crumbs": [
      "Probability and Distributions",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Sampling and Parameter Estimation</span>"
    ]
  },
  {
    "objectID": "chapters/10-sampling-estimation.html#maximum-likelihood-estimation",
    "href": "chapters/10-sampling-estimation.html#maximum-likelihood-estimation",
    "title": "10  Sampling and Parameter Estimation",
    "section": "10.12 Maximum Likelihood Estimation",
    "text": "10.12 Maximum Likelihood Estimation\nMaximum likelihood estimation (MLE) provides a principled framework for parameter estimation. The likelihood function measures how probable the observed data would be for different parameter values. MLE finds the parameter values that make the observed data most probable.\nFor a sample \\(x_1, x_2, \\ldots, x_n\\) from a distribution with parameter \\(\\theta\\), the likelihood function is:\n\\[L(\\theta | x_1, \\ldots, x_n) = \\prod_{i=1}^{n} f(x_i | \\theta)\\]\nwhere \\(f(x_i | \\theta)\\) is the probability density (or mass) function. We typically work with the log-likelihood for computational convenience:\n\\[\\ell(\\theta) = \\sum_{i=1}^{n} \\log f(x_i | \\theta)\\]\nThe MLE \\(\\hat{\\theta}\\) is the value that maximizes \\(\\ell(\\theta)\\).\n\n\nCode\n# MLE example: estimating the rate parameter of an exponential distribution\nset.seed(42)\nexp_data &lt;- rexp(100, rate = 0.5)  # True rate is 0.5\n\n# Log-likelihood function for exponential\nlog_likelihood &lt;- function(rate, data) {\n  sum(dexp(data, rate = rate, log = TRUE))\n}\n\n# Find MLE\nrates &lt;- seq(0.1, 1, by = 0.01)\nll_values &lt;- sapply(rates, log_likelihood, data = exp_data)\n\n# Plot likelihood surface\nplot(rates, ll_values, type = \"l\", lwd = 2,\n     xlab = \"Rate parameter\", ylab = \"Log-likelihood\",\n     main = \"Log-likelihood for Exponential Rate\")\nabline(v = 1/mean(exp_data), col = \"red\", lwd = 2)  # MLE = 1/mean for exponential\n\n\n\n\n\n\n\n\n\nCode\ncat(\"True rate: 0.5\\n\")\n\n\nTrue rate: 0.5\n\n\nCode\ncat(\"MLE estimate:\", round(1/mean(exp_data), 3), \"\\n\")\n\n\nMLE estimate: 0.445 \n\n\nMLEs have desirable properties: they are consistent (converge to true values as n increases) and asymptotically efficient (achieve the smallest possible variance for large samples).",
    "crumbs": [
      "Probability and Distributions",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Sampling and Parameter Estimation</span>"
    ]
  },
  {
    "objectID": "chapters/10-sampling-estimation.html#simulation-based-understanding-of-estimation",
    "href": "chapters/10-sampling-estimation.html#simulation-based-understanding-of-estimation",
    "title": "10  Sampling and Parameter Estimation",
    "section": "10.13 Simulation-Based Understanding of Estimation",
    "text": "10.13 Simulation-Based Understanding of Estimation\nSimulation provides powerful intuition for statistical concepts. By repeatedly sampling from known populations, we can directly observe sampling distributions.\n\n\nCode\n# Exploring properties of estimators through simulation\nset.seed(456)\n\n# True population parameters\npop_mean &lt;- 50\npop_sd &lt;- 10\n\n# Simulate many samples and compute estimates\nn_samples &lt;- 2000\nsample_sizes &lt;- c(5, 15, 50)\n\npar(mfrow = c(2, 3))\n\nfor (n in sample_sizes) {\n  # Collect sample means\n  sample_means &lt;- replicate(n_samples, {\n    samp &lt;- rnorm(n, mean = pop_mean, sd = pop_sd)\n    mean(samp)\n  })\n\n  # Plot distribution of sample means\n  hist(sample_means, breaks = 40, col = \"steelblue\",\n       main = paste(\"Sample Means (n =\", n, \")\"),\n       xlab = \"Sample Mean\", xlim = c(35, 65))\n  abline(v = pop_mean, col = \"red\", lwd = 2)\n\n  # Calculate actual SE vs theoretical\n  actual_se &lt;- sd(sample_means)\n  theoretical_se &lt;- pop_sd / sqrt(n)\n\n  legend(\"topright\", bty = \"n\", cex = 0.8,\n         legend = c(paste(\"Actual SE:\", round(actual_se, 2)),\n                    paste(\"Theoretical:\", round(theoretical_se, 2))))\n}\n\n# Now do the same for sample standard deviations\nfor (n in sample_sizes) {\n  # Collect sample SDs\n  sample_sds &lt;- replicate(n_samples, {\n    samp &lt;- rnorm(n, mean = pop_mean, sd = pop_sd)\n    sd(samp)\n  })\n\n  hist(sample_sds, breaks = 40, col = \"coral\",\n       main = paste(\"Sample SDs (n =\", n, \")\"),\n       xlab = \"Sample SD\")\n  abline(v = pop_sd, col = \"red\", lwd = 2)\n}\n\n\n\n\n\n\n\n\n\nThis simulation reveals several important facts: 1. Sample means are unbiased (centered on the population mean) 2. The spread of sample means decreases as \\(\\sqrt{n}\\) 3. Sample standard deviations are slightly biased for small samples but become unbiased as n increases",
    "crumbs": [
      "Probability and Distributions",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Sampling and Parameter Estimation</span>"
    ]
  },
  {
    "objectID": "chapters/10-sampling-estimation.html#principles-of-experimental-design",
    "href": "chapters/10-sampling-estimation.html#principles-of-experimental-design",
    "title": "10  Sampling and Parameter Estimation",
    "section": "10.14 Principles of Experimental Design",
    "text": "10.14 Principles of Experimental Design\nGood statistical analysis cannot rescue a poorly designed study. The way you collect data fundamentally determines what conclusions you can draw. Understanding key design principles is essential for planning experiments that yield valid, interpretable results.\n\n10.14.1 Randomization\nRandomization assigns subjects to treatment groups by chance, ensuring that treatment groups are comparable. Without randomization, systematic differences between groups (confounders) can bias results.\n\n\nCode\n# Randomly assign 20 subjects to treatment or control\nset.seed(42)\nsubjects &lt;- 1:20\ntreatment_group &lt;- sample(subjects, size = 10)\ncontrol_group &lt;- setdiff(subjects, treatment_group)\n\ncat(\"Treatment group:\", treatment_group, \"\\n\")\n\n\nTreatment group: 17 5 1 10 4 2 20 18 8 7 \n\n\nCode\ncat(\"Control group:\", control_group, \"\\n\")\n\n\nControl group: 3 6 9 11 12 13 14 15 16 19 \n\n\nRandomization provides the foundation for causal inference. Without it, we can only establish associations, not causation.\n\n\n10.14.2 Controls\nEvery experiment needs controls—groups that differ from treatment groups only in the variable of interest:\n\nNegative control: Receives no treatment; establishes baseline\nPositive control: Receives a treatment known to work; confirms the experimental system is functioning\nProcedural control: Receives all procedures except the active ingredient (e.g., sham surgery, vehicle-only injection)\n\nWithout proper controls, you cannot determine whether observed effects are due to your treatment or some other factor.\n\n\n10.14.3 Blinding\nBlinding prevents knowledge of group assignment from influencing results:\n\nSingle-blind: Subjects do not know which treatment they receive\nDouble-blind: Neither subjects nor experimenters know group assignments\nTriple-blind: Subjects, experimenters, and data analysts are all blinded\n\nBlinding prevents both placebo effects (subjects’ expectations influencing outcomes) and experimenter bias (conscious or unconscious influence on measurements).\n\n\n10.14.4 Replication\nReplication means having multiple independent observations in each treatment group. Replication is essential because it:\n\nProvides estimates of variability\nEnables statistical inference\nIncreases precision of estimates\nAllows detection of real effects\n\nThe unit of replication must match the unit of treatment. If you treat tanks with different water temperatures and measure multiple fish per tank, your replicates are tanks, not fish.\n\n\n\n\n\n\nTechnical vs. Biological Replicates\n\n\n\n\nTechnical replicates: Repeated measurements of the same sample (e.g., running the same sample through a machine twice)\nBiological replicates: Independent samples from different individuals or experimental units\n\nTechnical replicates measure precision of the measurement process. Biological replicates measure biological variability and enable inference to the population. Do not confuse them!\n\n\n\n\n10.14.5 Blocking\nBlocking groups experimental units that are similar to each other, then applies all treatments within each block. This reduces variability by accounting for known sources of heterogeneity.\nCommon blocking factors: - Time (experimental batches, days) - Location (different incubators, growth chambers) - Individual (paired designs, repeated measures)\n\n\nCode\n# Randomized complete block design\n# 4 treatments applied within each of 3 blocks\nset.seed(123)\nblocks &lt;- 1:3\ntreatments &lt;- c(\"A\", \"B\", \"C\", \"D\")\n\ndesign &lt;- expand.grid(Block = blocks, Treatment = treatments)\ndesign$Order &lt;- NA\n\nfor (b in blocks) {\n  block_rows &lt;- design$Block == b\n  design$Order[block_rows] &lt;- sample(1:4)\n}\n\ndesign[order(design$Block, design$Order), ]\n\n\n   Block Treatment Order\n7      1         C     1\n10     1         D     2\n1      1         A     3\n4      1         B     4\n11     2         D     1\n5      2         B     2\n2      2         A     3\n8      2         C     4\n6      3         B     1\n9      3         C     2\n3      3         A     3\n12     3         D     4\n\n\nBlocking is particularly valuable when blocks correspond to major sources of variation (e.g., different labs, experimental days, genetic backgrounds).\n\n\n10.14.6 Sample Size Considerations\nDetermining appropriate sample size before collecting data is crucial. Too few samples waste resources by producing inconclusive results; too many waste resources by studying effects that were detectable with smaller samples.\nSample size depends on: - Effect size: The minimum meaningful difference you want to detect - Variability: The expected noise in your measurements - Significance level (\\(\\alpha\\)): Usually 0.05 - Power: Usually 0.80 (80% chance of detecting a real effect)\nPower analysis (covered in detail in a later chapter) formalizes these considerations.",
    "crumbs": [
      "Probability and Distributions",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Sampling and Parameter Estimation</span>"
    ]
  },
  {
    "objectID": "chapters/10-sampling-estimation.html#key-takeaways",
    "href": "chapters/10-sampling-estimation.html#key-takeaways",
    "title": "10  Sampling and Parameter Estimation",
    "section": "10.15 Key Takeaways",
    "text": "10.15 Key Takeaways\nUnderstanding sampling distributions and estimation is fundamental to statistical inference. Key points to remember:\n\nStatistics vary from sample to sample; this variability is quantified by the standard error\nLarger samples give more precise estimates (smaller standard errors)\nConfidence intervals quantify uncertainty about parameter estimates\nThe Central Limit Theorem explains why the normal distribution appears so frequently\nBoth bias and variability affect the quality of estimates\nThe bootstrap provides a flexible, computer-intensive approach to estimation when analytical methods are limited\nMaximum likelihood provides a principled framework for parameter estimation\nGood experimental design—randomization, controls, blinding, proper replication—is essential for valid inference\n\nThese concepts provide the foundation for hypothesis testing and the statistical inference methods we develop in subsequent chapters.",
    "crumbs": [
      "Probability and Distributions",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Sampling and Parameter Estimation</span>"
    ]
  },
  {
    "objectID": "chapters/10-sampling-estimation.html#additional-resources",
    "href": "chapters/10-sampling-estimation.html#additional-resources",
    "title": "10  Sampling and Parameter Estimation",
    "section": "10.16 Additional Resources",
    "text": "10.16 Additional Resources\n\nEfron (1979) - The original bootstrap paper, a landmark in modern statistics\nIrizarry (2019) - Excellent chapters on sampling and estimation with R examples\n\n\n\n\n\n\n\nEfron, Bradley. 1979. “Bootstrap Methods: Another Look at the Jackknife.” The Annals of Statistics 7 (1): 1–26.\n\n\nIrizarry, Rafael A. 2019. Introduction to Data Science. Self-published. https://rafalab.github.io/dsbook/.",
    "crumbs": [
      "Probability and Distributions",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Sampling and Parameter Estimation</span>"
    ]
  },
  {
    "objectID": "chapters/11-hypothesis-testing.html",
    "href": "chapters/11-hypothesis-testing.html",
    "title": "11  Hypothesis Testing",
    "section": "",
    "text": "11.1 What is a Hypothesis?\nA hypothesis is a statement of belief about the world—a claim that can be evaluated with data. In statistics, we formalize hypothesis testing as a framework for using data to decide between competing claims.\nThe null hypothesis (\\(H_0\\)) represents a default position, typically stating that there is no effect, no difference, or no relationship. The alternative hypothesis (\\(H_A\\)) represents what we would conclude if we reject the null—typically that there is an effect, difference, or relationship.\nFor example, consider testing whether an amino acid substitution changes the catalytic rate of an enzyme:\nThe alternative hypothesis might be directional (the substitution increases the rate) or non-directional (the substitution changes the rate, in either direction). This distinction affects how we calculate p-values.",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "chapters/11-hypothesis-testing.html#what-is-a-hypothesis",
    "href": "chapters/11-hypothesis-testing.html#what-is-a-hypothesis",
    "title": "11  Hypothesis Testing",
    "section": "",
    "text": "\\(H_0\\): The substitution does not change the catalytic rate\n\\(H_A\\): The substitution does change the catalytic rate",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "chapters/11-hypothesis-testing.html#the-logic-of-hypothesis-testing",
    "href": "chapters/11-hypothesis-testing.html#the-logic-of-hypothesis-testing",
    "title": "11  Hypothesis Testing",
    "section": "11.2 The Logic of Hypothesis Testing",
    "text": "11.2 The Logic of Hypothesis Testing\nHypothesis testing follows a specific logic. We assume the null hypothesis is true and ask: how likely would we be to observe data as extreme as what we actually observed? If this probability is very small, we conclude that the null hypothesis is unlikely to be true and reject it in favor of the alternative.\nKey questions in hypothesis testing include:\n\nWhat is the probability of rejecting a true null hypothesis?\nWhat is the probability of failing to reject a false null hypothesis?\nHow do we decide when to reject the null hypothesis?\nWhat can we conclude if we fail to reject?",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "chapters/11-hypothesis-testing.html#type-i-and-type-ii-errors",
    "href": "chapters/11-hypothesis-testing.html#type-i-and-type-ii-errors",
    "title": "11  Hypothesis Testing",
    "section": "11.3 Type I and Type II Errors",
    "text": "11.3 Type I and Type II Errors\nTwo types of mistakes are possible in hypothesis testing.\n\n\n\n\n\nA Type I error occurs when we reject a true null hypothesis—concluding there is an effect when there is not. The probability of a Type I error is denoted \\(\\alpha\\) and is called the significance level. By convention, \\(\\alpha\\) is often set to 0.05, meaning we accept a 5% chance of falsely rejecting a true null.\nA Type II error occurs when we fail to reject a false null hypothesis—concluding there is no effect when there actually is one. The probability of a Type II error is denoted \\(\\beta\\).\nPower is the probability of correctly rejecting a false null hypothesis: Power = \\(1 - \\beta\\). Power depends on the effect size (how big the true effect is), sample size, significance level, and variability in the data.\n\n\n\n\n\\(H_0\\) True\n\\(H_0\\) False\n\n\n\n\nReject \\(H_0\\)\nType I Error (\\(\\alpha\\))\nCorrect Decision (Power)\n\n\nFail to Reject \\(H_0\\)\nCorrect Decision\nType II Error (\\(\\beta\\))\n\n\n\n\n11.3.1 Understanding Statistical Power\nPower analysis is essential for designing experiments that can actually detect effects of interest. A study with low power is unlikely to find real effects even when they exist—wasting resources and potentially leading to incorrect conclusions.\nThe key factors affecting power are:\n\nEffect size: Larger effects are easier to detect\nSample size: More data provides more information\nSignificance level (\\(\\alpha\\)): Higher \\(\\alpha\\) increases power but also increases Type I error rate\nVariability: Less noise makes signals easier to detect\n\nFor a two-sample t-test, the relationship between these factors can be expressed approximately as:\n\\[\\text{Power} \\approx \\Phi\\left(\\frac{|\\mu_1 - \\mu_2|}{\\sigma}\\sqrt{\\frac{n}{2}} - z_{1-\\alpha/2}\\right)\\]\nwhere \\(\\Phi\\) is the standard normal CDF, and \\(z_{1-\\alpha/2}\\) is the critical value.\n\n\nCode\n# Visualize how power depends on effect size and sample size\nlibrary(pwr)\n\n# Power curves for different effect sizes\neffect_sizes &lt;- c(0.2, 0.5, 0.8)  # Cohen's d: small, medium, large\nsample_sizes &lt;- seq(5, 100, by = 5)\n\npar(mfrow = c(1, 1))\ncolors &lt;- c(\"blue\", \"darkgreen\", \"red\")\n\nplot(NULL, xlim = c(5, 100), ylim = c(0, 1),\n     xlab = \"Sample Size (per group)\", ylab = \"Power\",\n     main = \"Power Curves for Two-Sample t-Test\")\nabline(h = 0.8, lty = 2, col = \"gray\")\n\nfor (i in 1:3) {\n  powers &lt;- sapply(sample_sizes, function(n) {\n    pwr.t.test(n = n, d = effect_sizes[i], sig.level = 0.05, type = \"two.sample\")$power\n  })\n  lines(sample_sizes, powers, col = colors[i], lwd = 2)\n}\n\nlegend(\"bottomright\",\n       legend = c(\"Small (d=0.2)\", \"Medium (d=0.5)\", \"Large (d=0.8)\"),\n       col = colors, lwd = 2)\n\n\n\n\n\n\n\n\n\n\n\n11.3.2 A Priori Power Analysis\nBefore conducting an experiment, power analysis helps determine the necessary sample size. The question is: “How many observations do I need to have an 80% chance of detecting an effect of a given size?”\n\n\nCode\n# Sample size calculation for 80% power\n# Detecting a medium effect (d = 0.5) with alpha = 0.05\npower_result &lt;- pwr.t.test(d = 0.5, sig.level = 0.05, power = 0.8, type = \"two.sample\")\ncat(\"Required sample size per group:\", ceiling(power_result$n), \"\\n\")\n\n\nRequired sample size per group: 64 \n\n\nCode\n# For a small effect (d = 0.2)\npower_small &lt;- pwr.t.test(d = 0.2, sig.level = 0.05, power = 0.8, type = \"two.sample\")\ncat(\"For small effect (d=0.2):\", ceiling(power_small$n), \"per group\\n\")\n\n\nFor small effect (d=0.2): 394 per group\n\n\nNotice how detecting small effects requires substantially larger samples. This is why pilot studies and literature-based effect size estimates are valuable for planning.",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "chapters/11-hypothesis-testing.html#p-values",
    "href": "chapters/11-hypothesis-testing.html#p-values",
    "title": "11  Hypothesis Testing",
    "section": "11.4 P-Values",
    "text": "11.4 P-Values\nThe p-value is the probability of observing a test statistic as extreme or more extreme than the one calculated from the data, assuming the null hypothesis is true.\nA small p-value indicates that the observed data would be unlikely if the null hypothesis were true, providing evidence against the null. A large p-value indicates that the data are consistent with the null hypothesis.\nThe p-value is NOT the probability that the null hypothesis is true. It is the probability of the data (or more extreme data) given the null hypothesis, not the probability of the null hypothesis given the data.",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "chapters/11-hypothesis-testing.html#significance-level-and-decision-rules",
    "href": "chapters/11-hypothesis-testing.html#significance-level-and-decision-rules",
    "title": "11  Hypothesis Testing",
    "section": "11.5 Significance Level and Decision Rules",
    "text": "11.5 Significance Level and Decision Rules\nThe significance level \\(\\alpha\\) is the threshold below which we reject the null hypothesis. If \\(p &lt; \\alpha\\), we reject \\(H_0\\). If \\(p \\geq \\alpha\\), we fail to reject \\(H_0\\).\n\n\n\n\n\nThe conventional choice of \\(\\alpha = 0.05\\) is arbitrary but widely used. In contexts where Type I errors are particularly costly (e.g., approving an ineffective drug), smaller \\(\\alpha\\) values may be appropriate. In exploratory research, larger \\(\\alpha\\) values might be acceptable.\nImportant: “fail to reject” is not the same as “accept.” Failing to reject the null hypothesis means the data did not provide sufficient evidence against it, not that the null hypothesis is true.",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "chapters/11-hypothesis-testing.html#test-statistics-and-statistical-distributions",
    "href": "chapters/11-hypothesis-testing.html#test-statistics-and-statistical-distributions",
    "title": "11  Hypothesis Testing",
    "section": "11.6 Test Statistics and Statistical Distributions",
    "text": "11.6 Test Statistics and Statistical Distributions\nA test statistic summarizes the data in a way that allows comparison to a known distribution under the null hypothesis. Different tests use different statistics: the t-statistic for t-tests, the F-statistic for ANOVA, the chi-squared statistic for contingency tables.\nJust like raw data, test statistics are random variables with their own sampling distributions. Under the null hypothesis, we know what distribution the test statistic should follow. We can then calculate how unusual our observed statistic is under this distribution.",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "chapters/11-hypothesis-testing.html#one-tailed-vs.-two-tailed-tests",
    "href": "chapters/11-hypothesis-testing.html#one-tailed-vs.-two-tailed-tests",
    "title": "11  Hypothesis Testing",
    "section": "11.7 One-Tailed vs. Two-Tailed Tests",
    "text": "11.7 One-Tailed vs. Two-Tailed Tests\nA two-tailed test considers extreme values in both directions. The alternative hypothesis is non-directional: \\(H_A: \\mu \\neq \\mu_0\\). Extreme values in either tail of the distribution count as evidence against the null.\nA one-tailed test considers extreme values in only one direction. The alternative hypothesis is directional: \\(H_A: \\mu &gt; \\mu_0\\) or \\(H_A: \\mu &lt; \\mu_0\\). Only extreme values in the specified direction count as evidence against the null.\n\n\n\n\n\nTwo-tailed tests are more conservative and are appropriate when you do not have a strong prior expectation about the direction of an effect. One-tailed tests have more power to detect effects in the specified direction but will miss effects in the opposite direction.",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "chapters/11-hypothesis-testing.html#multiple-testing",
    "href": "chapters/11-hypothesis-testing.html#multiple-testing",
    "title": "11  Hypothesis Testing",
    "section": "11.8 Multiple Testing",
    "text": "11.8 Multiple Testing\nWhen you perform many hypothesis tests, the probability of at least one Type I error increases. If you test 20 independent hypotheses at \\(\\alpha = 0.05\\), you expect about one false positive even when all null hypotheses are true.\nSeveral approaches address multiple testing:\nBonferroni correction divides \\(\\alpha\\) by the number of tests. For 20 tests, use \\(\\alpha = 0.05/20 = 0.0025\\). This is conservative and may miss true effects.\nFalse Discovery Rate (FDR) control allows some false positives but controls their proportion among rejected hypotheses. This is less conservative than Bonferroni and widely used in genomics and other high-throughput applications.",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "chapters/11-hypothesis-testing.html#practical-vs.-statistical-significance",
    "href": "chapters/11-hypothesis-testing.html#practical-vs.-statistical-significance",
    "title": "11  Hypothesis Testing",
    "section": "11.9 Practical vs. Statistical Significance",
    "text": "11.9 Practical vs. Statistical Significance\nStatistical significance does not imply practical importance. With a large enough sample, even trivially small effects become statistically significant. Conversely, practically important effects may not reach statistical significance with small samples.\nAlways consider effect sizes alongside p-values. Report confidence intervals, which convey both the magnitude of an effect and the uncertainty about it. A 95% confidence interval that excludes zero is equivalent to statistical significance at \\(\\alpha = 0.05\\), but also shows the range of plausible effect sizes.\n\n11.9.1 The Connection Between P-Values and Confidence Intervals\nP-values and confidence intervals are mathematically linked. Understanding this connection deepens your grasp of both concepts.\nFor testing whether a parameter equals some null value \\(\\theta_0\\) (e.g., testing if \\(\\mu = 0\\) or \\(\\mu_1 - \\mu_2 = 0\\)):\n\n\n\n\n\n\nThe P-Value / CI Duality\n\n\n\n\nIf the \\((1-\\alpha)\\) confidence interval excludes \\(\\theta_0\\), then \\(p &lt; \\alpha\\)\nIf the \\((1-\\alpha)\\) confidence interval includes \\(\\theta_0\\), then \\(p \\geq \\alpha\\)\n\nA 95% CI that doesn’t contain zero corresponds to p &lt; 0.05 for a two-tailed test.\n\n\nThis relationship makes sense when you think about it: the confidence interval represents the range of parameter values that are “compatible” with the data. If the null value falls outside this range, the data provide evidence against it (small p-value). If the null value falls inside, the data are consistent with it (large p-value).\n\n\nCode\n# Demonstrate the CI-pvalue relationship\nset.seed(42)\n\n# Three scenarios\nscenarios &lt;- list(\n  list(true_mean = 5, label = \"CI excludes 0\"),\n  list(true_mean = 2, label = \"CI barely excludes 0\"),\n  list(true_mean = 0.5, label = \"CI includes 0\")\n)\n\npar(mfrow = c(1, 3))\n\nfor (scenario in scenarios) {\n  sample_data &lt;- rnorm(30, mean = scenario$true_mean, sd = 5)\n  result &lt;- t.test(sample_data)\n\n  # Plot CI\n  ci &lt;- result$conf.int\n  mean_est &lt;- result$estimate\n\n  plot(1, mean_est, xlim = c(0.5, 1.5), ylim = c(-4, 12),\n       pch = 19, cex = 1.5, xaxt = \"n\", xlab = \"\",\n       ylab = \"Estimated Mean\",\n       main = paste0(scenario$label, \"\\np = \", round(result$p.value, 4)))\n\n  arrows(1, ci[1], 1, ci[2], angle = 90, code = 3, length = 0.1, lwd = 2)\n  abline(h = 0, col = \"red\", lty = 2, lwd = 2)\n}\n\n\n\n\n\n\n\n\n\nThe beauty of confidence intervals is that they provide more information than p-values alone:\n\nDirection: You see whether the effect is positive or negative\nMagnitude: You see the estimated size of the effect\nPrecision: The width shows your uncertainty\nSignificance: Whether zero is included tells you if p &lt; 0.05\n\nThis is why many statisticians advocate for reporting confidence intervals as the primary summary, with p-values as secondary.\n\n\n11.9.2 Standardized Effect Sizes\nEffect sizes quantify the magnitude of an effect in a standardized way, allowing comparison across studies.\nCohen’s d measures the difference between two means in standard deviation units:\n\\[d = \\frac{\\bar{x}_1 - \\bar{x}_2}{s_{\\text{pooled}}}\\]\nwhere \\(s_{\\text{pooled}} = \\sqrt{\\frac{(n_1-1)s_1^2 + (n_2-1)s_2^2}{n_1 + n_2 - 2}}\\)\nCohen’s guidelines suggest \\(d = 0.2\\) is small, \\(d = 0.5\\) is medium, and \\(d = 0.8\\) is large, though these benchmarks should be interpreted in context (Cohen 1988).\n\n\nCode\n# Calculate Cohen's d\ngroup1 &lt;- c(23, 25, 28, 31, 27, 29)\ngroup2 &lt;- c(18, 20, 22, 19, 21, 23)\n\n# Pooled standard deviation\nn1 &lt;- length(group1)\nn2 &lt;- length(group2)\ns_pooled &lt;- sqrt(((n1-1)*var(group1) + (n2-1)*var(group2)) / (n1 + n2 - 2))\n\n# Cohen's d\ncohens_d &lt;- (mean(group1) - mean(group2)) / s_pooled\ncat(\"Cohen's d:\", round(cohens_d, 3), \"\\n\")\n\n\nCohen's d: 2.76 \n\n\nOther effect sizes include: - Pearson’s r: Correlation coefficient (-1 to 1) - \\(\\eta^2\\) (eta-squared): Proportion of variance explained in ANOVA - Odds ratio: Effect size for binary outcomes",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "chapters/11-hypothesis-testing.html#critiques-of-nhst",
    "href": "chapters/11-hypothesis-testing.html#critiques-of-nhst",
    "title": "11  Hypothesis Testing",
    "section": "11.10 Critiques of NHST",
    "text": "11.10 Critiques of NHST\nThe Null Hypothesis Significance Testing (NHST) framework, while widely used, has important limitations that researchers should understand.\n\n\n\n\n\n\nLimitations of P-Values\n\n\n\n\nP-values do not measure effect size: A tiny, meaningless effect can have p &lt; 0.001 with enough data\nP-values do not measure probability that \\(H_0\\) is true: This is a common misinterpretation\nThe 0.05 threshold is arbitrary: There is nothing magical about \\(\\alpha = 0.05\\)\nDichotomous thinking: Treating p = 0.049 and p = 0.051 as fundamentally different is misleading\nPublication bias: Studies with p &lt; 0.05 are more likely to be published, distorting the literature\n\n\n\n\n11.10.1 Alternatives and Complements to NHST\nConfidence intervals provide more information than p-values alone, showing both the estimated effect and uncertainty.\nEffect sizes communicate the practical magnitude of results.\nBayesian methods provide probability statements about hypotheses themselves (not just the data given the hypothesis).\nEquivalence testing can demonstrate that an effect is negligibly small, not just “not significantly different from zero.”\nThe American Statistical Association’s 2016 statement on p-values emphasizes that p-values should not be used in isolation and that scientific conclusions should not be based solely on whether a p-value crosses a threshold.",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "chapters/11-hypothesis-testing.html#example-null-distribution-via-randomization",
    "href": "chapters/11-hypothesis-testing.html#example-null-distribution-via-randomization",
    "title": "11  Hypothesis Testing",
    "section": "11.11 Example: Null Distribution via Randomization",
    "text": "11.11 Example: Null Distribution via Randomization\nWe can create empirical null distributions through randomization, providing an alternative to parametric assumptions.\n\n\nCode\n# Two groups to compare\nset.seed(56)\npop_1 &lt;- rnorm(n = 50, mean = 20.1, sd = 2)\npop_2 &lt;- rnorm(n = 50, mean = 19.3, sd = 2)\n\n# Observed t-statistic\nt_obs &lt;- t.test(x = pop_1, y = pop_2, alternative = \"greater\")$statistic\n\n# Create null distribution by randomization\npops_comb &lt;- c(pop_1, pop_2)\n\nt_rand &lt;- replicate(1000, {\n  pops_shuf &lt;- sample(pops_comb)\n  t.test(x = pops_shuf[1:50], y = pops_shuf[51:100], alternative = \"greater\")$statistic\n})\n\n# Plot null distribution\nhist(t_rand, breaks = 30, main = \"Randomization Null Distribution\",\n     xlab = \"t-statistic\", col = \"lightblue\")\nabline(v = t_obs, col = \"red\", lwd = 2)\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Calculate p-value\np_value &lt;- sum(t_rand &gt;= t_obs) / 1000\ncat(\"Observed t:\", round(t_obs, 3), \"\\n\")\n\n\nObserved t: 2.211 \n\n\nCode\ncat(\"P-value:\", p_value, \"\\n\")\n\n\nP-value: 0.016",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "chapters/11-hypothesis-testing.html#summary",
    "href": "chapters/11-hypothesis-testing.html#summary",
    "title": "11  Hypothesis Testing",
    "section": "11.12 Summary",
    "text": "11.12 Summary\nHypothesis testing provides a framework for using data to evaluate claims about populations. Key concepts include:\n\nNull and alternative hypotheses formalize competing claims\nType I errors (false positives) and Type II errors (false negatives) represent the two ways we can be wrong\nP-values quantify evidence against the null hypothesis\nSignificance levels set thresholds for decision-making\nMultiple testing requires adjustment to control error rates\nStatistical significance does not imply practical importance\n\nIn the following chapters, we apply this framework to specific tests: t-tests for comparing means, chi-squared tests for categorical data, and nonparametric alternatives when assumptions are violated.",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "chapters/11-hypothesis-testing.html#additional-resources",
    "href": "chapters/11-hypothesis-testing.html#additional-resources",
    "title": "11  Hypothesis Testing",
    "section": "11.13 Additional Resources",
    "text": "11.13 Additional Resources\n\nCohen (1988) - The classic reference on statistical power analysis\nLogan (2010) - Comprehensive treatment of hypothesis testing in biological contexts\n\n\n\n\n\n\n\nCohen, Jacob. 1988. Statistical Power Analysis for the Behavioral Sciences. 2nd ed. Lawrence Erlbaum Associates.\n\n\nLogan, Murray. 2010. Biostatistical Design and Analysis Using r. Wiley-Blackwell.",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "chapters/12-t-tests.html",
    "href": "chapters/12-t-tests.html",
    "title": "12  T-Tests",
    "section": "",
    "text": "12.1 Comparing Means\nOne of the most common questions in data analysis is whether two groups differ. Is the mean expression level different between treatment and control? Does the new material have different strength than the standard? Do patients on drug A have different outcomes than patients on drug B?\nThe t-test is the classic method for comparing means. It compares the observed difference between groups to the variability expected by chance, producing a test statistic that follows a t-distribution under the null hypothesis of no difference.",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>T-Tests</span>"
    ]
  },
  {
    "objectID": "chapters/12-t-tests.html#the-t-distribution",
    "href": "chapters/12-t-tests.html#the-t-distribution",
    "title": "12  T-Tests",
    "section": "12.2 The T-Distribution",
    "text": "12.2 The T-Distribution\nThe t-distribution, discovered by William Sealy Gosset (who published under the pseudonym “Student”), resembles the normal distribution but has heavier tails. This accounts for the extra uncertainty that comes from estimating the population standard deviation from sample data.\nThe t-distribution is characterized by its degrees of freedom (df). As df increases, the t-distribution approaches the normal distribution. For small samples, the heavier tails mean that extreme values are more likely, leading to wider confidence intervals and more conservative tests.\n\n\nCode\n# Compare t-distributions with different df\nx &lt;- seq(-4, 4, length.out = 200)\nplot(x, dnorm(x), type = \"l\", lwd = 2, col = \"black\",\n     xlab = \"x\", ylab = \"Density\",\n     main = \"T-distributions vs. Normal\")\nlines(x, dt(x, df = 3), lwd = 2, col = \"red\")\nlines(x, dt(x, df = 10), lwd = 2, col = \"blue\")\nlegend(\"topright\",\n       legend = c(\"Normal\", \"t (df=3)\", \"t (df=10)\"),\n       col = c(\"black\", \"red\", \"blue\"), lwd = 2)\n\n\n\n\n\n\n\n\n\n\n12.2.1 Why Heavier Tails Matter in Practice\nThe heavier tails of the t-distribution have real practical consequences. When you estimate the standard deviation from a small sample, you might underestimate or overestimate the true value. The t-distribution accounts for this uncertainty by assigning more probability to extreme values.\nConsider this concrete example: suppose you’re estimating voter support from a poll of 25 likely voters. With the true population proportion unknown and estimated from the sample, how wide should your confidence interval be?\n\n\nCode\n# Demonstrate the practical difference between normal and t-based intervals\nset.seed(2016)\n\n# Simulate: true support is 48.5%, poll 25 people\ntrue_support &lt;- 0.485\nn_poll &lt;- 25\n\n# One poll result\npoll_result &lt;- rbinom(1, n_poll, true_support) / n_poll\npoll_se &lt;- sqrt(poll_result * (1 - poll_result) / n_poll)\n\n# Compare critical values\nz_crit &lt;- qnorm(0.975)      # Normal: 1.96\nt_crit &lt;- qt(0.975, df = n_poll - 1)  # t with 24 df: 2.06\n\n# Calculate intervals\nnormal_ci &lt;- c(poll_result - z_crit * poll_se, poll_result + z_crit * poll_se)\nt_ci &lt;- c(poll_result - t_crit * poll_se, poll_result + t_crit * poll_se)\n\ncat(\"Poll result:\", round(poll_result * 100, 1), \"%\\n\")\n\n\nPoll result: 40 %\n\n\nCode\ncat(\"Standard error:\", round(poll_se * 100, 1), \"%\\n\")\n\n\nStandard error: 9.8 %\n\n\nCode\ncat(\"\\nNormal-based 95% CI: [\", round(normal_ci[1]*100, 1), \"%, \",\n    round(normal_ci[2]*100, 1), \"%]\\n\")\n\n\n\nNormal-based 95% CI: [ 20.8 %,  59.2 %]\n\n\nCode\ncat(\"t-based 95% CI:      [\", round(t_ci[1]*100, 1), \"%, \",\n    round(t_ci[2]*100, 1), \"%]\\n\")\n\n\nt-based 95% CI:      [ 19.8 %,  60.2 %]\n\n\nCode\ncat(\"\\nDifference in width:\", round((t_ci[2] - t_ci[1] - (normal_ci[2] - normal_ci[1]))*100, 2),\n    \"percentage points\\n\")\n\n\n\nDifference in width: 2.04 percentage points\n\n\nWith only 25 observations, the t-distribution gives a critical value of about 2.06 instead of 1.96. This ~5% wider interval provides better coverage when the sample standard deviation might deviate substantially from the population value.\nThe difference matters most in the tails. For extreme values (like being 2.5+ standard errors away from the mean), the t-distribution assigns noticeably more probability:\n\n\nCode\n# Probability of being more than 2.5 SE from the mean\nprob_extreme_normal &lt;- 2 * pnorm(-2.5)\nprob_extreme_t &lt;- 2 * pt(-2.5, df = 24)\n\ncat(\"P(|Z| &gt; 2.5) with normal distribution:\", round(prob_extreme_normal, 4), \"\\n\")\n\n\nP(|Z| &gt; 2.5) with normal distribution: 0.0124 \n\n\nCode\ncat(\"P(|T| &gt; 2.5) with t(df=24):\", round(prob_extreme_t, 4), \"\\n\")\n\n\nP(|T| &gt; 2.5) with t(df=24): 0.0197 \n\n\nCode\ncat(\"The t-distribution gives\", round(prob_extreme_t/prob_extreme_normal, 1),\n    \"times higher probability to extreme values\\n\")\n\n\nThe t-distribution gives 1.6 times higher probability to extreme values\n\n\nThis is why using the normal distribution instead of the t-distribution for small samples leads to confidence intervals that are too narrow and p-values that are too small—both resulting in overconfident conclusions.",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>T-Tests</span>"
    ]
  },
  {
    "objectID": "chapters/12-t-tests.html#one-sample-t-test",
    "href": "chapters/12-t-tests.html#one-sample-t-test",
    "title": "12  T-Tests",
    "section": "12.3 One-Sample T-Test",
    "text": "12.3 One-Sample T-Test\nThe one-sample t-test compares a sample mean to a hypothesized population value. The null hypothesis is that the population mean equals the specified value: \\(H_0: \\mu = \\mu_0\\).\nThe test statistic is:\n\\[t = \\frac{\\bar{x} - \\mu_0}{s / \\sqrt{n}}\\]\nThis is the difference between the sample mean and hypothesized value, divided by the standard error of the mean. Under the null hypothesis, this statistic follows a t-distribution with \\(n-1\\) degrees of freedom.\n\n\nCode\n# One-sample t-test example\n# Does this sample come from a population with mean = 100?\nset.seed(42)\nsample_data &lt;- rnorm(25, mean = 105, sd = 15)\n\nt.test(sample_data, mu = 100)\n\n\n\n    One Sample t-test\n\ndata:  sample_data\nt = 1.9936, df = 24, p-value = 0.05768\nalternative hypothesis: true mean is not equal to 100\n95 percent confidence interval:\n  99.72443 115.90166\nsample estimates:\nmean of x \n  107.813 \n\n\nThe output shows the t-statistic, degrees of freedom, p-value, confidence interval, and sample mean. The small p-value indicates evidence that the true mean differs from 100.",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>T-Tests</span>"
    ]
  },
  {
    "objectID": "chapters/12-t-tests.html#two-sample-t-test",
    "href": "chapters/12-t-tests.html#two-sample-t-test",
    "title": "12  T-Tests",
    "section": "12.4 Two-Sample T-Test",
    "text": "12.4 Two-Sample T-Test\nThe two-sample (independent samples) t-test compares means from two independent groups. The null hypothesis is that the population means are equal: \\(H_0: \\mu_1 = \\mu_2\\).\nThe test assumes: - Independence of observations within and between groups - Normally distributed populations (or large samples) - Equal variances in both groups (for the standard version)\n\n\nCode\n# Two-sample t-test example\nset.seed(518)\ntreatment &lt;- rnorm(n = 30, mean = 12, sd = 3)\ncontrol &lt;- rnorm(n = 30, mean = 10, sd = 3)\n\n# Visualize the data\npar(mfrow = c(1, 2))\nboxplot(treatment, control, names = c(\"Treatment\", \"Control\"),\n        col = c(\"lightblue\", \"lightgreen\"), main = \"Boxplot\")\n        \n# Combined histogram\nhist(treatment, col = rgb(0, 0, 1, 0.5), xlim = c(0, 20),\n     main = \"Histograms\", xlab = \"Value\")\nhist(control, col = rgb(0, 1, 0, 0.5), add = TRUE)\nlegend(\"topright\", legend = c(\"Treatment\", \"Control\"),\n       fill = c(rgb(0, 0, 1, 0.5), rgb(0, 1, 0, 0.5)))\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Perform the t-test\nt.test(treatment, control)\n\n\n\n    Welch Two Sample t-test\n\ndata:  treatment and control\nt = 1.3224, df = 57.98, p-value = 0.1912\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -0.5256045  2.5718411\nsample estimates:\nmean of x mean of y \n 11.08437  10.06125",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>T-Tests</span>"
    ]
  },
  {
    "objectID": "chapters/12-t-tests.html#welchs-t-test",
    "href": "chapters/12-t-tests.html#welchs-t-test",
    "title": "12  T-Tests",
    "section": "12.5 Welch’s T-Test",
    "text": "12.5 Welch’s T-Test\nThe classic two-sample t-test assumes equal variances. When this assumption is violated, Welch’s t-test provides a better alternative. It adjusts the degrees of freedom to account for unequal variances.\nR’s t.test() function uses Welch’s test by default. To use the equal-variance version, set var.equal = TRUE.\n\n\nCode\n# When variances are unequal\nset.seed(42)\ngroup1 &lt;- rnorm(30, mean = 50, sd = 5)\ngroup2 &lt;- rnorm(30, mean = 52, sd = 15)\n\n# Welch's test (default)\nt.test(group1, group2)\n\n\n\n    Welch Two Sample t-test\n\ndata:  group1 and group2\nt = 0.055423, df = 37.98, p-value = 0.9561\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -6.095093  6.438216\nsample estimates:\nmean of x mean of y \n 50.34293  50.17137 \n\n\nCode\n# Equal variance assumed\nt.test(group1, group2, var.equal = TRUE)\n\n\n\n    Two Sample t-test\n\ndata:  group1 and group2\nt = 0.055423, df = 58, p-value = 0.956\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -6.024786  6.367910\nsample estimates:\nmean of x mean of y \n 50.34293  50.17137",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>T-Tests</span>"
    ]
  },
  {
    "objectID": "chapters/12-t-tests.html#paired-t-test",
    "href": "chapters/12-t-tests.html#paired-t-test",
    "title": "12  T-Tests",
    "section": "12.6 Paired T-Test",
    "text": "12.6 Paired T-Test\nWhen observations in two groups are naturally paired—the same subjects measured twice, matched pairs, or before-and-after measurements—the paired t-test is more appropriate. It tests whether the mean difference within pairs is zero.\nThe paired t-test is more powerful than the two-sample test when pairs are positively correlated, because it removes between-subject variability.\n\n\nCode\n# Paired t-test example: before and after treatment\nset.seed(123)\nn &lt;- 20\nbefore &lt;- rnorm(n, mean = 100, sd = 15)\n# After measurements are correlated with before\nafter &lt;- before + rnorm(n, mean = 5, sd = 5)\n\n# Paired test (correct for this data)\nt.test(after, before, paired = TRUE)\n\n\n\n    Paired t-test\n\ndata:  after and before\nt = 5.1123, df = 19, p-value = 6.19e-05\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n 2.801598 6.685830\nsample estimates:\nmean difference \n       4.743714 \n\n\nCode\n# Compare to unpaired (less power)\nt.test(after, before, paired = FALSE)\n\n\n\n    Welch Two Sample t-test\n\ndata:  after and before\nt = 1.0209, df = 37.992, p-value = 0.3138\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -4.663231 14.150660\nsample estimates:\nmean of x mean of y \n 106.8681  102.1244 \n\n\nNotice that the paired test produces a smaller p-value because it accounts for the correlation between measurements on the same subject.",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>T-Tests</span>"
    ]
  },
  {
    "objectID": "chapters/12-t-tests.html#one-tailed-vs.-two-tailed-tests",
    "href": "chapters/12-t-tests.html#one-tailed-vs.-two-tailed-tests",
    "title": "12  T-Tests",
    "section": "12.7 One-Tailed vs. Two-Tailed Tests",
    "text": "12.7 One-Tailed vs. Two-Tailed Tests\nBy default, t.test() performs a two-tailed test. For a one-tailed test, specify the alternative hypothesis:\n\n\nCode\n# Two-tailed (default): H_A: treatment ≠ control\nt.test(treatment, control, alternative = \"two.sided\")$p.value\n\n\n[1] 0.1912327\n\n\nCode\n# One-tailed: H_A: treatment &gt; control\nt.test(treatment, control, alternative = \"greater\")$p.value\n\n\n[1] 0.09561633\n\n\nCode\n# One-tailed: H_A: treatment &lt; control\nt.test(treatment, control, alternative = \"less\")$p.value\n\n\n[1] 0.9043837\n\n\nUse one-tailed tests only when you have a strong prior reason to expect an effect in a specific direction and would not act on an effect in the opposite direction.",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>T-Tests</span>"
    ]
  },
  {
    "objectID": "chapters/12-t-tests.html#checking-assumptions",
    "href": "chapters/12-t-tests.html#checking-assumptions",
    "title": "12  T-Tests",
    "section": "12.8 Checking Assumptions",
    "text": "12.8 Checking Assumptions\nT-tests assume normally distributed data (or large samples) and, for the standard two-sample test, equal variances. Check these assumptions before interpreting results.\nNormality: Use histograms, Q-Q plots, or formal tests like Shapiro-Wilk.\n\n\nCode\n# Check normality with Q-Q plot\nqqnorm(treatment)\nqqline(treatment, col = \"red\")\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Shapiro-Wilk test for normality\nshapiro.test(treatment)\n\n\n\n    Shapiro-Wilk normality test\n\ndata:  treatment\nW = 0.9115, p-value = 0.01624\n\n\nA non-significant Shapiro-Wilk test suggests the data are consistent with normality. However, this test has low power for small samples and may reject normality for trivial deviations with large samples.\nEqual variances: Compare standard deviations or use Levene’s test.\n\n\nCode\n# Compare standard deviations\nsd(treatment)\n\n\n[1] 3.024138\n\n\nCode\nsd(control)\n\n\n[1] 2.968592\n\n\nCode\n# Levene's test (from car package)\n# car::leveneTest(c(treatment, control), \n#                 factor(rep(c(\"treatment\", \"control\"), each = 30)))",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>T-Tests</span>"
    ]
  },
  {
    "objectID": "chapters/12-t-tests.html#effect-size-cohens-d",
    "href": "chapters/12-t-tests.html#effect-size-cohens-d",
    "title": "12  T-Tests",
    "section": "12.9 Effect Size: Cohen’s d",
    "text": "12.9 Effect Size: Cohen’s d\nStatistical significance does not tell you how large an effect is. Cohen’s d measures effect size as the standardized difference between means:\n\\[d = \\frac{\\bar{x}_1 - \\bar{x}_2}{s_{pooled}}\\]\nwhere \\(s_{pooled}\\) is the pooled standard deviation.\nConventional interpretations: \\(|d| = 0.2\\) is small, \\(|d| = 0.5\\) is medium, \\(|d| = 0.8\\) is large. However, context matters—a small d might be practically important in some fields.\n\n\nCode\n# Calculate Cohen's d\nmean_diff &lt;- mean(treatment) - mean(control)\ns_pooled &lt;- sqrt((var(treatment) + var(control)) / 2)\ncohens_d &lt;- mean_diff / s_pooled\n\ncat(\"Cohen's d:\", round(cohens_d, 2), \"\\n\")\n\n\nCohen's d: 0.34",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>T-Tests</span>"
    ]
  },
  {
    "objectID": "chapters/12-t-tests.html#practical-example",
    "href": "chapters/12-t-tests.html#practical-example",
    "title": "12  T-Tests",
    "section": "12.10 Practical Example",
    "text": "12.10 Practical Example\nLet’s work through a complete analysis comparing two groups:\n\n\nCode\n# Simulated drug trial data\nset.seed(999)\ndrug &lt;- rnorm(40, mean = 75, sd = 12)\nplacebo &lt;- rnorm(40, mean = 70, sd = 12)\n\n# Step 1: Visualize\npar(mfrow = c(2, 2))\nboxplot(drug, placebo, names = c(\"Drug\", \"Placebo\"), \n        col = c(\"coral\", \"lightblue\"), main = \"Response by Group\")\n\n# Step 2: Check normality\nqqnorm(drug, main = \"Q-Q Plot: Drug\")\nqqline(drug, col = \"red\")\nqqnorm(placebo, main = \"Q-Q Plot: Placebo\")\nqqline(placebo, col = \"red\")\n\n# Combined histogram\nhist(drug, col = rgb(1, 0.5, 0.5, 0.5), xlim = c(40, 110),\n     main = \"Distribution Comparison\", xlab = \"Response\")\nhist(placebo, col = rgb(0.5, 0.5, 1, 0.5), add = TRUE)\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Step 3: Perform t-test\nresult &lt;- t.test(drug, placebo)\nprint(result)\n\n\n\n    Welch Two Sample t-test\n\ndata:  drug and placebo\nt = 1.2147, df = 75.923, p-value = 0.2282\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -1.990367  8.213525\nsample estimates:\nmean of x mean of y \n 72.26982  69.15824 \n\n\nCode\n# Step 4: Calculate effect size\ncohens_d &lt;- (mean(drug) - mean(placebo)) / \n            sqrt((var(drug) + var(placebo)) / 2)\ncat(\"\\nCohen's d:\", round(cohens_d, 2), \"\\n\")\n\n\n\nCohen's d: 0.27 \n\n\nThe t-test shows a significant difference (p &lt; 0.05), and Cohen’s d indicates a medium effect size. We can conclude that the drug group shows higher response than the placebo group, with the mean difference being about 0.4 standard deviations.",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>T-Tests</span>"
    ]
  },
  {
    "objectID": "chapters/12-t-tests.html#randomization-tests-as-an-alternative",
    "href": "chapters/12-t-tests.html#randomization-tests-as-an-alternative",
    "title": "12  T-Tests",
    "section": "12.11 Randomization Tests as an Alternative",
    "text": "12.11 Randomization Tests as an Alternative\nWhen normality assumptions are questionable and sample sizes are small, randomization (permutation) tests provide a non-parametric alternative to the t-test. The logic is elegant: if there is no difference between groups, then the group labels are arbitrary and could be shuffled without affecting the distribution of the test statistic.\n\n\nCode\n# Randomization test example\nset.seed(42)\ngroup_A &lt;- c(23, 25, 28, 31, 35, 29)\ngroup_B &lt;- c(18, 20, 22, 19, 21, 23)\n\n# Observed difference\nobs_diff &lt;- mean(group_A) - mean(group_B)\n\n# Combine all observations\nall_data &lt;- c(group_A, group_B)\nn_A &lt;- length(group_A)\nn_B &lt;- length(group_B)\n\n# Generate null distribution by permutation\nn_perms &lt;- 10000\nperm_diffs &lt;- numeric(n_perms)\n\nfor (i in 1:n_perms) {\n  shuffled &lt;- sample(all_data)\n  perm_diffs[i] &lt;- mean(shuffled[1:n_A]) - mean(shuffled[(n_A+1):(n_A+n_B)])\n}\n\n# Plot null distribution\nhist(perm_diffs, breaks = 50, col = \"lightblue\",\n     main = \"Randomization Null Distribution\",\n     xlab = \"Difference in Means\")\nabline(v = obs_diff, col = \"red\", lwd = 2)\nabline(v = -obs_diff, col = \"red\", lwd = 2, lty = 2)\n\n\n\n\n\n\n\n\n\nCode\n# Two-tailed p-value\np_value &lt;- mean(abs(perm_diffs) &gt;= abs(obs_diff))\ncat(\"Observed difference:\", round(obs_diff, 2), \"\\n\")\n\n\nObserved difference: 8 \n\n\nCode\ncat(\"Permutation p-value:\", p_value, \"\\n\")\n\n\nPermutation p-value: 0.0039 \n\n\nThe randomization test makes no assumptions about the underlying distribution—it only assumes that observations are exchangeable under the null hypothesis. This makes it robust to non-normality and outliers.\n\n\n\n\n\n\nWhen to Use Randomization Tests\n\n\n\n\nSample sizes are small (n &lt; 20 per group)\nData are clearly non-normal or contain outliers\nYou want to avoid distributional assumptions\nAs a sensitivity analysis to complement parametric results",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>T-Tests</span>"
    ]
  },
  {
    "objectID": "chapters/12-t-tests.html#choosing-the-right-t-test",
    "href": "chapters/12-t-tests.html#choosing-the-right-t-test",
    "title": "12  T-Tests",
    "section": "12.12 Choosing the Right T-Test",
    "text": "12.12 Choosing the Right T-Test\n\n\n\n\n\n\n\n\nScenario\nTest\nR Function\n\n\n\n\nCompare sample mean to known value\nOne-sample\nt.test(x, mu = value)\n\n\nCompare two independent groups\nTwo-sample (Welch’s)\nt.test(x, y)\n\n\nCompare two independent groups (equal variance)\nTwo-sample (Student’s)\nt.test(x, y, var.equal = TRUE)\n\n\nCompare paired measurements\nPaired\nt.test(x, y, paired = TRUE)\n\n\n\nDecision guidelines:\n\nIf comparing to a fixed, known value: one-sample t-test\nIf observations in groups are naturally paired: paired t-test\nIf groups are independent with potentially unequal variances: Welch’s t-test (the default)\nIf groups are independent and you have strong evidence of equal variances: Student’s t-test\n\nWhen in doubt, use Welch’s t-test—it performs nearly as well as Student’s t-test when variances are equal and much better when they are not.",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>T-Tests</span>"
    ]
  },
  {
    "objectID": "chapters/12-t-tests.html#summary",
    "href": "chapters/12-t-tests.html#summary",
    "title": "12  T-Tests",
    "section": "12.13 Summary",
    "text": "12.13 Summary\nThe t-test family provides essential tools for comparing means:\n\nOne-sample tests compare a sample to a hypothesized value\nTwo-sample tests compare independent groups\nPaired tests compare matched or repeated measurements\nWelch’s version handles unequal variances (recommended default)\nRandomization tests provide a distribution-free alternative\n\nAlways visualize your data, check assumptions, and report effect sizes alongside p-values. A statistically significant result is only meaningful if the underlying assumptions are reasonable and the effect size is practically relevant.",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>T-Tests</span>"
    ]
  },
  {
    "objectID": "chapters/12-t-tests.html#practice-exercises",
    "href": "chapters/12-t-tests.html#practice-exercises",
    "title": "12  T-Tests",
    "section": "12.14 Practice Exercises",
    "text": "12.14 Practice Exercises\nFor hands-on practice with t-tests and hypothesis testing, see Section 35.9 in the Practice Exercises appendix. The exercises include:\n\nPerforming one-sample and two-sample t-tests\nExploring the effects of sample size on statistical power\nChi-square tests for categorical data\nTesting for Hardy-Weinberg equilibrium",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>T-Tests</span>"
    ]
  },
  {
    "objectID": "chapters/12-t-tests.html#additional-resources",
    "href": "chapters/12-t-tests.html#additional-resources",
    "title": "12  T-Tests",
    "section": "12.15 Additional Resources",
    "text": "12.15 Additional Resources\n\nLogan (2010) - Detailed coverage of t-tests with biological examples\nIrizarry (2019) - Excellent treatment of randomization and permutation methods\n\n\n\n\n\n\n\nIrizarry, Rafael A. 2019. Introduction to Data Science. Self-published. https://rafalab.github.io/dsbook/.\n\n\nLogan, Murray. 2010. Biostatistical Design and Analysis Using r. Wiley-Blackwell.",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>T-Tests</span>"
    ]
  },
  {
    "objectID": "chapters/13-nonparametric-tests.html",
    "href": "chapters/13-nonparametric-tests.html",
    "title": "13  Nonparametric Tests",
    "section": "",
    "text": "13.1 When Assumptions Fail\nParametric tests like the t-test make assumptions about the underlying data distribution—typically that data are normally distributed with equal variances across groups. When these assumptions are violated, the tests may give misleading results. Nonparametric tests provide alternatives that make fewer assumptions about the data.\nNonparametric methods are sometimes called distribution-free methods because they do not assume a specific probability distribution. Instead, they typically work with ranks or signs of data rather than the raw values. This makes them robust to outliers and applicable to ordinal data where parametric methods would be inappropriate.",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Nonparametric Tests</span>"
    ]
  },
  {
    "objectID": "chapters/13-nonparametric-tests.html#the-mann-whitney-u-test",
    "href": "chapters/13-nonparametric-tests.html#the-mann-whitney-u-test",
    "title": "13  Nonparametric Tests",
    "section": "13.2 The Mann-Whitney U Test",
    "text": "13.2 The Mann-Whitney U Test\nThe Mann-Whitney U test (also called the Wilcoxon rank-sum test) is the nonparametric equivalent of the two-sample t-test. It tests whether two independent groups tend to have different values, based on comparing the ranks of observations rather than the observations themselves.\nThe null hypothesis is that the distributions of the two groups are identical. The alternative is that one group tends to have larger values than the other.\n\n\nCode\n# Generate data with non-normal distributions\nset.seed(518)\ngroup1 &lt;- sample(rnorm(n = 10000, mean = 2, sd = 0.5), size = 100)\ngroup2 &lt;- sample(rnorm(n = 10000, mean = 5, sd = 1.5), size = 100)\n\n# Mann-Whitney U test\nwilcox.test(group1, group2)\n\n\n\n    Wilcoxon rank sum test with continuity correction\n\ndata:  group1 and group2\nW = 440, p-value &lt; 2.2e-16\nalternative hypothesis: true location shift is not equal to 0\n\n\nThe test works by combining all observations, ranking them, and comparing the sum of ranks in each group. If one group tends to have higher values, its rank sum will be larger than expected by chance.",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Nonparametric Tests</span>"
    ]
  },
  {
    "objectID": "chapters/13-nonparametric-tests.html#wilcoxon-signed-rank-test",
    "href": "chapters/13-nonparametric-tests.html#wilcoxon-signed-rank-test",
    "title": "13  Nonparametric Tests",
    "section": "13.3 Wilcoxon Signed-Rank Test",
    "text": "13.3 Wilcoxon Signed-Rank Test\nFor paired data, the Wilcoxon signed-rank test is the nonparametric alternative to the paired t-test. It tests whether the median difference between pairs is zero.\n\n\nCode\n# Paired data example\nset.seed(123)\nbefore &lt;- rnorm(20, mean = 100, sd = 15)\nafter &lt;- before + rexp(20, rate = 0.2)  # Skewed improvement\n\nwilcox.test(after, before, paired = TRUE)\n\n\n\n    Wilcoxon signed rank exact test\n\ndata:  after and before\nV = 210, p-value = 1.907e-06\nalternative hypothesis: true location shift is not equal to 0\n\n\nThe test calculates the differences between pairs, ranks their absolute values, and considers the signs of the differences. Under the null hypothesis, positive and negative differences should be equally likely and of similar magnitude.",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Nonparametric Tests</span>"
    ]
  },
  {
    "objectID": "chapters/13-nonparametric-tests.html#kruskal-wallis-test",
    "href": "chapters/13-nonparametric-tests.html#kruskal-wallis-test",
    "title": "13  Nonparametric Tests",
    "section": "13.4 Kruskal-Wallis Test",
    "text": "13.4 Kruskal-Wallis Test\nThe Kruskal-Wallis test extends the Mann-Whitney U test to more than two groups, serving as a nonparametric alternative to one-way ANOVA. It tests whether at least one group tends to have different values from the others.\n\n\nCode\n# Example with three groups\nset.seed(42)\ndata &lt;- data.frame(\n  value = c(rexp(30, 0.1), rexp(30, 0.15), rexp(30, 0.2)),\n  group = factor(rep(c(\"A\", \"B\", \"C\"), each = 30))\n)\n\nkruskal.test(value ~ group, data = data)\n\n\n\n    Kruskal-Wallis rank sum test\n\ndata:  value by group\nKruskal-Wallis chi-squared = 9.3507, df = 2, p-value = 0.009322\n\n\nLike ANOVA, a significant Kruskal-Wallis test tells you that groups differ but not which specific groups differ from which others. Post-hoc pairwise comparisons can follow up on a significant result.",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Nonparametric Tests</span>"
    ]
  },
  {
    "objectID": "chapters/13-nonparametric-tests.html#advantages-and-limitations",
    "href": "chapters/13-nonparametric-tests.html#advantages-and-limitations",
    "title": "13  Nonparametric Tests",
    "section": "13.5 Advantages and Limitations",
    "text": "13.5 Advantages and Limitations\nAdvantages of nonparametric tests:\nNonparametric tests do not require normally distributed data. They are robust to outliers since they work with ranks rather than raw values. They can be applied to ordinal data where the assumption of interval-level measurement would be violated. They often have good power relative to parametric tests even when parametric assumptions are met.\nLimitations:\nWhen parametric assumptions are met, nonparametric tests are slightly less powerful than their parametric counterparts. They test hypotheses about distributions or medians rather than means, which may not always align with research questions. They can be more difficult to extend to complex designs with multiple factors or covariates.",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Nonparametric Tests</span>"
    ]
  },
  {
    "objectID": "chapters/13-nonparametric-tests.html#choosing-between-parametric-and-nonparametric",
    "href": "chapters/13-nonparametric-tests.html#choosing-between-parametric-and-nonparametric",
    "title": "13  Nonparametric Tests",
    "section": "13.6 Choosing Between Parametric and Nonparametric",
    "text": "13.6 Choosing Between Parametric and Nonparametric\nThe choice depends on your data and research question. If your data are reasonably normal (or your sample is large enough for the Central Limit Theorem to apply) and you care about means, parametric tests are appropriate and efficient. If your data are severely non-normal, contain outliers, or are ordinal in nature, nonparametric tests provide a safer alternative.\nWith large samples, the Central Limit Theorem ensures that parametric tests are robust to non-normality, so the choice matters less. With small samples, checking assumptions becomes more important.",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Nonparametric Tests</span>"
    ]
  },
  {
    "objectID": "chapters/13-nonparametric-tests.html#frequency-analysis-chi-square-tests",
    "href": "chapters/13-nonparametric-tests.html#frequency-analysis-chi-square-tests",
    "title": "13  Nonparametric Tests",
    "section": "13.7 Frequency Analysis: Chi-Square Tests",
    "text": "13.7 Frequency Analysis: Chi-Square Tests\nWhen data consist of counts in categories rather than continuous measurements, we need tests designed for categorical data. The chi-square (\\(\\chi^2\\)) test compares observed frequencies to expected frequencies under a null hypothesis.\n\n13.7.1 Goodness-of-Fit Test\nThe chi-square goodness-of-fit test asks whether observed frequencies match expected proportions. For example, do offspring genotypes follow expected Mendelian ratios?\n\n\nCode\n# Test whether observed counts match expected 3:1 ratio\nobserved &lt;- c(75, 25)  # Dominant, Recessive phenotypes\nexpected_ratio &lt;- c(3, 1)\nexpected &lt;- sum(observed) * expected_ratio / sum(expected_ratio)\n\n# Chi-square test\nchisq.test(observed, p = expected_ratio / sum(expected_ratio))\n\n\n\n    Chi-squared test for given probabilities\n\ndata:  observed\nX-squared = 0, df = 1, p-value = 1\n\n\nThe test statistic is:\n\\[\\chi^2 = \\sum \\frac{(O_i - E_i)^2}{E_i}\\]\nwhere \\(O_i\\) are observed counts and \\(E_i\\) are expected counts. Under the null hypothesis (observed = expected), this follows a chi-square distribution with \\(k-1\\) degrees of freedom, where \\(k\\) is the number of categories.\n\n\n13.7.2 Tests of Independence: Contingency Tables\nWhen we have counts cross-classified by two categorical variables, a contingency table displays the frequencies. The chi-square test of independence asks whether the two variables are associated.\n\n\nCode\n# Example: Is treatment outcome associated with gender?\ntreatment_data &lt;- matrix(c(\n  45, 35,   # Males: Success, Failure\n  55, 15    # Females: Success, Failure\n), nrow = 2, byrow = TRUE)\nrownames(treatment_data) &lt;- c(\"Male\", \"Female\")\ncolnames(treatment_data) &lt;- c(\"Success\", \"Failure\")\n\ntreatment_data\n\n\n       Success Failure\nMale        45      35\nFemale      55      15\n\n\nCode\n# Chi-square test of independence\nchisq.test(treatment_data)\n\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  treatment_data\nX-squared = 7.3962, df = 1, p-value = 0.006536\n\n\nExpected counts under independence are calculated as:\n\\[E_{ij} = \\frac{(\\text{Row Total}_i) \\times (\\text{Column Total}_j)}{\\text{Grand Total}}\\]\n\n\n\n\n\n\nAssumptions of Chi-Square Tests\n\n\n\n\nObservations must be independent\nExpected counts should be at least 5 in each cell (some sources say 80% of cells should have expected counts ≥ 5)\nFor 2×2 tables with small expected counts, use Fisher’s exact test instead\n\n\n\n\n\n13.7.3 Fisher’s Exact Test\nWhen sample sizes are small, Fisher’s exact test provides exact p-values rather than relying on the chi-square approximation:\n\n\nCode\n# Small sample example\nsmall_table &lt;- matrix(c(3, 1, 1, 3), nrow = 2)\nfisher.test(small_table)\n\n\n\n    Fisher's Exact Test for Count Data\n\ndata:  small_table\np-value = 0.4857\nalternative hypothesis: true odds ratio is not equal to 1\n95 percent confidence interval:\n   0.2117329 621.9337505\nsample estimates:\nodds ratio \n  6.408309 \n\n\n\n\n13.7.4 G-Test (Likelihood Ratio Test)\nThe G-test is an alternative to the chi-square test based on the likelihood ratio. It has better theoretical properties and is preferred by some statisticians:\n\\[G = 2 \\sum O_i \\ln\\left(\\frac{O_i}{E_i}\\right)\\]\n\n\nCode\n# G-test for the treatment data\n# Using observed and expected from chi-square\ntest_result &lt;- chisq.test(treatment_data)\nobserved_counts &lt;- as.vector(treatment_data)\nexpected_counts &lt;- as.vector(test_result$expected)\n\nG &lt;- 2 * sum(observed_counts * log(observed_counts / expected_counts))\np_value &lt;- 1 - pchisq(G, df = 1)\n\ncat(\"G statistic:\", round(G, 3), \"\\n\")\n\n\nG statistic: 8.563 \n\n\nCode\ncat(\"p-value:\", round(p_value, 4), \"\\n\")\n\n\np-value: 0.0034 \n\n\n\n\n13.7.5 Odds Ratios\nFor 2×2 tables, the odds ratio quantifies the strength of association between two binary variables:\n\\[OR = \\frac{a/b}{c/d} = \\frac{ad}{bc}\\]\nwhere the table is:\n\n\n\n\nOutcome+\nOutcome-\n\n\n\n\nExposure+\na\nb\n\n\nExposure-\nc\nd\n\n\n\nAn odds ratio of 1 indicates no association. OR &gt; 1 indicates positive association; OR &lt; 1 indicates negative association.\n\n\nCode\n# Calculate odds ratio for treatment data\na &lt;- treatment_data[1, 1]  # Male, Success\nb &lt;- treatment_data[1, 2]  # Male, Failure\nc &lt;- treatment_data[2, 1]  # Female, Success\nd &lt;- treatment_data[2, 2]  # Female, Failure\n\nodds_ratio &lt;- (a * d) / (b * c)\ncat(\"Odds ratio:\", round(odds_ratio, 3), \"\\n\")\n\n\nOdds ratio: 0.351 \n\n\nCode\n# Using fisher.test to get OR with confidence interval\nfisher.test(treatment_data)\n\n\n\n    Fisher's Exact Test for Count Data\n\ndata:  treatment_data\np-value = 0.005273\nalternative hypothesis: true odds ratio is not equal to 1\n95 percent confidence interval:\n 0.1579843 0.7609357\nsample estimates:\nodds ratio \n 0.3531441 \n\n\nAn odds ratio of 0.35 indicates that males have lower odds of success compared to females in this example.\n\n\n13.7.6 McNemar’s Test for Paired Data\nWhen categorical data are paired (e.g., before/after measurements on the same subjects), McNemar’s test is appropriate:\n\n\nCode\n# Before/after treatment: did opinion change?\nbefore_after &lt;- matrix(c(\n  40, 10,  # Agree before: Agree after, Disagree after\n  25, 25   # Disagree before: Agree after, Disagree after\n), nrow = 2, byrow = TRUE)\n\nmcnemar.test(before_after)\n\n\n\n    McNemar's Chi-squared test with continuity correction\n\ndata:  before_after\nMcNemar's chi-squared = 5.6, df = 1, p-value = 0.01796\n\n\nThe test focuses on the discordant pairs—cases where the response changed—and asks whether changes in one direction are more common than changes in the other direction.",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Nonparametric Tests</span>"
    ]
  },
  {
    "objectID": "chapters/13-nonparametric-tests.html#summary",
    "href": "chapters/13-nonparametric-tests.html#summary",
    "title": "13  Nonparametric Tests",
    "section": "13.8 Summary",
    "text": "13.8 Summary\nNonparametric and frequency-based tests provide alternatives when parametric assumptions fail or data are categorical:\n\nMann-Whitney U and Kruskal-Wallis for comparing groups with non-normal data\nWilcoxon signed-rank for paired non-normal data\nChi-square tests for categorical data (goodness-of-fit and independence)\nFisher’s exact test for small samples\nOdds ratios to quantify association strength\nMcNemar’s test for paired categorical data",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Nonparametric Tests</span>"
    ]
  },
  {
    "objectID": "chapters/13-nonparametric-tests.html#additional-resources",
    "href": "chapters/13-nonparametric-tests.html#additional-resources",
    "title": "13  Nonparametric Tests",
    "section": "13.9 Additional Resources",
    "text": "13.9 Additional Resources\n\nLogan (2010) - Comprehensive coverage of nonparametric and categorical data analysis\nCrawley (2007) - Detailed treatment of chi-square and contingency table methods in R\n\n\n\n\n\n\n\nCrawley, Michael J. 2007. The r Book. John Wiley & Sons.\n\n\nLogan, Murray. 2010. Biostatistical Design and Analysis Using r. Wiley-Blackwell.",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Nonparametric Tests</span>"
    ]
  },
  {
    "objectID": "chapters/14-bootstrapping.html",
    "href": "chapters/14-bootstrapping.html",
    "title": "14  Bootstrapping",
    "section": "",
    "text": "14.1 The Bootstrap Idea\nFor the sample mean, we have elegant formulas for standard errors and confidence intervals derived from probability theory. But what about other statistics—the median, a correlation coefficient, the ratio of two means? For many estimators, no convenient formula exists.\nThe bootstrap, invented by Bradley Efron in 1979, provides a general solution. The key insight is that we can learn about the sampling distribution of a statistic by resampling from our data. If our sample is representative of the population, then samples drawn from our sample (with replacement) mimic what we would get from repeated sampling from the population.",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Bootstrapping</span>"
    ]
  },
  {
    "objectID": "chapters/14-bootstrapping.html#why-the-bootstrap-works",
    "href": "chapters/14-bootstrapping.html#why-the-bootstrap-works",
    "title": "14  Bootstrapping",
    "section": "14.2 Why the Bootstrap Works",
    "text": "14.2 Why the Bootstrap Works\nThe bootstrap treats the observed sample as if it were the population. By drawing many samples with replacement from this “population,” we create a distribution of the statistic of interest. This bootstrap distribution approximates the true sampling distribution.\nThe bootstrap standard error is the standard deviation of the bootstrap distribution. Bootstrap confidence intervals can be constructed from the percentiles of the bootstrap distribution—the 2.5th and 97.5th percentiles give an approximate 95% confidence interval.",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Bootstrapping</span>"
    ]
  },
  {
    "objectID": "chapters/14-bootstrapping.html#bootstrap-procedure",
    "href": "chapters/14-bootstrapping.html#bootstrap-procedure",
    "title": "14  Bootstrapping",
    "section": "14.3 Bootstrap Procedure",
    "text": "14.3 Bootstrap Procedure\nThe basic algorithm is straightforward:\n\nDraw a random sample of size n from your data with replacement (the bootstrap sample)\nCalculate the statistic of interest from this bootstrap sample\nRepeat steps 1 and 2 many times (1000 or more)\nUse the distribution of bootstrap statistics to estimate standard error or confidence intervals\n\n\n\nCode\n# Bootstrap example: estimating standard error of median\nset.seed(42)\noriginal_data &lt;- rexp(50, rate = 0.1)  # Skewed distribution\n\n# Observed median\nobserved_median &lt;- median(original_data)\n\n# Bootstrap\nn_boot &lt;- 1000\nboot_medians &lt;- replicate(n_boot, {\n  boot_sample &lt;- sample(original_data, replace = TRUE)\n  median(boot_sample)\n})\n\n# Bootstrap standard error\nboot_se &lt;- sd(boot_medians)\n\n# Bootstrap confidence interval (percentile method)\nboot_ci &lt;- quantile(boot_medians, c(0.025, 0.975))\n\ncat(\"Observed median:\", round(observed_median, 2), \"\\n\")\n\n\nObserved median: 6.59 \n\n\nCode\ncat(\"Bootstrap SE:\", round(boot_se, 2), \"\\n\")\n\n\nBootstrap SE: 1.46 \n\n\nCode\ncat(\"95% CI:\", round(boot_ci, 2), \"\\n\")\n\n\n95% CI: 4.39 11.92 \n\n\nCode\nhist(boot_medians, breaks = 30, main = \"Bootstrap Distribution of Median\",\n     xlab = \"Median\", col = \"lightblue\")\nabline(v = observed_median, col = \"red\", lwd = 2)\nabline(v = boot_ci, col = \"blue\", lwd = 2, lty = 2)",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Bootstrapping</span>"
    ]
  },
  {
    "objectID": "chapters/14-bootstrapping.html#advantages-of-the-bootstrap",
    "href": "chapters/14-bootstrapping.html#advantages-of-the-bootstrap",
    "title": "14  Bootstrapping",
    "section": "14.4 Advantages of the Bootstrap",
    "text": "14.4 Advantages of the Bootstrap\nThe bootstrap is remarkably versatile. It can be applied to almost any statistic—means, medians, correlations, regression coefficients, eigenvalues, and more. It works when no formula for standard errors exists. It is nonparametric, making no assumptions about the underlying distribution. It handles complex sampling designs and calculations that would be intractable analytically.\nThe bootstrap is widely used for assessing confidence in phylogenetic trees, where the complexity of tree-building algorithms makes analytical approaches impractical. In machine learning, bootstrap aggregating (bagging) improves prediction accuracy by combining models trained on bootstrap samples.",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Bootstrapping</span>"
    ]
  },
  {
    "objectID": "chapters/14-bootstrapping.html#when-the-bootstrap-fails",
    "href": "chapters/14-bootstrapping.html#when-the-bootstrap-fails",
    "title": "14  Bootstrapping",
    "section": "14.5 When the Bootstrap Fails",
    "text": "14.5 When the Bootstrap Fails\nThe bootstrap is not a magic solution to all problems. It requires that the original sample be representative of the population—a biased sample produces biased bootstrap estimates. It can struggle with very small samples where the original data may not adequately represent the population.\nCertain statistics, like the maximum of a sample, are poorly estimated by the bootstrap because the bootstrap distribution is bounded by the observed data. The bootstrap also assumes that observations are independent; for dependent data (like time series), specialized bootstrap methods are needed.",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Bootstrapping</span>"
    ]
  },
  {
    "objectID": "chapters/14-bootstrapping.html#bootstrap-confidence-intervals",
    "href": "chapters/14-bootstrapping.html#bootstrap-confidence-intervals",
    "title": "14  Bootstrapping",
    "section": "14.6 Bootstrap Confidence Intervals",
    "text": "14.6 Bootstrap Confidence Intervals\nSeveral methods exist for constructing bootstrap confidence intervals. The percentile method uses the quantiles of the bootstrap distribution directly. The basic bootstrap method reflects the bootstrap distribution around the observed estimate. The BCa (bias-corrected and accelerated) method adjusts for bias and skewness in the bootstrap distribution.\n\n\nCode\n# Different bootstrap CI methods\nlibrary(boot)\n\n# Define statistic function\nmedian_fun &lt;- function(data, indices) {\n  median(data[indices])\n}\n\n# Run bootstrap\nboot_result &lt;- boot(original_data, median_fun, R = 1000)\n\n# Different CI methods\nboot.ci(boot_result, type = c(\"perc\", \"basic\", \"bca\"))\n\n\nBOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS\nBased on 1000 bootstrap replicates\n\nCALL : \nboot.ci(boot.out = boot_result, type = c(\"perc\", \"basic\", \"bca\"))\n\nIntervals : \nLevel      Basic              Percentile            BCa          \n95%   ( 1.256,  8.841 )   ( 4.331, 11.916 )   ( 4.244, 11.582 )  \nCalculations and Intervals on Original Scale\n\n\nThe BCa method is generally preferred when computationally feasible, as it provides better coverage in many situations.",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Bootstrapping</span>"
    ]
  },
  {
    "objectID": "chapters/14-bootstrapping.html#practical-recommendations",
    "href": "chapters/14-bootstrapping.html#practical-recommendations",
    "title": "14  Bootstrapping",
    "section": "14.7 Practical Recommendations",
    "text": "14.7 Practical Recommendations\nFor most applications, 1000 bootstrap replications provide adequate precision for standard errors. For confidence intervals, especially when using the BCa method, 10,000 replications may be preferable. Always set a random seed for reproducibility.\nRemember that the bootstrap estimates sampling variability—it cannot fix problems with biased samples or invalid measurements. Use it as a tool for understanding uncertainty, not as a cure for poor data quality.",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Bootstrapping</span>"
    ]
  },
  {
    "objectID": "chapters/14b-what-are-models.html",
    "href": "chapters/14b-what-are-models.html",
    "title": "15  What are Models?",
    "section": "",
    "text": "15.1 The Essence of Modeling\nA model is a simplified representation of reality that helps us understand, explain, or predict phenomena. In statistics and data science, models are mathematical relationships between variables that capture patterns in data while ignoring irrelevant details.\nThis famous quote captures the fundamental truth of modeling: no model perfectly represents reality, but a good model can still provide valuable insights and predictions.",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>What are Models?</span>"
    ]
  },
  {
    "objectID": "chapters/14b-what-are-models.html#the-essence-of-modeling",
    "href": "chapters/14b-what-are-models.html#the-essence-of-modeling",
    "title": "15  What are Models?",
    "section": "",
    "text": "“All models are wrong, but some are useful.” — George Box",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>What are Models?</span>"
    ]
  },
  {
    "objectID": "chapters/14b-what-are-models.html#two-cultures-of-statistical-modeling",
    "href": "chapters/14b-what-are-models.html#two-cultures-of-statistical-modeling",
    "title": "15  What are Models?",
    "section": "15.2 Two Cultures of Statistical Modeling",
    "text": "15.2 Two Cultures of Statistical Modeling\nIn his influential 2001 paper, Leo Breiman identified two cultures in statistical modeling:\nThe Data Modeling Culture (traditional statistics):\n\nAssumes data are generated by a specific stochastic model\nFocus on estimating parameters and testing hypotheses\nEmphasis on interpretability and understanding mechanisms\nExamples: linear regression, ANOVA, generalized linear models\n\nThe Algorithmic Modeling Culture (machine learning):\n\nTreats the data-generating mechanism as unknown\nFocus on predictive accuracy\nEmphasis on performance over interpretability\nExamples: random forests, neural networks, boosting\n\nBoth approaches have value. Traditional models excel at inference and explanation; algorithmic approaches often produce better predictions. The choice depends on whether your goal is understanding or prediction.",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>What are Models?</span>"
    ]
  },
  {
    "objectID": "chapters/14b-what-are-models.html#the-general-linear-model-framework",
    "href": "chapters/14b-what-are-models.html#the-general-linear-model-framework",
    "title": "15  What are Models?",
    "section": "15.3 The General Linear Model Framework",
    "text": "15.3 The General Linear Model Framework\nMost statistical models you encounter are special cases of the General Linear Model (GLM):\n\\[Y = X\\beta + \\epsilon\\]\nwhere:\n\n\\(Y\\) is the response variable (what we want to predict/explain)\n\\(X\\) is the design matrix of predictor variables\n\\(\\beta\\) are coefficients we estimate\n\\(\\epsilon\\) is random error\n\nThis framework unifies many methods:\n\n\n\nMethod\nResponse Type\nPredictors\n\n\n\n\nOne-sample t-test\nContinuous\nNone (intercept only)\n\n\nTwo-sample t-test\nContinuous\nOne categorical (2 levels)\n\n\nANOVA\nContinuous\nOne or more categorical\n\n\nSimple regression\nContinuous\nOne continuous\n\n\nMultiple regression\nContinuous\nMultiple (any type)\n\n\nANCOVA\nContinuous\nMixed categorical and continuous\n\n\n\nThe beauty of this unified framework is that once you understand regression, you understand the entire family of linear models.",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>What are Models?</span>"
    ]
  },
  {
    "objectID": "chapters/14b-what-are-models.html#components-of-a-statistical-model",
    "href": "chapters/14b-what-are-models.html#components-of-a-statistical-model",
    "title": "15  What are Models?",
    "section": "15.4 Components of a Statistical Model",
    "text": "15.4 Components of a Statistical Model\nEvery statistical model specifies:\n\nResponse variable: What we want to predict or explain\nPredictor variables: Information we use to make predictions\nFunctional form: How predictors relate to the response (linear, polynomial, etc.)\nError structure: Assumptions about variability (normally distributed, etc.)\n\n\n\nCode\n# Visualizing a simple linear model\nset.seed(42)\nx &lt;- 1:50\ny &lt;- 2 + 0.5*x + rnorm(50, sd = 3)\n\nplot(x, y, pch = 19, col = \"steelblue\",\n     main = \"Components of a Linear Model\",\n     xlab = \"Predictor (X)\", ylab = \"Response (Y)\")\n\n# Fitted line (the model)\nmodel &lt;- lm(y ~ x)\nabline(model, col = \"red\", lwd = 2)\n\n# Show residuals for a few points\nsegments(x[c(10,25,40)], y[c(10,25,40)],\n         x[c(10,25,40)], fitted(model)[c(10,25,40)],\n         col = \"darkgreen\", lwd = 2)\ntext(x[25] + 3, (y[25] + fitted(model)[25])/2,\n     \"ε (residual)\", col = \"darkgreen\")\n\nlegend(\"topleft\",\n       c(\"Data points\", \"Model (E[Y|X])\", \"Residuals (ε)\"),\n       pch = c(19, NA, NA), lty = c(NA, 1, 1),\n       col = c(\"steelblue\", \"red\", \"darkgreen\"), lwd = 2)",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>What are Models?</span>"
    ]
  },
  {
    "objectID": "chapters/14b-what-are-models.html#model-fitting-finding-the-best-parameters",
    "href": "chapters/14b-what-are-models.html#model-fitting-finding-the-best-parameters",
    "title": "15  What are Models?",
    "section": "15.5 Model Fitting: Finding the Best Parameters",
    "text": "15.5 Model Fitting: Finding the Best Parameters\nModel fitting is the process of finding parameter values that make the model best explain the observed data.\n\n15.5.1 Least Squares\nFor linear models, least squares minimizes the sum of squared residuals:\n\\[\\text{minimize} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\\]\nThis produces the best linear unbiased estimators (BLUE) under certain conditions.\n\n\n15.5.2 Maximum Likelihood\nMaximum likelihood estimation (MLE) finds parameters that maximize the probability of observing the data:\n\\[\\hat{\\theta} = \\arg\\max_{\\theta} L(\\theta | \\text{data}) = \\arg\\max_{\\theta} \\prod_{i=1}^{n} f(y_i | \\theta)\\]\nFor normally distributed errors, least squares and MLE give identical results. MLE extends to non-normal distributions and complex models.\n\n\nCode\n# Visualize likelihood for estimating a mean\nset.seed(123)\ndata &lt;- rnorm(20, mean = 5, sd = 2)\n\n# Calculate log-likelihood for different values of mu\nmu_values &lt;- seq(3, 7, length.out = 100)\nlog_lik &lt;- sapply(mu_values, function(mu) {\n  sum(dnorm(data, mean = mu, sd = 2, log = TRUE))\n})\n\npar(mfrow = c(1, 2))\n\n# Data histogram\nhist(data, breaks = 10, main = \"Sample Data\", xlab = \"Value\",\n     col = \"lightblue\", border = \"white\")\nabline(v = mean(data), col = \"red\", lwd = 2)\n\n# Log-likelihood curve\nplot(mu_values, log_lik, type = \"l\", lwd = 2, col = \"blue\",\n     xlab = expression(mu), ylab = \"Log-likelihood\",\n     main = \"Maximum Likelihood Estimation\")\nabline(v = mu_values[which.max(log_lik)], col = \"red\", lwd = 2, lty = 2)\ntext(mu_values[which.max(log_lik)], min(log_lik) + 2,\n     paste(\"MLE =\", round(mu_values[which.max(log_lik)], 2)))",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>What are Models?</span>"
    ]
  },
  {
    "objectID": "chapters/14b-what-are-models.html#overfitting-when-models-learn-too-much",
    "href": "chapters/14b-what-are-models.html#overfitting-when-models-learn-too-much",
    "title": "15  What are Models?",
    "section": "15.6 Overfitting: When Models Learn Too Much",
    "text": "15.6 Overfitting: When Models Learn Too Much\nOverfitting occurs when a model captures noise rather than signal—it fits the training data extremely well but fails to generalize to new data.\n\n\nCode\n# Demonstrate overfitting with polynomial regression\nset.seed(42)\nn &lt;- 20\nx &lt;- seq(0, 1, length.out = n)\ny_true &lt;- sin(2*pi*x)\ny &lt;- y_true + rnorm(n, sd = 0.3)\n\npar(mfrow = c(1, 2))\n\n# Underfitting (too simple)\nplot(x, y, pch = 19, main = \"Underfitting (degree 1)\")\nabline(lm(y ~ x), col = \"red\", lwd = 2)\n\n# Overfitting (too complex)\nplot(x, y, pch = 19, main = \"Overfitting (degree 15)\")\nx_new &lt;- seq(0, 1, length.out = 100)\nlines(x_new, predict(lm(y ~ poly(x, 15)),\n                     newdata = data.frame(x = x_new)),\n      col = \"red\", lwd = 2)\n\n\n\n\n\n\n\n\n\nSigns of overfitting:\n\nModel fits training data nearly perfectly\nPredictions on new data are poor\nCoefficients are extremely large or unstable\nSmall changes in data produce very different models\n\n\n15.6.1 The Bias-Variance Tradeoff\nPrediction error has two sources:\nBias: Error from oversimplifying—missing important patterns\nVariance: Error from oversensitivity—fitting noise\n\\[\\text{Total Error} = \\text{Bias}^2 + \\text{Variance} + \\text{Irreducible Error}\\]\nSimple models have high bias but low variance. Complex models have low bias but high variance. The goal is to find the sweet spot.\n\n\nCode\n# Conceptual bias-variance plot\ncomplexity &lt;- 1:20\nbias_sq &lt;- 10 / complexity\nvariance &lt;- 0.5 * complexity\ntotal &lt;- bias_sq + variance + 2  # irreducible error = 2\n\nplot(complexity, total, type = \"l\", lwd = 2,\n     ylim = c(0, max(total) + 1),\n     xlab = \"Model Complexity\", ylab = \"Error\",\n     main = \"Bias-Variance Tradeoff\")\nlines(complexity, bias_sq, col = \"blue\", lwd = 2, lty = 2)\nlines(complexity, variance, col = \"red\", lwd = 2, lty = 2)\nabline(h = 2, col = \"gray\", lty = 3)\n\n# Optimal complexity\noptimal &lt;- which.min(total)\npoints(optimal, total[optimal], pch = 19, cex = 1.5, col = \"darkgreen\")\n\nlegend(\"topright\",\n       c(\"Total Error\", \"Bias²\", \"Variance\", \"Irreducible\"),\n       col = c(\"black\", \"blue\", \"red\", \"gray\"),\n       lty = c(1, 2, 2, 3), lwd = 2)",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>What are Models?</span>"
    ]
  },
  {
    "objectID": "chapters/14b-what-are-models.html#feature-engineering-and-transformations",
    "href": "chapters/14b-what-are-models.html#feature-engineering-and-transformations",
    "title": "15  What are Models?",
    "section": "15.7 Feature Engineering and Transformations",
    "text": "15.7 Feature Engineering and Transformations\nThe raw predictor variables may not capture the true relationship. Feature engineering creates new variables that better represent the underlying patterns.\nCommon transformations:\n\nPolynomial terms: \\(x^2\\), \\(x^3\\) for curved relationships\nLog transform: \\(\\log(x)\\) for multiplicative relationships\nInteractions: \\(x_1 \\times x_2\\) when effects depend on each other\nCategorical encoding: Converting categories to numbers\n\n\n\nCode\n# Example: log transformation\nset.seed(42)\nx &lt;- runif(50, 1, 100)\ny &lt;- 2 * log(x) + rnorm(50, sd = 0.5)\n\npar(mfrow = c(1, 2))\n\n# Raw scale - looks nonlinear\nplot(x, y, pch = 19, main = \"Original Scale\",\n     xlab = \"X\", ylab = \"Y\")\nabline(lm(y ~ x), col = \"red\", lwd = 2)\n\n# Log scale - linear\nplot(log(x), y, pch = 19, main = \"After Log Transform\",\n     xlab = \"log(X)\", ylab = \"Y\")\nabline(lm(y ~ log(x)), col = \"red\", lwd = 2)",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>What are Models?</span>"
    ]
  },
  {
    "objectID": "chapters/14b-what-are-models.html#model-selection",
    "href": "chapters/14b-what-are-models.html#model-selection",
    "title": "15  What are Models?",
    "section": "15.8 Model Selection",
    "text": "15.8 Model Selection\nWhen multiple models are possible, how do we choose? Several criteria exist:\nAdjusted R²: Penalizes for additional parameters \\[R^2_{adj} = 1 - \\frac{(1-R^2)(n-1)}{n-p-1}\\]\nAIC (Akaike Information Criterion): Balances fit and complexity \\[AIC = -2\\ln(L) + 2k\\]\nBIC (Bayesian Information Criterion): Heavier penalty for complexity \\[BIC = -2\\ln(L) + k\\ln(n)\\]\nLower AIC/BIC values indicate better models (balancing fit and parsimony).\n\n\nCode\n# Model selection example\ndata(mtcars)\n\n# Compare models of increasing complexity\nm1 &lt;- lm(mpg ~ wt, data = mtcars)\nm2 &lt;- lm(mpg ~ wt + hp, data = mtcars)\nm3 &lt;- lm(mpg ~ wt + hp + disp, data = mtcars)\nm4 &lt;- lm(mpg ~ wt + hp + disp + drat + qsec, data = mtcars)\n\n# Compare using AIC\nmodels &lt;- list(m1, m2, m3, m4)\ncomparison &lt;- data.frame(\n  Model = c(\"mpg ~ wt\", \"mpg ~ wt + hp\",\n            \"mpg ~ wt + hp + disp\",\n            \"mpg ~ wt + hp + disp + drat + qsec\"),\n  R_squared = sapply(models, function(m) summary(m)$r.squared),\n  Adj_R_squared = sapply(models, function(m) summary(m)$adj.r.squared),\n  AIC = sapply(models, AIC),\n  BIC = sapply(models, BIC)\n)\n\nknitr::kable(comparison, digits = 2)\n\n\n\n\n\n\n\n\n\n\n\n\nModel\nR_squared\nAdj_R_squared\nAIC\nBIC\n\n\n\n\nmpg ~ wt\n0.75\n0.74\n166.03\n170.43\n\n\nmpg ~ wt + hp\n0.83\n0.81\n156.65\n162.52\n\n\nmpg ~ wt + hp + disp\n0.83\n0.81\n158.64\n165.97\n\n\nmpg ~ wt + hp + disp + drat + qsec\n0.85\n0.82\n158.28\n168.54",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>What are Models?</span>"
    ]
  },
  {
    "objectID": "chapters/14b-what-are-models.html#cross-validation-for-model-assessment",
    "href": "chapters/14b-what-are-models.html#cross-validation-for-model-assessment",
    "title": "15  What are Models?",
    "section": "15.9 Cross-Validation for Model Assessment",
    "text": "15.9 Cross-Validation for Model Assessment\nThe gold standard for evaluating predictive performance is cross-validation: testing the model on data it hasn’t seen.\nK-fold cross-validation: 1. Split data into K parts 2. For each part: train on the other K-1 parts, test on the held-out part 3. Average performance across all K tests\n\n\nCode\nlibrary(boot)\n\n# Compare polynomial degrees using cross-validation\nset.seed(42)\nn &lt;- 100\nx &lt;- seq(0, 4*pi, length.out = n)\ny &lt;- sin(x) + rnorm(n, sd = 0.5)\ndata_cv &lt;- data.frame(x, y)\n\n# Calculate CV error for different polynomial degrees\ndegrees &lt;- 1:15\ncv_errors &lt;- sapply(degrees, function(d) {\n  model &lt;- glm(y ~ poly(x, d), data = data_cv)\n  cv.glm(data_cv, model, K = 10)$delta[1]\n})\n\nplot(degrees, cv_errors, type = \"b\", pch = 19,\n     xlab = \"Polynomial Degree\", ylab = \"CV Error\",\n     main = \"Cross-Validation for Model Selection\")\nabline(v = which.min(cv_errors), col = \"red\", lty = 2)",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>What are Models?</span>"
    ]
  },
  {
    "objectID": "chapters/14b-what-are-models.html#prediction-vs.-explanation",
    "href": "chapters/14b-what-are-models.html#prediction-vs.-explanation",
    "title": "15  What are Models?",
    "section": "15.10 Prediction vs. Explanation",
    "text": "15.10 Prediction vs. Explanation\nDifferent goals require different approaches:\nFor Explanation:\n\nSimpler models are often preferable\nFocus on coefficient interpretation\nStatistical significance matters\nUnderstand which variables drive the relationship\n\nFor Prediction:\n\nModel complexity can be higher if it helps\nFocus on out-of-sample performance\nAccuracy metrics matter most\nUnderstanding why is secondary\n\nIn biology and bioengineering, we often want both—models that predict well AND provide mechanistic insight. This tension shapes many modeling decisions.",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>What are Models?</span>"
    ]
  },
  {
    "objectID": "chapters/14b-what-are-models.html#practical-modeling-workflow",
    "href": "chapters/14b-what-are-models.html#practical-modeling-workflow",
    "title": "15  What are Models?",
    "section": "15.11 Practical Modeling Workflow",
    "text": "15.11 Practical Modeling Workflow\n\nDefine the question: What are you trying to learn or predict?\nExplore the data: Visualize relationships, check distributions, identify issues\nChoose candidate models: Based on data type, assumptions, and goals\nFit and evaluate: Use appropriate metrics and validation\nCheck assumptions: Residual analysis, diagnostic plots\nIterate: Refine based on diagnostics\nReport honestly: Including limitations and uncertainty",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>What are Models?</span>"
    ]
  },
  {
    "objectID": "chapters/14b-what-are-models.html#summary",
    "href": "chapters/14b-what-are-models.html#summary",
    "title": "15  What are Models?",
    "section": "15.12 Summary",
    "text": "15.12 Summary\n\nModels are simplified representations of reality that help us understand and predict\nThe general linear model framework unifies many common statistical methods\nModel fitting finds parameters that best explain the data (least squares, MLE)\nOverfitting occurs when models learn noise instead of signal\nThe bias-variance tradeoff governs model complexity choices\nFeature engineering can improve model performance\nCross-validation provides honest estimates of predictive performance\nDifferent goals (prediction vs. explanation) may favor different approaches",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>What are Models?</span>"
    ]
  },
  {
    "objectID": "chapters/14b-what-are-models.html#additional-resources",
    "href": "chapters/14b-what-are-models.html#additional-resources",
    "title": "15  What are Models?",
    "section": "15.13 Additional Resources",
    "text": "15.13 Additional Resources\n\nJames et al. (2023) - Comprehensive introduction to statistical learning concepts\nCrawley (2007) - Practical guide to statistical modeling in R\nBreiman, L. (2001). Statistical Modeling: The Two Cultures. Statistical Science, 16(3), 199-231.\n\n\n\n\n\n\n\nCrawley, Michael J. 2007. The r Book. John Wiley & Sons.\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2023. An Introduction to Statistical Learning with Applications in r. 2nd ed. Springer. https://www.statlearning.com.",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>What are Models?</span>"
    ]
  },
  {
    "objectID": "chapters/15-correlation.html",
    "href": "chapters/15-correlation.html",
    "title": "16  Correlation",
    "section": "",
    "text": "16.1 Measuring Association\nWhen two variables vary together, we say they are correlated. Understanding whether and how variables are related is fundamental to science—it helps us identify potential causal relationships, make predictions, and understand systems.\nCorrelation quantifies the strength and direction of the linear relationship between two variables. A positive correlation means that high values of one variable tend to occur with high values of the other. A negative correlation means that high values of one variable tend to occur with low values of the other.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Correlation</span>"
    ]
  },
  {
    "objectID": "chapters/15-correlation.html#covariance",
    "href": "chapters/15-correlation.html#covariance",
    "title": "16  Correlation",
    "section": "16.2 Covariance",
    "text": "16.2 Covariance\nThe covariance measures how two variables vary together. If X and Y tend to be above their means at the same time (and below their means at the same time), the covariance is positive. If one tends to be above its mean when the other is below, the covariance is negative.\n\\[Cov(X, Y) = \\frac{\\sum_{i=1}^{n}(x_i - \\bar{x})(y_i - \\bar{y})}{n-1}\\]\n\n16.2.1 Understanding Covariance Visually\nThe covariance formula involves products of deviations from the mean. Consider each point in a scatterplot:\n\nIf a point is in the upper-right quadrant (both X and Y above their means), the product of deviations is positive\nIf a point is in the lower-left quadrant (both below their means), the product is also positive\nIf a point is in the upper-left or lower-right (one above, one below), the product is negative\n\n\n\nCode\n# Visualizing covariance with quadrants\nset.seed(42)\nx &lt;- rnorm(50, mean = 10, sd = 2)\ny &lt;- 0.8 * x + rnorm(50, sd = 1.5)\n\n# Calculate means\nmean_x &lt;- mean(x)\nmean_y &lt;- mean(y)\n\n# Plot with quadrants\nplot(x, y, pch = 19, col = \"steelblue\",\n     xlab = \"X\", ylab = \"Y\",\n     main = \"Covariance: Products of Deviations from Means\")\nabline(v = mean_x, h = mean_y, col = \"red\", lty = 2, lwd = 2)\n\n# Add quadrant labels\ntext(max(x) - 0.5, max(y) - 0.5, \"(+)(+) = +\", col = \"darkgreen\", cex = 0.9)\ntext(min(x) + 0.5, min(y) + 0.5, \"(-)(−) = +\", col = \"darkgreen\", cex = 0.9)\ntext(max(x) - 0.5, min(y) + 0.5, \"(+)(−) = −\", col = \"darkred\", cex = 0.9)\ntext(min(x) + 0.5, max(y) - 0.5, \"(−)(+) = −\", col = \"darkred\", cex = 0.9)\n\n\n\n\n\n\n\n\n\nCode\n# Report covariance\ncat(\"Covariance:\", round(cov(x, y), 3), \"\\n\")\n\n\nCovariance: 3.479 \n\n\nWhen points cluster in the positive quadrants (upper-right and lower-left), the covariance is positive. When points cluster in the negative quadrants, the covariance is negative. When points are evenly distributed, covariance is near zero.\nThe problem with covariance is that its magnitude depends on the scales of X and Y, making it hard to interpret. Is a covariance of 100 strong or weak? It depends entirely on the units of measurement.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Correlation</span>"
    ]
  },
  {
    "objectID": "chapters/15-correlation.html#pearsons-correlation-coefficient",
    "href": "chapters/15-correlation.html#pearsons-correlation-coefficient",
    "title": "16  Correlation",
    "section": "16.3 Pearson’s Correlation Coefficient",
    "text": "16.3 Pearson’s Correlation Coefficient\nThe Pearson correlation coefficient standardizes covariance by dividing by the product of the standard deviations:\n\\[r = \\frac{Cov(X, Y)}{s_X s_Y} = \\frac{\\sum_{i=1}^{n}(x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum(x_i - \\bar{x})^2 \\sum(y_i - \\bar{y})^2}}\\]\nThis produces a value between -1 and +1:\n\n\\(r = 1\\): perfect positive linear relationship\n\\(r = -1\\): perfect negative linear relationship\n\n\\(r = 0\\): no linear relationship\n\n\n\n\n\n\n\n\nCode\n# Examples of different correlations\nset.seed(42)\nn &lt;- 100\n\npar(mfrow = c(2, 2))\n\n# Strong positive\nx1 &lt;- rnorm(n)\ny1 &lt;- 0.9 * x1 + rnorm(n, sd = 0.4)\nplot(x1, y1, main = paste(\"r =\", round(cor(x1, y1), 2)), pch = 19, col = \"blue\")\n\n# Moderate negative\ny2 &lt;- -0.6 * x1 + rnorm(n, sd = 0.8)\nplot(x1, y2, main = paste(\"r =\", round(cor(x1, y2), 2)), pch = 19, col = \"red\")\n\n# No correlation\ny3 &lt;- rnorm(n)\nplot(x1, y3, main = paste(\"r =\", round(cor(x1, y3), 2)), pch = 19, col = \"gray\")\n\n# Non-linear relationship (correlation misleading)\nx4 &lt;- runif(n, -3, 3)\ny4 &lt;- x4^2 + rnorm(n, sd = 0.5)\nplot(x4, y4, main = paste(\"r =\", round(cor(x4, y4), 2), \"(non-linear!)\"), \n     pch = 19, col = \"purple\")",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Correlation</span>"
    ]
  },
  {
    "objectID": "chapters/15-correlation.html#anscombes-quartet",
    "href": "chapters/15-correlation.html#anscombes-quartet",
    "title": "16  Correlation",
    "section": "16.4 Anscombe’s Quartet",
    "text": "16.4 Anscombe’s Quartet\nFrancis Anscombe created a famous set of four datasets that all have nearly identical statistical properties—same means, variances, correlations, and regression lines—yet look completely different when plotted. This demonstrates why visualization is essential.\n\n\n\n\n\nAlways plot your data before calculating correlations. The correlation coefficient captures only linear relationships and can be misleading for non-linear patterns.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Correlation</span>"
    ]
  },
  {
    "objectID": "chapters/15-correlation.html#testing-correlation",
    "href": "chapters/15-correlation.html#testing-correlation",
    "title": "16  Correlation",
    "section": "16.5 Testing Correlation",
    "text": "16.5 Testing Correlation\nThe cor.test() function tests whether a correlation is significantly different from zero:\n\n\nCode\n# Example: zebrafish length and weight\nset.seed(123)\nlength &lt;- rnorm(50, mean = 2.5, sd = 0.5)\nweight &lt;- 10 * length^2 + rnorm(50, sd = 5)\n\ncor.test(length, weight)\n\n\n\n    Pearson's product-moment correlation\n\ndata:  length and weight\nt = 29.857, df = 48, p-value &lt; 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.9546054 0.9853063\nsample estimates:\n     cor \n0.974118 \n\n\nThe null hypothesis is that the population correlation is zero (\\(H_0: \\rho = 0\\)). A small p-value indicates evidence of a non-zero correlation.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Correlation</span>"
    ]
  },
  {
    "objectID": "chapters/15-correlation.html#sample-correlation-as-a-random-variable",
    "href": "chapters/15-correlation.html#sample-correlation-as-a-random-variable",
    "title": "16  Correlation",
    "section": "16.6 Sample Correlation as a Random Variable",
    "text": "16.6 Sample Correlation as a Random Variable\nJust like the sample mean, the sample correlation coefficient is a random variable—it varies from sample to sample. If we could repeatedly draw samples from the same population and compute r for each, we would get a distribution of r values centered around the true population correlation \\(\\rho\\).\nThis sampling variability has important implications for interpretation. A sample correlation of r = 0.3 from a small study might arise even when the true correlation is zero (or is actually 0.5). Understanding this uncertainty is essential for proper inference.\n\n\nCode\n# Demonstrate sampling variability of correlation\nset.seed(42)\n\n# True population parameters\nrho_true &lt;- 0.5  # True population correlation\nn_small &lt;- 20\nn_large &lt;- 100\nn_reps &lt;- 1000\n\n# Function to generate correlated data\ngenerate_correlated_data &lt;- function(n, rho) {\n  x &lt;- rnorm(n)\n  y &lt;- rho * x + sqrt(1 - rho^2) * rnorm(n)\n  return(cor(x, y))\n}\n\n# Generate sampling distributions for different sample sizes\nr_small &lt;- replicate(n_reps, generate_correlated_data(n_small, rho_true))\nr_large &lt;- replicate(n_reps, generate_correlated_data(n_large, rho_true))\n\n# Plot sampling distributions\npar(mfrow = c(1, 2))\n\nhist(r_small, breaks = 30, col = \"lightblue\",\n     main = paste(\"Sampling Distribution of r\\n(n =\", n_small, \")\"),\n     xlab = \"Sample Correlation\", xlim = c(-0.2, 1))\nabline(v = rho_true, col = \"red\", lwd = 2, lty = 2)\nabline(v = mean(r_small), col = \"blue\", lwd = 2)\nlegend(\"topleft\", legend = c(paste(\"True ρ =\", rho_true),\n                              paste(\"Mean r =\", round(mean(r_small), 3))),\n       col = c(\"red\", \"blue\"), lty = c(2, 1), lwd = 2, cex = 0.8)\n\nhist(r_large, breaks = 30, col = \"lightgreen\",\n     main = paste(\"Sampling Distribution of r\\n(n =\", n_large, \")\"),\n     xlab = \"Sample Correlation\", xlim = c(-0.2, 1))\nabline(v = rho_true, col = \"red\", lwd = 2, lty = 2)\nabline(v = mean(r_large), col = \"blue\", lwd = 2)\nlegend(\"topleft\", legend = c(paste(\"True ρ =\", rho_true),\n                              paste(\"Mean r =\", round(mean(r_large), 3))),\n       col = c(\"red\", \"blue\"), lty = c(2, 1), lwd = 2, cex = 0.8)\n\n\n\n\n\n\n\n\n\nNotice the dramatic difference in variability. With n = 20, sample correlations range widely around the true value—sometimes even appearing negative when the true correlation is 0.5! With n = 100, the distribution is much tighter, and our sample r is a more reliable estimate of \\(\\rho\\).\n\n\nCode\n# Quantify the variability\ncat(\"True population correlation: ρ =\", rho_true, \"\\n\\n\")\n\n\nTrue population correlation: ρ = 0.5 \n\n\nCode\ncat(\"With n =\", n_small, \":\\n\")\n\n\nWith n = 20 :\n\n\nCode\ncat(\"  Mean of sample r:\", round(mean(r_small), 3), \"\\n\")\n\n\n  Mean of sample r: 0.483 \n\n\nCode\ncat(\"  SD of sample r:\", round(sd(r_small), 3), \"\\n\")\n\n\n  SD of sample r: 0.181 \n\n\nCode\ncat(\"  95% of samples give r between\", round(quantile(r_small, 0.025), 3),\n    \"and\", round(quantile(r_small, 0.975), 3), \"\\n\\n\")\n\n\n  95% of samples give r between 0.056 and 0.781 \n\n\nCode\ncat(\"With n =\", n_large, \":\\n\")\n\n\nWith n = 100 :\n\n\nCode\ncat(\"  Mean of sample r:\", round(mean(r_large), 3), \"\\n\")\n\n\n  Mean of sample r: 0.497 \n\n\nCode\ncat(\"  SD of sample r:\", round(sd(r_large), 3), \"\\n\")\n\n\n  SD of sample r: 0.075 \n\n\nCode\ncat(\"  95% of samples give r between\", round(quantile(r_large, 0.025), 3),\n    \"and\", round(quantile(r_large, 0.975), 3), \"\\n\")\n\n\n  95% of samples give r between 0.347 and 0.634 \n\n\n\n\n\n\n\n\nPractical Implications\n\n\n\n\nSmall samples yield unreliable correlations: With n &lt; 30, sample r can differ substantially from \\(\\rho\\)\nConfidence intervals are essential: Report CIs to communicate uncertainty, not just the point estimate\nReplication matters: A single study with r = 0.4 (n = 25) is consistent with true correlations anywhere from near-zero to quite strong\nPublication bias distorts the literature: Studies with “significant” correlations are more likely to be published, inflating effect sizes in the literature\n\n\n\n\n16.6.1 Fisher’s Z-Transformation\nThe sampling distribution of r is not symmetric, especially when \\(\\rho\\) is far from zero. Fisher’s z-transformation stabilizes the variance and makes the distribution approximately normal:\n\\[z = \\frac{1}{2} \\ln\\left(\\frac{1 + r}{1 - r}\\right) = \\text{arctanh}(r)\\]\nThe standard error of z is approximately \\(\\frac{1}{\\sqrt{n-3}}\\), which depends only on sample size—not on the true correlation. This transformation is used to construct confidence intervals for correlation and to compare correlations across groups.\n\n\nCode\n# Fisher's z-transformation for confidence interval\nr_observed &lt;- 0.6\nn &lt;- 50\n\n# Transform to z\nz &lt;- atanh(r_observed)  # Same as 0.5 * log((1 + r) / (1 - r))\nse_z &lt;- 1 / sqrt(n - 3)\n\n# 95% CI in z scale\nz_lower &lt;- z - 1.96 * se_z\nz_upper &lt;- z + 1.96 * se_z\n\n# Transform back to r scale\nr_lower &lt;- tanh(z_lower)\nr_upper &lt;- tanh(z_upper)\n\ncat(\"Observed r:\", r_observed, \"with n =\", n, \"\\n\")\n\n\nObserved r: 0.6 with n = 50 \n\n\nCode\ncat(\"95% CI for ρ: [\", round(r_lower, 3), \",\", round(r_upper, 3), \"]\\n\")\n\n\n95% CI for ρ: [ 0.386 , 0.753 ]",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Correlation</span>"
    ]
  },
  {
    "objectID": "chapters/15-correlation.html#parametric-assumptions",
    "href": "chapters/15-correlation.html#parametric-assumptions",
    "title": "16  Correlation",
    "section": "16.7 Parametric Assumptions",
    "text": "16.7 Parametric Assumptions\nPearson’s correlation assumes that both variables are normally distributed (or at least that the relationship is linear and homoscedastic). When these assumptions are violated, nonparametric alternatives may be more appropriate.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Correlation</span>"
    ]
  },
  {
    "objectID": "chapters/15-correlation.html#nonparametric-correlation",
    "href": "chapters/15-correlation.html#nonparametric-correlation",
    "title": "16  Correlation",
    "section": "16.8 Nonparametric Correlation",
    "text": "16.8 Nonparametric Correlation\nSpearman’s rank correlation replaces values with their ranks before calculating correlation. It measures monotonic (consistently increasing or decreasing) rather than strictly linear relationships and is robust to outliers.\nKendall’s tau is another rank-based measure that counts concordant and discordant pairs. It is particularly appropriate for small samples or data with many ties.\n\n\nCode\n# Compare methods on non-normal data\nset.seed(42)\nx &lt;- rexp(30, rate = 0.1)\ny &lt;- x + rexp(30, rate = 0.2)\n\ncat(\"Pearson:\", round(cor(x, y, method = \"pearson\"), 3), \"\\n\")\n\n\nPearson: 0.764 \n\n\nCode\ncat(\"Spearman:\", round(cor(x, y, method = \"spearman\"), 3), \"\\n\")\n\n\nSpearman: 0.808 \n\n\nCode\ncat(\"Kendall:\", round(cor(x, y, method = \"kendall\"), 3), \"\\n\")\n\n\nKendall: 0.623",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Correlation</span>"
    ]
  },
  {
    "objectID": "chapters/15-correlation.html#correlation-is-not-causation",
    "href": "chapters/15-correlation.html#correlation-is-not-causation",
    "title": "16  Correlation",
    "section": "16.9 Correlation Is Not Causation",
    "text": "16.9 Correlation Is Not Causation\nA correlation between X and Y might arise because X causes Y, because Y causes X, because a third variable Z causes both, or simply by chance. Correlation alone cannot distinguish these possibilities.\nTo establish causation, you need experimental manipulation (changing X and observing Y), temporal precedence (X occurs before Y), and ruling out confounding variables. Observational correlations are valuable for generating hypotheses but insufficient for establishing causal relationships.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Correlation</span>"
    ]
  },
  {
    "objectID": "chapters/15-correlation.html#interpreting-correlation-magnitude",
    "href": "chapters/15-correlation.html#interpreting-correlation-magnitude",
    "title": "16  Correlation",
    "section": "16.10 Interpreting Correlation Magnitude",
    "text": "16.10 Interpreting Correlation Magnitude\nWhile there are no universal standards, these guidelines provide rough interpretation:\n\n\n\n\nr\n\n\n\n\n0.00 - 0.19\nNegligible\n\n\n0.20 - 0.39\nWeak\n\n\n0.40 - 0.59\nModerate\n\n\n0.60 - 0.79\nStrong\n\n\n0.80 - 1.00\nVery strong\n\n\n\nContext matters greatly. In physics, correlations below 0.99 might be disappointing. In psychology or ecology, correlations of 0.3 can be considered meaningful.\n\n16.10.1 Coefficient of Determination\nThe square of the correlation coefficient, \\(r^2\\), is called the coefficient of determination. It represents the proportion of variance in one variable that is explained by its linear relationship with the other.\nIf \\(r = 0.7\\), then \\(r^2 = 0.49\\), meaning about 49% of the variance in Y is explained by its relationship with X. This leaves 51% unexplained—due to other factors, measurement error, or the relationship not being perfectly linear.\n\n\nCode\n# Visualize explained vs unexplained variance\nset.seed(123)\nx &lt;- 1:50\ny &lt;- 2 * x + rnorm(50, sd = 15)\n\nr &lt;- cor(x, y)\nr_squared &lt;- r^2\n\ncat(\"Correlation (r):\", round(r, 3), \"\\n\")\n\n\nCorrelation (r): 0.901 \n\n\nCode\ncat(\"R-squared (r²):\", round(r_squared, 3), \"\\n\")\n\n\nR-squared (r²): 0.812 \n\n\nCode\ncat(\"Variance explained:\", round(r_squared * 100, 1), \"%\\n\")\n\n\nVariance explained: 81.2 %",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Correlation</span>"
    ]
  },
  {
    "objectID": "chapters/15-correlation.html#common-pitfalls-with-correlation",
    "href": "chapters/15-correlation.html#common-pitfalls-with-correlation",
    "title": "16  Correlation",
    "section": "16.11 Common Pitfalls with Correlation",
    "text": "16.11 Common Pitfalls with Correlation\n\n\n\n\n\n\nWatch Out For These Mistakes\n\n\n\n\nRestricted range: If you only sample part of the range of X or Y, correlation will appear weaker than it truly is\nOutliers: A single extreme point can dramatically inflate or deflate the correlation\nNon-linearity: Correlation only measures linear relationships; a perfect curved relationship can have r ≈ 0\nAggregation effects: Correlations computed on grouped data (e.g., country averages) are often much stronger than correlations on individual data (ecological fallacy)\nConfounding: A third variable may create a spurious correlation between X and Y\n\n\n\n\n\nCode\n# Demonstrating the effect of outliers\nset.seed(42)\nx_base &lt;- rnorm(30)\ny_base &lt;- rnorm(30)\n\npar(mfrow = c(1, 2))\n\n# Without outlier\nplot(x_base, y_base, pch = 19, col = \"blue\",\n     main = paste(\"Without outlier: r =\", round(cor(x_base, y_base), 3)),\n     xlab = \"X\", ylab = \"Y\", xlim = c(-3, 5), ylim = c(-3, 5))\n\n# With outlier\nx_out &lt;- c(x_base, 4)\ny_out &lt;- c(y_base, 4)\nplot(x_out, y_out, pch = 19, col = c(rep(\"blue\", 30), \"red\"),\n     main = paste(\"With outlier: r =\", round(cor(x_out, y_out), 3)),\n     xlab = \"X\", ylab = \"Y\", xlim = c(-3, 5), ylim = c(-3, 5))\n\n\n\n\n\n\n\n\n\nA single outlier has shifted the correlation from near zero to moderately positive. Always visualize your data and consider robust alternatives like Spearman’s correlation when outliers are present.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Correlation</span>"
    ]
  },
  {
    "objectID": "chapters/15-correlation.html#summary",
    "href": "chapters/15-correlation.html#summary",
    "title": "16  Correlation",
    "section": "16.12 Summary",
    "text": "16.12 Summary\nCorrelation quantifies the strength and direction of linear relationships between variables:\n\nCovariance measures how variables move together, but depends on units\nPearson’s r standardizes covariance to range from -1 to +1\nSpearman and Kendall provide robust rank-based alternatives\nCorrelation does not imply causation\nAlways visualize data and check for non-linearity and outliers",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Correlation</span>"
    ]
  },
  {
    "objectID": "chapters/15-correlation.html#practice-exercises",
    "href": "chapters/15-correlation.html#practice-exercises",
    "title": "16  Correlation",
    "section": "16.13 Practice Exercises",
    "text": "16.13 Practice Exercises\nFor hands-on practice with correlation and linear models, see Section 35.10 in the Practice Exercises appendix. The exercises include:\n\nCalculating and interpreting correlation coefficients\nCreating scatterplots with trend lines\nUnderstanding the effect of outliers on correlation\nSimple linear regression analysis",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Correlation</span>"
    ]
  },
  {
    "objectID": "chapters/15-correlation.html#additional-resources",
    "href": "chapters/15-correlation.html#additional-resources",
    "title": "16  Correlation",
    "section": "16.14 Additional Resources",
    "text": "16.14 Additional Resources\n\nLogan (2010) - Detailed coverage of correlation with biological examples\nJames et al. (2023) - Excellent discussion of correlation in the context of statistical learning\n\n\n\n\n\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2023. An Introduction to Statistical Learning with Applications in r. 2nd ed. Springer. https://www.statlearning.com.\n\n\nLogan, Murray. 2010. Biostatistical Design and Analysis Using r. Wiley-Blackwell.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Correlation</span>"
    ]
  },
  {
    "objectID": "chapters/16-simple-linear-regression.html",
    "href": "chapters/16-simple-linear-regression.html",
    "title": "17  Simple Linear Regression",
    "section": "",
    "text": "17.1 From Correlation to Prediction\nCorrelation tells us that two variables are related, but it does not allow us to predict one from the other or to describe the nature of that relationship. Linear regression goes further—it models the relationship between variables, allowing us to make predictions and to quantify how changes in one variable are associated with changes in another.\nIn simple linear regression, we model a response variable Y as a linear function of a predictor variable X:\n\\[y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i\\]\nHere \\(\\beta_0\\) is the intercept (the expected value of Y when X equals zero), \\(\\beta_1\\) is the slope (how much Y changes for a one-unit change in X), and \\(\\epsilon_i\\) represents the random error—the part of Y not explained by X.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "chapters/16-simple-linear-regression.html#origins-of-the-term-regression",
    "href": "chapters/16-simple-linear-regression.html#origins-of-the-term-regression",
    "title": "17  Simple Linear Regression",
    "section": "17.2 Origins of the Term “Regression”",
    "text": "17.2 Origins of the Term “Regression”\nThe term “regression” comes from Francis Galton’s studies of heredity in the 1880s. He observed that tall parents tended to have children who were tall, but not as extremely tall as the parents—children’s heights “regressed” toward the population mean. This phenomenon, now called regression to the mean, is a statistical artifact that appears whenever two variables are imperfectly correlated.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "chapters/16-simple-linear-regression.html#the-regression-fallacy",
    "href": "chapters/16-simple-linear-regression.html#the-regression-fallacy",
    "title": "17  Simple Linear Regression",
    "section": "17.3 The Regression Fallacy",
    "text": "17.3 The Regression Fallacy\nUnderstanding regression to the mean is crucial because ignoring it leads to a common error in reasoning called the regression fallacy. This occurs when we attribute regression to the mean to some other cause—typically claiming credit for improvement that was simply statistical regression.\nThe most famous example is the “Sophomore Slump” in baseball. The player who wins Rookie of the Year typically performs worse in their second season. Sportswriters often blame this on pressure, complacency, or teams “figuring out” the player. But much of this decline is simply regression to the mean.\nConsider why: to win Rookie of the Year, a player typically needs exceptional performance—often their personal best. This outstanding season likely involved skill plus some good luck (favorable conditions, timely hits, etc.). In the following season, luck averages out, and performance regresses toward the player’s true ability level.\n\n\nCode\n# Simulating the Sophomore Slump\nset.seed(42)\nn_players &lt;- 500\n\n# True talent level for each player (varies between players)\ntrue_talent &lt;- rnorm(n_players, mean = 0.265, sd = 0.020)\n\n# Season 1: observed performance = true talent + luck\nluck_season1 &lt;- rnorm(n_players, mean = 0, sd = 0.025)\nbatting_avg_yr1 &lt;- true_talent + luck_season1\n\n# Season 2: observed performance = same true talent + different luck\nluck_season2 &lt;- rnorm(n_players, mean = 0, sd = 0.025)\nbatting_avg_yr2 &lt;- true_talent + luck_season2\n\n# Find the \"Rookie of the Year\" - best performer in year 1\nroy_idx &lt;- which.max(batting_avg_yr1)\n\n# Look at top 10 performers in year 1\ntop_10 &lt;- order(batting_avg_yr1, decreasing = TRUE)[1:10]\n\ncat(\"Top 10 performers in Year 1 vs Year 2:\\n\")\n\n\nTop 10 performers in Year 1 vs Year 2:\n\n\nCode\ncat(\"========================================\\n\")\n\n\n========================================\n\n\nCode\nfor (i in 1:10) {\n  idx &lt;- top_10[i]\n  change &lt;- batting_avg_yr2[idx] - batting_avg_yr1[idx]\n  cat(sprintf(\"Player %d: Yr1 = %.3f, Yr2 = %.3f, Change = %+.3f\\n\",\n              i, batting_avg_yr1[idx], batting_avg_yr2[idx], change))\n}\n\n\nPlayer 1: Yr1 = 0.384, Yr2 = 0.291, Change = -0.093\nPlayer 2: Yr1 = 0.366, Yr2 = 0.272, Change = -0.094\nPlayer 3: Yr1 = 0.357, Yr2 = 0.298, Change = -0.059\nPlayer 4: Yr1 = 0.352, Yr2 = 0.240, Change = -0.112\nPlayer 5: Yr1 = 0.349, Yr2 = 0.328, Change = -0.021\nPlayer 6: Yr1 = 0.343, Yr2 = 0.301, Change = -0.042\nPlayer 7: Yr1 = 0.337, Yr2 = 0.299, Change = -0.038\nPlayer 8: Yr1 = 0.334, Yr2 = 0.325, Change = -0.009\nPlayer 9: Yr1 = 0.329, Yr2 = 0.242, Change = -0.087\nPlayer 10: Yr1 = 0.329, Yr2 = 0.283, Change = -0.046\n\n\nCode\n# How many of top 10 declined?\ndeclines &lt;- sum(batting_avg_yr2[top_10] &lt; batting_avg_yr1[top_10])\ncat(sprintf(\"\\n%d of top 10 performers showed a decline (the 'slump')\\n\", declines))\n\n\n\n10 of top 10 performers showed a decline (the 'slump')\n\n\nNotice that nearly all of the top performers declined—not because of anything about being a sophomore, but because extreme initial performance tends to be followed by more average performance.\n\n\nCode\n# Visualize regression to the mean\nplot(batting_avg_yr1, batting_avg_yr2,\n     pch = 19, col = rgb(0, 0, 0, 0.3),\n     xlab = \"Year 1 Batting Average\",\n     ylab = \"Year 2 Batting Average\",\n     main = \"Regression to the Mean: The Sophomore Slump\",\n     xlim = c(0.20, 0.35), ylim = c(0.20, 0.35))\n\n# Add y = x line (what we'd see with no regression)\nabline(a = 0, b = 1, col = \"gray\", lty = 2, lwd = 2)\n\n# Add regression line\nreg_line &lt;- lm(batting_avg_yr2 ~ batting_avg_yr1)\nabline(reg_line, col = \"red\", lwd = 2)\n\n# Highlight top performers\npoints(batting_avg_yr1[top_10], batting_avg_yr2[top_10],\n       pch = 19, col = \"blue\", cex = 1.5)\n\n# Mark the \"Rookie of the Year\"\npoints(batting_avg_yr1[roy_idx], batting_avg_yr2[roy_idx],\n       pch = 17, col = \"red\", cex = 2)\n\nlegend(\"topleft\",\n       legend = c(\"All players\", \"Top 10 Year 1\", \"Best performer\",\n                  \"No regression (y=x)\", \"Regression line\"),\n       pch = c(19, 19, 17, NA, NA),\n       lty = c(NA, NA, NA, 2, 1),\n       col = c(rgb(0,0,0,0.3), \"blue\", \"red\", \"gray\", \"red\"),\n       lwd = c(NA, NA, NA, 2, 2))\n\n\n\n\n\n\n\n\n\nThe dashed line shows what we would see if Year 1 perfectly predicted Year 2 (no regression to the mean). The red regression line shows reality—it’s flatter, meaning extreme Year 1 performers tend to move toward the center in Year 2.\n\n\n\n\n\n\nAvoiding the Regression Fallacy\n\n\n\nThe regression fallacy appears in many contexts:\n\nMedical treatments: Patients seek treatment when symptoms are worst; subsequent improvement may be regression, not treatment effect\nPerformance management: Workers reprimanded for poor performance often improve; those praised for good performance often decline—both may be regression\nEducational interventions: Students identified as struggling (tested at their worst) often improve regardless of intervention\n\nWhen evaluating any intervention applied to extreme cases, always consider whether observed changes might simply be regression to the mean.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "chapters/16-simple-linear-regression.html#fitting-the-model-ordinary-least-squares",
    "href": "chapters/16-simple-linear-regression.html#fitting-the-model-ordinary-least-squares",
    "title": "17  Simple Linear Regression",
    "section": "17.4 Fitting the Model: Ordinary Least Squares",
    "text": "17.4 Fitting the Model: Ordinary Least Squares\nThe most common method for fitting a regression line is ordinary least squares (OLS). OLS finds the line that minimizes the sum of squared residuals—the squared vertical distances between observed points and the fitted line.\n\n\n\n\n\nThe OLS estimates for the slope and intercept are:\n\\[\\hat{\\beta}_1 = \\frac{\\sum(x_i - \\bar{x})(y_i - \\bar{y})}{\\sum(x_i - \\bar{x})^2} = r \\frac{s_y}{s_x}\\]\n\\[\\hat{\\beta}_0 = \\bar{y} - \\hat{\\beta}_1 \\bar{x}\\]\nNotice that the slope equals the correlation coefficient times the ratio of standard deviations. This makes clear the connection between correlation and regression.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "chapters/16-simple-linear-regression.html#linear-regression-in-r",
    "href": "chapters/16-simple-linear-regression.html#linear-regression-in-r",
    "title": "17  Simple Linear Regression",
    "section": "17.5 Linear Regression in R",
    "text": "17.5 Linear Regression in R\n\n\nCode\n# Example: zebrafish size data\nset.seed(42)\nn &lt;- 100\nlength_cm &lt;- runif(n, 0.5, 3.5)\nweight_mg &lt;- 15 * length_cm^2 + rnorm(n, sd = 10)\n\n# Fit the model\nfish_lm &lt;- lm(weight_mg ~ length_cm)\nsummary(fish_lm)\n\n\n\nCall:\nlm(formula = weight_mg ~ length_cm)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-39.277  -9.033  -0.432   9.998  29.934 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  -51.429      3.579  -14.37   &lt;2e-16 ***\nlength_cm     61.660      1.583   38.95   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 14.27 on 98 degrees of freedom\nMultiple R-squared:  0.9393,    Adjusted R-squared:  0.9387 \nF-statistic:  1517 on 1 and 98 DF,  p-value: &lt; 2.2e-16\n\n\nThe output shows the estimated coefficients with their standard errors, t-statistics, and p-values. The Multiple R-squared indicates how much of the variance in Y is explained by X.\n\n\nCode\n# Visualize the fit\nplot(length_cm, weight_mg, pch = 19, col = \"blue\",\n     xlab = \"Length (cm)\", ylab = \"Weight (mg)\",\n     main = \"Linear Regression: Weight vs Length\")\nabline(fish_lm, col = \"red\", lwd = 2)",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "chapters/16-simple-linear-regression.html#interpretation-of-coefficients",
    "href": "chapters/16-simple-linear-regression.html#interpretation-of-coefficients",
    "title": "17  Simple Linear Regression",
    "section": "17.6 Interpretation of Coefficients",
    "text": "17.6 Interpretation of Coefficients\nThe intercept \\(\\hat{\\beta}_0\\) is the predicted value of Y when X equals zero. This may or may not be meaningful depending on whether X = 0 makes sense in your context.\nThe slope \\(\\hat{\\beta}_1\\) is the predicted change in Y for a one-unit increase in X. If the slope is 15, then each additional unit of X is associated with 15 more units of Y on average.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "chapters/16-simple-linear-regression.html#hypothesis-testing-in-regression",
    "href": "chapters/16-simple-linear-regression.html#hypothesis-testing-in-regression",
    "title": "17  Simple Linear Regression",
    "section": "17.7 Hypothesis Testing in Regression",
    "text": "17.7 Hypothesis Testing in Regression\nThe hypothesis test for the slope asks whether there is evidence of a relationship between X and Y:\n\\[H_0: \\beta_1 = 0 \\quad \\text{(no relationship)}\\] \\[H_A: \\beta_1 \\neq 0 \\quad \\text{(relationship exists)}\\]\nThe test uses a t-statistic, comparing the estimated slope to its standard error. The p-value indicates the probability of observing a slope this far from zero if the true slope were zero.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "chapters/16-simple-linear-regression.html#r-squared-measuring-model-fit",
    "href": "chapters/16-simple-linear-regression.html#r-squared-measuring-model-fit",
    "title": "17  Simple Linear Regression",
    "section": "17.8 R-Squared: Measuring Model Fit",
    "text": "17.8 R-Squared: Measuring Model Fit\nR-squared (\\(R^2\\)) measures the proportion of variance in Y explained by the model:\n\\[R^2 = 1 - \\frac{SS_{error}}{SS_{total}} = \\frac{SS_{model}}{SS_{total}}\\]\nIn simple linear regression, \\(R^2\\) equals the square of the correlation coefficient. An \\(R^2\\) of 0.7 means the model explains 70% of the variance in Y; the remaining 30% is unexplained.\nBe cautious with \\(R^2\\)—it always increases when you add predictors, even useless ones. Adjusted \\(R^2\\) penalizes for model complexity.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "chapters/16-simple-linear-regression.html#making-predictions",
    "href": "chapters/16-simple-linear-regression.html#making-predictions",
    "title": "17  Simple Linear Regression",
    "section": "17.9 Making Predictions",
    "text": "17.9 Making Predictions\nOnce you have a fitted model, you can predict Y for new values of X:\n\n\nCode\n# Predict weight for new lengths\nnew_lengths &lt;- data.frame(length_cm = c(1.0, 2.0, 3.0))\npredict(fish_lm, newdata = new_lengths, interval = \"confidence\")\n\n\n        fit        lwr       upr\n1  10.23108   5.828292  14.63387\n2  71.89135  69.050838  74.73186\n3 133.55162 129.491293 137.61194\n\n\nThe confidence interval indicates uncertainty about the mean Y at each X value. A prediction interval (using interval = \"prediction\") would be wider, accounting for individual variability around that mean.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "chapters/16-simple-linear-regression.html#model-assumptions",
    "href": "chapters/16-simple-linear-regression.html#model-assumptions",
    "title": "17  Simple Linear Regression",
    "section": "17.10 Model Assumptions",
    "text": "17.10 Model Assumptions\nLinear regression assumptions include:\n\nLinearity: The relationship between X and Y is linear\nIndependence: Observations are independent of each other\nNormality: Residuals are normally distributed\nHomoscedasticity: Residuals have constant variance across X\n\nThese assumptions should be checked through residual analysis.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "chapters/16-simple-linear-regression.html#residual-analysis",
    "href": "chapters/16-simple-linear-regression.html#residual-analysis",
    "title": "17  Simple Linear Regression",
    "section": "17.11 Residual Analysis",
    "text": "17.11 Residual Analysis\nResiduals are the differences between observed and fitted values: \\(e_i = y_i - \\hat{y}_i\\). Examining residuals reveals whether model assumptions are satisfied.\n\n\nCode\n# Residual diagnostic plots\npar(mfrow = c(2, 2))\nplot(fish_lm)\n\n\n\n\n\n\n\n\n\nKey diagnostic plots:\n\nResiduals vs Fitted: Should show random scatter around zero. Patterns suggest non-linearity or heteroscedasticity.\nQ-Q Plot: Residuals should fall along the diagonal line if normally distributed. Deviations at the tails indicate non-normality.\nScale-Location: Should show constant spread. A funnel shape indicates heteroscedasticity.\nResiduals vs Leverage: Identifies influential points. Points with high leverage and large residuals may unduly influence the fit.\n\n\n\nCode\n# Example of problematic residuals\nset.seed(123)\nx_prob &lt;- seq(1, 10, length.out = 50)\ny_prob &lt;- x_prob^2 + rnorm(50, sd = 5)  # Quadratic relationship\n\nlm_prob &lt;- lm(y_prob ~ x_prob)\n\npar(mfrow = c(1, 2))\nplot(x_prob, y_prob, pch = 19, main = \"Data with Non-linear Pattern\")\nabline(lm_prob, col = \"red\", lwd = 2)\n\nplot(fitted(lm_prob), residuals(lm_prob), pch = 19,\n     xlab = \"Fitted values\", ylab = \"Residuals\",\n     main = \"Residuals Show Clear Pattern\")\nabline(h = 0, col = \"red\", lty = 2)\n\n\n\n\n\n\n\n\n\nThe curved pattern in the residuals reveals that a linear model is inappropriate—the true relationship is non-linear.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "chapters/16-simple-linear-regression.html#model-i-vs-model-ii-regression",
    "href": "chapters/16-simple-linear-regression.html#model-i-vs-model-ii-regression",
    "title": "17  Simple Linear Regression",
    "section": "17.12 Model I vs Model II Regression",
    "text": "17.12 Model I vs Model II Regression\nStandard OLS regression (Model I) assumes X is measured without error and minimizes vertical distances to the line. This is appropriate when:\n\nX is fixed by the experimenter (controlled variable)\nX is measured with negligible error compared to Y\nThe goal is prediction of Y from X\n\nWhen both variables are measured with error (common in observational studies), Model II regression may be more appropriate. Model II methods include:\n\nMajor Axis (MA) regression: Minimizes perpendicular distances to the line\nReduced Major Axis (RMA): Often preferred when both variables have similar measurement error\n\n\n\nCode\n# Model I slope estimate\nslope_model1 &lt;- coef(fish_lm)[2]\n\n# Reduced Major Axis slope estimate (ratio of standard deviations)\nslope_rma &lt;- sd(weight_mg) / sd(length_cm) * sign(cor(length_cm, weight_mg))\n\ncat(\"Model I (OLS) slope:\", round(slope_model1, 3), \"\\n\")\n\n\nModel I (OLS) slope: 61.66 \n\n\nCode\ncat(\"Model II (RMA) slope:\", round(slope_rma, 3), \"\\n\")\n\n\nModel II (RMA) slope: 63.62 \n\n\n\n\n\n\n\n\nWhen to Use Model II Regression\n\n\n\nUse Model II regression when: - Both X and Y are random variables measured with error - You want to describe the relationship rather than predict Y from X - You need to compare slopes across groups (e.g., allometric scaling)\nThe lmodel2 package in R provides Model II regression methods.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "chapters/16-simple-linear-regression.html#extrapolation-warning",
    "href": "chapters/16-simple-linear-regression.html#extrapolation-warning",
    "title": "17  Simple Linear Regression",
    "section": "17.13 Extrapolation Warning",
    "text": "17.13 Extrapolation Warning\nRegression models should only be used to make predictions within the range of observed X values. Extrapolation—predicting beyond this range—is risky because the linear relationship may not hold.\n\n\nCode\n# Danger of extrapolation\nplot(length_cm, weight_mg, pch = 19, col = \"blue\",\n     xlim = c(0, 6), ylim = c(-50, 600),\n     xlab = \"Length (cm)\", ylab = \"Weight (mg)\",\n     main = \"Extrapolation Risk\")\nabline(fish_lm, col = \"red\", lwd = 2)\nabline(v = c(min(length_cm), max(length_cm)), col = \"gray\", lty = 2)\n\n# Mark extrapolation zone\nrect(max(length_cm), -50, 6, 600, col = rgb(1, 0, 0, 0.1), border = NA)\ntext(5, 100, \"Extrapolation\\nzone\", col = \"red\")",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "chapters/16-simple-linear-regression.html#summary",
    "href": "chapters/16-simple-linear-regression.html#summary",
    "title": "17  Simple Linear Regression",
    "section": "17.14 Summary",
    "text": "17.14 Summary\nSimple linear regression models the relationship between a predictor and response variable:\n\nOLS finds the line minimizing squared residuals\nThe slope indicates how Y changes per unit change in X\nR-squared measures proportion of variance explained\nResidual analysis checks model assumptions\nModel II regression is appropriate when both variables have measurement error\nAvoid extrapolating beyond the range of observed data",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "chapters/16-simple-linear-regression.html#practice-exercises",
    "href": "chapters/16-simple-linear-regression.html#practice-exercises",
    "title": "17  Simple Linear Regression",
    "section": "17.15 Practice Exercises",
    "text": "17.15 Practice Exercises\nFor hands-on practice with regression concepts, see Section 35.10 in the Practice Exercises appendix. The exercises include:\n\nFitting and interpreting linear models\nVisualizing regression lines and residuals\nChecking model assumptions\nANOVA for comparing group means",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "chapters/16-simple-linear-regression.html#additional-resources",
    "href": "chapters/16-simple-linear-regression.html#additional-resources",
    "title": "17  Simple Linear Regression",
    "section": "17.16 Additional Resources",
    "text": "17.16 Additional Resources\n\nJames et al. (2023) - Excellent introduction to regression in the context of statistical learning\nLogan (2010) - Detailed coverage of regression with biological applications\n\n\n\n\n\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2023. An Introduction to Statistical Learning with Applications in r. 2nd ed. Springer. https://www.statlearning.com.\n\n\nLogan, Murray. 2010. Biostatistical Design and Analysis Using r. Wiley-Blackwell.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "chapters/17-residual-analysis.html",
    "href": "chapters/17-residual-analysis.html",
    "title": "18  Residual Analysis",
    "section": "",
    "text": "18.1 What Are Residuals?\nResiduals are the differences between observed values and values predicted by the model:\n\\[e_i = y_i - \\hat{y}_i\\]\nThey represent the part of the data not explained by the model—the “leftover” variation. Analyzing residuals helps us check whether the assumptions of our model are met and identify potential problems.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Residual Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/17-residual-analysis.html#why-residual-analysis-matters",
    "href": "chapters/17-residual-analysis.html#why-residual-analysis-matters",
    "title": "18  Residual Analysis",
    "section": "18.2 Why Residual Analysis Matters",
    "text": "18.2 Why Residual Analysis Matters\nA regression model might fit the data well according to R-squared while still being inappropriate. The model might capture the wrong pattern, miss non-linear relationships, or be unduly influenced by outliers. Residual analysis reveals these problems.\nRemember Anscombe’s Quartet—four datasets with identical regression lines but very different patterns. Looking only at the regression output would miss these differences entirely.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Residual Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/17-residual-analysis.html#checking-assumptions",
    "href": "chapters/17-residual-analysis.html#checking-assumptions",
    "title": "18  Residual Analysis",
    "section": "18.3 Checking Assumptions",
    "text": "18.3 Checking Assumptions\n\n18.3.1 Linearity\nIf the relationship is truly linear, residuals should show no systematic pattern when plotted against fitted values or the predictor variable. A curved pattern suggests non-linearity.\n\n\nCode\n# Create data with non-linear relationship\nset.seed(42)\nx &lt;- seq(0, 10, length.out = 100)\ny &lt;- 2 + 0.5 * x + 0.1 * x^2 + rnorm(100, sd = 1)\n\nmodel &lt;- lm(y ~ x)\n\npar(mfrow = c(1, 2))\nplot(x, y, main = \"Data with Quadratic Pattern\")\nabline(model, col = \"red\")\n\nplot(fitted(model), residuals(model), \n     main = \"Residuals vs Fitted\",\n     xlab = \"Fitted values\", ylab = \"Residuals\")\nabline(h = 0, col = \"red\", lty = 2)\n\n\n\n\n\n\n\n\n\nThe curved pattern in the residual plot reveals that a linear model is inadequate.\n\n\n18.3.2 Normality\nResiduals should be approximately normally distributed. Check with a histogram or Q-Q plot:\n\n\nCode\n# Good model for comparison\nx2 &lt;- rnorm(100)\ny2 &lt;- 2 + 3 * x2 + rnorm(100)\ngood_model &lt;- lm(y2 ~ x2)\n\npar(mfrow = c(1, 2))\nhist(residuals(good_model), breaks = 20, main = \"Histogram of Residuals\",\n     xlab = \"Residuals\", col = \"lightblue\")\nqqnorm(residuals(good_model))\nqqline(residuals(good_model), col = \"red\")\n\n\n\n\n\n\n\n\n\nPoints on the Q-Q plot should fall approximately along the diagonal line. Systematic departures indicate non-normality.\n\n\n18.3.3 Homoscedasticity\nResiduals should have constant variance across the range of fitted values. A fan or cone shape indicates heteroscedasticity (unequal variance).\n\n\n\n\n\n\n\n18.3.4 Independence\nResiduals should be independent of each other. This is hard to check visually but is violated when observations are related (e.g., repeated measurements on the same subjects, or time series data).",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Residual Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/17-residual-analysis.html#diagnostic-plots-in-r",
    "href": "chapters/17-residual-analysis.html#diagnostic-plots-in-r",
    "title": "18  Residual Analysis",
    "section": "18.4 Diagnostic Plots in R",
    "text": "18.4 Diagnostic Plots in R\nR provides built-in diagnostic plots for linear models:\n\n\nCode\n# Standard diagnostic plots\npar(mfrow = c(2, 2))\nplot(good_model)\n\n\n\n\n\n\n\n\n\nThese four plots show: 1. Residuals vs Fitted: Check for linearity and homoscedasticity 2. Q-Q Plot: Check for normality 3. Scale-Location: Check for homoscedasticity 4. Residuals vs Leverage: Identify influential points",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Residual Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/17-residual-analysis.html#leverage-and-influence",
    "href": "chapters/17-residual-analysis.html#leverage-and-influence",
    "title": "18  Residual Analysis",
    "section": "18.5 Leverage and Influence",
    "text": "18.5 Leverage and Influence\nNot all observations affect the regression equally. Leverage measures how unusual an observation’s X value is—points with extreme X values have more potential to influence the fitted line.\nCook’s Distance measures how much the regression would change if an observation were removed. High Cook’s D values indicate influential points that merit closer examination.\n\n\n\n\n\n\n\nCode\n# Check for influential points\ninfluence.measures(good_model)$is.inf[1:5,]  # First 5 observations\n\n\n  dfb.1_ dfb.x2 dffit cov.r cook.d   hat\n1  FALSE  FALSE FALSE FALSE  FALSE FALSE\n2  FALSE  FALSE FALSE FALSE  FALSE FALSE\n3  FALSE  FALSE FALSE FALSE  FALSE FALSE\n4  FALSE  FALSE  TRUE FALSE  FALSE FALSE\n5  FALSE  FALSE FALSE FALSE  FALSE FALSE",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Residual Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/17-residual-analysis.html#handling-violations",
    "href": "chapters/17-residual-analysis.html#handling-violations",
    "title": "18  Residual Analysis",
    "section": "18.6 Handling Violations",
    "text": "18.6 Handling Violations\nWhen assumptions are violated, several approaches may help:\nTransform the data: Log, square root, or other transformations can stabilize variance and improve linearity.\nUse robust regression: Methods like rlm() from the MASS package down-weight influential observations.\nTry a different model: Non-linear regression, generalized linear models, or generalized additive models may be more appropriate.\nRemove outliers: Only if you have substantive reasons—never simply to improve fit.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Residual Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/17-residual-analysis.html#residual-analysis-workflow",
    "href": "chapters/17-residual-analysis.html#residual-analysis-workflow",
    "title": "18  Residual Analysis",
    "section": "18.7 Residual Analysis Workflow",
    "text": "18.7 Residual Analysis Workflow\nA systematic approach to residual analysis:\n\nFit the model\nGenerate diagnostic plots\nCheck for patterns in residuals vs. fitted values\nExamine the Q-Q plot for normality\nLook for influential points\nIf problems exist, consider transformations or alternative models\nRe-check diagnostics after any changes\n\nResidual analysis is not optional—it is an essential part of any regression analysis. Models that look good on paper may tell misleading stories if their assumptions are violated.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Residual Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/18-statistical-power.html",
    "href": "chapters/18-statistical-power.html",
    "title": "19  Statistical Power",
    "section": "",
    "text": "19.1 What is Statistical Power?\nPower is the probability of correctly rejecting a false null hypothesis—the probability of detecting an effect when one truly exists. If the true effect size is non-zero, power tells us how likely our study is to find it.\nPower = 1 - \\(\\beta\\), where \\(\\beta\\) is the probability of a Type II error (failing to reject a false null hypothesis). We typically aim for power of at least 80%, meaning we accept a 20% chance of missing a true effect.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Statistical Power</span>"
    ]
  },
  {
    "objectID": "chapters/18-statistical-power.html#why-power-matters",
    "href": "chapters/18-statistical-power.html#why-power-matters",
    "title": "19  Statistical Power",
    "section": "19.2 Why Power Matters",
    "text": "19.2 Why Power Matters\nA study with low power has poor chances of detecting true effects. Even if an effect exists, the study may fail to find statistical significance. Worse, significant results from underpowered studies tend to overestimate effect sizes—a phenomenon called the “winner’s curse.”\nUnderstanding power helps us interpret results appropriately. If we fail to reject the null hypothesis, was it because no effect exists, or because our study lacked the power to detect it?",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Statistical Power</span>"
    ]
  },
  {
    "objectID": "chapters/18-statistical-power.html#determinants-of-power",
    "href": "chapters/18-statistical-power.html#determinants-of-power",
    "title": "19  Statistical Power",
    "section": "19.3 Determinants of Power",
    "text": "19.3 Determinants of Power\nPower depends on four factors that are mathematically related:\n\\[\\text{Power} \\propto \\frac{(\\text{Effect Size}) \\times (\\alpha) \\times (\\sqrt{n})}{\\sigma}\\]\nEffect Size: Larger effects are easier to detect. Effect size can be measured in original units or standardized (like Cohen’s d).\nSample Size (n): Larger samples provide more information and higher power.\nSignificance Level (\\(\\alpha\\)): Using a more lenient alpha (e.g., 0.10 instead of 0.05) increases power but also increases Type I error risk.\nVariability (\\(\\sigma\\)): Less variable data makes effects easier to detect.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Statistical Power</span>"
    ]
  },
  {
    "objectID": "chapters/18-statistical-power.html#cohens-d-standardized-effect-size",
    "href": "chapters/18-statistical-power.html#cohens-d-standardized-effect-size",
    "title": "19  Statistical Power",
    "section": "19.4 Cohen’s d: Standardized Effect Size",
    "text": "19.4 Cohen’s d: Standardized Effect Size\nCohen’s d expresses the difference between means in standard deviation units:\n\\[d = \\frac{\\mu_1 - \\mu_2}{s_{pooled}}\\]\nConventional benchmarks (Cohen, 1988): - d = 0.2: small effect - d = 0.5: medium effect - d = 0.8: large effect\nThese benchmarks are only guidelines—what counts as “small” depends on the research context.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Statistical Power</span>"
    ]
  },
  {
    "objectID": "chapters/18-statistical-power.html#a-priori-power-analysis",
    "href": "chapters/18-statistical-power.html#a-priori-power-analysis",
    "title": "19  Statistical Power",
    "section": "19.5 A Priori Power Analysis",
    "text": "19.5 A Priori Power Analysis\nBefore collecting data, power analysis helps determine the sample size needed to detect effects of interest. This requires specifying:\n\nThe expected effect size\nThe desired power (typically 0.80)\nThe significance level (typically 0.05)\nThe statistical test to be used\n\n\n\nCode\n# How many subjects needed to detect d = 0.5 with 80% power?\npwr.t.test(d = 0.5, sig.level = 0.05, power = 0.80, type = \"two.sample\")\n\n\n\n     Two-sample t test power calculation \n\n              n = 63.76561\n              d = 0.5\n      sig.level = 0.05\n          power = 0.8\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n\n\nAbout 64 subjects per group are needed to detect a medium effect with 80% power using a two-sample t-test.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Statistical Power</span>"
    ]
  },
  {
    "objectID": "chapters/18-statistical-power.html#power-curves",
    "href": "chapters/18-statistical-power.html#power-curves",
    "title": "19  Statistical Power",
    "section": "19.6 Power Curves",
    "text": "19.6 Power Curves\nPower curves show how power changes with sample size or effect size:\n\n\nCode\n# Power curve for different effect sizes\nsample_sizes &lt;- seq(10, 200, by = 5)\neffect_sizes &lt;- c(0.2, 0.5, 0.8)\n\npower_data &lt;- expand.grid(n = sample_sizes, d = effect_sizes)\npower_data$power &lt;- mapply(function(n, d) {\n  pwr.t.test(n = n, d = d, sig.level = 0.05, type = \"two.sample\")$power\n}, power_data$n, power_data$d)\n\nggplot(power_data, aes(x = n, y = power, color = factor(d))) +\n  geom_line(size = 1.2) +\n  geom_hline(yintercept = 0.8, linetype = \"dashed\") +\n  labs(title = \"Power Curves for Two-Sample t-Test\",\n       x = \"Sample Size per Group\",\n       y = \"Power\",\n       color = \"Effect Size (d)\") +\n  theme_minimal()",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Statistical Power</span>"
    ]
  },
  {
    "objectID": "chapters/18-statistical-power.html#power-for-anova",
    "href": "chapters/18-statistical-power.html#power-for-anova",
    "title": "19  Statistical Power",
    "section": "19.7 Power for ANOVA",
    "text": "19.7 Power for ANOVA\nFor ANOVA, effect size is measured by Cohen’s f:\n\\[f = \\frac{\\sigma_{between}}{\\sigma_{within}}\\]\nBenchmarks: f = 0.10 (small), f = 0.25 (medium), f = 0.40 (large).\n\n\nCode\n# Sample size for ANOVA with 3 groups\npwr.anova.test(k = 3, f = 0.25, sig.level = 0.05, power = 0.80)\n\n\n\n     Balanced one-way analysis of variance power calculation \n\n              k = 3\n              n = 52.3966\n              f = 0.25\n      sig.level = 0.05\n          power = 0.8\n\nNOTE: n is number in each group",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Statistical Power</span>"
    ]
  },
  {
    "objectID": "chapters/18-statistical-power.html#simulation-based-power-analysis",
    "href": "chapters/18-statistical-power.html#simulation-based-power-analysis",
    "title": "19  Statistical Power",
    "section": "19.8 Simulation-Based Power Analysis",
    "text": "19.8 Simulation-Based Power Analysis\nFor complex designs, simulation provides a flexible approach:\n\n\nCode\n# Simulation-based power for comparing two Poisson distributions\nset.seed(42)\n\npower_sim &lt;- function(n, lambda1, lambda2, n_sims = 1000) {\n  significant &lt;- replicate(n_sims, {\n    x1 &lt;- rpois(n, lambda1)\n    x2 &lt;- rpois(n, lambda2)\n    t.test(x1, x2)$p.value &lt; 0.05\n  })\n  mean(significant)\n}\n\n# Power for different sample sizes\nsample_sizes &lt;- seq(10, 100, by = 10)\npowers &lt;- sapply(sample_sizes, power_sim, lambda1 = 10, lambda2 = 12)\n\nplot(sample_sizes, powers, type = \"b\", pch = 19,\n     xlab = \"Sample Size per Group\", ylab = \"Power\",\n     main = \"Simulated Power (λ1=10 vs λ2=12)\")\nabline(h = 0.8, lty = 2, col = \"red\")",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Statistical Power</span>"
    ]
  },
  {
    "objectID": "chapters/18-statistical-power.html#post-hoc-power-analysis",
    "href": "chapters/18-statistical-power.html#post-hoc-power-analysis",
    "title": "19  Statistical Power",
    "section": "19.9 Post Hoc Power Analysis",
    "text": "19.9 Post Hoc Power Analysis\nCalculating power after a study is completed is controversial. Post hoc power calculated from observed effect sizes is mathematically determined by the p-value and adds no new information. It cannot tell you whether a non-significant result reflects a true null or insufficient power.\nIf you want to understand what your study could detect, specify effect sizes based on scientific considerations, not observed results.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Statistical Power</span>"
    ]
  },
  {
    "objectID": "chapters/18-statistical-power.html#practical-recommendations",
    "href": "chapters/18-statistical-power.html#practical-recommendations",
    "title": "19  Statistical Power",
    "section": "19.10 Practical Recommendations",
    "text": "19.10 Practical Recommendations\nAlways conduct power analysis before data collection. Use realistic effect size estimates based on pilot data or previous literature. Consider what effect size would be practically meaningful, not just what you think exists.\nBe conservative—effects are often smaller than expected. Plan for some attrition or missing data. When in doubt, collect more data if feasible.",
    "crumbs": [
      "Linear Models",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Statistical Power</span>"
    ]
  },
  {
    "objectID": "chapters/19-anova.html",
    "href": "chapters/19-anova.html",
    "title": "20  Analysis of Variance",
    "section": "",
    "text": "20.1 Beyond Two Groups\nThe t-test compares two groups, but many experiments involve more than two. We might compare three drug treatments, five temperature conditions, or four genetic strains. Running multiple t-tests creates problems: with many comparisons, false positives become likely even when no true differences exist.\nAnalysis of Variance (ANOVA) provides a solution. It tests whether any of the group means differ from the others in a single test, controlling the overall Type I error rate.",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Analysis of Variance</span>"
    ]
  },
  {
    "objectID": "chapters/19-anova.html#the-anova-framework",
    "href": "chapters/19-anova.html#the-anova-framework",
    "title": "20  Analysis of Variance",
    "section": "20.2 The ANOVA Framework",
    "text": "20.2 The ANOVA Framework\nANOVA partitions the total variation in the data into components: variation between groups (due to treatment effects) and variation within groups (due to random error).\n\n\n\n\n\nThe key insight is that if groups have equal means, the between-group variation should be similar to the within-group variation. If the between-group variation is much larger, the group means probably differ.",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Analysis of Variance</span>"
    ]
  },
  {
    "objectID": "chapters/19-anova.html#the-f-test",
    "href": "chapters/19-anova.html#the-f-test",
    "title": "20  Analysis of Variance",
    "section": "20.3 The F-Test",
    "text": "20.3 The F-Test\nANOVA uses the F-statistic:\n\\[F = \\frac{MS_{between}}{MS_{within}} = \\frac{\\text{Variance between groups}}{\\text{Variance within groups}}\\]\nUnder the null hypothesis (all group means equal), F follows an F-distribution. Large F values indicate that group means differ more than expected by chance.",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Analysis of Variance</span>"
    ]
  },
  {
    "objectID": "chapters/19-anova.html#one-way-anova-in-r",
    "href": "chapters/19-anova.html#one-way-anova-in-r",
    "title": "20  Analysis of Variance",
    "section": "20.4 One-Way ANOVA in R",
    "text": "20.4 One-Way ANOVA in R\n\n\nCode\n# Example using iris data\niris_aov &lt;- aov(Sepal.Length ~ Species, data = iris)\nsummary(iris_aov)\n\n\n             Df Sum Sq Mean Sq F value Pr(&gt;F)    \nSpecies       2  63.21  31.606   119.3 &lt;2e-16 ***\nResiduals   147  38.96   0.265                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe significant p-value tells us that sepal length differs among species, but not which species differ from which.",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Analysis of Variance</span>"
    ]
  },
  {
    "objectID": "chapters/19-anova.html#anova-assumptions",
    "href": "chapters/19-anova.html#anova-assumptions",
    "title": "20  Analysis of Variance",
    "section": "20.5 ANOVA Assumptions",
    "text": "20.5 ANOVA Assumptions\nLike the t-test, ANOVA assumes:\n\nNormality: Observations within each group are normally distributed\nHomogeneity of variance: Groups have equal variances\nIndependence: Observations are independent\n\nANOVA is robust to mild violations of normality, especially with balanced designs and large samples. Serious violations of homogeneity of variance are more problematic but can be addressed with Welch’s ANOVA or transformations.",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Analysis of Variance</span>"
    ]
  },
  {
    "objectID": "chapters/19-anova.html#post-hoc-comparisons",
    "href": "chapters/19-anova.html#post-hoc-comparisons",
    "title": "20  Analysis of Variance",
    "section": "20.6 Post-Hoc Comparisons",
    "text": "20.6 Post-Hoc Comparisons\nA significant ANOVA tells us groups differ but not how. Post-hoc tests compare specific pairs of groups while controlling for multiple comparisons.\nTukey’s HSD (Honestly Significant Difference) compares all pairs:\n\n\nCode\nTukeyHSD(iris_aov)\n\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = Sepal.Length ~ Species, data = iris)\n\n$Species\n                      diff       lwr       upr p adj\nversicolor-setosa    0.930 0.6862273 1.1737727     0\nvirginica-setosa     1.582 1.3382273 1.8257727     0\nvirginica-versicolor 0.652 0.4082273 0.8957727     0\n\n\nEach pairwise comparison includes the difference in means, confidence interval, and adjusted p-value.",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Analysis of Variance</span>"
    ]
  },
  {
    "objectID": "chapters/19-anova.html#planned-contrasts",
    "href": "chapters/19-anova.html#planned-contrasts",
    "title": "20  Analysis of Variance",
    "section": "20.7 Planned Contrasts",
    "text": "20.7 Planned Contrasts\nIf you have specific hypotheses about which groups should differ (decided before seeing the data), planned contrasts are more powerful than post-hoc tests. They focus statistical power on the comparisons you care about.\n\n\nCode\n# Example: Compare setosa to the average of the other two species\ncontrasts(iris$Species) &lt;- cbind(\n  setosa_vs_others = c(2, -1, -1)\n)\nsummary.lm(aov(Sepal.Length ~ Species, data = iris))\n\n\n\nCall:\naov(formula = Sepal.Length ~ Species, data = iris)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.6880 -0.3285 -0.0060  0.3120  1.3120 \n\nCoefficients:\n                        Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)              5.84333    0.04203 139.020  &lt; 2e-16 ***\nSpeciessetosa_vs_others -0.41867    0.02972 -14.086  &lt; 2e-16 ***\nSpecies                  0.46103    0.07280   6.333 2.77e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5148 on 147 degrees of freedom\nMultiple R-squared:  0.6187,    Adjusted R-squared:  0.6135 \nF-statistic: 119.3 on 2 and 147 DF,  p-value: &lt; 2.2e-16",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Analysis of Variance</span>"
    ]
  },
  {
    "objectID": "chapters/19-anova.html#fixed-vs.-random-effects",
    "href": "chapters/19-anova.html#fixed-vs.-random-effects",
    "title": "20  Analysis of Variance",
    "section": "20.8 Fixed vs. Random Effects",
    "text": "20.8 Fixed vs. Random Effects\nFixed effects are specific treatments of interest that would be the same if the study were replicated—drug A, drug B, drug C. Conclusions apply only to these specific treatments.\nRandom effects are levels sampled from a larger population—particular subjects, batches, or locations. The goal is to generalize to the population of possible levels, not just those observed.\nThe distinction matters because it affects how F-ratios are calculated and what conclusions can be drawn.",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Analysis of Variance</span>"
    ]
  },
  {
    "objectID": "chapters/19-anova.html#two-way-anova",
    "href": "chapters/19-anova.html#two-way-anova",
    "title": "20  Analysis of Variance",
    "section": "20.9 Two-Way ANOVA",
    "text": "20.9 Two-Way ANOVA\nWhen experiments have two factors, two-way ANOVA examines main effects of each factor and their interaction.\n\n\nCode\n# Simulated factorial design\nset.seed(42)\nn &lt;- 20\ndata_factorial &lt;- data.frame(\n  response = c(rnorm(n, 10, 2), rnorm(n, 12, 2), \n               rnorm(n, 11, 2), rnorm(n, 18, 2)),\n  factor_A = rep(c(\"A1\", \"A1\", \"A2\", \"A2\"), each = n),\n  factor_B = rep(c(\"B1\", \"B2\", \"B1\", \"B2\"), each = n)\n)\n\ntwo_way &lt;- aov(response ~ factor_A * factor_B, data = data_factorial)\nsummary(two_way)\n\n\n                  Df Sum Sq Mean Sq F value   Pr(&gt;F)    \nfactor_A           1  279.6   279.6   59.95 3.41e-11 ***\nfactor_B           1  352.6   352.6   75.61 5.12e-13 ***\nfactor_A:factor_B  1  195.2   195.2   41.87 8.57e-09 ***\nResiduals         76  354.4     4.7                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nAn interaction means the effect of one factor depends on the level of the other. Significant interactions often require examining simple effects rather than main effects.",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Analysis of Variance</span>"
    ]
  },
  {
    "objectID": "chapters/19-anova.html#interaction-plots",
    "href": "chapters/19-anova.html#interaction-plots",
    "title": "20  Analysis of Variance",
    "section": "20.10 Interaction Plots",
    "text": "20.10 Interaction Plots\n\n\nCode\n# Visualize interaction\ninteraction.plot(data_factorial$factor_A, data_factorial$factor_B, \n                 data_factorial$response,\n                 col = c(\"blue\", \"red\"), lwd = 2,\n                 xlab = \"Factor A\", ylab = \"Response\",\n                 trace.label = \"Factor B\")\n\n\n\n\n\n\n\n\n\nNon-parallel lines suggest an interaction. Parallel lines suggest additive (non-interacting) effects.",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Analysis of Variance</span>"
    ]
  },
  {
    "objectID": "chapters/19-anova.html#nested-designs",
    "href": "chapters/19-anova.html#nested-designs",
    "title": "20  Analysis of Variance",
    "section": "20.11 Nested Designs",
    "text": "20.11 Nested Designs\nIn nested designs, levels of one factor exist only within levels of another. For example, technicians might be nested within labs—each technician works in only one lab.\nNested designs have no interaction term because not all combinations of factor levels exist. They are common when sampling is hierarchical.",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Analysis of Variance</span>"
    ]
  },
  {
    "objectID": "chapters/19-anova.html#practical-considerations",
    "href": "chapters/19-anova.html#practical-considerations",
    "title": "20  Analysis of Variance",
    "section": "20.12 Practical Considerations",
    "text": "20.12 Practical Considerations\nReport effect sizes (like \\(\\eta^2\\)) alongside p-values. A significant ANOVA with tiny effect size may not be practically meaningful.\nCheck assumptions with residual plots. Consider transformations or nonparametric alternatives (Kruskal-Wallis) when assumptions are violated.\nPlan your sample size using power analysis before collecting data, specifying the minimum effect size you want to detect.",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Analysis of Variance</span>"
    ]
  },
  {
    "objectID": "chapters/19-anova.html#anova-as-a-general-linear-model",
    "href": "chapters/19-anova.html#anova-as-a-general-linear-model",
    "title": "20  Analysis of Variance",
    "section": "20.13 ANOVA as a General Linear Model",
    "text": "20.13 ANOVA as a General Linear Model\nANOVA is a special case of the general linear model (GLM). Both t-tests and ANOVA can be expressed as regression with indicator variables (dummy coding). This unified framework shows that these seemingly different methods are fundamentally the same.\n\n\nCode\n# ANOVA using lm() with dummy coding\n# Equivalent to aov()\niris_lm &lt;- lm(Sepal.Length ~ Species, data = iris)\nanova(iris_lm)\n\n\nAnalysis of Variance Table\n\nResponse: Sepal.Length\n           Df Sum Sq Mean Sq F value    Pr(&gt;F)    \nSpecies     2 63.212  31.606  119.26 &lt; 2.2e-16 ***\nResiduals 147 38.956   0.265                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe connection becomes clear when you realize: - A one-sample t-test is regression on an intercept - A two-sample t-test is regression with one binary predictor - One-way ANOVA is regression with multiple indicator variables\nThis unified view is powerful: once you understand regression, you understand the entire family of linear models.",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Analysis of Variance</span>"
    ]
  },
  {
    "objectID": "chapters/19-anova.html#effect-sizes-in-anova",
    "href": "chapters/19-anova.html#effect-sizes-in-anova",
    "title": "20  Analysis of Variance",
    "section": "20.14 Effect Sizes in ANOVA",
    "text": "20.14 Effect Sizes in ANOVA\nBeyond statistical significance, report how much of the variance is explained by your factors.\nEta-squared (\\(\\eta^2\\)): Proportion of total variance explained by the factor\n\\[\\eta^2 = \\frac{SS_{between}}{SS_{total}}\\]\nPartial eta-squared (\\(\\eta^2_p\\)): Proportion of variance explained after accounting for other factors\nOmega-squared (\\(\\omega^2\\)): Less biased estimate of variance explained in the population\n\n\nCode\n# Calculate effect sizes\nss &lt;- summary(iris_aov)[[1]]\nss_between &lt;- ss[\"Species\", \"Sum Sq\"]\nss_within &lt;- ss[\"Residuals\", \"Sum Sq\"]\nss_total &lt;- ss_between + ss_within\n\neta_squared &lt;- ss_between / ss_total\ncat(\"Eta-squared:\", round(eta_squared, 3), \"\\n\")\n\n\nEta-squared: 0.619 \n\n\nCode\n# Omega-squared (less biased)\nms_within &lt;- ss[\"Residuals\", \"Mean Sq\"]\nn &lt;- nrow(iris)\nk &lt;- length(unique(iris$Species))\nomega_squared &lt;- (ss_between - (k-1) * ms_within) / (ss_total + ms_within)\ncat(\"Omega-squared:\", round(omega_squared, 3), \"\\n\")\n\n\nOmega-squared: 0.612",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Analysis of Variance</span>"
    ]
  },
  {
    "objectID": "chapters/19-anova.html#pseudoreplication",
    "href": "chapters/19-anova.html#pseudoreplication",
    "title": "20  Analysis of Variance",
    "section": "20.15 Pseudoreplication",
    "text": "20.15 Pseudoreplication\n\n\n\n\n\n\nA Common Design Flaw\n\n\n\nPseudoreplication occurs when non-independent observations are treated as independent replicates. This inflates the apparent sample size and leads to artificially small p-values.\nCommon examples: - Multiple measurements from the same individual treated as independent - Multiple cells from the same culture dish - Multiple fish from the same tank when treatment was applied to tanks - Technical replicates confused with biological replicates\n\n\nThe unit of replication must be the unit to which the treatment was independently applied. If you treat three tanks with drug A and three with drug B, you have n=3 per group regardless of how many fish are in each tank.\n\n\nCode\n# Wrong: treats individual fish as independent\n# If 10 fish per tank, and tanks are the true units:\nset.seed(42)\n# This overstates the evidence because fish within tanks are correlated\ntank_A &lt;- rep(c(10, 12, 11), each = 10) + rnorm(30, sd = 1)  # 3 tanks, 10 fish each\ntank_B &lt;- rep(c(8, 9, 8.5), each = 10) + rnorm(30, sd = 1)\n\n# Pseudoreplicated analysis (WRONG - n appears to be 30 per group)\ncat(\"Pseudoreplicated p-value:\", t.test(tank_A, tank_B)$p.value, \"\\n\")\n\n\nPseudoreplicated p-value: 2.315344e-11 \n\n\nCode\n# Correct analysis (using tank means, n = 3 per group)\nmeans_A &lt;- c(mean(tank_A[1:10]), mean(tank_A[11:20]), mean(tank_A[21:30]))\nmeans_B &lt;- c(mean(tank_B[1:10]), mean(tank_B[11:20]), mean(tank_B[21:30]))\ncat(\"Correct p-value:\", t.test(means_A, means_B)$p.value, \"\\n\")\n\n\nCorrect p-value: 0.008405113 \n\n\nThe correct analysis has less power (larger p-value) because it honestly reflects the true sample size.",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Analysis of Variance</span>"
    ]
  },
  {
    "objectID": "chapters/19-anova.html#repeated-measures-anova",
    "href": "chapters/19-anova.html#repeated-measures-anova",
    "title": "20  Analysis of Variance",
    "section": "20.16 Repeated Measures ANOVA",
    "text": "20.16 Repeated Measures ANOVA\nWhen the same subjects are measured under multiple conditions, observations are not independent—each subject creates correlated measurements. Repeated measures ANOVA accounts for this by partitioning out between-subject variability.\n\n\nCode\n# Simulated repeated measures data\nset.seed(123)\nn_subjects &lt;- 10\nsubject &lt;- factor(rep(1:n_subjects, 3))\ntime &lt;- factor(rep(c(\"Before\", \"During\", \"After\"), each = n_subjects))\n\n# Subjects have baseline differences, plus time effect\nbaseline &lt;- rnorm(n_subjects, 50, 10)\nresponse &lt;- c(baseline, baseline + 5 + rnorm(n_subjects, 0, 3),\n              baseline + 2 + rnorm(n_subjects, 0, 3))\n\nrm_data &lt;- data.frame(subject, time, response)\n\n# Repeated measures ANOVA\nrm_aov &lt;- aov(response ~ time + Error(subject/time), data = rm_data)\nsummary(rm_aov)\n\n\n\nError: subject\n          Df Sum Sq Mean Sq F value Pr(&gt;F)\nResiduals  9   2593   288.1               \n\nError: subject:time\n          Df Sum Sq Mean Sq F value   Pr(&gt;F)    \ntime       2  187.3   93.64   12.52 0.000391 ***\nResiduals 18  134.6    7.48                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nRepeated measures designs are powerful because they control for individual differences, but require additional assumptions (sphericity) that should be checked.",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Analysis of Variance</span>"
    ]
  },
  {
    "objectID": "chapters/19-anova.html#summary",
    "href": "chapters/19-anova.html#summary",
    "title": "20  Analysis of Variance",
    "section": "20.17 Summary",
    "text": "20.17 Summary\nANOVA provides a flexible framework for comparing groups:\n\nOne-way ANOVA compares means across multiple groups\nThe F-test assesses whether between-group variance exceeds within-group variance\nPost-hoc tests identify which specific groups differ\nTwo-way ANOVA examines main effects and interactions\nFixed effects are specific treatments; random effects are sampled from populations\nANOVA is part of the general linear model family\nAlways report effect sizes, not just p-values\nBeware of pseudoreplication—the unit of analysis must match the unit of replication",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Analysis of Variance</span>"
    ]
  },
  {
    "objectID": "chapters/19-anova.html#additional-resources",
    "href": "chapters/19-anova.html#additional-resources",
    "title": "20  Analysis of Variance",
    "section": "20.18 Additional Resources",
    "text": "20.18 Additional Resources\n\nLogan (2010) - Comprehensive treatment of ANOVA designs in biological research\nCrawley (2007) - Detailed coverage of linear models in R including ANOVA\n\n\n\n\n\n\n\nCrawley, Michael J. 2007. The r Book. John Wiley & Sons.\n\n\nLogan, Murray. 2010. Biostatistical Design and Analysis Using r. Wiley-Blackwell.",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Analysis of Variance</span>"
    ]
  },
  {
    "objectID": "chapters/20-multiple-regression.html",
    "href": "chapters/20-multiple-regression.html",
    "title": "21  Multiple Regression",
    "section": "",
    "text": "21.1 Beyond One Predictor\nSimple linear regression uses a single predictor. But the response variable often depends on multiple factors. A patient’s blood pressure might depend on age, weight, sodium intake, and medication. Gene expression might depend on temperature, time, and treatment condition.\nMultiple regression extends linear regression to multiple predictors:\n\\[y_i = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\ldots + \\beta_p x_p + \\epsilon_i\\]",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Multiple Regression</span>"
    ]
  },
  {
    "objectID": "chapters/20-multiple-regression.html#goals-of-multiple-regression",
    "href": "chapters/20-multiple-regression.html#goals-of-multiple-regression",
    "title": "21  Multiple Regression",
    "section": "21.2 Goals of Multiple Regression",
    "text": "21.2 Goals of Multiple Regression\nMultiple regression serves two main purposes. First, it often improves prediction by incorporating multiple sources of information. Second, it allows us to investigate the effect of each predictor while controlling for the others—the effect of X1 “holding X2 constant.”\nThis second goal is powerful but requires caution. In observational data, controlling for variables statistically is not the same as controlling them experimentally. Confounding variables you do not measure cannot be controlled.",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Multiple Regression</span>"
    ]
  },
  {
    "objectID": "chapters/20-multiple-regression.html#understanding-confounding-through-stratification",
    "href": "chapters/20-multiple-regression.html#understanding-confounding-through-stratification",
    "title": "21  Multiple Regression",
    "section": "21.3 Understanding Confounding Through Stratification",
    "text": "21.3 Understanding Confounding Through Stratification\nA powerful way to understand confounding—and why multiple regression is necessary—is through stratification. When two predictors are correlated, their apparent individual effects can be misleading.\nConsider a biological example: suppose we measure both body size and metabolic rate in animals, and we want to know if each independently predicts lifespan. If larger animals have both higher metabolic rates AND longer lifespans (because both correlate with species type), we might see a spurious positive relationship between metabolic rate and lifespan when the true relationship is negative within any given body size.\nThe solution is to stratify by the confounding variable. If we group animals by body size and look at the relationship between metabolic rate and lifespan within each group, we see the true (negative) relationship.\n\n\nCode\n# Simulated confounding example\nset.seed(42)\nn &lt;- 200\n\n# Body size (the confounder)\nbody_size &lt;- runif(n, 1, 10)\n\n# Metabolic rate increases with body size (positive correlation with confounder)\nmetabolic_rate &lt;- 2 * body_size + rnorm(n, sd = 2)\n\n# Lifespan: increases with body size, DECREASES with metabolic rate\n# But without controlling for body size, it appears metabolic rate increases lifespan!\nlifespan &lt;- 5 + 3 * body_size - 0.5 * metabolic_rate + rnorm(n, sd = 2)\n\nconfound_data &lt;- data.frame(body_size, metabolic_rate, lifespan)\n\n# Naive regression (ignoring confounder)\nnaive_fit &lt;- lm(lifespan ~ metabolic_rate, data = confound_data)\n\n# Proper multiple regression (controlling for body size)\nproper_fit &lt;- lm(lifespan ~ metabolic_rate + body_size, data = confound_data)\n\ncat(\"Naive model (ignoring body size):\\n\")\n\n\nNaive model (ignoring body size):\n\n\nCode\ncat(\"Metabolic rate coefficient:\", round(coef(naive_fit)[2], 3), \"\\n\\n\")\n\n\nMetabolic rate coefficient: 0.787 \n\n\nCode\ncat(\"Multiple regression (controlling for body size):\\n\")\n\n\nMultiple regression (controlling for body size):\n\n\nCode\ncat(\"Metabolic rate coefficient:\", round(coef(proper_fit)[2], 3), \"\\n\")\n\n\nMetabolic rate coefficient: -0.466 \n\n\nCode\ncat(\"Body size coefficient:\", round(coef(proper_fit)[3], 3), \"\\n\")\n\n\nBody size coefficient: 2.844 \n\n\nThe naive model shows a positive relationship between metabolic rate and lifespan. But once we control for body size, we see the true negative relationship—higher metabolic rate is associated with shorter lifespan, as biological theory predicts.\nWe can visualize this confounding through stratification:\n\n\nCode\n# Stratify by body size\nconfound_data$size_strata &lt;- cut(confound_data$body_size,\n                                  breaks = quantile(body_size, c(0, 0.33, 0.67, 1)),\n                                  labels = c(\"Small\", \"Medium\", \"Large\"),\n                                  include.lowest = TRUE)\n\n# Plot relationship within each stratum\nggplot(confound_data, aes(x = metabolic_rate, y = lifespan)) +\n  geom_point(alpha = 0.5) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  facet_wrap(~ size_strata) +\n  labs(title = \"Stratification Reveals True Relationship\",\n       subtitle = \"Within each body size group, higher metabolic rate predicts shorter lifespan\",\n       x = \"Metabolic Rate\", y = \"Lifespan\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nWithin each stratum (holding body size approximately constant), we see the true negative relationship. The slopes within strata approximate what multiple regression gives us.\n\n\n\n\n\n\nWhy This Matters\n\n\n\nWhen predictors are correlated, simple regression coefficients can be misleading—even showing the wrong sign! Multiple regression “adjusts” for confounders, revealing relationships that are closer to (though not necessarily equal to) causal effects. However, you can only adjust for confounders you measure. Unmeasured confounders remain a threat to causal interpretation.",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Multiple Regression</span>"
    ]
  },
  {
    "objectID": "chapters/20-multiple-regression.html#additive-vs.-multiplicative-models",
    "href": "chapters/20-multiple-regression.html#additive-vs.-multiplicative-models",
    "title": "21  Multiple Regression",
    "section": "21.4 Additive vs. Multiplicative Models",
    "text": "21.4 Additive vs. Multiplicative Models\nAn additive model assumes predictors contribute independently:\n\\[y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\epsilon\\]\nA multiplicative model includes interactions—the effect of one predictor depends on the value of another:\n\\[y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_1 x_2 + \\epsilon\\]\n\n\nCode\n# Example with two predictors\nset.seed(42)\nn &lt;- 100\nx1 &lt;- rnorm(n)\nx2 &lt;- rnorm(n)\ny &lt;- 2 + 3*x1 + 2*x2 + 1.5*x1*x2 + rnorm(n)\n\n# Additive model\nadd_model &lt;- lm(y ~ x1 + x2)\n\n# Model with interaction\nint_model &lt;- lm(y ~ x1 * x2)\n\nsummary(int_model)\n\n\n\nCall:\nlm(formula = y ~ x1 * x2)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.55125 -0.69885 -0.03771  0.56441  2.42157 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  2.00219    0.10108   19.81   &lt;2e-16 ***\nx1           2.84494    0.09734   29.23   &lt;2e-16 ***\nx2           2.04126    0.11512   17.73   &lt;2e-16 ***\nx1:x2        1.35163    0.09228   14.65   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.005 on 96 degrees of freedom\nMultiple R-squared:  0.9289,    Adjusted R-squared:  0.9267 \nF-statistic:   418 on 3 and 96 DF,  p-value: &lt; 2.2e-16",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Multiple Regression</span>"
    ]
  },
  {
    "objectID": "chapters/20-multiple-regression.html#interpretation-of-coefficients",
    "href": "chapters/20-multiple-regression.html#interpretation-of-coefficients",
    "title": "21  Multiple Regression",
    "section": "21.5 Interpretation of Coefficients",
    "text": "21.5 Interpretation of Coefficients\nIn multiple regression, each coefficient represents the expected change in Y for a one-unit change in that predictor, holding all other predictors constant.\nThis “holding constant” interpretation makes the coefficients different from what you would get from separate simple regressions. The coefficient for X1 in multiple regression represents the unique contribution of X1 after accounting for X2.",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Multiple Regression</span>"
    ]
  },
  {
    "objectID": "chapters/20-multiple-regression.html#multicollinearity",
    "href": "chapters/20-multiple-regression.html#multicollinearity",
    "title": "21  Multiple Regression",
    "section": "21.6 Multicollinearity",
    "text": "21.6 Multicollinearity\nWhen predictors are correlated with each other, interpreting individual coefficients becomes problematic. This multicollinearity inflates standard errors and can make coefficients unstable.\n\n\nCode\n# Check for multicollinearity visually\nlibrary(car)\npairs(~ x1 + x2, main = \"Scatterplot Matrix\")\n\n\n\n\n\n\n\n\n\nThe Variance Inflation Factor (VIF) quantifies multicollinearity. VIF &gt; 10 suggests serious problems; VIF &gt; 5 warrants attention.\n\n\nCode\nvif(int_model)\n\n\n      x1       x2    x1:x2 \n1.006276 1.061022 1.066455",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Multiple Regression</span>"
    ]
  },
  {
    "objectID": "chapters/20-multiple-regression.html#model-selection",
    "href": "chapters/20-multiple-regression.html#model-selection",
    "title": "21  Multiple Regression",
    "section": "21.7 Model Selection",
    "text": "21.7 Model Selection\nWith many potential predictors, how do we choose which to include? Adding variables always improves fit to the training data but may hurt prediction on new data through overfitting.\nSeveral criteria balance fit and complexity:\nAdjusted R² penalizes for the number of predictors.\nAIC (Akaike Information Criterion) estimates prediction error, penalizing complexity. Lower is better.\nBIC (Bayesian Information Criterion) similar to AIC but penalizes complexity more heavily.\n\n\nCode\n# Compare models\nAIC(add_model, int_model)\n\n\n          df      AIC\nadd_model  4 406.1808\nint_model  5 290.7926\n\n\nCode\nBIC(add_model, int_model)\n\n\n          df      BIC\nadd_model  4 416.6015\nint_model  5 303.8185",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Multiple Regression</span>"
    ]
  },
  {
    "objectID": "chapters/20-multiple-regression.html#model-selection-strategies",
    "href": "chapters/20-multiple-regression.html#model-selection-strategies",
    "title": "21  Multiple Regression",
    "section": "21.8 Model Selection Strategies",
    "text": "21.8 Model Selection Strategies\nForward selection starts with no predictors and adds them one at a time based on statistical criteria.\nBackward elimination starts with all predictors and removes them one at a time.\nAll subsets examines all possible combinations and selects the best.\nNo strategy is universally best. Automated selection can lead to overfitting and unstable models. Theory-driven model building—starting with predictors you have scientific reasons to include—is often preferable.",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Multiple Regression</span>"
    ]
  },
  {
    "objectID": "chapters/20-multiple-regression.html#polynomial-regression",
    "href": "chapters/20-multiple-regression.html#polynomial-regression",
    "title": "21  Multiple Regression",
    "section": "21.9 Polynomial Regression",
    "text": "21.9 Polynomial Regression\nPolynomial terms can capture non-linear relationships while still using the linear regression framework:\n\n\nCode\n# Non-linear relationship\nx &lt;- seq(0, 10, length.out = 100)\ny &lt;- 2 + 0.5*x - 0.1*x^2 + rnorm(100, sd = 0.5)\n\n# Fit polynomial models\nmodel1 &lt;- lm(y ~ poly(x, 1))  # Linear\nmodel2 &lt;- lm(y ~ poly(x, 2))  # Quadratic\nmodel3 &lt;- lm(y ~ poly(x, 5))  # Degree 5\n\n# Compare\nAIC(model1, model2, model3)\n\n\n       df      AIC\nmodel1  3 250.3022\nmodel2  4 121.8041\nmodel3  7 126.6399\n\n\nHigher-degree polynomials fit better but risk overfitting. The principle of parsimony suggests using the simplest adequate model.",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Multiple Regression</span>"
    ]
  },
  {
    "objectID": "chapters/20-multiple-regression.html#assumptions",
    "href": "chapters/20-multiple-regression.html#assumptions",
    "title": "21  Multiple Regression",
    "section": "21.10 Assumptions",
    "text": "21.10 Assumptions\nMultiple regression shares assumptions with simple regression: linearity (in each predictor), independence, normality of residuals, and constant variance. Additionally, predictors should not be perfectly correlated (no perfect multicollinearity).\nCheck assumptions with residual plots. Partial regression plots can help diagnose problems with individual predictors.",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Multiple Regression</span>"
    ]
  },
  {
    "objectID": "chapters/20-multiple-regression.html#practical-guidelines",
    "href": "chapters/20-multiple-regression.html#practical-guidelines",
    "title": "21  Multiple Regression",
    "section": "21.11 Practical Guidelines",
    "text": "21.11 Practical Guidelines\nStart with a theoretically motivated model rather than throwing in all available predictors. Check for multicollinearity before interpreting coefficients. Use cross-validation to assess prediction performance. Report standardized coefficients when comparing the relative importance of predictors on different scales.\nBe humble about causation. Multiple regression describes associations; experimental manipulation is needed to establish causation.",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Multiple Regression</span>"
    ]
  },
  {
    "objectID": "chapters/21-glm.html",
    "href": "chapters/21-glm.html",
    "title": "22  Generalized Linear Models",
    "section": "",
    "text": "22.1 Beyond Normal Distributions\nStandard linear regression assumes that the response variable is continuous and normally distributed. But many important response variables violate these assumptions. Binary outcomes (success/failure, alive/dead) follow binomial distributions. Count data (number of events, cells, species) often follow Poisson distributions.\nGeneralized Linear Models (GLMs) extend linear regression to handle these situations. They provide a unified framework for modeling responses that follow different distributions from the exponential family.",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Generalized Linear Models</span>"
    ]
  },
  {
    "objectID": "chapters/21-glm.html#frequency-analysis-categorical-response-variables",
    "href": "chapters/21-glm.html#frequency-analysis-categorical-response-variables",
    "title": "22  Generalized Linear Models",
    "section": "22.2 Frequency Analysis: Categorical Response Variables",
    "text": "22.2 Frequency Analysis: Categorical Response Variables\nBefore diving into GLMs, it’s important to understand how we analyze categorical response variables. When observations fall into categories rather than being measured on a continuous scale, we count the frequency in each category and compare observed frequencies to expected values.\n\n22.2.1 Chi-Square Goodness of Fit Test\nThe goodness of fit test asks whether observed frequencies match a hypothesized distribution. The classic example comes from Mendelian genetics.\n\n\nCode\n# Mendel's pea experiment - F2 phenotype ratios\n# Expected: 9:3:3:1 for Yellow-Smooth:Yellow-Wrinkled:Green-Smooth:Green-Wrinkled\nobserved &lt;- c(315, 101, 108, 32)  # Mendel's actual data\nexpected_ratios &lt;- c(9/16, 3/16, 3/16, 1/16)\n\n# Perform chi-square test\nchisq.test(observed, p = expected_ratios)\n\n\n\n    Chi-squared test for given probabilities\n\ndata:  observed\nX-squared = 0.47002, df = 3, p-value = 0.9254\n\n\nThe test statistic measures deviation from expected:\n\\[\\chi^2 = \\sum \\frac{(O_i - E_i)^2}{E_i}\\]\nwhere \\(O_i\\) are observed and \\(E_i\\) are expected counts. Under the null hypothesis (observed frequencies match expected), this follows a chi-square distribution with \\(df = k - 1\\) where \\(k\\) is the number of categories.\n\n\n\n\n\n\nChi-Square Assumptions\n\n\n\n\nIndependence: Observations must be independent\nExpected counts: No more than 20% of expected counts should be &lt; 5\nSample size: Total sample should be reasonably large\n\nCheck expected values before interpreting results:\n\n\nCode\nn &lt;- sum(observed)\nexpected &lt;- n * expected_ratios\ncat(\"Expected counts:\", round(expected, 1))\n\n\nExpected counts: 312.8 104.2 104.2 34.8\n\n\n\n\n\n\n22.2.2 Contingency Table Analysis\nWhen we have two categorical variables, we use a contingency table to examine their association. The null hypothesis is that the variables are independent.\n\n\nCode\n# Example: Hair color and eye color association\n# Data from 1000 students\nhair_eye &lt;- matrix(c(347, 191,    # Blue eyes: blonde, brunette\n                     177, 329),   # Brown eyes: blonde, brunette\n                   nrow = 2, byrow = TRUE)\nrownames(hair_eye) &lt;- c(\"Blue_eyes\", \"Brown_eyes\")\ncolnames(hair_eye) &lt;- c(\"Blonde\", \"Brunette\")\n\n# View the contingency table\nhair_eye\n\n\n           Blonde Brunette\nBlue_eyes     347      191\nBrown_eyes    177      329\n\n\nCode\n# Chi-square test of independence\nchisq.test(hair_eye)\n\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  hair_eye\nX-squared = 89.703, df = 1, p-value &lt; 2.2e-16\n\n\nFor contingency tables, \\(df = (r-1)(c-1)\\) where \\(r\\) and \\(c\\) are the number of rows and columns.\n\n\nCode\n# Visualize with a mosaic plot\nmosaicplot(hair_eye, main = \"Hair and Eye Color Association\",\n           color = c(\"gold\", \"brown\"), shade = FALSE)\n\n\n\n\n\n\n\n\n\n\n\n22.2.3 Standardized Residuals\nTo understand where associations are strongest, examine standardized residuals:\n\n\nCode\n# Which cells deviate most from independence?\ntest_result &lt;- chisq.test(hair_eye)\ntest_result$residuals\n\n\n              Blonde  Brunette\nBlue_eyes   4.683940 -4.701920\nBrown_eyes -4.829778  4.848318\n\n\nResiduals &gt; 2 or &lt; -2 indicate cells contributing substantially to the association. Positive residuals mean more observations than expected under independence; negative means fewer.\n\n\n22.2.4 G-Test (Log-Likelihood Ratio Test)\nThe G-test is an alternative to chi-square based on likelihood ratios:\n\\[G = 2 \\sum O_i \\ln\\left(\\frac{O_i}{E_i}\\right)\\]\nThe G-test and chi-square give similar results for large samples, but G-tests are preferred when: - Sample sizes are small - Differences between observed and expected are small - You want to decompose complex tables\n\n\nCode\n# Manual G-test calculation\nobserved_flat &lt;- as.vector(hair_eye)\nexpected_flat &lt;- as.vector(test_result$expected)\nG &lt;- 2 * sum(observed_flat * log(observed_flat / expected_flat))\np_value &lt;- 1 - pchisq(G, df = 1)\n\ncat(\"G statistic:\", round(G, 3), \"\\n\")\n\n\nG statistic: 92.248 \n\n\nCode\ncat(\"p-value:\", format(p_value, scientific = TRUE), \"\\n\")\n\n\np-value: 0e+00 \n\n\n\n\n22.2.5 Odds Ratios: Measuring Effect Size\nThe chi-square test tells us whether variables are associated, but not the strength of association. For 2×2 tables, the odds ratio quantifies effect size.\nThe odds of an event are \\(\\frac{p}{1-p}\\). The odds ratio compares odds between groups:\n\\[OR = \\frac{odds_1}{odds_2} = \\frac{a/b}{c/d} = \\frac{ad}{bc}\\]\n\n\nCode\n# Odds ratio for hair/eye color data\na &lt;- hair_eye[1,1]  # Blue eyes, Blonde\nb &lt;- hair_eye[1,2]  # Blue eyes, Brunette\nc &lt;- hair_eye[2,1]  # Brown eyes, Blonde\nd &lt;- hair_eye[2,2]  # Brown eyes, Brunette\n\nodds_ratio &lt;- (a * d) / (b * c)\ncat(\"Odds Ratio:\", round(odds_ratio, 2), \"\\n\")\n\n\nOdds Ratio: 3.38 \n\n\nAn OR of 3.38 means blue-eyed individuals have about 3.4 times the odds of being blonde compared to brown-eyed individuals.\n\nOR = 1: No association\nOR &gt; 1: Positive association\nOR &lt; 1: Negative association\n\n\n\nCode\n# Confidence interval for odds ratio (using log transform)\nlog_OR &lt;- log(odds_ratio)\nse_log_OR &lt;- sqrt(1/a + 1/b + 1/c + 1/d)\nci_log &lt;- log_OR + c(-1.96, 1.96) * se_log_OR\nci_OR &lt;- exp(ci_log)\n\ncat(\"95% CI for OR: [\", round(ci_OR[1], 2), \",\", round(ci_OR[2], 2), \"]\\n\")\n\n\n95% CI for OR: [ 2.62 , 4.35 ]\n\n\n\n\n22.2.6 Fisher’s Exact Test\nFor small sample sizes (expected counts &lt; 5), Fisher’s exact test is more appropriate. It calculates exact probabilities rather than relying on the chi-square approximation.\n\n\nCode\n# Small sample example\nsmall_table &lt;- matrix(c(3, 1, 1, 3), nrow = 2)\nfisher.test(small_table)\n\n\n\n    Fisher's Exact Test for Count Data\n\ndata:  small_table\np-value = 0.4857\nalternative hypothesis: true odds ratio is not equal to 1\n95 percent confidence interval:\n   0.2117329 621.9337505\nsample estimates:\nodds ratio \n  6.408309 \n\n\nFisher’s test is preferred for 2×2 tables with small samples and provides a confidence interval for the odds ratio.",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Generalized Linear Models</span>"
    ]
  },
  {
    "objectID": "chapters/21-glm.html#components-of-a-glm",
    "href": "chapters/21-glm.html#components-of-a-glm",
    "title": "22  Generalized Linear Models",
    "section": "22.3 Components of a GLM",
    "text": "22.3 Components of a GLM\nGLMs have three components:\nRandom component: Specifies the probability distribution of the response variable (e.g., binomial, Poisson, normal).\nSystematic component: The linear predictor, a linear combination of explanatory variables: \\[\\eta = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\ldots\\]\nLink function: Connects the random and systematic components, transforming the expected value of the response to the scale of the linear predictor.",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Generalized Linear Models</span>"
    ]
  },
  {
    "objectID": "chapters/21-glm.html#the-link-function",
    "href": "chapters/21-glm.html#the-link-function",
    "title": "22  Generalized Linear Models",
    "section": "22.4 The Link Function",
    "text": "22.4 The Link Function\nDifferent distributions use different link functions:\n\n\n\nDistribution\nTypical Link\nLink Function\n\n\n\n\nNormal\nIdentity\n\\(\\eta = \\mu\\)\n\n\nBinomial\nLogit\n\\(\\eta = \\log(\\frac{\\mu}{1-\\mu})\\)\n\n\nPoisson\nLog\n\\(\\eta = \\log(\\mu)\\)",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Generalized Linear Models</span>"
    ]
  },
  {
    "objectID": "chapters/21-glm.html#logistic-regression",
    "href": "chapters/21-glm.html#logistic-regression",
    "title": "22  Generalized Linear Models",
    "section": "22.5 Logistic Regression",
    "text": "22.5 Logistic Regression\nLogistic regression models binary outcomes. The response is 0 or 1 (failure or success), and we model the probability of success as a function of predictors.\nThe logistic function maps the linear predictor to probabilities:\n\\[P(Y = 1 | X) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 X)}}\\]\nEquivalently, we model the log-odds:\n\\[\\log\\left(\\frac{P}{1-P}\\right) = \\beta_0 + \\beta_1 X\\]\n\n\nCode\n# The logistic function\ncurve(1 / (1 + exp(-x)), from = -6, to = 6, \n      xlab = \"Linear Predictor\", ylab = \"Probability\",\n      main = \"The Logistic Function\", lwd = 2, col = \"blue\")",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Generalized Linear Models</span>"
    ]
  },
  {
    "objectID": "chapters/21-glm.html#fitting-logistic-regression",
    "href": "chapters/21-glm.html#fitting-logistic-regression",
    "title": "22  Generalized Linear Models",
    "section": "22.6 Fitting Logistic Regression",
    "text": "22.6 Fitting Logistic Regression\n\n\nCode\n# Example: predicting transmission type from mpg\ndata(mtcars)\nmtcars$am &lt;- factor(mtcars$am, labels = c(\"automatic\", \"manual\"))\n\nlogit_model &lt;- glm(am ~ mpg, data = mtcars, family = binomial)\nsummary(logit_model)\n\n\n\nCall:\nglm(formula = am ~ mpg, family = binomial, data = mtcars)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)   \n(Intercept)  -6.6035     2.3514  -2.808  0.00498 **\nmpg           0.3070     0.1148   2.673  0.00751 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 43.230  on 31  degrees of freedom\nResidual deviance: 29.675  on 30  degrees of freedom\nAIC: 33.675\n\nNumber of Fisher Scoring iterations: 5",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Generalized Linear Models</span>"
    ]
  },
  {
    "objectID": "chapters/21-glm.html#interpreting-logistic-coefficients",
    "href": "chapters/21-glm.html#interpreting-logistic-coefficients",
    "title": "22  Generalized Linear Models",
    "section": "22.7 Interpreting Logistic Coefficients",
    "text": "22.7 Interpreting Logistic Coefficients\nCoefficients are on the log-odds scale. To interpret them:\nExponentiate to get odds ratios:\n\n\nCode\nexp(coef(logit_model))\n\n\n(Intercept)         mpg \n0.001355579 1.359379288 \n\n\nThe odds ratio for mpg (1.36) means that each additional mpg is associated with 36% higher odds of having a manual transmission.",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Generalized Linear Models</span>"
    ]
  },
  {
    "objectID": "chapters/21-glm.html#making-predictions",
    "href": "chapters/21-glm.html#making-predictions",
    "title": "22  Generalized Linear Models",
    "section": "22.8 Making Predictions",
    "text": "22.8 Making Predictions\n\n\nCode\n# Predict probability for specific mpg values\nnew_data &lt;- data.frame(mpg = c(15, 20, 25, 30))\npredict(logit_model, newdata = new_data, type = \"response\")\n\n\n        1         2         3         4 \n0.1194021 0.3862832 0.7450109 0.9313311 \n\n\nThe type = \"response\" argument returns probabilities rather than log-odds.",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Generalized Linear Models</span>"
    ]
  },
  {
    "objectID": "chapters/21-glm.html#multiple-logistic-regression",
    "href": "chapters/21-glm.html#multiple-logistic-regression",
    "title": "22  Generalized Linear Models",
    "section": "22.9 Multiple Logistic Regression",
    "text": "22.9 Multiple Logistic Regression\nLike linear regression, logistic regression can include multiple predictors. This allows us to:\n\nControl for confounding variables\nExamine how multiple factors together predict the outcome\nTest for interactions between predictors\n\n\n\nCode\n# Multiple logistic regression: am ~ mpg + wt + hp\nmulti_logit &lt;- glm(am ~ mpg + wt + hp, data = mtcars, family = binomial)\nsummary(multi_logit)\n\n\n\nCall:\nglm(formula = am ~ mpg + wt + hp, family = binomial, data = mtcars)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)  \n(Intercept) -15.72137   40.00281  -0.393   0.6943  \nmpg           1.22930    1.58109   0.778   0.4369  \nwt           -6.95492    3.35297  -2.074   0.0381 *\nhp            0.08389    0.08228   1.020   0.3079  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 43.2297  on 31  degrees of freedom\nResidual deviance:  8.7661  on 28  degrees of freedom\nAIC: 16.766\n\nNumber of Fisher Scoring iterations: 10\n\n\n\n\nCode\n# Odds ratios for all predictors\nexp(coef(multi_logit))\n\n\n (Intercept)          mpg           wt           hp \n1.486947e-07 3.418843e+00 9.539266e-04 1.087513e+00 \n\n\nNotice how coefficients change compared to the simple model—this is the effect of controlling for other variables.\n\n\nCode\n# Compare models with likelihood ratio test\nanova(logit_model, multi_logit, test = \"Chisq\")\n\n\nAnalysis of Deviance Table\n\nModel 1: am ~ mpg\nModel 2: am ~ mpg + wt + hp\n  Resid. Df Resid. Dev Df Deviance  Pr(&gt;Chi)    \n1        30    29.6752                          \n2        28     8.7661  2   20.909 2.882e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe likelihood ratio test compares nested models. A significant p-value indicates the fuller model fits significantly better.",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Generalized Linear Models</span>"
    ]
  },
  {
    "objectID": "chapters/21-glm.html#poisson-regression",
    "href": "chapters/21-glm.html#poisson-regression",
    "title": "22  Generalized Linear Models",
    "section": "22.10 Poisson Regression",
    "text": "22.10 Poisson Regression\nPoisson regression models count data—the number of events in a fixed period or area. The response must be non-negative integers, and we assume events occur independently at a constant rate.\n\\[\\log(\\mu) = \\beta_0 + \\beta_1 X\\]\n\n\nCode\n# Example: modeling count data\nset.seed(42)\nexposure &lt;- runif(100, 1, 10)\ncounts &lt;- rpois(100, lambda = exp(0.5 + 0.3 * exposure))\n\npois_model &lt;- glm(counts ~ exposure, family = poisson)\nsummary(pois_model)\n\n\n\nCall:\nglm(formula = counts ~ exposure, family = poisson)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  0.42499    0.10624    4.00 6.32e-05 ***\nexposure     0.30714    0.01345   22.84  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 744.110  on 99  degrees of freedom\nResidual deviance:  97.826  on 98  degrees of freedom\nAIC: 498.52\n\nNumber of Fisher Scoring iterations: 4",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Generalized Linear Models</span>"
    ]
  },
  {
    "objectID": "chapters/21-glm.html#overdispersion",
    "href": "chapters/21-glm.html#overdispersion",
    "title": "22  Generalized Linear Models",
    "section": "22.11 Overdispersion",
    "text": "22.11 Overdispersion\nA key assumption of Poisson regression is that the mean equals the variance. When variance exceeds the mean (overdispersion), standard errors are underestimated and p-values become too small.\n\n\nCode\n# Check for overdispersion\n# Ratio of residual deviance to df should be near 1\ndispersion_ratio &lt;- pois_model$deviance / pois_model$df.residual\ncat(\"Dispersion ratio:\", round(dispersion_ratio, 3), \"\\n\")\n\n\nDispersion ratio: 0.998 \n\n\nCode\ncat(\"Values &gt; 1.5 suggest overdispersion\\n\")\n\n\nValues &gt; 1.5 suggest overdispersion\n\n\n\n22.11.1 Handling Overdispersion\nQuasi-Poisson estimates the dispersion parameter from the data rather than assuming it equals 1:\n\n\nCode\n# Create overdispersed data for demonstration\nset.seed(42)\nn &lt;- 100\nx &lt;- runif(n, 1, 10)\n# Generate overdispersed counts (negative binomial acts like overdispersed Poisson)\ny_overdispersed &lt;- rnbinom(n, size = 2, mu = exp(0.5 + 0.3 * x))\n\n# Standard Poisson (ignores overdispersion)\npois_fit &lt;- glm(y_overdispersed ~ x, family = poisson)\n\n# Quasi-Poisson (accounts for overdispersion)\nquasi_fit &lt;- glm(y_overdispersed ~ x, family = quasipoisson)\n\n# Compare standard errors\ncat(\"Poisson SE:\", round(summary(pois_fit)$coefficients[2, 2], 4), \"\\n\")\n\n\nPoisson SE: 0.0129 \n\n\nCode\ncat(\"Quasi-Poisson SE:\", round(summary(quasi_fit)$coefficients[2, 2], 4), \"\\n\")\n\n\nQuasi-Poisson SE: 0.0327 \n\n\nCode\ncat(\"SE inflation factor:\", round(summary(quasi_fit)$coefficients[2, 2] /\n                                   summary(pois_fit)$coefficients[2, 2], 2), \"\\n\")\n\n\nSE inflation factor: 2.53 \n\n\nSimilarly, quasibinomial handles overdispersion in binomial data:\n\n\nCode\n# Quasibinomial example\n# family = quasibinomial adjusts for extra-binomial variation\n\n\n\n\n\n\n\n\nWhen to Use Quasi-Likelihood\n\n\n\nUse quasipoisson or quasibinomial when:\n\nDispersion ratio is substantially &gt; 1 (overdispersion)\nYou don’t need AIC for model comparison (quasi-models don’t have AIC)\nThe basic model structure is correct but variance assumptions are violated\n\nFor severe overdispersion, consider negative binomial regression (package MASS) which models overdispersion explicitly.",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Generalized Linear Models</span>"
    ]
  },
  {
    "objectID": "chapters/21-glm.html#model-assessment",
    "href": "chapters/21-glm.html#model-assessment",
    "title": "22  Generalized Linear Models",
    "section": "22.12 Model Assessment",
    "text": "22.12 Model Assessment\nGLMs use deviance rather than R² to assess fit. Deviance compares the fitted model to a saturated model (one parameter per observation).\nNull deviance: Deviance with only the intercept Residual deviance: Deviance of the fitted model\nA large drop from null to residual deviance indicates the predictors explain substantial variation.\n\n\nCode\n# Compare deviances\nwith(logit_model, null.deviance - deviance)\n\n\n[1] 13.55457\n\n\nCode\n# Chi-square test for improvement\nwith(logit_model, pchisq(null.deviance - deviance, \n                         df.null - df.residual, \n                         lower.tail = FALSE))\n\n\n[1] 0.0002317271",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Generalized Linear Models</span>"
    ]
  },
  {
    "objectID": "chapters/21-glm.html#model-comparison-with-aic",
    "href": "chapters/21-glm.html#model-comparison-with-aic",
    "title": "22  Generalized Linear Models",
    "section": "22.13 Model Comparison with AIC",
    "text": "22.13 Model Comparison with AIC\nAs with linear models, AIC helps compare GLMs:\n\n\nCode\n# Compare models\nmodel1 &lt;- glm(am ~ mpg, data = mtcars, family = binomial)\nmodel2 &lt;- glm(am ~ mpg + wt, data = mtcars, family = binomial)\nmodel3 &lt;- glm(am ~ mpg * wt, data = mtcars, family = binomial)\n\nAIC(model1, model2, model3)\n\n\n       df      AIC\nmodel1  2 33.67517\nmodel2  3 23.18426\nmodel3  4 24.49947",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Generalized Linear Models</span>"
    ]
  },
  {
    "objectID": "chapters/21-glm.html#assumptions-and-diagnostics",
    "href": "chapters/21-glm.html#assumptions-and-diagnostics",
    "title": "22  Generalized Linear Models",
    "section": "22.14 Assumptions and Diagnostics",
    "text": "22.14 Assumptions and Diagnostics\nGLM assumptions include: - Correct specification of the distribution - Correct link function - Independence of observations - No extreme multicollinearity\nDiagnostic tools include: - Residual plots (deviance or Pearson residuals) - Influence measures - Goodness-of-fit tests\n\n\nCode\npar(mfrow = c(1, 2))\nplot(logit_model, which = c(1, 2))",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Generalized Linear Models</span>"
    ]
  },
  {
    "objectID": "chapters/21-glm.html#summary",
    "href": "chapters/21-glm.html#summary",
    "title": "22  Generalized Linear Models",
    "section": "22.15 Summary",
    "text": "22.15 Summary\nGLMs provide a flexible framework for modeling non-normal response variables while maintaining the interpretability of linear models. Logistic regression for binary outcomes and Poisson regression for counts are the most common applications, but the framework extends to other distributions as needed.",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Generalized Linear Models</span>"
    ]
  },
  {
    "objectID": "chapters/22-statistical-learning.html",
    "href": "chapters/22-statistical-learning.html",
    "title": "23  Statistical Learning",
    "section": "",
    "text": "23.1 From Inference to Prediction\nTraditional statistics emphasizes inference—understanding relationships, testing hypotheses, and quantifying uncertainty. Statistical learning (or machine learning) shifts focus toward prediction—building models that accurately predict outcomes for new data.\nBoth approaches use similar mathematical tools, but the goals differ. In inference, we want to understand the true relationship between variables. In prediction, we want accurate predictions, even if the model does not perfectly capture the underlying mechanism.",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Statistical Learning</span>"
    ]
  },
  {
    "objectID": "chapters/22-statistical-learning.html#the-overfitting-problem",
    "href": "chapters/22-statistical-learning.html#the-overfitting-problem",
    "title": "23  Statistical Learning",
    "section": "23.2 The Overfitting Problem",
    "text": "23.2 The Overfitting Problem\nModels are built to fit training data as closely as possible. A linear regression minimizes squared errors; a logistic regression maximizes likelihood. But models that fit training data too well often predict poorly on new data.\nOverfitting occurs when a model captures noise specific to the training data rather than the true underlying pattern. Complex models with many parameters are especially susceptible.\nThe solution is to evaluate models on data they have not seen—held-out test data or through cross-validation.",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Statistical Learning</span>"
    ]
  },
  {
    "objectID": "chapters/22-statistical-learning.html#cross-validation",
    "href": "chapters/22-statistical-learning.html#cross-validation",
    "title": "23  Statistical Learning",
    "section": "23.3 Cross-Validation",
    "text": "23.3 Cross-Validation\nCross-validation estimates how well a model will generalize to new data.\nK-fold cross-validation: 1. Split data into k roughly equal parts (folds) 2. For each fold: train on k-1 folds, test on the held-out fold 3. Average performance across all folds\n\n\nCode\n# Simple CV example with linear regression\nlibrary(boot)\n\n# Generate data\nset.seed(42)\nx &lt;- rnorm(100)\ny &lt;- 2 + 3*x + rnorm(100)\ndata &lt;- data.frame(x, y)\n\n# Fit model and perform CV\nmodel &lt;- glm(y ~ x, data = data)\n\n# 10-fold cross-validation\ncv_result &lt;- cv.glm(data, model, K = 10)\ncat(\"CV estimate of prediction error:\", round(cv_result$delta[1], 3), \"\\n\")\n\n\nCV estimate of prediction error: 0.846 \n\n\nLeave-one-out cross-validation (LOOCV) is k-fold with k = n: each observation is held out once. More computationally expensive but lower variance.",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Statistical Learning</span>"
    ]
  },
  {
    "objectID": "chapters/22-statistical-learning.html#bias-variance-tradeoff",
    "href": "chapters/22-statistical-learning.html#bias-variance-tradeoff",
    "title": "23  Statistical Learning",
    "section": "23.4 Bias-Variance Tradeoff",
    "text": "23.4 Bias-Variance Tradeoff\nPrediction error has two components:\nBias: Error from approximating a complex reality with a simpler model. Simple models have high bias—they may miss important patterns.\nVariance: Error from sensitivity to training data. Complex models have high variance—they change substantially with different training samples.\nThe best predictions come from models that balance bias and variance. As model complexity increases, bias decreases but variance increases. The optimal complexity minimizes total prediction error.",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Statistical Learning</span>"
    ]
  },
  {
    "objectID": "chapters/22-statistical-learning.html#loess-flexible-non-parametric-smoothing",
    "href": "chapters/22-statistical-learning.html#loess-flexible-non-parametric-smoothing",
    "title": "23  Statistical Learning",
    "section": "23.5 LOESS: Flexible Non-Parametric Smoothing",
    "text": "23.5 LOESS: Flexible Non-Parametric Smoothing\nLOESS (Locally Estimated Scatterplot Smoothing) fits local regressions to subsets of data, weighted by distance from each point.\n\n\nCode\n# Compare linear regression and LOESS\nset.seed(123)\nx &lt;- seq(0, 4*pi, length.out = 100)\ny &lt;- sin(x) + rnorm(100, sd = 0.3)\n\nplot(x, y, pch = 16, col = \"gray60\", main = \"Linear vs LOESS\")\nabline(lm(y ~ x), col = \"red\", lwd = 2)\nlines(x, predict(loess(y ~ x, span = 0.3)), col = \"blue\", lwd = 2)\nlegend(\"topright\", c(\"Linear\", \"LOESS\"), col = c(\"red\", \"blue\"), lwd = 2)\n\n\n\n\n\n\n\n\n\nThe span parameter controls smoothness: smaller values fit more locally (more flexible), larger values average more broadly (smoother).",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Statistical Learning</span>"
    ]
  },
  {
    "objectID": "chapters/22-statistical-learning.html#classification",
    "href": "chapters/22-statistical-learning.html#classification",
    "title": "23  Statistical Learning",
    "section": "23.6 Classification",
    "text": "23.6 Classification\nWhen the response is categorical, we have a classification problem rather than regression. The goal is to predict which category an observation belongs to.\nLogistic regression produces probabilities that can be converted to class predictions.\nDecision trees recursively partition the feature space based on simple rules.\nRandom forests combine many decision trees for more robust predictions.",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Statistical Learning</span>"
    ]
  },
  {
    "objectID": "chapters/22-statistical-learning.html#confusion-matrices",
    "href": "chapters/22-statistical-learning.html#confusion-matrices",
    "title": "23  Statistical Learning",
    "section": "23.7 Confusion Matrices",
    "text": "23.7 Confusion Matrices\nClassification performance is evaluated with a confusion matrix:\n\n\n\n\nPredicted Positive\nPredicted Negative\n\n\n\n\nActual Positive\nTrue Positive (TP)\nFalse Negative (FN)\n\n\nActual Negative\nFalse Positive (FP)\nTrue Negative (TN)\n\n\n\nKey metrics: - Accuracy: (TP + TN) / Total - Sensitivity (Recall): TP / (TP + FN) — how many positives were caught - Specificity: TN / (TN + FP) — how many negatives were correctly identified - Precision: TP / (TP + FP) — among positive predictions, how many were correct",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Statistical Learning</span>"
    ]
  },
  {
    "objectID": "chapters/22-statistical-learning.html#decision-trees",
    "href": "chapters/22-statistical-learning.html#decision-trees",
    "title": "23  Statistical Learning",
    "section": "23.8 Decision Trees",
    "text": "23.8 Decision Trees\nDecision trees make predictions by asking a series of yes/no questions about the features:\n\n\nCode\nlibrary(rpart)\nlibrary(rpart.plot)\n\n# Build a simple decision tree\ndata(iris)\ntree_model &lt;- rpart(Species ~ Sepal.Length + Sepal.Width, data = iris)\nrpart.plot(tree_model)\n\n\n\n\n\n\n\n\n\nTrees are interpretable but prone to overfitting. Pruning (removing branches) or using ensembles helps.",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Statistical Learning</span>"
    ]
  },
  {
    "objectID": "chapters/22-statistical-learning.html#random-forests",
    "href": "chapters/22-statistical-learning.html#random-forests",
    "title": "23  Statistical Learning",
    "section": "23.9 Random Forests",
    "text": "23.9 Random Forests\nRandom forests improve on single trees by: 1. Building many trees on bootstrap samples (bagging) 2. Using a random subset of features at each split 3. Averaging predictions across all trees\n\n\nCode\nlibrary(randomForest)\n\nrf_model &lt;- randomForest(Species ~ ., data = iris, ntree = 100)\nrf_model\n\n\n\nCall:\n randomForest(formula = Species ~ ., data = iris, ntree = 100) \n               Type of random forest: classification\n                     Number of trees: 100\nNo. of variables tried at each split: 2\n\n        OOB estimate of  error rate: 6%\nConfusion matrix:\n           setosa versicolor virginica class.error\nsetosa         50          0         0        0.00\nversicolor      0         47         3        0.06\nvirginica       0          6        44        0.12\n\n\nCode\n# Variable importance\nvarImpPlot(rf_model)",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Statistical Learning</span>"
    ]
  },
  {
    "objectID": "chapters/22-statistical-learning.html#practical-workflow",
    "href": "chapters/22-statistical-learning.html#practical-workflow",
    "title": "23  Statistical Learning",
    "section": "23.10 Practical Workflow",
    "text": "23.10 Practical Workflow\nA typical statistical learning workflow:\n\nSplit data into training and test sets\nExplore the training data\nBuild candidate models with different algorithms or parameters\nEvaluate using cross-validation on training data\nSelect the best model\nFinal evaluation on held-out test data\nReport honest estimates of performance\n\nNever use test data for model building or selection—that defeats the purpose of holding it out.",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Statistical Learning</span>"
    ]
  },
  {
    "objectID": "chapters/22-statistical-learning.html#when-to-use-statistical-learning",
    "href": "chapters/22-statistical-learning.html#when-to-use-statistical-learning",
    "title": "23  Statistical Learning",
    "section": "23.11 When to Use Statistical Learning",
    "text": "23.11 When to Use Statistical Learning\nStatistical learning excels when: - Prediction is the primary goal - Relationships are complex or non-linear - You have substantial data - Interpretability is less critical\nTraditional statistical methods may be preferable when: - Understanding relationships matters more than prediction - Sample sizes are small - You need confidence intervals and hypothesis tests - Interpretability is essential",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Statistical Learning</span>"
    ]
  },
  {
    "objectID": "chapters/22-statistical-learning.html#connection-to-dimensionality-reduction",
    "href": "chapters/22-statistical-learning.html#connection-to-dimensionality-reduction",
    "title": "23  Statistical Learning",
    "section": "23.12 Connection to Dimensionality Reduction",
    "text": "23.12 Connection to Dimensionality Reduction\nHigh-dimensional data often benefit from dimensionality reduction before applying statistical learning methods. Techniques like PCA, clustering, and discriminant analysis are covered in detail in Chapter 24.",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Statistical Learning</span>"
    ]
  },
  {
    "objectID": "chapters/22-statistical-learning.html#summary",
    "href": "chapters/22-statistical-learning.html#summary",
    "title": "23  Statistical Learning",
    "section": "23.13 Summary",
    "text": "23.13 Summary\nStatistical learning provides powerful tools for prediction and pattern discovery:\n\nOverfitting is the central challenge—models that fit training data too well predict poorly\nCross-validation provides honest estimates of predictive performance\nThe bias-variance tradeoff governs model complexity choices\nLOESS offers flexible non-parametric smoothing\nClassification methods (decision trees, random forests) handle categorical outcomes\nConfusion matrices summarize classification performance\nThe choice between traditional statistics and machine learning depends on goals",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Statistical Learning</span>"
    ]
  },
  {
    "objectID": "chapters/22-statistical-learning.html#additional-resources",
    "href": "chapters/22-statistical-learning.html#additional-resources",
    "title": "23  Statistical Learning",
    "section": "23.14 Additional Resources",
    "text": "23.14 Additional Resources\n\nJames et al. (2023) - The standard introduction to statistical learning\nThulin (2025) - Modern perspectives on statistics with R\nCrawley (2007) - Practical statistical methods in R\n\n\n\n\n\n\n\nCrawley, Michael J. 2007. The r Book. John Wiley & Sons.\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2023. An Introduction to Statistical Learning with Applications in r. 2nd ed. Springer. https://www.statlearning.com.\n\n\nThulin, Måns. 2025. Modern Statistics with r. CRC Press. https://www.modernstatisticswithr.com.",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Statistical Learning</span>"
    ]
  },
  {
    "objectID": "chapters/23-dimensionality-reduction.html",
    "href": "chapters/23-dimensionality-reduction.html",
    "title": "24  Dimensionality Reduction and Multivariate Methods",
    "section": "",
    "text": "24.1 The Challenge of High-Dimensional Data\nModern biology generates datasets with many variables: gene expression across thousands of genes, metabolomic profiles with hundreds of compounds, morphological measurements on many traits. When datasets have many variables, visualization becomes impossible and statistical analysis becomes complicated.\nDimensionality reduction creates a smaller set of new variables that capture most of the information in the original data. These techniques help us:",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Dimensionality Reduction and Multivariate Methods</span>"
    ]
  },
  {
    "objectID": "chapters/23-dimensionality-reduction.html#the-challenge-of-high-dimensional-data",
    "href": "chapters/23-dimensionality-reduction.html#the-challenge-of-high-dimensional-data",
    "title": "24  Dimensionality Reduction and Multivariate Methods",
    "section": "",
    "text": "Visualize high-dimensional data in 2D or 3D\nIdentify patterns and clusters\nRemove noise and redundancy\nCreate composite variables for downstream analysis",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Dimensionality Reduction and Multivariate Methods</span>"
    ]
  },
  {
    "objectID": "chapters/23-dimensionality-reduction.html#principal-component-analysis-pca",
    "href": "chapters/23-dimensionality-reduction.html#principal-component-analysis-pca",
    "title": "24  Dimensionality Reduction and Multivariate Methods",
    "section": "24.2 Principal Component Analysis (PCA)",
    "text": "24.2 Principal Component Analysis (PCA)\nPrincipal Component Analysis (PCA) is the most widely used dimensionality reduction technique. It finds new variables (principal components) that are linear combinations of the originals, chosen to capture maximum variance.\n\n24.2.1 The Eigenanalysis Foundation\nThe key insight involves eigenanalysis: decomposing the covariance (or correlation) matrix of variables to find directions of maximum variation.\nGiven a covariance matrix \\(\\Sigma\\), eigenanalysis finds:\n\nEigenvectors: The directions of the principal components\nEigenvalues: The variance explained by each component\n\nThe first principal component points in the direction of maximum variance. Each subsequent component is orthogonal (uncorrelated) and captures remaining variance in decreasing order.\n\n\n\n\n\n\n\n24.2.2 PCA in R\n\n\nCode\n# PCA on iris data\niris_pca &lt;- prcomp(iris[, 1:4], scale. = TRUE)\n\n# Variance explained\nsummary(iris_pca)\n\n\nImportance of components:\n                          PC1    PC2     PC3     PC4\nStandard deviation     1.7084 0.9560 0.38309 0.14393\nProportion of Variance 0.7296 0.2285 0.03669 0.00518\nCumulative Proportion  0.7296 0.9581 0.99482 1.00000\n\n\nCode\n# Scree plot\npar(mfrow = c(1, 2))\nplot(iris_pca, type = \"l\", main = \"Scree Plot\")\n\n# PC scores colored by species\nplot(iris_pca$x[, 1:2],\n     col = c(\"red\", \"green\", \"blue\")[iris$Species],\n     pch = 19, xlab = \"PC1\", ylab = \"PC2\",\n     main = \"PCA of Iris Data\")\nlegend(\"topright\", levels(iris$Species),\n       col = c(\"red\", \"green\", \"blue\"), pch = 19)\n\n\n\n\n\n\n\n\n\n\n\n24.2.3 Loadings: What Variables Drive Each PC?\nEach principal component is defined by its loadings—the coefficients showing how much each original variable contributes:\n\n\nCode\n# Loadings (rotation matrix)\niris_pca$rotation\n\n\n                    PC1         PC2        PC3        PC4\nSepal.Length  0.5210659 -0.37741762  0.7195664  0.2612863\nSepal.Width  -0.2693474 -0.92329566 -0.2443818 -0.1235096\nPetal.Length  0.5804131 -0.02449161 -0.1421264 -0.8014492\nPetal.Width   0.5648565 -0.06694199 -0.6342727  0.5235971\n\n\nLarge absolute loadings indicate that a variable strongly influences that component. The sign indicates the direction of the relationship.\n\n\n24.2.4 Interpreting PCA Results\nKey elements of PCA output:\n\nEigenvalues: Variance explained by each component (shown in scree plot)\nProportion of variance: How much of total variance each PC captures\nLoadings: Coefficients relating original variables to PCs\nScores: Values of the new variables for each observation\n\nThe first few PCs often capture most of the meaningful variation, allowing you to reduce many variables to just 2-3 for visualization and analysis.\n\n\n\n\n\n\nHow Many Components to Keep?\n\n\n\nCommon approaches:\n\nKeep components with eigenvalues &gt; 1 (Kaiser criterion)\nKeep enough to explain 80-90% of variance\nLook for an “elbow” in the scree plot\nUse cross-validation if using PCs for prediction\n\n\n\n\n\n24.2.5 Biplot Visualization\nA biplot shows both observations (scores) and variables (loadings) on the same plot:\n\n\nCode\nbiplot(iris_pca, col = c(\"gray50\", \"red\"), cex = 0.7,\n       main = \"PCA Biplot of Iris Data\")\n\n\n\n\n\n\n\n\n\nArrows show variable loadings—their direction and length indicate how each variable relates to the principal components.",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Dimensionality Reduction and Multivariate Methods</span>"
    ]
  },
  {
    "objectID": "chapters/23-dimensionality-reduction.html#principal-coordinate-analysis-pcoa",
    "href": "chapters/23-dimensionality-reduction.html#principal-coordinate-analysis-pcoa",
    "title": "24  Dimensionality Reduction and Multivariate Methods",
    "section": "24.3 Principal Coordinate Analysis (PCoA)",
    "text": "24.3 Principal Coordinate Analysis (PCoA)\nWhile PCA uses correlations among variables, Principal Coordinate Analysis (PCoA) (also called Metric Multidimensional Scaling) starts with a dissimilarity matrix among observations. This is valuable when:\n\nYou have a meaningful distance metric (e.g., genetic distances)\nVariables are mixed types or non-numeric\nThe data are counts (e.g., microbiome data)\n\n\n\nCode\n# PCoA example using Euclidean distances\ndist_matrix &lt;- dist(iris[, 1:4])\npcoa_result &lt;- cmdscale(dist_matrix, k = 2, eig = TRUE)\n\n# Proportion of variance explained\neig_vals &lt;- pcoa_result$eig[pcoa_result$eig &gt; 0]\nvar_explained &lt;- eig_vals / sum(eig_vals)\ncat(\"Variance explained by first two axes:\",\n    round(sum(var_explained[1:2]) * 100, 1), \"%\\n\")\n\n\nVariance explained by first two axes: 97.8 %\n\n\nCode\n# Plot\nplot(pcoa_result$points,\n     col = c(\"red\", \"green\", \"blue\")[iris$Species],\n     pch = 19,\n     xlab = paste0(\"PCoA1 (\", round(var_explained[1]*100, 1), \"%)\"),\n     ylab = paste0(\"PCoA2 (\", round(var_explained[2]*100, 1), \"%)\"),\n     main = \"PCoA of Iris Data\")\nlegend(\"topright\", levels(iris$Species),\n       col = c(\"red\", \"green\", \"blue\"), pch = 19)\n\n\n\n\n\n\n\n\n\n\n24.3.1 When to Use PCoA vs. PCA\n\nPCA: Variables are measured on a common scale; interested in variable contributions\nPCoA: Have a distance matrix; want to preserve distances among samples\nFor Euclidean distances, PCA and PCoA give equivalent results",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Dimensionality Reduction and Multivariate Methods</span>"
    ]
  },
  {
    "objectID": "chapters/23-dimensionality-reduction.html#non-metric-multidimensional-scaling-nmds",
    "href": "chapters/23-dimensionality-reduction.html#non-metric-multidimensional-scaling-nmds",
    "title": "24  Dimensionality Reduction and Multivariate Methods",
    "section": "24.4 Non-Metric Multidimensional Scaling (NMDS)",
    "text": "24.4 Non-Metric Multidimensional Scaling (NMDS)\nNMDS is an ordination technique that preserves rank-order of distances rather than exact distances. It’s widely used in ecology because it makes no assumptions about the data distribution.\n\n\nCode\n# NMDS example\nlibrary(vegan)\nnmds_result &lt;- metaMDS(iris[, 1:4], k = 2, trymax = 100, trace = FALSE)\n\n# Stress value indicates fit (&lt; 0.1 is good, &lt; 0.2 is acceptable)\ncat(\"Stress:\", round(nmds_result$stress, 3), \"\\n\")\n\n\nStress: 0.038 \n\n\nCode\nplot(nmds_result$points,\n     col = c(\"red\", \"green\", \"blue\")[iris$Species],\n     pch = 19, xlab = \"NMDS1\", ylab = \"NMDS2\",\n     main = \"NMDS of Iris Data\")\nlegend(\"topright\", levels(iris$Species),\n       col = c(\"red\", \"green\", \"blue\"), pch = 19)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMetric vs. Non-Metric Methods\n\n\n\nPCA and metric PCoA produce scores on a ratio scale—differences between scores are meaningful. These can be used directly in linear models.\nNon-metric multidimensional scaling (NMDS) produces ordinal rankings only. NMDS scores should not be used in parametric analyses like ANOVA or regression.",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Dimensionality Reduction and Multivariate Methods</span>"
    ]
  },
  {
    "objectID": "chapters/23-dimensionality-reduction.html#cluster-analysis",
    "href": "chapters/23-dimensionality-reduction.html#cluster-analysis",
    "title": "24  Dimensionality Reduction and Multivariate Methods",
    "section": "24.5 Cluster Analysis",
    "text": "24.5 Cluster Analysis\nCluster analysis groups observations based on their similarity. Unlike PCA, which creates new continuous variables, clustering assigns observations to discrete groups.\n\n24.5.1 Hierarchical Clustering\nHierarchical clustering builds a tree (dendrogram) of nested clusters. At each step, it either combines the most similar observations/clusters (agglomerative) or splits the most dissimilar (divisive).\n\n\nCode\n# Hierarchical clustering\niris_scaled &lt;- scale(iris[, 1:4])\nhc &lt;- hclust(dist(iris_scaled), method = \"complete\")\nplot(hc, labels = FALSE, main = \"Hierarchical Clustering of Iris\")\nrect.hclust(hc, k = 3, border = \"red\")\n\n\n\n\n\n\n\n\n\nDifferent linkage methods define how cluster distances are calculated:\n\nComplete: Maximum distance between points in different clusters\nSingle: Minimum distance (tends to chain)\nAverage: Mean distance between all pairs\nWard’s: Minimizes within-cluster variance\n\n\n\n24.5.2 K-Means Clustering\nK-means partitions data into K groups by minimizing within-cluster variance. It requires specifying K in advance.\n\n\nCode\nset.seed(42)\nkm &lt;- kmeans(iris_scaled, centers = 3, nstart = 20)\n\n# Compare to true species\ntable(Cluster = km$cluster, Species = iris$Species)\n\n\n       Species\nCluster setosa versicolor virginica\n      1     50          0         0\n      2      0         39        14\n      3      0         11        36\n\n\nThe nstart parameter runs the algorithm multiple times with different starting points, reducing sensitivity to initialization.\n\n\n24.5.3 Choosing the Number of Clusters\nSeveral methods help determine the optimal number of clusters:\n\n\nCode\n# Elbow method: look for bend in within-cluster sum of squares\nwss &lt;- sapply(1:10, function(k) {\n  kmeans(iris_scaled, centers = k, nstart = 20)$tot.withinss\n})\n\npar(mfrow = c(1, 2))\nplot(1:10, wss, type = \"b\", pch = 19,\n     xlab = \"Number of Clusters\", ylab = \"Within-cluster SS\",\n     main = \"Elbow Method\")\n\n# Silhouette method\nlibrary(cluster)\nsil_width &lt;- sapply(2:10, function(k) {\n  km &lt;- kmeans(iris_scaled, centers = k, nstart = 20)\n  mean(silhouette(km$cluster, dist(iris_scaled))[, \"sil_width\"])\n})\nplot(2:10, sil_width, type = \"b\", pch = 19,\n     xlab = \"Number of Clusters\", ylab = \"Average Silhouette Width\",\n     main = \"Silhouette Method\")",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Dimensionality Reduction and Multivariate Methods</span>"
    ]
  },
  {
    "objectID": "chapters/23-dimensionality-reduction.html#manova-multivariate-analysis-of-variance",
    "href": "chapters/23-dimensionality-reduction.html#manova-multivariate-analysis-of-variance",
    "title": "24  Dimensionality Reduction and Multivariate Methods",
    "section": "24.6 MANOVA: Multivariate Analysis of Variance",
    "text": "24.6 MANOVA: Multivariate Analysis of Variance\nWhen you have multiple response variables and want to test for group differences, MANOVA (Multivariate Analysis of Variance) is the appropriate technique. It extends ANOVA to multiple dependent variables simultaneously.\n\n24.6.1 Why Not Multiple ANOVAs?\nRunning separate ANOVAs on each variable:\n\nIgnores correlations among response variables\nInflates Type I error rate with multiple tests\nMay miss differences only apparent when variables are considered together\n\nMANOVA tests whether group centroids differ in multivariate space.\n\n\n24.6.2 The MANOVA Framework\nMANOVA decomposes the total multivariate variation:\n\\[\\mathbf{T} = \\mathbf{H} + \\mathbf{E}\\]\nwhere:\n\nT: Total sum of squares and cross-products matrix\nH: Hypothesis (between-groups) matrix\nE: Error (within-groups) matrix\n\nThese are matrices because we have multiple response variables.\n\n\n\n\n\n\n\n24.6.3 Test Statistics\nSeveral test statistics exist for MANOVA, each a function of the eigenvalues of \\(\\mathbf{HE}^{-1}\\):\n\n\n\n\n\n\n\nStatistic\nDescription\n\n\n\n\nWilks’ Lambda (Λ)\nProduct of 1/(1+λᵢ); most commonly used\n\n\nHotelling-Lawley Trace\nSum of eigenvalues\n\n\nPillai’s Trace\nSum of λᵢ/(1+λᵢ); most robust\n\n\nRoy’s Largest Root\nMaximum eigenvalue; most powerful but sensitive\n\n\n\nPillai’s Trace is generally recommended because it’s most robust to violations of assumptions.\n\n\n24.6.4 MANOVA in R\n\n\nCode\n# MANOVA on iris data\nmanova_model &lt;- manova(cbind(Sepal.Length, Sepal.Width,\n                              Petal.Length, Petal.Width) ~ Species,\n                       data = iris)\n\n# Summary with different test statistics\nsummary(manova_model, test = \"Pillai\")\n\n\n           Df Pillai approx F num Df den Df    Pr(&gt;F)    \nSpecies     2 1.1919   53.466      8    290 &lt; 2.2e-16 ***\nResiduals 147                                            \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCode\nsummary(manova_model, test = \"Wilks\")\n\n\n           Df    Wilks approx F num Df den Df    Pr(&gt;F)    \nSpecies     2 0.023439   199.15      8    288 &lt; 2.2e-16 ***\nResiduals 147                                              \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe significant result tells us that species differ in their multivariate centroid—the combination of all four measurements.\n\n\n24.6.5 Follow-Up Analyses\nA significant MANOVA should be followed by:\n\nUnivariate ANOVAs to see which variables differ\nDiscriminant Function Analysis to understand how groups differ\n\n\n\nCode\n# Univariate follow-ups\nsummary.aov(manova_model)\n\n\n Response Sepal.Length :\n             Df Sum Sq Mean Sq F value    Pr(&gt;F)    \nSpecies       2 63.212  31.606  119.26 &lt; 2.2e-16 ***\nResiduals   147 38.956   0.265                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n Response Sepal.Width :\n             Df Sum Sq Mean Sq F value    Pr(&gt;F)    \nSpecies       2 11.345  5.6725   49.16 &lt; 2.2e-16 ***\nResiduals   147 16.962  0.1154                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n Response Petal.Length :\n             Df Sum Sq Mean Sq F value    Pr(&gt;F)    \nSpecies       2 437.10 218.551  1180.2 &lt; 2.2e-16 ***\nResiduals   147  27.22   0.185                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n Response Petal.Width :\n             Df Sum Sq Mean Sq F value    Pr(&gt;F)    \nSpecies       2 80.413  40.207  960.01 &lt; 2.2e-16 ***\nResiduals   147  6.157   0.042                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n24.6.6 MANOVA Assumptions\nMANOVA assumes:\n\nMultivariate normality within groups\nHomogeneity of covariance matrices across groups\nIndependence of observations\nNo multicollinearity among response variables\n\nTest homogeneity of covariance matrices with Box’s M test (though it’s sensitive to non-normality):\n\n\nCode\n# Box's M test (requires biotools package)\n# library(biotools)\n# boxM(iris[, 1:4], iris$Species)",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Dimensionality Reduction and Multivariate Methods</span>"
    ]
  },
  {
    "objectID": "chapters/23-dimensionality-reduction.html#discriminant-function-analysis-dfa",
    "href": "chapters/23-dimensionality-reduction.html#discriminant-function-analysis-dfa",
    "title": "24  Dimensionality Reduction and Multivariate Methods",
    "section": "24.7 Discriminant Function Analysis (DFA)",
    "text": "24.7 Discriminant Function Analysis (DFA)\nDiscriminant Function Analysis (DFA, also called Linear Discriminant Analysis or LDA) finds linear combinations of variables that best separate groups. It complements MANOVA by showing how groups differ.\n\n24.7.1 The Goal of DFA\nDFA finds discriminant functions—weighted combinations of original variables—that maximize separation between groups while minimizing variation within groups.\nThe first discriminant function captures the most separation, the second captures remaining separation orthogonal to the first, and so on.\n\n\n\n\n\n\n\n24.7.2 DFA in R\n\n\nCode\n# Linear Discriminant Analysis\nlda_model &lt;- lda(Species ~ ., data = iris)\n\n# View the model\nlda_model\n\n\nCall:\nlda(Species ~ ., data = iris)\n\nPrior probabilities of groups:\n    setosa versicolor  virginica \n 0.3333333  0.3333333  0.3333333 \n\nGroup means:\n           Sepal.Length Sepal.Width Petal.Length Petal.Width\nsetosa            5.006       3.428        1.462       0.246\nversicolor        5.936       2.770        4.260       1.326\nvirginica         6.588       2.974        5.552       2.026\n\nCoefficients of linear discriminants:\n                    LD1         LD2\nSepal.Length  0.8293776 -0.02410215\nSepal.Width   1.5344731 -2.16452123\nPetal.Length -2.2012117  0.93192121\nPetal.Width  -2.8104603 -2.83918785\n\nProportion of trace:\n   LD1    LD2 \n0.9912 0.0088 \n\n\nCode\n# Discriminant scores\nlda_scores &lt;- predict(lda_model)$x\n\n# Plot\nplot(lda_scores,\n     col = c(\"red\", \"green\", \"blue\")[iris$Species],\n     pch = 19,\n     main = \"Discriminant Function Scores\",\n     xlab = \"LD1\", ylab = \"LD2\")\nlegend(\"topright\", levels(iris$Species),\n       col = c(\"red\", \"green\", \"blue\"), pch = 19)\n\n\n\n\n\n\n\n\n\n\n\n24.7.3 Interpreting DFA Output\nKey components:\n\nCoefficients of linear discriminants: Weights for creating discriminant scores\nProportion of trace: Variance explained by each discriminant function\nGroup means: Average score on each discriminant function for each group\n\n\n\nCode\n# Coefficients (loadings)\nlda_model$scaling\n\n\n                    LD1         LD2\nSepal.Length  0.8293776 -0.02410215\nSepal.Width   1.5344731 -2.16452123\nPetal.Length -2.2012117  0.93192121\nPetal.Width  -2.8104603 -2.83918785\n\n\nCode\n# Proportion of separation explained\nlda_model$svd^2 / sum(lda_model$svd^2)\n\n\n[1] 0.991212605 0.008787395\n\n\n\n\n24.7.4 Using DFA for Prediction\nDFA can classify new observations into groups based on their discriminant scores:\n\n\nCode\n# Classification accuracy\npredictions &lt;- predict(lda_model)$class\ntable(Predicted = predictions, Actual = iris$Species)\n\n\n            Actual\nPredicted    setosa versicolor virginica\n  setosa         50          0         0\n  versicolor      0         48         1\n  virginica       0          2        49\n\n\nCode\n# Classification accuracy\nmean(predictions == iris$Species)\n\n\n[1] 0.98\n\n\n\n\n24.7.5 Cross-Validated Classification\nFor honest estimates of classification accuracy, use leave-one-out cross-validation:\n\n\nCode\n# Cross-validated LDA\nlda_cv &lt;- lda(Species ~ ., data = iris, CV = TRUE)\n\n# Cross-validated classification table\ntable(Predicted = lda_cv$class, Actual = iris$Species)\n\n\n            Actual\nPredicted    setosa versicolor virginica\n  setosa         50          0         0\n  versicolor      0         48         1\n  virginica       0          2        49\n\n\nCode\n# Cross-validated accuracy\nmean(lda_cv$class == iris$Species)\n\n\n[1] 0.98\n\n\n\n\n24.7.6 DFA for Biomarker Discovery\nDFA is valuable for identifying which variables best distinguish groups—useful in biomarker discovery:\n\n\nCode\n# Which variables contribute most to separation?\nscaling_df &lt;- data.frame(\n  Variable = rownames(lda_model$scaling),\n  LD1 = abs(lda_model$scaling[, 1]),\n  LD2 = abs(lda_model$scaling[, 2])\n)\n\nbarplot(scaling_df$LD1, names.arg = scaling_df$Variable,\n        main = \"Variable Contributions to LD1\",\n        ylab = \"Absolute Coefficient\",\n        col = \"steelblue\")",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Dimensionality Reduction and Multivariate Methods</span>"
    ]
  },
  {
    "objectID": "chapters/23-dimensionality-reduction.html#comparing-methods",
    "href": "chapters/23-dimensionality-reduction.html#comparing-methods",
    "title": "24  Dimensionality Reduction and Multivariate Methods",
    "section": "24.8 Comparing Methods",
    "text": "24.8 Comparing Methods\n\n\n\n\n\n\n\n\n\n\nMethod\nInput\nOutput\nSupervision\nBest For\n\n\n\n\nPCA\nVariables\nContinuous scores\nNone\nReducing correlated variables\n\n\nPCoA\nDistance matrix\nContinuous scores\nNone\nPreserving sample distances\n\n\nNMDS\nDistance matrix\nOrdinal scores\nNone\nEcological community data\n\n\nCluster Analysis\nVariables or distances\nGroup assignments\nNone\nFinding natural groupings\n\n\nMANOVA\nVariables + groups\nTest statistics\nGroups known\nTesting group differences\n\n\nDFA\nVariables + groups\nDiscriminant scores\nGroups known\nClassifying observations",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Dimensionality Reduction and Multivariate Methods</span>"
    ]
  },
  {
    "objectID": "chapters/23-dimensionality-reduction.html#using-ordination-scores-in-further-analyses",
    "href": "chapters/23-dimensionality-reduction.html#using-ordination-scores-in-further-analyses",
    "title": "24  Dimensionality Reduction and Multivariate Methods",
    "section": "24.9 Using Ordination Scores in Further Analyses",
    "text": "24.9 Using Ordination Scores in Further Analyses\nPC scores and discriminant scores are legitimate new variables that can be used in downstream analysis:\n\nRegression of scores on other continuous variables\nANOVA comparing groups on ordination scores\nCorrelation of scores with environmental gradients\n\nThis is valuable when you have many correlated variables and want to reduce dimensionality before hypothesis testing.\n\n\nCode\n# Use PC scores in ANOVA\npc_scores &lt;- data.frame(\n  PC1 = iris_pca$x[, 1],\n  PC2 = iris_pca$x[, 2],\n  Species = iris$Species\n)\n\nsummary(aov(PC1 ~ Species, data = pc_scores))\n\n\n             Df Sum Sq Mean Sq F value Pr(&gt;F)    \nSpecies       2  406.4  203.21    1051 &lt;2e-16 ***\nResiduals   147   28.4    0.19                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Dimensionality Reduction and Multivariate Methods</span>"
    ]
  },
  {
    "objectID": "chapters/23-dimensionality-reduction.html#practical-workflow",
    "href": "chapters/23-dimensionality-reduction.html#practical-workflow",
    "title": "24  Dimensionality Reduction and Multivariate Methods",
    "section": "24.10 Practical Workflow",
    "text": "24.10 Practical Workflow\n\nExplore data: Check for outliers, missing values, scaling issues\nStandardize if needed: Especially important when variables are on different scales\nChoose appropriate method: Based on your data type and question\nExamine output: Scree plots, loadings, clustering diagnostics\nValidate: Cross-validation for classification; permutation tests for significance\nInterpret biologically: What do the patterns mean in your system?",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Dimensionality Reduction and Multivariate Methods</span>"
    ]
  },
  {
    "objectID": "chapters/23-dimensionality-reduction.html#summary",
    "href": "chapters/23-dimensionality-reduction.html#summary",
    "title": "24  Dimensionality Reduction and Multivariate Methods",
    "section": "24.11 Summary",
    "text": "24.11 Summary\n\nDimensionality reduction creates fewer variables that capture most information\nPCA finds linear combinations maximizing variance; useful for correlated variables\nPCoA works from distance matrices; useful for ecological and genetic data\nCluster analysis groups similar observations together\nMANOVA tests whether groups differ on multiple response variables simultaneously\nDFA finds combinations that best discriminate known groups\nThese methods can be combined: use PCA to reduce dimensions, then cluster or classify",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Dimensionality Reduction and Multivariate Methods</span>"
    ]
  },
  {
    "objectID": "chapters/23-dimensionality-reduction.html#additional-resources",
    "href": "chapters/23-dimensionality-reduction.html#additional-resources",
    "title": "24  Dimensionality Reduction and Multivariate Methods",
    "section": "24.12 Additional Resources",
    "text": "24.12 Additional Resources\n\nJames et al. (2023) - Modern treatment of dimensionality reduction and clustering\nLogan (2010) - MANOVA and DFA in biological research contexts\nBorcard, D., Gillet, F., & Legendre, P. (2018). Numerical Ecology with R - Comprehensive ordination methods\n\n\n\n\n\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2023. An Introduction to Statistical Learning with Applications in r. 2nd ed. Springer. https://www.statlearning.com.\n\n\nLogan, Murray. 2010. Biostatistical Design and Analysis Using r. Wiley-Blackwell.",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Dimensionality Reduction and Multivariate Methods</span>"
    ]
  },
  {
    "objectID": "chapters/24-high-performance-computing.html",
    "href": "chapters/24-high-performance-computing.html",
    "title": "25  High Performance Computing",
    "section": "",
    "text": "25.1 Why High Performance Computing?\nAs datasets grow and analyses become more complex, your laptop may not be enough. Genomic datasets can be terabytes in size. Simulations might require millions of iterations. Machine learning models may need to be trained on billions of data points. High Performance Computing (HPC) provides the resources to tackle problems that exceed what personal computers can handle.\nHPC systems come in different forms. Computing clusters—collections of interconnected computers working together—are common at universities and research institutions. Cloud computing services from Amazon (AWS), Google, and Microsoft (Azure) provide on-demand access to computing resources. GPUs (Graphics Processing Units) accelerate certain types of parallel computations.",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>High Performance Computing</span>"
    ]
  },
  {
    "objectID": "chapters/24-high-performance-computing.html#computing-clusters",
    "href": "chapters/24-high-performance-computing.html#computing-clusters",
    "title": "25  High Performance Computing",
    "section": "25.2 Computing Clusters",
    "text": "25.2 Computing Clusters\nA typical university computing cluster consists of a head node (login node) where you submit jobs, and many compute nodes where jobs actually run. The head node manages the queue of waiting jobs and allocates resources.\nAt the University of Oregon, the Talapas cluster provides researchers with access to thousands of CPU cores and specialized hardware including GPUs. Access requires an account, which graduate students can request through their research groups.",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>High Performance Computing</span>"
    ]
  },
  {
    "objectID": "chapters/24-high-performance-computing.html#connecting-to-remote-systems",
    "href": "chapters/24-high-performance-computing.html#connecting-to-remote-systems",
    "title": "25  High Performance Computing",
    "section": "25.3 Connecting to Remote Systems",
    "text": "25.3 Connecting to Remote Systems\nYou access remote systems through SSH (Secure Shell):\nssh username@talapas-login.uoregon.edu\nAfter authenticating, you are in a terminal on the remote system, working in a Unix environment just as you would locally. File transfer between your computer and the cluster uses scp or rsync:\n# Copy file to cluster\nscp data.csv username@talapas-login.uoregon.edu:~/project/\n\n# Copy file from cluster\nscp username@talapas-login.uoregon.edu:~/project/results.csv ./",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>High Performance Computing</span>"
    ]
  },
  {
    "objectID": "chapters/24-high-performance-computing.html#job-schedulers",
    "href": "chapters/24-high-performance-computing.html#job-schedulers",
    "title": "25  High Performance Computing",
    "section": "25.4 Job Schedulers",
    "text": "25.4 Job Schedulers\nYou do not run computationally intensive jobs directly on the login node. Instead, you submit them to a job scheduler (like SLURM on Talapas) that queues jobs and runs them when resources become available.\nA basic SLURM submission script:\n#!/bin/bash\n#SBATCH --job-name=my_analysis\n#SBATCH --account=your_account\n#SBATCH --partition=short\n#SBATCH --time=2:00:00\n#SBATCH --nodes=1\n#SBATCH --ntasks=1\n#SBATCH --cpus-per-task=4\n#SBATCH --mem=16G\n\n# Load required software\nmodule load R/4.2.1\n\n# Run your script\nRscript my_analysis.R\nSubmit with sbatch script.sh. Check job status with squeue -u username. Cancel jobs with scancel job_id.",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>High Performance Computing</span>"
    ]
  },
  {
    "objectID": "chapters/24-high-performance-computing.html#resource-requests",
    "href": "chapters/24-high-performance-computing.html#resource-requests",
    "title": "25  High Performance Computing",
    "section": "25.5 Resource Requests",
    "text": "25.5 Resource Requests\nJobs must request resources: time, memory, and CPUs. Request enough to complete your job but not so much that it waits unnecessarily in the queue. Start with conservative estimates and adjust based on actual usage.\nCommon SLURM directives: - --time: Maximum runtime (job is killed if exceeded) - --mem: Memory per node - --cpus-per-task: Number of CPU cores - --array: For running many similar jobs",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>High Performance Computing</span>"
    ]
  },
  {
    "objectID": "chapters/24-high-performance-computing.html#environment-modules",
    "href": "chapters/24-high-performance-computing.html#environment-modules",
    "title": "25  High Performance Computing",
    "section": "25.6 Environment Modules",
    "text": "25.6 Environment Modules\nHPC systems use environment modules to manage software. Instead of installing software yourself, you load pre-installed modules:\nmodule avail              # List available software\nmodule load R/4.2.1       # Load R\nmodule load python/3.10   # Load Python\nmodule list               # Show loaded modules\nmodule purge              # Unload all modules",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>High Performance Computing</span>"
    ]
  },
  {
    "objectID": "chapters/24-high-performance-computing.html#running-r-on-a-cluster",
    "href": "chapters/24-high-performance-computing.html#running-r-on-a-cluster",
    "title": "25  High Performance Computing",
    "section": "25.7 Running R on a Cluster",
    "text": "25.7 Running R on a Cluster\nR scripts run non-interactively on clusters. Instead of using RStudio, you write your analysis as a script and run it with Rscript:\n# my_analysis.R\nlibrary(tidyverse)\n\n# Read data\ndata &lt;- read.csv(\"large_dataset.csv\")\n\n# Perform analysis\nresults &lt;- data |&gt;\n  group_by(category) |&gt;\n  summarize(mean_value = mean(value))\n\n# Save results\nwrite.csv(results, \"output.csv\")",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>High Performance Computing</span>"
    ]
  },
  {
    "objectID": "chapters/24-high-performance-computing.html#parallelization-in-r",
    "href": "chapters/24-high-performance-computing.html#parallelization-in-r",
    "title": "25  High Performance Computing",
    "section": "25.8 Parallelization in R",
    "text": "25.8 Parallelization in R\nR can use multiple CPU cores to speed up computations. The parallel package provides tools for parallel processing:\nlibrary(parallel)\n\n# Detect number of cores\nn_cores &lt;- detectCores()\n\n# Create a cluster\ncl &lt;- makeCluster(n_cores - 1)\n\n# Parallel apply\nresults &lt;- parLapply(cl, data_list, analysis_function)\n\n# Stop the cluster\nstopCluster(cl)\nThe future and furrr packages provide more user-friendly parallelization.",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>High Performance Computing</span>"
    ]
  },
  {
    "objectID": "chapters/24-high-performance-computing.html#cloud-computing",
    "href": "chapters/24-high-performance-computing.html#cloud-computing",
    "title": "25  High Performance Computing",
    "section": "25.9 Cloud Computing",
    "text": "25.9 Cloud Computing\nCloud platforms (AWS, Google Cloud, Azure) offer computing resources on demand. You pay for what you use rather than having fixed resources.\nAdvantages: - Scale up quickly when needed - No hardware maintenance - Access to specialized hardware (GPUs, large memory instances)\nDisadvantages: - Costs can accumulate quickly - Requires learning platform-specific tools - Data transfer can be slow and expensive",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>High Performance Computing</span>"
    ]
  },
  {
    "objectID": "chapters/24-high-performance-computing.html#best-practices",
    "href": "chapters/24-high-performance-computing.html#best-practices",
    "title": "25  High Performance Computing",
    "section": "25.10 Best Practices",
    "text": "25.10 Best Practices\nStart small: Test your code on a small subset before running on full data.\nUse version control: Keep your scripts in Git for reproducibility.\nDocument everything: Future you (and others) need to understand what you did.\nSave intermediate results: If a job fails, you do not want to start from scratch.\nMonitor resource usage: Check how much time and memory your jobs actually use.\nClean up: Delete unnecessary files; storage is shared.",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>High Performance Computing</span>"
    ]
  },
  {
    "objectID": "chapters/24-high-performance-computing.html#getting-help",
    "href": "chapters/24-high-performance-computing.html#getting-help",
    "title": "25  High Performance Computing",
    "section": "25.11 Getting Help",
    "text": "25.11 Getting Help\nMost HPC systems have documentation and support staff. At UO, Research Advanced Computing Services (RACS) provides Talapas documentation and consultations. Reading the documentation before asking questions will make your interactions more productive.\nLearning to use HPC effectively takes time, but the ability to run large-scale analyses is essential for modern bioengineering research.",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>High Performance Computing</span>"
    ]
  },
  {
    "objectID": "chapters/25-presenting-results.html",
    "href": "chapters/25-presenting-results.html",
    "title": "26  Presenting Statistical Results",
    "section": "",
    "text": "26.1 The Art of Scientific Communication\nYou have collected your data, performed your statistical analyses, and obtained results. Now comes a critical challenge that many students underestimate: communicating those results clearly and professionally. The results section of a scientific paper serves as the bridge between your methods and your conclusions, and writing it well requires understanding both statistical concepts and scientific writing conventions.\nThis chapter covers the practical aspects of presenting statistical results—the style conventions, formatting standards, and communication strategies that transform raw statistical output into compelling scientific narrative.",
    "crumbs": [
      "Scientific Communication",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Presenting Statistical Results</span>"
    ]
  },
  {
    "objectID": "chapters/25-presenting-results.html#style-and-function-of-the-results-section",
    "href": "chapters/25-presenting-results.html#style-and-function-of-the-results-section",
    "title": "26  Presenting Statistical Results",
    "section": "26.2 Style and Function of the Results Section",
    "text": "26.2 Style and Function of the Results Section\nThe results section has a specific purpose: to present your findings objectively and systematically, without interpretation or speculation. This section answers the question “What did you find?” while leaving “What does it mean?” for the discussion.\n\n26.2.1 Writing Style Conventions\nScientific results are traditionally written in past tense and passive voice:\n\n“The mean expression level was significantly higher in the treatment group compared to the control group (t = 3.45, df = 28, p = 0.002).”\n\nRather than:\n\n“We find that the mean expression level is significantly higher…”\n\nThe past tense reflects that the research has been completed. The passive voice keeps the focus on the findings rather than the researchers. While some journals now accept active voice, the past tense remains standard for results sections.\n\n\n26.2.2 Objectivity and Precision\nResults sections should be factual and precise: - Report exact values, not vague descriptions - Avoid emotional or evaluative language - Let the numbers speak for themselves\nAvoid: “The results were very impressive and showed a dramatic improvement.”\nBetter: “Cell viability increased from 45.2% (±3.1) to 78.6% (±4.2) following treatment.”",
    "crumbs": [
      "Scientific Communication",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Presenting Statistical Results</span>"
    ]
  },
  {
    "objectID": "chapters/25-presenting-results.html#summarizing-statistical-analyses",
    "href": "chapters/25-presenting-results.html#summarizing-statistical-analyses",
    "title": "26  Presenting Statistical Results",
    "section": "26.3 Summarizing Statistical Analyses",
    "text": "26.3 Summarizing Statistical Analyses\nWhen reporting results, you need to convey what test you performed and what it revealed. The key elements include:\n\nThe statistical test used\nThe test statistic value\nDegrees of freedom (when applicable)\nThe p-value or confidence interval\nEffect size (increasingly expected in modern publications)\n\n\n26.3.1 Common Reporting Formats\nDifferent statistical tests have standard reporting formats:\nt-tests: &gt; “Mean protein concentration was significantly higher in treated cells (M = 45.3 ng/mL, SD = 8.2) than in control cells (M = 31.7 ng/mL, SD = 7.4), t(48) = 6.12, p &lt; 0.001, d = 1.74.”\nANOVA: &gt; “Gene expression differed significantly among the three treatment groups, F(2, 87) = 15.34, p &lt; 0.001, η² = 0.26.”\nChi-square test: &gt; “There was a significant association between genotype and disease status, χ²(2) = 12.45, p = 0.002, V = 0.31.”\nCorrelation: &gt; “Body mass was positively correlated with metabolic rate, r(58) = 0.67, p &lt; 0.001.”\nRegression: &gt; “Temperature significantly predicted reaction rate, β = 0.82, t(43) = 7.89, p &lt; 0.001, R² = 0.59.”\n\n\n26.3.2 Formatting Statistical Symbols\nStatistical symbols follow specific conventions:\n\nUse italics for statistical symbols: p, t, F, r, n, M, SD\nDo not italicize Greek letters: α, β, χ², η²\nReport exact p-values to 2-3 decimal places (p = 0.034) rather than inequalities (p &lt; 0.05), except when p &lt; 0.001\nRound most statistics to 2 decimal places\nReport means and standard deviations to one more decimal place than the original measurements",
    "crumbs": [
      "Scientific Communication",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Presenting Statistical Results</span>"
    ]
  },
  {
    "objectID": "chapters/25-presenting-results.html#reporting-differences-and-directionality",
    "href": "chapters/25-presenting-results.html#reporting-differences-and-directionality",
    "title": "26  Presenting Statistical Results",
    "section": "26.4 Reporting Differences and Directionality",
    "text": "26.4 Reporting Differences and Directionality\nStatistical tests tell you whether a difference exists, but your results section must also communicate the direction and magnitude of that difference.\n\n26.4.1 Stating Directionality Clearly\nAlways specify which group was higher, lower, faster, or slower:\nUnclear: “There was a significant difference between groups (p = 0.003).”\nClear: “The treatment group showed significantly higher expression levels than the control group (p = 0.003).”\n\n\n26.4.2 Reporting Magnitude\nInclude the actual values so readers can judge biological significance:\nIncomplete: “Treatment significantly increased survival (p &lt; 0.01).”\nComplete: “Treatment increased 30-day survival from 34% to 67% (p &lt; 0.01).”\n\n\n26.4.3 Effect Sizes\nP-values tell you whether an effect is statistically distinguishable from zero; effect sizes tell you how large the effect is. Common effect size measures include:\n\n\n\nStatistic\nEffect Size Measure\nSmall\nMedium\nLarge\n\n\n\n\nt-test\nCohen’s d\n0.2\n0.5\n0.8\n\n\nANOVA\nη² (eta squared)\n0.01\n0.06\n0.14\n\n\nCorrelation\nr\n0.1\n0.3\n0.5\n\n\nChi-square\nCramér’s V\n0.1\n0.3\n0.5\n\n\n\nReport effect sizes alongside p-values:\n\n“The drug treatment significantly improved memory scores, t(46) = 3.21, p = 0.002, d = 0.94, indicating a large effect.”",
    "crumbs": [
      "Scientific Communication",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Presenting Statistical Results</span>"
    ]
  },
  {
    "objectID": "chapters/25-presenting-results.html#units-and-measurement",
    "href": "chapters/25-presenting-results.html#units-and-measurement",
    "title": "26  Presenting Statistical Results",
    "section": "26.5 Units and Measurement",
    "text": "26.5 Units and Measurement\nAlways include units for measured quantities. Readers need units to interpret your findings.\nWithout units: “Mean concentration was 45.3 (SD = 8.2).”\nWith units: “Mean concentration was 45.3 ng/mL (SD = 8.2 ng/mL).”\n\n26.5.1 SI Units\nUse International System of Units (SI) consistently: - Mass: kg, g, mg, μg - Length: m, cm, mm, μm, nm - Time: s, min, h - Concentration: M, mM, μM, mol/L - Temperature: °C or K\n\n\n26.5.2 Formatting Numbers\n\nUse consistent decimal places within a comparison\nUse scientific notation for very large or very small numbers: 3.2 × 10⁶ cells/mL\nInclude leading zeros: 0.05, not .05\nUse spaces or commas for large numbers: 10,000 or 10 000",
    "crumbs": [
      "Scientific Communication",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Presenting Statistical Results</span>"
    ]
  },
  {
    "objectID": "chapters/25-presenting-results.html#integrating-text-tables-and-figures",
    "href": "chapters/25-presenting-results.html#integrating-text-tables-and-figures",
    "title": "26  Presenting Statistical Results",
    "section": "26.6 Integrating Text, Tables, and Figures",
    "text": "26.6 Integrating Text, Tables, and Figures\nEffective results sections coordinate text, tables, and figures. Each serves a different purpose:\n\nText: Highlights key findings and guides interpretation\nTables: Present precise numerical values for comparison\nFigures: Show patterns, trends, and relationships visually\n\n\n26.6.1 Referring to Figures and Tables\nEvery figure and table must be referenced in the text:\n\n“Expression levels varied significantly among tissue types (Figure 3.2). The highest expression was observed in liver tissue, while muscle showed minimal expression (Table 3.1).”\n\n\n\n26.6.2 Avoiding Redundancy\nDon’t repeat the same information in text, table, and figure. Instead, present detailed data in tables or figures and summarize the key findings in text:\nRedundant: “As shown in Table 1, Group A had a mean of 45.3, Group B had a mean of 52.1, and Group C had a mean of 48.7.”\nBetter: “Mean values differed significantly among groups (Table 1), with Group B showing the highest response.”",
    "crumbs": [
      "Scientific Communication",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Presenting Statistical Results</span>"
    ]
  },
  {
    "objectID": "chapters/25-presenting-results.html#non-significant-results",
    "href": "chapters/25-presenting-results.html#non-significant-results",
    "title": "26  Presenting Statistical Results",
    "section": "26.7 Non-Significant Results",
    "text": "26.7 Non-Significant Results\nNon-significant results are still results and should be reported clearly:\n\n“There was no significant difference in survival rate between treatment and control groups (78% vs. 72%, χ²(1) = 1.24, p = 0.27).”\n\nReport exact p-values for non-significant results rather than simply stating “p &gt; 0.05” or “ns.” This gives readers information about how close the result was to significance and allows for meta-analysis.\n\n\n\n\n\n\nAbsence of Evidence vs. Evidence of Absence\n\n\n\nA non-significant result means you failed to detect an effect—it does not prove no effect exists. Avoid language like “the treatment had no effect.” Instead use “we did not detect a significant effect” or “there was no significant difference.”",
    "crumbs": [
      "Scientific Communication",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Presenting Statistical Results</span>"
    ]
  },
  {
    "objectID": "chapters/25-presenting-results.html#handling-multiple-comparisons",
    "href": "chapters/25-presenting-results.html#handling-multiple-comparisons",
    "title": "26  Presenting Statistical Results",
    "section": "26.8 Handling Multiple Comparisons",
    "text": "26.8 Handling Multiple Comparisons\nWhen conducting multiple statistical tests, address the issue of multiple comparisons:\n\n“To control for multiple comparisons, we applied Bonferroni correction, setting the significance threshold at α = 0.008 (0.05/6). After correction, three of the six comparisons remained significant.”\n\nOr:\n\n“We report uncorrected p-values but note that with 12 tests, approximately one significant result would be expected by chance at α = 0.05.”",
    "crumbs": [
      "Scientific Communication",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Presenting Statistical Results</span>"
    ]
  },
  {
    "objectID": "chapters/25-presenting-results.html#confidence-intervals",
    "href": "chapters/25-presenting-results.html#confidence-intervals",
    "title": "26  Presenting Statistical Results",
    "section": "26.9 Confidence Intervals",
    "text": "26.9 Confidence Intervals\nConfidence intervals often communicate more information than p-values alone. They show both statistical significance and precision of estimation:\n\n“Mean improvement in the treatment group was 12.3 points (95% CI: 8.1–16.5), significantly greater than zero.”\n\nWhen confidence intervals don’t cross zero (for differences) or one (for ratios), the result is statistically significant at the corresponding α level.",
    "crumbs": [
      "Scientific Communication",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Presenting Statistical Results</span>"
    ]
  },
  {
    "objectID": "chapters/25-presenting-results.html#practical-example-writing-a-results-paragraph",
    "href": "chapters/25-presenting-results.html#practical-example-writing-a-results-paragraph",
    "title": "26  Presenting Statistical Results",
    "section": "26.10 Practical Example: Writing a Results Paragraph",
    "text": "26.10 Practical Example: Writing a Results Paragraph\nConsider an experiment testing whether a new fertilizer affects plant growth. Here’s how to construct a results paragraph:\nData summary: - Control group: n = 30, mean height = 24.3 cm, SD = 4.2 cm - Treatment group: n = 30, mean height = 31.7 cm, SD = 5.1 cm - t-test: t(58) = 6.17, p &lt; 0.001 - Cohen’s d = 1.59\nWell-written results paragraph:\n\n“Plants receiving the experimental fertilizer grew significantly taller than control plants. Mean height in the treatment group (M = 31.7 cm, SD = 5.1) was 7.4 cm greater than in the control group (M = 24.3 cm, SD = 4.2), representing a 30% increase in growth. This difference was statistically significant, t(58) = 6.17, p &lt; 0.001, with a large effect size (d = 1.59). Figure 1 shows the distribution of heights in both groups.”\n\nNotice how this paragraph: - States the main finding clearly - Provides specific numerical values - Reports both percentage and absolute differences - Includes all statistical details - References a figure - Uses past tense and maintains objectivity",
    "crumbs": [
      "Scientific Communication",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Presenting Statistical Results</span>"
    ]
  },
  {
    "objectID": "chapters/25-presenting-results.html#common-mistakes-to-avoid",
    "href": "chapters/25-presenting-results.html#common-mistakes-to-avoid",
    "title": "26  Presenting Statistical Results",
    "section": "26.11 Common Mistakes to Avoid",
    "text": "26.11 Common Mistakes to Avoid\n\n26.11.1 Statistical Reporting Errors\n\nMisreporting degrees of freedom: For a t-test comparing two groups of 25 each, df = 48, not 50\nInconsistent decimal places: Report “M = 45.32, SD = 8.10” not “M = 45.32, SD = 8.1”\nMissing test statistics: “p &lt; 0.05” without the test statistic is incomplete\nConfusing SD and SE: Standard deviation describes variability; standard error describes precision of the mean\n\n\n\n26.11.2 Interpretation Errors\n\nClaiming causation from correlation: Correlation does not imply causation\nOver-interpreting non-significance: Failure to reject H₀ is not acceptance of H₀\nEquating statistical and practical significance: A tiny effect can be statistically significant with large n\n\n\n\n26.11.3 Presentation Errors\n\nBurying important results: Lead with key findings\nMixing results and interpretation: Save interpretation for the discussion\nIncomplete reporting: Include all tests performed, including non-significant results",
    "crumbs": [
      "Scientific Communication",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Presenting Statistical Results</span>"
    ]
  },
  {
    "objectID": "chapters/25-presenting-results.html#reporting-guidelines-and-standards",
    "href": "chapters/25-presenting-results.html#reporting-guidelines-and-standards",
    "title": "26  Presenting Statistical Results",
    "section": "26.12 Reporting Guidelines and Standards",
    "text": "26.12 Reporting Guidelines and Standards\nMany fields have developed reporting guidelines for specific types of studies:\n\nCONSORT: Randomized controlled trials\nSTROBE: Observational studies\nPRISMA: Systematic reviews and meta-analyses\nARRIVE: Animal research\n\nThese guidelines specify what information to report and how to structure your presentation. Following them improves transparency and reproducibility.",
    "crumbs": [
      "Scientific Communication",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Presenting Statistical Results</span>"
    ]
  },
  {
    "objectID": "chapters/25-presenting-results.html#creating-reproducible-results",
    "href": "chapters/25-presenting-results.html#creating-reproducible-results",
    "title": "26  Presenting Statistical Results",
    "section": "26.13 Creating Reproducible Results",
    "text": "26.13 Creating Reproducible Results\nModern scientific practice increasingly emphasizes reproducibility. Consider:\n\nReport exact values from statistical software rather than rounded approximations\nInclude sample sizes for all analyses\nDescribe any data exclusions and the rationale\nNote software and versions used for analysis\nShare data and code when possible\n\n\n\nCode\n# Example: Extracting exact values for reporting\n# Instead of rounding manually, extract from model objects\n\nmodel_data &lt;- data.frame(\n  treatment = rep(c(\"Control\", \"Drug\"), each = 20),\n  response = c(rnorm(20, 50, 10), rnorm(20, 58, 12))\n)\n\nt_result &lt;- t.test(response ~ treatment, data = model_data)\n\n# Extract values programmatically\ncat(\"t =\", round(t_result$statistic, 2), \"\\n\")\n\n\nt = -0.68 \n\n\nCode\ncat(\"df =\", round(t_result$parameter, 1), \"\\n\")\n\n\ndf = 38 \n\n\nCode\ncat(\"p =\", round(t_result$p.value, 4), \"\\n\")\n\n\np = 0.5028 \n\n\nCode\ncat(\"95% CI: [\", round(t_result$conf.int[1], 2), \",\",\n    round(t_result$conf.int[2], 2), \"]\\n\")\n\n\n95% CI: [ -11.29 , 5.64 ]",
    "crumbs": [
      "Scientific Communication",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Presenting Statistical Results</span>"
    ]
  },
  {
    "objectID": "chapters/25-presenting-results.html#summary",
    "href": "chapters/25-presenting-results.html#summary",
    "title": "26  Presenting Statistical Results",
    "section": "26.14 Summary",
    "text": "26.14 Summary\nPresenting statistical results effectively requires:\n\nProper style: Past tense, passive voice, objective tone\nComplete reporting: Test statistic, degrees of freedom, p-value, effect size\nClear directionality: State which group was higher/lower/faster\nAppropriate precision: Consistent decimal places and units\nIntegration: Coordinate text, tables, and figures without redundancy\nTransparency: Report all results, including non-significant findings\n\nThe goal is to present your findings so clearly that readers can evaluate the evidence themselves. Statistical results should inform, not obscure. Master these conventions, and your scientific writing will communicate with precision and authority.",
    "crumbs": [
      "Scientific Communication",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Presenting Statistical Results</span>"
    ]
  },
  {
    "objectID": "chapters/25-presenting-results.html#additional-resources",
    "href": "chapters/25-presenting-results.html#additional-resources",
    "title": "26  Presenting Statistical Results",
    "section": "26.15 Additional Resources",
    "text": "26.15 Additional Resources\n\nAmerican Psychological Association. (2020). Publication Manual of the American Psychological Association (7th ed.) - The standard reference for statistical reporting format\nLang, T. A., & Secic, M. (2006). How to Report Statistics in Medicine - Medical statistics reporting guidelines\nThulin (2025) - Modern approaches to presenting statistical analyses\n\n\n\n\n\n\n\nThulin, Måns. 2025. Modern Statistics with r. CRC Press. https://www.modernstatisticswithr.com.",
    "crumbs": [
      "Scientific Communication",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Presenting Statistical Results</span>"
    ]
  },
  {
    "objectID": "chapters/A1-eugenics-history.html",
    "href": "chapters/A1-eugenics-history.html",
    "title": "27  The Eugenics History of Statistics",
    "section": "",
    "text": "27.1 Why This History Matters\nMany of the statistical methods we use today—correlation, regression, ANOVA, and others—were developed by scientists whose primary motivation was eugenics. Understanding this history is important not to discredit the methods themselves, which remain mathematically valid and useful, but to understand the context in which science develops and to remain vigilant about how scientific tools can be misused.\nScience does not exist in a vacuum. The questions scientists ask, the data they collect, and the interpretations they favor are shaped by the social contexts in which they work. The history of statistics and eugenics offers a stark example of this principle.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>The Eugenics History of Statistics</span>"
    ]
  },
  {
    "objectID": "chapters/A1-eugenics-history.html#francis-galton-founder-of-eugenics",
    "href": "chapters/A1-eugenics-history.html#francis-galton-founder-of-eugenics",
    "title": "27  The Eugenics History of Statistics",
    "section": "27.2 Francis Galton: Founder of Eugenics",
    "text": "27.2 Francis Galton: Founder of Eugenics\n\n\n\n\n\nFrancis Galton (1822–1911) was Charles Darwin’s half-cousin and a polymath who made genuine contributions to statistics, meteorology, and other fields. He invented correlation, pioneered the use of questionnaires, and conducted early twin studies. He is also the person who coined the term “eugenics” and devoted much of his career to promoting it.\nGalton believed that human intelligence was hereditary and that the “improvement” of the human race could be achieved through selective breeding. He studied prominent academics and concluded that talent ran in families, attributing this entirely to heredity while ignoring the advantages of wealth, education, and social connections.\nHis statistical innovations—correlation, regression to the mean—were developed specifically to analyze human inheritance and support eugenic arguments. The concept of “regression to the mean” came from his observation that tall parents had children who were tall but not as extremely tall as the parents, which he interpreted through a lens of hereditary “quality.”",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>The Eugenics History of Statistics</span>"
    ]
  },
  {
    "objectID": "chapters/A1-eugenics-history.html#eugenics-in-america",
    "href": "chapters/A1-eugenics-history.html#eugenics-in-america",
    "title": "27  The Eugenics History of Statistics",
    "section": "27.3 Eugenics in America",
    "text": "27.3 Eugenics in America\nEugenic ideas found fertile ground in the United States. The Eugenics Record Office (ERO) was founded at Cold Spring Harbor in 1910, conducting research aimed at identifying “unfit” individuals who should be prevented from reproducing.\n\n\n\n\n\nState fairs featured “fitter family” contests. Educational materials promoted eugenic thinking. Thirty states passed laws allowing forced sterilization of people deemed “unfit”—a category that disproportionately targeted poor people, immigrants, people with disabilities, and people of color.\nBetween 1907 and 1963, over 64,000 people were forcibly sterilized in the United States under eugenic legislation. California led the nation in forced sterilizations.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>The Eugenics History of Statistics</span>"
    ]
  },
  {
    "objectID": "chapters/A1-eugenics-history.html#r.a.-fisher-and-eugenics",
    "href": "chapters/A1-eugenics-history.html#r.a.-fisher-and-eugenics",
    "title": "27  The Eugenics History of Statistics",
    "section": "27.4 R.A. Fisher and Eugenics",
    "text": "27.4 R.A. Fisher and Eugenics\n\n\n\n\n\nRonald A. Fisher (1890–1962) is one of the most influential statisticians in history. He developed analysis of variance (ANOVA), the concept of statistical significance, maximum likelihood estimation, and experimental design principles that remain standard today.\nFisher was also deeply committed to eugenics throughout his career. He was the founding chairman of the Cambridge University Eugenics Society. Approximately one-third of his landmark book “The Genetical Theory of Natural Selection” (1930) addresses eugenics, arguing that the fall of civilizations was caused by differential fertility between social classes.\nFisher used his statistical methods to analyze human variation in ways meant to support racial hierarchies. His scientific work and his eugenic advocacy were not separate activities—they were intertwined.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>The Eugenics History of Statistics</span>"
    ]
  },
  {
    "objectID": "chapters/A1-eugenics-history.html#connections-to-nazi-germany",
    "href": "chapters/A1-eugenics-history.html#connections-to-nazi-germany",
    "title": "27  The Eugenics History of Statistics",
    "section": "27.5 Connections to Nazi Germany",
    "text": "27.5 Connections to Nazi Germany\nAmerican eugenics directly influenced Nazi Germany. The Nazi sterilization program, which forcibly sterilized hundreds of thousands of people, was explicitly modeled on American laws, particularly California’s.\n\n\n\n\n\nThe Holocaust itself was the most extreme expression of eugenic ideology—the belief that human populations could and should be “improved” through preventing certain people from reproducing, taken to its murderous conclusion.\nAfter World War II, the horrors of Nazi eugenics discredited the movement publicly, but eugenic practices continued. Forced sterilizations continued in the United States into the 1970s. California did not pass legislation explicitly banning sterilization of prison inmates until 2014.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>The Eugenics History of Statistics</span>"
    ]
  },
  {
    "objectID": "chapters/A1-eugenics-history.html#what-do-we-do-with-this-history",
    "href": "chapters/A1-eugenics-history.html#what-do-we-do-with-this-history",
    "title": "27  The Eugenics History of Statistics",
    "section": "27.6 What Do We Do With This History?",
    "text": "27.6 What Do We Do With This History?\nThe statistical methods developed by Galton, Fisher, and others are mathematically sound. A t-test does not care about the motives of the person who developed it. These tools have been used to improve medicine, agriculture, and countless other fields.\nBut acknowledging this history serves several purposes:\nIt reminds us that science is not neutral. The questions scientists ask are shaped by their values and social context. Eugenic statistics were developed to answer eugenic questions.\nIt encourages vigilance. Similar misuses of science can occur today. Claims about genetic differences between groups, about who deserves resources or rights, should be scrutinized carefully.\nIt honors the victims. Tens of thousands of people were forcibly sterilized, and millions were murdered, under policies justified by scientific authority. Acknowledging this history recognizes their suffering.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>The Eugenics History of Statistics</span>"
    ]
  },
  {
    "objectID": "chapters/A1-eugenics-history.html#moving-forward",
    "href": "chapters/A1-eugenics-history.html#moving-forward",
    "title": "27  The Eugenics History of Statistics",
    "section": "27.7 Moving Forward",
    "text": "27.7 Moving Forward\nUnderstanding this history does not mean abandoning statistics—it means using these tools thoughtfully and ethically. It means being skeptical when scientific claims align too conveniently with existing prejudices. It means recognizing that data and analysis can be weaponized.\nScience has the potential to improve human welfare, but only when practiced with awareness of its history and constant attention to its ethics.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>The Eugenics History of Statistics</span>"
    ]
  },
  {
    "objectID": "chapters/A1-eugenics-history.html#further-reading",
    "href": "chapters/A1-eugenics-history.html#further-reading",
    "title": "27  The Eugenics History of Statistics",
    "section": "27.8 Further Reading",
    "text": "27.8 Further Reading\nFor those interested in exploring this history further:\n\n“Superior: The Return of Race Science” by Angela Saini\n“The Gene: An Intimate History” by Siddhartha Mukherjee\n\n“Control: The Dark History and Troubling Present of Eugenics” by Adam Rutherford",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>The Eugenics History of Statistics</span>"
    ]
  },
  {
    "objectID": "chapters/A2-r-reference.html",
    "href": "chapters/A2-r-reference.html",
    "title": "28  R Command Reference",
    "section": "",
    "text": "28.1 Base R Fundamentals\nThis appendix provides a comprehensive reference for R commands, covering both base R and tidyverse functions.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>R Command Reference</span>"
    ]
  },
  {
    "objectID": "chapters/A2-r-reference.html#sec-r-base",
    "href": "chapters/A2-r-reference.html#sec-r-base",
    "title": "28  R Command Reference",
    "section": "",
    "text": "28.1.1 Assignment and Basic Operations\n\n\n\nOperation\nSyntax\nExample\n\n\n\n\nAssignment\n&lt;- or =\nx &lt;- 5\n\n\nPrint value\nVariable name or print()\nx or print(x)\n\n\nComment\n#\n# This is a comment\n\n\nHelp\n? or help()\n?mean\n\n\nExamples\nexample()\nexample(mean)\n\n\nSearch help\n?? or help.search()\n??regression\n\n\n\n\n\n28.1.2 Arithmetic Operators\n\n\n\nOperator\nDescription\nExample\n\n\n\n\n+\nAddition\n5 + 3\n\n\n-\nSubtraction\n5 - 3\n\n\n*\nMultiplication\n5 * 3\n\n\n/\nDivision\n5 / 3\n\n\n^ or **\nExponentiation\n5^3\n\n\n%%\nModulo (remainder)\n5 %% 3\n\n\n%/%\nInteger division\n5 %/% 3\n\n\n\n\n\n28.1.3 Comparison Operators\n\n\n\nOperator\nDescription\nExample\n\n\n\n\n==\nEqual to\nx == 5\n\n\n!=\nNot equal to\nx != 5\n\n\n&lt;\nLess than\nx &lt; 5\n\n\n&gt;\nGreater than\nx &gt; 5\n\n\n&lt;=\nLess than or equal\nx &lt;= 5\n\n\n&gt;=\nGreater than or equal\nx &gt;= 5\n\n\n\n\n\n28.1.4 Logical Operators\n\n\n\nOperator\nDescription\nExample\n\n\n\n\n&\nAND (element-wise)\nx &gt; 0 & x &lt; 10\n\n\n|\nOR (element-wise)\nx &lt; 0 | x &gt; 10\n\n\n!\nNOT\n!is.na(x)\n\n\n&&\nAND (single value)\ncond1 && cond2\n\n\n||\nOR (single value)\ncond1 || cond2\n\n\n%in%\nValue in set\nx %in% c(1, 2, 3)\n\n\nxor()\nExclusive OR\nxor(TRUE, FALSE)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>R Command Reference</span>"
    ]
  },
  {
    "objectID": "chapters/A2-r-reference.html#sec-r-vectors",
    "href": "chapters/A2-r-reference.html#sec-r-vectors",
    "title": "28  R Command Reference",
    "section": "28.2 Vector Operations",
    "text": "28.2 Vector Operations\n\n28.2.1 Creating Vectors\n\n\n\nFunction\nDescription\nExample\n\n\n\n\nc()\nCombine values\nc(1, 2, 3, 4, 5)\n\n\n:\nSequence\n1:10\n\n\nseq()\nSequence with step\nseq(0, 1, by = 0.1)\n\n\nseq_len()\nSequence of length n\nseq_len(10)\n\n\nseq_along()\nSequence along object\nseq_along(x)\n\n\nrep()\nRepeat values\nrep(1, times = 5)\n\n\nrep()\nRepeat each\nrep(1:3, each = 2)\n\n\nvector()\nCreate empty vector\nvector(\"numeric\", 10)\n\n\n\n\n\n28.2.2 Vector Indexing\n\n\n\nSyntax\nDescription\nExample\n\n\n\n\nx[i]\nElement at position i\nx[3]\n\n\nx[c(i,j)]\nMultiple elements\nx[c(1, 3, 5)]\n\n\nx[-i]\nExclude element\nx[-1]\n\n\nx[condition]\nLogical subsetting\nx[x &gt; 5]\n\n\nx[1:n]\nRange of elements\nx[1:5]\n\n\nx[\"name\"]\nBy name\nx[\"first\"]\n\n\n\n\n\n28.2.3 Vector Functions\n\n\n\nFunction\nDescription\nExample\n\n\n\n\nlength()\nNumber of elements\nlength(x)\n\n\nsum()\nSum of elements\nsum(x)\n\n\nmean()\nArithmetic mean\nmean(x)\n\n\nmedian()\nMedian value\nmedian(x)\n\n\nsd()\nStandard deviation\nsd(x)\n\n\nvar()\nVariance\nvar(x)\n\n\nmin()\nMinimum\nmin(x)\n\n\nmax()\nMaximum\nmax(x)\n\n\nrange()\nRange (min and max)\nrange(x)\n\n\nquantile()\nQuantiles\nquantile(x, 0.5)\n\n\nsort()\nSort values\nsort(x)\n\n\norder()\nIndices for sorting\norder(x)\n\n\nrev()\nReverse order\nrev(x)\n\n\nunique()\nUnique values\nunique(x)\n\n\ntable()\nFrequency table\ntable(x)\n\n\ncumsum()\nCumulative sum\ncumsum(x)\n\n\ndiff()\nDifferences\ndiff(x)\n\n\nwhich()\nIndices where TRUE\nwhich(x &gt; 5)\n\n\nwhich.max()\nIndex of max\nwhich.max(x)\n\n\nwhich.min()\nIndex of min\nwhich.min(x)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>R Command Reference</span>"
    ]
  },
  {
    "objectID": "chapters/A2-r-reference.html#sec-r-dataframes",
    "href": "chapters/A2-r-reference.html#sec-r-dataframes",
    "title": "28  R Command Reference",
    "section": "28.3 Data Frames",
    "text": "28.3 Data Frames\n\n28.3.1 Creating Data Frames\n\n\n\n\n\n\n\n\nFunction\nDescription\nExample\n\n\n\n\ndata.frame()\nCreate data frame\ndata.frame(x = 1:3, y = c(\"a\", \"b\", \"c\"))\n\n\ntibble()\nCreate tibble\ntibble(x = 1:3, y = c(\"a\", \"b\", \"c\"))\n\n\nas.data.frame()\nConvert to data frame\nas.data.frame(matrix)\n\n\nas_tibble()\nConvert to tibble\nas_tibble(df)\n\n\n\n\n\n28.3.2 Data Frame Indexing\n\n\n\nSyntax\nDescription\nExample\n\n\n\n\ndf$col\nColumn by name\ndf$age\n\n\ndf[, \"col\"]\nColumn as vector\ndf[, \"age\"]\n\n\ndf[\"col\"]\nColumn as data frame\ndf[\"age\"]\n\n\ndf[i, ]\nRow by position\ndf[1, ]\n\n\ndf[i, j]\nElement\ndf[1, 2]\n\n\ndf[condition, ]\nFilter rows\ndf[df$age &gt; 25, ]\n\n\n\n\n\n28.3.3 Data Frame Functions\n\n\n\nFunction\nDescription\nExample\n\n\n\n\nnrow()\nNumber of rows\nnrow(df)\n\n\nncol()\nNumber of columns\nncol(df)\n\n\ndim()\nDimensions\ndim(df)\n\n\nnames()\nColumn names\nnames(df)\n\n\ncolnames()\nColumn names\ncolnames(df)\n\n\nrownames()\nRow names\nrownames(df)\n\n\nhead()\nFirst rows\nhead(df, 10)\n\n\ntail()\nLast rows\ntail(df, 10)\n\n\nstr()\nStructure\nstr(df)\n\n\nsummary()\nSummary statistics\nsummary(df)\n\n\nglimpse()\nTidyverse structure\nglimpse(df)\n\n\nView()\nOpen viewer\nView(df)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>R Command Reference</span>"
    ]
  },
  {
    "objectID": "chapters/A2-r-reference.html#sec-r-io",
    "href": "chapters/A2-r-reference.html#sec-r-io",
    "title": "28  R Command Reference",
    "section": "28.4 Reading and Writing Data",
    "text": "28.4 Reading and Writing Data\n\n28.4.1 Base R I/O\n\n\n\n\n\n\n\n\nFunction\nDescription\nExample\n\n\n\n\nread.csv()\nRead CSV\nread.csv(\"file.csv\")\n\n\nread.table()\nRead table\nread.table(\"file.txt\", header = TRUE)\n\n\nread.delim()\nRead tab-delimited\nread.delim(\"file.tsv\")\n\n\nwrite.csv()\nWrite CSV\nwrite.csv(df, \"file.csv\", row.names = FALSE)\n\n\nwrite.table()\nWrite table\nwrite.table(df, \"file.txt\")\n\n\nsaveRDS()\nSave R object\nsaveRDS(obj, \"file.rds\")\n\n\nreadRDS()\nRead R object\nreadRDS(\"file.rds\")\n\n\nsave()\nSave multiple objects\nsave(x, y, file = \"data.RData\")\n\n\nload()\nLoad RData file\nload(\"data.RData\")\n\n\n\n\n\n28.4.2 Tidyverse I/O (readr)\n\n\n\n\n\n\n\n\nFunction\nDescription\nExample\n\n\n\n\nread_csv()\nRead CSV (fast)\nread_csv(\"file.csv\")\n\n\nread_tsv()\nRead TSV\nread_tsv(\"file.tsv\")\n\n\nread_delim()\nRead with delimiter\nread_delim(\"file.txt\", delim = \"|\")\n\n\nread_fwf()\nRead fixed-width\nread_fwf(\"file.txt\", col_positions)\n\n\nwrite_csv()\nWrite CSV\nwrite_csv(df, \"file.csv\")\n\n\nwrite_tsv()\nWrite TSV\nwrite_tsv(df, \"file.tsv\")",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>R Command Reference</span>"
    ]
  },
  {
    "objectID": "chapters/A2-r-reference.html#sec-r-dplyr",
    "href": "chapters/A2-r-reference.html#sec-r-dplyr",
    "title": "28  R Command Reference",
    "section": "28.5 Tidyverse: dplyr",
    "text": "28.5 Tidyverse: dplyr\n\n28.5.1 Core Verbs\n\n\n\n\n\n\n\n\nFunction\nDescription\nExample\n\n\n\n\nfilter()\nFilter rows\nfilter(df, age &gt; 25)\n\n\nselect()\nSelect columns\nselect(df, name, age)\n\n\nmutate()\nCreate/modify columns\nmutate(df, age_sq = age^2)\n\n\narrange()\nSort rows\narrange(df, age)\n\n\nsummarise()\nSummarize data\nsummarise(df, mean_age = mean(age))\n\n\ngroup_by()\nGroup data\ngroup_by(df, category)\n\n\n\n\n\n28.5.2 Selection Helpers\n\n\n\n\n\n\n\n\nFunction\nDescription\nExample\n\n\n\n\nstarts_with()\nColumns starting with\nselect(df, starts_with(\"temp\"))\n\n\nends_with()\nColumns ending with\nselect(df, ends_with(\"_id\"))\n\n\ncontains()\nColumns containing\nselect(df, contains(\"score\"))\n\n\nmatches()\nColumns matching regex\nselect(df, matches(\"^x[0-9]\"))\n\n\neverything()\nAll columns\nselect(df, name, everything())\n\n\nwhere()\nColumns where condition\nselect(df, where(is.numeric))\n\n\nall_of()\nAll specified columns\nselect(df, all_of(col_names))\n\n\nany_of()\nAny of specified columns\nselect(df, any_of(col_names))\n\n\n\n\n\n28.5.3 Additional dplyr Functions\n\n\n\n\n\n\n\n\nFunction\nDescription\nExample\n\n\n\n\ndistinct()\nUnique rows\ndistinct(df, category)\n\n\ncount()\nCount occurrences\ncount(df, category)\n\n\nslice()\nSelect rows by position\nslice(df, 1:10)\n\n\nslice_head()\nFirst n rows\nslice_head(df, n = 5)\n\n\nslice_tail()\nLast n rows\nslice_tail(df, n = 5)\n\n\nslice_sample()\nRandom rows\nslice_sample(df, n = 10)\n\n\nslice_max()\nRows with max values\nslice_max(df, age, n = 3)\n\n\nslice_min()\nRows with min values\nslice_min(df, age, n = 3)\n\n\npull()\nExtract column as vector\npull(df, name)\n\n\nrename()\nRename columns\nrename(df, new_name = old_name)\n\n\nrelocate()\nReorder columns\nrelocate(df, name, .before = age)\n\n\nacross()\nApply to multiple columns\nmutate(df, across(where(is.numeric), scale))\n\n\nrowwise()\nRow-wise operations\nrowwise(df)\n\n\nungroup()\nRemove grouping\nungroup(df)\n\n\nn()\nCount in group\nsummarise(df, count = n())\n\n\nn_distinct()\nCount unique values\nsummarise(df, unique = n_distinct(x))\n\n\nfirst()\nFirst value\nsummarise(df, first = first(x))\n\n\nlast()\nLast value\nsummarise(df, last = last(x))\n\n\nnth()\nNth value\nsummarise(df, third = nth(x, 3))\n\n\n\n\n\n28.5.4 Joins\n\n\n\n\n\n\n\n\nFunction\nDescription\nExample\n\n\n\n\nleft_join()\nKeep all left rows\nleft_join(df1, df2, by = \"id\")\n\n\nright_join()\nKeep all right rows\nright_join(df1, df2, by = \"id\")\n\n\ninner_join()\nKeep matching rows\ninner_join(df1, df2, by = \"id\")\n\n\nfull_join()\nKeep all rows\nfull_join(df1, df2, by = \"id\")\n\n\nsemi_join()\nFilter left by right\nsemi_join(df1, df2, by = \"id\")\n\n\nanti_join()\nFilter left, no match in right\nanti_join(df1, df2, by = \"id\")\n\n\nbind_rows()\nStack data frames\nbind_rows(df1, df2)\n\n\nbind_cols()\nCombine columns\nbind_cols(df1, df2)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>R Command Reference</span>"
    ]
  },
  {
    "objectID": "chapters/A2-r-reference.html#sec-r-tidyr",
    "href": "chapters/A2-r-reference.html#sec-r-tidyr",
    "title": "28  R Command Reference",
    "section": "28.6 Tidyverse: tidyr",
    "text": "28.6 Tidyverse: tidyr\n\n\n\n\n\n\n\n\nFunction\nDescription\nExample\n\n\n\n\npivot_longer()\nWide to long\npivot_longer(df, cols = -id, names_to = \"var\", values_to = \"val\")\n\n\npivot_wider()\nLong to wide\npivot_wider(df, names_from = var, values_from = val)\n\n\nseparate()\nSplit column\nseparate(df, col, into = c(\"a\", \"b\"), sep = \"_\")\n\n\nseparate_wider_delim()\nSplit by delimiter\nseparate_wider_delim(df, col, delim = \"_\", names = c(\"a\", \"b\"))\n\n\nunite()\nCombine columns\nunite(df, new_col, a, b, sep = \"_\")\n\n\ndrop_na()\nRemove NA rows\ndrop_na(df)\n\n\nfill()\nFill NA with previous\nfill(df, column, .direction = \"down\")\n\n\nreplace_na()\nReplace NA values\nreplace_na(df, list(x = 0))\n\n\ncomplete()\nComplete missing combinations\ncomplete(df, x, y)\n\n\nexpand()\nCreate all combinations\nexpand(df, x, y)\n\n\nnest()\nNest data\nnest(df, data = -group)\n\n\nunnest()\nUnnest data\nunnest(df, data)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>R Command Reference</span>"
    ]
  },
  {
    "objectID": "chapters/A2-r-reference.html#sec-r-ggplot",
    "href": "chapters/A2-r-reference.html#sec-r-ggplot",
    "title": "28  R Command Reference",
    "section": "28.7 Tidyverse: ggplot2",
    "text": "28.7 Tidyverse: ggplot2\n\n28.7.1 Basic Structure\nggplot(data, aes(x = x_var, y = y_var)) +\n  geom_*() +\n  labs() +\n  theme_*()\n\n\n28.7.2 Geometries\n\n\n\nFunction\nPlot Type\nUsage\n\n\n\n\ngeom_point()\nScatter plot\nContinuous x and y\n\n\ngeom_line()\nLine plot\nContinuous x and y\n\n\ngeom_smooth()\nSmoothed line\nAdd trend line\n\n\ngeom_bar()\nBar chart\nCounts\n\n\ngeom_col()\nBar chart\nValues\n\n\ngeom_histogram()\nHistogram\nDistribution\n\n\ngeom_density()\nDensity plot\nSmooth distribution\n\n\ngeom_boxplot()\nBox plot\nDistribution by group\n\n\ngeom_violin()\nViolin plot\nDistribution shape\n\n\ngeom_jitter()\nJittered points\nAvoid overplotting\n\n\ngeom_area()\nArea plot\nFilled area\n\n\ngeom_tile()\nHeatmap tiles\nGrid data\n\n\ngeom_text()\nText labels\nAdd text\n\n\ngeom_label()\nText with background\nLabeled points\n\n\ngeom_errorbar()\nError bars\nUncertainty\n\n\ngeom_hline()\nHorizontal line\nReference line\n\n\ngeom_vline()\nVertical line\nReference line\n\n\ngeom_abline()\nDiagonal line\ny = a + bx\n\n\n\n\n\n28.7.3 Aesthetics\n\n\n\nAesthetic\nDescription\nExample\n\n\n\n\nx\nX-axis variable\naes(x = var)\n\n\ny\nY-axis variable\naes(y = var)\n\n\ncolor\nPoint/line color\naes(color = group)\n\n\nfill\nFill color\naes(fill = group)\n\n\nsize\nPoint/line size\naes(size = value)\n\n\nshape\nPoint shape\naes(shape = group)\n\n\nlinetype\nLine type\naes(linetype = group)\n\n\nalpha\nTransparency\naes(alpha = value)\n\n\ngroup\nGrouping\naes(group = id)\n\n\n\n\n\n28.7.4 Scales and Labels\n\n\n\n\n\n\n\n\nFunction\nDescription\nExample\n\n\n\n\nlabs()\nAdd labels\nlabs(title = \"Title\", x = \"X\", y = \"Y\")\n\n\nscale_x_continuous()\nContinuous x scale\nscale_x_continuous(limits = c(0, 100))\n\n\nscale_y_continuous()\nContinuous y scale\nscale_y_continuous(breaks = seq(0, 10, 2))\n\n\nscale_x_log10()\nLog10 x scale\nscale_x_log10()\n\n\nscale_color_manual()\nManual colors\nscale_color_manual(values = c(\"red\", \"blue\"))\n\n\nscale_fill_brewer()\nColorBrewer palette\nscale_fill_brewer(palette = \"Set1\")\n\n\nscale_fill_viridis_d()\nViridis discrete\nscale_fill_viridis_d()\n\n\ncoord_flip()\nFlip coordinates\ncoord_flip()\n\n\ncoord_polar()\nPolar coordinates\ncoord_polar()\n\n\n\n\n\n28.7.5 Faceting\n\n\n\nFunction\nDescription\nExample\n\n\n\n\nfacet_wrap()\nWrap into panels\nfacet_wrap(~variable)\n\n\nfacet_grid()\nGrid of panels\nfacet_grid(rows ~ cols)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>R Command Reference</span>"
    ]
  },
  {
    "objectID": "chapters/A2-r-reference.html#sec-r-stats",
    "href": "chapters/A2-r-reference.html#sec-r-stats",
    "title": "28  R Command Reference",
    "section": "28.8 Statistical Functions",
    "text": "28.8 Statistical Functions\n\n\n\nFunction\nDescription\nExample\n\n\n\n\nt.test()\nT-test\nt.test(x, y)\n\n\ncor()\nCorrelation\ncor(x, y)\n\n\ncor.test()\nCorrelation test\ncor.test(x, y)\n\n\nlm()\nLinear model\nlm(y ~ x, data = df)\n\n\nglm()\nGeneralized linear model\nglm(y ~ x, family = binomial)\n\n\naov()\nANOVA\naov(y ~ group, data = df)\n\n\nchisq.test()\nChi-squared test\nchisq.test(table)\n\n\nwilcox.test()\nWilcoxon test\nwilcox.test(x, y)\n\n\nks.test()\nKolmogorov-Smirnov test\nks.test(x, y)\n\n\nsummary()\nModel summary\nsummary(model)\n\n\ncoef()\nModel coefficients\ncoef(model)\n\n\nresiduals()\nModel residuals\nresiduals(model)\n\n\npredict()\nPredictions\npredict(model, newdata)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>R Command Reference</span>"
    ]
  },
  {
    "objectID": "chapters/A2-r-reference.html#sec-r-stringr",
    "href": "chapters/A2-r-reference.html#sec-r-stringr",
    "title": "28  R Command Reference",
    "section": "28.9 String Functions (stringr)",
    "text": "28.9 String Functions (stringr)\n\n\n\n\n\n\n\n\nFunction\nDescription\nExample\n\n\n\n\nstr_length()\nString length\nstr_length(\"hello\")\n\n\nstr_sub()\nSubstring\nstr_sub(\"hello\", 1, 3)\n\n\nstr_c()\nConcatenate\nstr_c(\"a\", \"b\", sep = \"-\")\n\n\nstr_detect()\nDetect pattern\nstr_detect(x, \"pattern\")\n\n\nstr_replace()\nReplace first match\nstr_replace(x, \"old\", \"new\")\n\n\nstr_replace_all()\nReplace all matches\nstr_replace_all(x, \"old\", \"new\")\n\n\nstr_split()\nSplit string\nstr_split(x, \",\")\n\n\nstr_trim()\nRemove whitespace\nstr_trim(x)\n\n\nstr_to_lower()\nLowercase\nstr_to_lower(x)\n\n\nstr_to_upper()\nUppercase\nstr_to_upper(x)\n\n\nstr_to_title()\nTitle case\nstr_to_title(x)\n\n\nstr_extract()\nExtract match\nstr_extract(x, \"[0-9]+\")\n\n\nstr_count()\nCount matches\nstr_count(x, \"a\")",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>R Command Reference</span>"
    ]
  },
  {
    "objectID": "chapters/A2-r-reference.html#sec-r-control",
    "href": "chapters/A2-r-reference.html#sec-r-control",
    "title": "28  R Command Reference",
    "section": "28.10 Control Flow",
    "text": "28.10 Control Flow\n\n28.10.1 Conditionals\n# if-else\nif (condition) {\n  # code if TRUE\n} else if (other_condition) {\n  # code if other TRUE\n} else {\n  # code if all FALSE\n}\n\n# Vectorized if-else\nifelse(condition, value_if_true, value_if_false)\n\n# dplyr case_when\ncase_when(\n  condition1 ~ value1,\n  condition2 ~ value2,\n  TRUE ~ default_value\n)\n\n\n28.10.2 Loops\n# for loop\nfor (i in 1:10) {\n  print(i)\n}\n\n# while loop\nwhile (condition) {\n  # code\n}\n\n# Apply functions (preferred)\nlapply(list, function)   # Returns list\nsapply(list, function)   # Returns vector\nmap(list, function)      # purrr, returns list\nmap_dbl(list, function)  # purrr, returns double vector",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>R Command Reference</span>"
    ]
  },
  {
    "objectID": "chapters/A2-r-reference.html#sec-r-packages",
    "href": "chapters/A2-r-reference.html#sec-r-packages",
    "title": "28  R Command Reference",
    "section": "28.11 Package Management",
    "text": "28.11 Package Management\n\n\n\n\n\n\n\n\nFunction\nDescription\nExample\n\n\n\n\ninstall.packages()\nInstall from CRAN\ninstall.packages(\"dplyr\")\n\n\nlibrary()\nLoad package\nlibrary(dplyr)\n\n\nrequire()\nLoad (returns TRUE/FALSE)\nrequire(dplyr)\n\n\ninstalled.packages()\nList installed\ninstalled.packages()\n\n\nupdate.packages()\nUpdate all\nupdate.packages()\n\n\nremove.packages()\nRemove package\nremove.packages(\"dplyr\")\n\n\npackageVersion()\nPackage version\npackageVersion(\"dplyr\")",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>R Command Reference</span>"
    ]
  },
  {
    "objectID": "chapters/A3-probability-distributions.html",
    "href": "chapters/A3-probability-distributions.html",
    "title": "29  Common Probability Distributions",
    "section": "",
    "text": "29.1 Discrete Distributions\nThis appendix provides a reference for probability distributions commonly encountered in biological and bioengineering applications. Each distribution is characterized by its probability function, parameters, mean, variance, and typical applications.\nDiscrete distributions describe random variables that take on countable values (integers).",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Common Probability Distributions</span>"
    ]
  },
  {
    "objectID": "chapters/A3-probability-distributions.html#discrete-distributions",
    "href": "chapters/A3-probability-distributions.html#discrete-distributions",
    "title": "29  Common Probability Distributions",
    "section": "",
    "text": "29.1.1 Bernoulli Distribution\nThe Bernoulli distribution describes a single trial with two possible outcomes (success/failure).\nProbability mass function: \\[P(X = k) = p^k(1-p)^{1-k}, \\quad k \\in \\{0, 1\\}\\]\nParameters: \\(p\\) = probability of success (0 ≤ p ≤ 1)\nMean: \\(E[X] = p\\)\nVariance: \\(\\text{Var}(X) = p(1-p)\\)\nApplications:\n\nSingle coin flip\nWhether a patient responds to treatment\nWhether an allele is inherited\n\n\n\nCode\n# Bernoulli with different p values\np_vals &lt;- c(0.2, 0.5, 0.8)\ndata.frame(\n  outcome = rep(c(\"Failure\", \"Success\"), 3),\n  p = rep(p_vals, each = 2),\n  prob = c(1-p_vals[1], p_vals[1], 1-p_vals[2], p_vals[2], 1-p_vals[3], p_vals[3])\n) %&gt;%\n  ggplot(aes(x = outcome, y = prob, fill = factor(p))) +\n  geom_col(position = \"dodge\") +\n  labs(title = \"Bernoulli Distribution\", y = \"Probability\", fill = \"p\")\n\n\n\n\n\n\n\n\n\n\n\n29.1.2 Binomial Distribution\nThe binomial distribution describes the number of successes in \\(n\\) independent Bernoulli trials.\nProbability mass function: \\[P(X = k) = \\binom{n}{k} p^k (1-p)^{n-k}\\]\nParameters:\n\n\\(n\\) = number of trials\n\\(p\\) = probability of success on each trial\n\nMean: \\(E[X] = np\\)\nVariance: \\(\\text{Var}(X) = np(1-p)\\)\nApplications:\n\nNumber of heads in \\(n\\) coin flips\nNumber of mutant alleles in offspring\nNumber of patients responding to treatment out of \\(n\\) treated\n\n\n\nCode\n# Binomial distributions with n=20\npar(mfrow = c(1, 3))\nfor (p in c(0.2, 0.5, 0.8)) {\n  x &lt;- 0:20\n  barplot(dbinom(x, 20, p), names.arg = x,\n          main = paste(\"Binomial(n=20, p=\", p, \")\"),\n          xlab = \"k\", ylab = \"P(X=k)\", col = \"steelblue\")\n}\n\n\n\n\n\n\n\n\n\nR functions: dbinom(), pbinom(), qbinom(), rbinom()\n\n\n29.1.3 Geometric Distribution\nThe geometric distribution describes the number of trials needed to get the first success.\nProbability mass function: \\[P(X = k) = (1-p)^{k-1}p, \\quad k = 1, 2, 3, \\ldots\\]\nParameters: \\(p\\) = probability of success\nMean: \\(E[X] = \\frac{1}{p}\\)\nVariance: \\(\\text{Var}(X) = \\frac{1-p}{p^2}\\)\nApplications:\n\nNumber of trials until first success\nTime until extinction of an endangered population\nNumber of reads until finding a specific sequence\n\n\n\nCode\n# Geometric distribution example\n# If extinction probability is 0.1 per year, expected time to extinction?\np &lt;- 0.1\nx &lt;- 1:50\nplot(x, dgeom(x-1, p), type = \"h\", lwd = 2, col = \"steelblue\",\n     xlab = \"Years until extinction\", ylab = \"Probability\",\n     main = paste(\"Geometric Distribution (p =\", p, \")\\nMean =\", 1/p, \"years\"))\n\n\n\n\n\n\n\n\n\nR functions: dgeom(), pgeom(), qgeom(), rgeom()\n\n\n29.1.4 Negative Binomial Distribution\nThe negative binomial extends the geometric distribution to describe the number of trials needed to achieve \\(r\\) successes.\nProbability mass function: \\[P(X = k) = \\binom{k-1}{r-1} p^r (1-p)^{k-r}, \\quad k = r, r+1, r+2, \\ldots\\]\nParameters:\n\n\\(r\\) = number of successes needed\n\\(p\\) = probability of success\n\nMean: \\(E[X] = \\frac{r}{p}\\)\nVariance: \\(\\text{Var}(X) = \\frac{r(1-p)}{p^2}\\)\nApplications:\n\nOverdispersed count data (variance &gt; mean)\nRNA-seq count modeling\nNumber of trials until \\(r\\) successes\n\n\n\nCode\n# Negative binomial example\n# Predator must capture 10 prey before reproduction\nr &lt;- 10\np &lt;- 0.1\nx &lt;- r:100\nplot(x, dnbinom(x - r, size = r, prob = p), type = \"h\", lwd = 2, col = \"steelblue\",\n     xlab = \"Days until reproduction\", ylab = \"Probability\",\n     main = paste(\"Negative Binomial (r=10, p=0.1)\\nMean =\", r/p, \"days\"))\n\n\n\n\n\n\n\n\n\nR functions: dnbinom(), pnbinom(), qnbinom(), rnbinom()\n\n\n29.1.5 Poisson Distribution\nThe Poisson distribution describes the number of events occurring in a fixed interval when events occur independently at a constant average rate.\nProbability mass function: \\[P(X = k) = \\frac{e^{-\\lambda}\\lambda^k}{k!}, \\quad k = 0, 1, 2, \\ldots\\]\nParameters: \\(\\lambda\\) = rate parameter (expected count)\nMean: \\(E[X] = \\lambda\\)\nVariance: \\(\\text{Var}(X) = \\lambda\\)\nNote: For a Poisson distribution, mean equals variance. When observed variance exceeds the mean, the data are called “overdispersed.”\nApplications:\n\nNumber of mutations per gene\nNumber of cells in a microscope field\nNumber of organisms per sample area\nRare event counts in genomics\n\n\n\nCode\n# Poisson distributions with different lambda values\npar(mfrow = c(1, 3))\nfor (lambda in c(1, 5, 15)) {\n  x &lt;- 0:30\n  barplot(dpois(x, lambda), names.arg = x,\n          main = paste(\"Poisson(λ =\", lambda, \")\"),\n          xlab = \"k\", ylab = \"P(X=k)\", col = \"steelblue\")\n}\n\n\n\n\n\n\n\n\n\nR functions: dpois(), ppois(), qpois(), rpois()",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Common Probability Distributions</span>"
    ]
  },
  {
    "objectID": "chapters/A3-probability-distributions.html#continuous-distributions",
    "href": "chapters/A3-probability-distributions.html#continuous-distributions",
    "title": "29  Common Probability Distributions",
    "section": "29.2 Continuous Distributions",
    "text": "29.2 Continuous Distributions\nContinuous distributions describe random variables that can take any value in some range.\n\n29.2.1 Uniform Distribution\nThe uniform distribution assigns equal probability to all values in a specified range.\nProbability density function: \\[f(x) = \\frac{1}{b-a}, \\quad a \\leq x \\leq b\\]\nParameters:\n\n\\(a\\) = minimum value\n\\(b\\) = maximum value\n\nMean: \\(E[X] = \\frac{a+b}{2}\\)\nVariance: \\(\\text{Var}(X) = \\frac{(b-a)^2}{12}\\)\nApplications:\n\nRandom number generation\nUninformative priors in Bayesian analysis\nEqual probability among alternatives\n\n\n\nCode\nx &lt;- seq(-1, 6, length.out = 200)\nplot(x, dunif(x, min = 0, max = 5), type = \"l\", lwd = 2, col = \"steelblue\",\n     xlab = \"x\", ylab = \"f(x)\", main = \"Uniform Distribution (a=0, b=5)\",\n     ylim = c(0, 0.3))\npolygon(c(0, 0, 5, 5), c(0, 0.2, 0.2, 0), col = rgb(0, 0, 1, 0.3), border = NA)\n\n\n\n\n\n\n\n\n\nR functions: dunif(), punif(), qunif(), runif()\n\n\n29.2.2 Exponential Distribution\nThe exponential distribution describes the time between events in a Poisson process.\nProbability density function: \\[f(x) = \\lambda e^{-\\lambda x}, \\quad x \\geq 0\\]\nParameters: \\(\\lambda\\) = rate parameter\nMean: \\(E[X] = \\frac{1}{\\lambda}\\)\nVariance: \\(\\text{Var}(X) = \\frac{1}{\\lambda^2}\\)\nApplications:\n\nTime until next event (radioactive decay, mutations)\nLifespan distributions (constant hazard rate)\nWaiting times\n\n\n\nCode\n# Exponential with different rate parameters\nx &lt;- seq(0, 5, length.out = 200)\nplot(x, dexp(x, rate = 0.5), type = \"l\", lwd = 2, col = \"blue\",\n     xlab = \"x\", ylab = \"f(x)\", main = \"Exponential Distribution\",\n     ylim = c(0, 2))\nlines(x, dexp(x, rate = 1), lwd = 2, col = \"red\")\nlines(x, dexp(x, rate = 2), lwd = 2, col = \"darkgreen\")\nlegend(\"topright\", c(\"λ = 0.5\", \"λ = 1\", \"λ = 2\"),\n       col = c(\"blue\", \"red\", \"darkgreen\"), lwd = 2)\n\n\n\n\n\n\n\n\n\nR functions: dexp(), pexp(), qexp(), rexp()\n\n\n29.2.3 Gamma Distribution\nThe gamma distribution generalizes the exponential distribution to describe waiting time until the \\(r\\)th event.\nProbability density function: \\[f(x) = \\frac{\\lambda^r}{\\Gamma(r)} x^{r-1} e^{-\\lambda x}, \\quad x \\geq 0\\]\nParameters:\n\n\\(r\\) = shape parameter (number of events)\n\\(\\lambda\\) = rate parameter\n\nMean: \\(E[X] = \\frac{r}{\\lambda}\\)\nVariance: \\(\\text{Var}(X) = \\frac{r}{\\lambda^2}\\)\nApplications:\n\nTime for multiple events to occur\nDuration of processes with multiple stages\nPrior distributions in Bayesian analysis\n\n\n\nCode\n# Gamma distribution with different shape parameters\nx &lt;- seq(0, 15, length.out = 200)\nplot(x, dgamma(x, shape = 1, rate = 0.5), type = \"l\", lwd = 2, col = \"blue\",\n     xlab = \"x\", ylab = \"f(x)\", main = \"Gamma Distribution (rate = 0.5)\",\n     ylim = c(0, 0.5))\nlines(x, dgamma(x, shape = 2, rate = 0.5), lwd = 2, col = \"red\")\nlines(x, dgamma(x, shape = 5, rate = 0.5), lwd = 2, col = \"darkgreen\")\nlegend(\"topright\", c(\"shape = 1\", \"shape = 2\", \"shape = 5\"),\n       col = c(\"blue\", \"red\", \"darkgreen\"), lwd = 2)\n\n\n\n\n\n\n\n\n\nR functions: dgamma(), pgamma(), qgamma(), rgamma()\n\n\n29.2.4 Normal (Gaussian) Distribution\nThe normal distribution is the most important continuous distribution in statistics. It arises naturally when many independent factors contribute additively to an outcome.\nProbability density function: \\[f(x) = \\frac{1}{\\sqrt{2\\pi}\\sigma} e^{-\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)^2}\\]\nParameters:\n\n\\(\\mu\\) = mean (location)\n\\(\\sigma\\) = standard deviation (scale)\n\nMean: \\(E[X] = \\mu\\)\nVariance: \\(\\text{Var}(X) = \\sigma^2\\)\nNotation: \\(X \\sim \\mathcal{N}(\\mu, \\sigma^2)\\)\nApplications:\n\nHeights, weights, and other biological measurements\nMeasurement errors\nSampling distributions (via Central Limit Theorem)\nBasis for many statistical tests\n\n\n\nCode\nx &lt;- seq(-4, 8, length.out = 200)\nplot(x, dnorm(x, mean = 2, sd = 1), type = \"l\", lwd = 2, col = \"blue\",\n     xlab = \"x\", ylab = \"f(x)\", main = \"Normal Distribution\",\n     ylim = c(0, 0.45))\nlines(x, dnorm(x, mean = 2, sd = 0.5), lwd = 2, col = \"red\")\nlines(x, dnorm(x, mean = 2, sd = 2), lwd = 2, col = \"darkgreen\")\nlegend(\"topright\", c(\"μ=2, σ=1\", \"μ=2, σ=0.5\", \"μ=2, σ=2\"),\n       col = c(\"blue\", \"red\", \"darkgreen\"), lwd = 2)\n\n\n\n\n\n\n\n\n\nKey properties:\n\n68% of values fall within 1 SD of the mean\n95% of values fall within 2 SDs of the mean\n99.7% of values fall within 3 SDs of the mean\n\nR functions: dnorm(), pnorm(), qnorm(), rnorm()\n\n\n29.2.5 Standard Normal Distribution\nThe standard normal distribution is a normal distribution with \\(\\mu = 0\\) and \\(\\sigma = 1\\).\nAny normal variable \\(X \\sim \\mathcal{N}(\\mu, \\sigma^2)\\) can be converted to a standard normal \\(Z\\) using:\n\\[Z = \\frac{X - \\mu}{\\sigma}\\]\nThis z-score tells us how many standard deviations a value is from the mean.\n\n\nCode\nx &lt;- seq(-4, 4, length.out = 200)\ny &lt;- dnorm(x)\n\nplot(x, y, type = \"l\", lwd = 2,\n     xlab = \"z\", ylab = \"f(z)\",\n     main = \"Standard Normal Distribution\")\n\n# Shade areas\nx_1sd &lt;- seq(-1, 1, length.out = 100)\npolygon(c(-1, x_1sd, 1), c(0, dnorm(x_1sd), 0),\n        col = rgb(0, 0, 1, 0.3), border = NA)\n\nx_2sd &lt;- c(seq(-2, -1, length.out = 50), seq(1, 2, length.out = 50))\nfor (region in list(c(-2, -1), c(1, 2))) {\n  xx &lt;- seq(region[1], region[2], length.out = 50)\n  polygon(c(region[1], xx, region[2]), c(0, dnorm(xx), 0),\n          col = rgb(0, 1, 0, 0.3), border = NA)\n}\n\nlegend(\"topright\", c(\"68% within 1σ\", \"95% within 2σ\"),\n       fill = c(rgb(0, 0, 1, 0.3), rgb(0, 1, 0, 0.3)))\n\n\n\n\n\n\n\n\n\n\n\n29.2.6 Log-Normal Distribution\nIf \\(\\ln(X)\\) follows a normal distribution, then \\(X\\) follows a log-normal distribution. This arises when effects are multiplicative rather than additive.\nProbability density function: \\[f(x) = \\frac{1}{x\\sigma\\sqrt{2\\pi}} e^{-\\frac{(\\ln x - \\mu)^2}{2\\sigma^2}}, \\quad x &gt; 0\\]\nParameters:\n\n\\(\\mu\\) = mean of the log-transformed variable\n\\(\\sigma\\) = SD of the log-transformed variable\n\nMean: \\(E[X] = e^{\\mu + \\sigma^2/2}\\)\nVariance: \\(\\text{Var}(X) = (e^{\\sigma^2} - 1)e^{2\\mu + \\sigma^2}\\)\nApplications:\n\nGene expression levels\nOrganism sizes\nConcentrations\nIncome distributions\n\n\n\nCode\npar(mfrow = c(1, 2))\n\n# Log-normal data\nset.seed(42)\nx &lt;- rlnorm(1000, meanlog = 1, sdlog = 0.5)\n\nhist(x, breaks = 30, main = \"Log-Normal Data\", col = \"lightblue\",\n     xlab = \"x\", probability = TRUE)\ncurve(dlnorm(x, meanlog = 1, sdlog = 0.5), add = TRUE, col = \"red\", lwd = 2)\n\n# After log transformation\nhist(log(x), breaks = 30, main = \"After Log Transform\", col = \"lightblue\",\n     xlab = \"log(x)\", probability = TRUE)\ncurve(dnorm(x, mean = 1, sd = 0.5), add = TRUE, col = \"red\", lwd = 2)\n\n\n\n\n\n\n\n\n\nR functions: dlnorm(), plnorm(), qlnorm(), rlnorm()\n\n\n29.2.7 Chi-Square Distribution\nThe chi-square distribution is the distribution of a sum of squared standard normal variables. It is fundamental to many statistical tests.\nParameters: \\(k\\) = degrees of freedom\nMean: \\(E[X] = k\\)\nVariance: \\(\\text{Var}(X) = 2k\\)\nApplications:\n\nGoodness-of-fit tests\nTests of independence\nVariance estimation\n\n\n\nCode\nx &lt;- seq(0, 20, length.out = 200)\nplot(x, dchisq(x, df = 2), type = \"l\", lwd = 2, col = \"blue\",\n     xlab = \"x\", ylab = \"f(x)\", main = \"Chi-Square Distribution\",\n     ylim = c(0, 0.5))\nlines(x, dchisq(x, df = 5), lwd = 2, col = \"red\")\nlines(x, dchisq(x, df = 10), lwd = 2, col = \"darkgreen\")\nlegend(\"topright\", c(\"df = 2\", \"df = 5\", \"df = 10\"),\n       col = c(\"blue\", \"red\", \"darkgreen\"), lwd = 2)\n\n\n\n\n\n\n\n\n\nR functions: dchisq(), pchisq(), qchisq(), rchisq()\n\n\n29.2.8 Student’s t Distribution\nThe t-distribution arises when estimating the mean of a normally distributed population when the sample size is small and the population standard deviation is unknown.\nParameters: \\(\\nu\\) = degrees of freedom\nAs \\(\\nu \\to \\infty\\), the t-distribution approaches the standard normal.\nApplications:\n\nt-tests\nConfidence intervals for means\nRobust regression\n\n\n\nCode\nx &lt;- seq(-4, 4, length.out = 200)\nplot(x, dnorm(x), type = \"l\", lwd = 2, col = \"black\", lty = 2,\n     xlab = \"x\", ylab = \"f(x)\", main = \"t-Distribution vs Normal\")\nlines(x, dt(x, df = 2), lwd = 2, col = \"blue\")\nlines(x, dt(x, df = 5), lwd = 2, col = \"red\")\nlines(x, dt(x, df = 30), lwd = 2, col = \"darkgreen\")\nlegend(\"topright\", c(\"Normal\", \"t (df=2)\", \"t (df=5)\", \"t (df=30)\"),\n       col = c(\"black\", \"blue\", \"red\", \"darkgreen\"), lwd = 2, lty = c(2, 1, 1, 1))\n\n\n\n\n\n\n\n\n\nR functions: dt(), pt(), qt(), rt()\n\n\n29.2.9 F Distribution\nThe F-distribution is the ratio of two chi-square distributions divided by their degrees of freedom. It is used in ANOVA and comparing variances.\nParameters:\n\n\\(d_1\\) = numerator degrees of freedom\n\\(d_2\\) = denominator degrees of freedom\n\nApplications:\n\nANOVA F-tests\nComparing variances\nRegression significance tests\n\n\n\nCode\nx &lt;- seq(0, 5, length.out = 200)\nplot(x, df(x, df1 = 5, df2 = 20), type = \"l\", lwd = 2, col = \"blue\",\n     xlab = \"x\", ylab = \"f(x)\", main = \"F Distribution\",\n     ylim = c(0, 1))\nlines(x, df(x, df1 = 10, df2 = 20), lwd = 2, col = \"red\")\nlines(x, df(x, df1 = 20, df2 = 20), lwd = 2, col = \"darkgreen\")\nlegend(\"topright\", c(\"F(5,20)\", \"F(10,20)\", \"F(20,20)\"),\n       col = c(\"blue\", \"red\", \"darkgreen\"), lwd = 2)\n\n\n\n\n\n\n\n\n\nR functions: df(), pf(), qf(), rf()\n\n\n29.2.10 Beta Distribution\nThe beta distribution is defined on the interval [0, 1] and is useful for modeling proportions and probabilities.\nParameters:\n\n\\(\\alpha\\) = shape parameter 1\n\\(\\beta\\) = shape parameter 2\n\nMean: \\(E[X] = \\frac{\\alpha}{\\alpha + \\beta}\\)\nVariance: \\(\\text{Var}(X) = \\frac{\\alpha\\beta}{(\\alpha+\\beta)^2(\\alpha+\\beta+1)}\\)\nApplications:\n\nPrior distributions for probabilities (Bayesian analysis)\nModeling proportions\nAllele frequencies\n\n\n\nCode\nx &lt;- seq(0, 1, length.out = 200)\nplot(x, dbeta(x, 2, 5), type = \"l\", lwd = 2, col = \"blue\",\n     xlab = \"x\", ylab = \"f(x)\", main = \"Beta Distribution\",\n     ylim = c(0, 3))\nlines(x, dbeta(x, 5, 2), lwd = 2, col = \"red\")\nlines(x, dbeta(x, 2, 2), lwd = 2, col = \"darkgreen\")\nlines(x, dbeta(x, 0.5, 0.5), lwd = 2, col = \"purple\")\nlegend(\"top\", c(\"Beta(2,5)\", \"Beta(5,2)\", \"Beta(2,2)\", \"Beta(0.5,0.5)\"),\n       col = c(\"blue\", \"red\", \"darkgreen\", \"purple\"), lwd = 2)\n\n\n\n\n\n\n\n\n\nR functions: dbeta(), pbeta(), qbeta(), rbeta()",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Common Probability Distributions</span>"
    ]
  },
  {
    "objectID": "chapters/A3-probability-distributions.html#summary-table",
    "href": "chapters/A3-probability-distributions.html#summary-table",
    "title": "29  Common Probability Distributions",
    "section": "29.3 Summary Table",
    "text": "29.3 Summary Table\n\n\n\n\n\n\n\n\n\n\n\nDistribution\nType\nParameters\nMean\nVariance\nR prefix\n\n\n\n\nBernoulli\nDiscrete\np\np\np(1-p)\nbinom (n=1)\n\n\nBinomial\nDiscrete\nn, p\nnp\nnp(1-p)\nbinom\n\n\nGeometric\nDiscrete\np\n1/p\n(1-p)/p²\ngeom\n\n\nNeg. Binomial\nDiscrete\nr, p\nr/p\nr(1-p)/p²\nnbinom\n\n\nPoisson\nDiscrete\nλ\nλ\nλ\npois\n\n\nUniform\nContinuous\na, b\n(a+b)/2\n(b-a)²/12\nunif\n\n\nExponential\nContinuous\nλ\n1/λ\n1/λ²\nexp\n\n\nGamma\nContinuous\nr, λ\nr/λ\nr/λ²\ngamma\n\n\nNormal\nContinuous\nμ, σ\nμ\nσ²\nnorm\n\n\nLog-normal\nContinuous\nμ, σ\nexp(μ+σ²/2)\n(exp(σ²)-1)exp(2μ+σ²)\nlnorm\n\n\nChi-square\nContinuous\nk\nk\n2k\nchisq\n\n\nt\nContinuous\nν\n0 (ν&gt;1)\nν/(ν-2) (ν&gt;2)\nt\n\n\nF\nContinuous\nd₁, d₂\nd₂/(d₂-2)\n…\nf\n\n\nBeta\nContinuous\nα, β\nα/(α+β)\n…\nbeta",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Common Probability Distributions</span>"
    ]
  },
  {
    "objectID": "chapters/A3-probability-distributions.html#r-function-naming-convention",
    "href": "chapters/A3-probability-distributions.html#r-function-naming-convention",
    "title": "29  Common Probability Distributions",
    "section": "29.4 R Function Naming Convention",
    "text": "29.4 R Function Naming Convention\nR uses a consistent naming convention for distribution functions:\n\ndxxx - density/mass function (PDF/PMF)\npxxx - cumulative distribution function (CDF)\nqxxx - quantile function (inverse CDF)\nrxxx - random number generation\n\nFor example, for the normal distribution:\n\n\nCode\n# Density at x = 0 for standard normal\ndnorm(0)\n\n\n[1] 0.3989423\n\n\nCode\n# Probability that X ≤ 1.96\npnorm(1.96)\n\n\n[1] 0.9750021\n\n\nCode\n# Value where P(X ≤ q) = 0.975\nqnorm(0.975)\n\n\n[1] 1.959964\n\n\nCode\n# Generate 5 random normal values\nset.seed(42)\nrnorm(5)\n\n\n[1]  1.3709584 -0.5646982  0.3631284  0.6328626  0.4042683",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Common Probability Distributions</span>"
    ]
  },
  {
    "objectID": "chapters/A3-probability-distributions.html#choosing-the-right-distribution",
    "href": "chapters/A3-probability-distributions.html#choosing-the-right-distribution",
    "title": "29  Common Probability Distributions",
    "section": "29.5 Choosing the Right Distribution",
    "text": "29.5 Choosing the Right Distribution\n\n\n\n\n\n\n\n\n\nGuidelines for selection:\n\nIs your variable discrete or continuous?\n\nDiscrete: Binomial, Poisson, Negative Binomial\nContinuous: Normal, Gamma, Beta, etc.\n\nFor counts:\n\nFixed number of trials with yes/no: Binomial\nEvents per unit time/space, rare: Poisson\nOverdispersed counts (variance &gt; mean): Negative Binomial\n\nFor continuous measurements:\n\nSymmetric, unbounded: Normal\nPositive, right-skewed: Log-normal or Gamma\nBounded between 0 and 1: Beta\nWaiting times: Exponential or Gamma\n\nFor testing:\n\nMeans (unknown variance): t-distribution\nVariances: Chi-square or F-distribution",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Common Probability Distributions</span>"
    ]
  },
  {
    "objectID": "chapters/A4-greek-letters.html",
    "href": "chapters/A4-greek-letters.html",
    "title": "30  Greek Letters in Mathematics and Statistics",
    "section": "",
    "text": "30.1 The Greek Alphabet\nGreek letters are ubiquitous in mathematics and statistics. This appendix provides a reference for the Greek alphabet, including pronunciation and common uses in statistical contexts.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Greek Letters in Mathematics and Statistics</span>"
    ]
  },
  {
    "objectID": "chapters/A4-greek-letters.html#the-greek-alphabet",
    "href": "chapters/A4-greek-letters.html#the-greek-alphabet",
    "title": "30  Greek Letters in Mathematics and Statistics",
    "section": "",
    "text": "Letter\nLowercase\nUppercase\nName\nPronunciation\nCommon Statistical Uses\n\n\n\n\n1\nα\nΑ\nAlpha\nAL-fah\nSignificance level (Type I error rate); regression intercept\n\n\n2\nβ\nΒ\nBeta\nBAY-tah\nType II error rate; regression coefficients; slope parameters\n\n\n3\nγ\nΓ\nGamma\nGAM-ah\nGamma distribution; shape parameter\n\n\n4\nδ\nΔ\nDelta\nDEL-tah\nChange or difference; effect size (Cohen’s d uses Roman d)\n\n\n5\nε\nΕ\nEpsilon\nEP-si-lon\nError term in models; small quantity approaching zero\n\n\n6\nζ\nΖ\nZeta\nZAY-tah\nRarely used in statistics\n\n\n7\nη\nΗ\nEta\nAY-tah\nEffect size (η²); learning rate\n\n\n8\nθ\nΘ\nTheta\nTHAY-tah\nGeneric parameter; angle\n\n\n9\nι\nΙ\nIota\neye-OH-tah\nRarely used in statistics\n\n\n10\nκ\nΚ\nKappa\nKAP-ah\nCohen’s kappa (agreement); condition number\n\n\n11\nλ\nΛ\nLambda\nLAM-dah\nRate parameter (Poisson, exponential); eigenvalue; Wilks’ lambda\n\n\n12\nμ\nΜ\nMu\nMYOO\nPopulation mean\n\n\n13\nν\nΝ\nNu\nNOO\nDegrees of freedom\n\n\n14\nξ\nΞ\nXi\nKSEE or ZIGH\nRarely used; sometimes for random variables\n\n\n15\nο\nΟ\nOmicron\nOM-i-kron\nRarely used (resembles Roman O)\n\n\n16\nπ\nΠ\nPi\nPIE\nMathematical constant (≈ 3.14159); product notation (Π)\n\n\n17\nρ\nΡ\nRho\nROW\nPopulation correlation coefficient; autocorrelation\n\n\n18\nσ\nΣ\nSigma\nSIG-mah\nPopulation standard deviation (σ); summation (Σ)\n\n\n19\nτ\nΤ\nTau\nTAW (rhymes with cow)\nKendall’s tau; time constant\n\n\n20\nυ\nΥ\nUpsilon\nOOP-si-lon\nRarely used in statistics\n\n\n21\nφ\nΦ\nPhi\nFYE or FEE\nPhi coefficient; standard normal PDF (φ); golden ratio\n\n\n22\nχ\nΧ\nChi\nKYE (rhymes with sky)\nChi-square distribution and test (χ²)\n\n\n23\nψ\nΨ\nPsi\nSIGH or PSEE\nRarely used; sometimes for angles or wave functions\n\n\n24\nω\nΩ\nOmega\noh-MAY-gah\nEffect size (ω²); angular frequency",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Greek Letters in Mathematics and Statistics</span>"
    ]
  },
  {
    "objectID": "chapters/A4-greek-letters.html#most-commonly-used-letters-in-statistics",
    "href": "chapters/A4-greek-letters.html#most-commonly-used-letters-in-statistics",
    "title": "30  Greek Letters in Mathematics and Statistics",
    "section": "30.2 Most Commonly Used Letters in Statistics",
    "text": "30.2 Most Commonly Used Letters in Statistics\n\n30.2.1 Population Parameters\nThe following Greek letters conventionally represent population parameters (true but unknown values):\n\nμ (mu): Population mean\nσ (sigma): Population standard deviation\nσ² (sigma squared): Population variance\nρ (rho): Population correlation coefficient\nπ (pi): Population proportion (also the mathematical constant)\nβ (beta): Population regression coefficients\n\n\n\n30.2.2 Hypothesis Testing\n\nα (alpha): Significance level; probability of Type I error (rejecting a true null hypothesis). Commonly set to 0.05.\nβ (beta): Probability of Type II error (failing to reject a false null hypothesis). Power = 1 - β.\nχ² (chi-square): Test statistic for categorical data analysis\n\n\n\n30.2.3 Effect Sizes\n\nη² (eta squared): Proportion of variance explained in ANOVA\nω² (omega squared): Less biased estimate of variance explained\nφ (phi): Effect size for 2×2 contingency tables\n\n\n\n30.2.4 Distributions\n\nλ (lambda): Rate parameter for Poisson and exponential distributions\nΓ (Gamma): The Gamma function and Gamma distribution\nθ (theta): Generic parameter in probability distributions\n\n\n\n30.2.5 Summation and Products\n\nΣ (capital sigma): Summation notation: \\(\\sum_{i=1}^{n} x_i\\)\nΠ (capital pi): Product notation: \\(\\prod_{i=1}^{n} x_i\\)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Greek Letters in Mathematics and Statistics</span>"
    ]
  },
  {
    "objectID": "chapters/A4-greek-letters.html#writing-greek-letters",
    "href": "chapters/A4-greek-letters.html#writing-greek-letters",
    "title": "30  Greek Letters in Mathematics and Statistics",
    "section": "30.3 Writing Greek Letters",
    "text": "30.3 Writing Greek Letters\n\n30.3.1 In LaTeX and Quarto\nGreek letters are written in LaTeX (and thus in Quarto documents) using backslash commands:\n$\\alpha$    → α        $\\Alpha$    → Α\n$\\beta$     → β        $\\Beta$     → Β\n$\\gamma$    → γ        $\\Gamma$    → Γ\n$\\delta$    → δ        $\\Delta$    → Δ\n$\\epsilon$  → ε        $\\mu$       → μ\n$\\sigma$    → σ        $\\Sigma$    → Σ\n$\\chi$      → χ        $\\lambda$   → λ\n$\\theta$    → θ        $\\rho$      → ρ\n\n\n30.3.2 In R\nR supports Greek letters in plots using the expression() function:\n\n\nCode\n# Axis labels with Greek letters\nplot(x, y,\n     xlab = expression(mu),\n     ylab = expression(sigma^2))\n\n# More complex expressions\ntitle(expression(paste(\"Mean = \", mu, \", SD = \", sigma)))\n\n# In ggplot2\nlibrary(ggplot2)\nggplot(data, aes(x, y)) +\n  geom_point() +\n  labs(x = expression(alpha),\n       y = expression(beta))",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Greek Letters in Mathematics and Statistics</span>"
    ]
  },
  {
    "objectID": "chapters/A4-greek-letters.html#conventions-and-mnemonics",
    "href": "chapters/A4-greek-letters.html#conventions-and-mnemonics",
    "title": "30  Greek Letters in Mathematics and Statistics",
    "section": "30.4 Conventions and Mnemonics",
    "text": "30.4 Conventions and Mnemonics\n\n30.4.1 Roman vs. Greek Letters\nA useful convention in statistics:\n\nGreek letters represent population parameters (unknown, fixed values)\nRoman (Latin) letters represent sample statistics (calculated from data)\n\n\n\n\nPopulation Parameter\nSample Statistic\n\n\n\n\nμ (mu) - population mean\n\\(\\bar{x}\\) - sample mean\n\n\nσ (sigma) - population SD\ns - sample SD\n\n\nρ (rho) - population correlation\nr - sample correlation\n\n\nβ (beta) - population slope\nb - sample slope\n\n\nπ (pi) - population proportion\np̂ - sample proportion\n\n\n\n\n\n30.4.2 Memory Aids\n\nα (alpha) for significance level: “A” comes first, and we set alpha first before testing\nβ (beta) for slope: “B” is for the coefficient “B” in y = a + bx\nμ (mu) for mean: Think “μ” looks like a “u” turned sideways—“u” for “average of you all”\nσ (sigma) for standard deviation: “S” for Sigma, “S” for Standard deviation\nΣ (capital sigma) for sum: “S” for Sum\nρ (rho) for correlation: “R” for Rho, “R” for R-value (correlation)\nχ (chi) for chi-square: Looks like an “X”—think “X marks the spot” for testing categories",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Greek Letters in Mathematics and Statistics</span>"
    ]
  },
  {
    "objectID": "chapters/A4-greek-letters.html#common-formulas-using-greek-letters",
    "href": "chapters/A4-greek-letters.html#common-formulas-using-greek-letters",
    "title": "30  Greek Letters in Mathematics and Statistics",
    "section": "30.5 Common Formulas Using Greek Letters",
    "text": "30.5 Common Formulas Using Greek Letters\n\n30.5.1 Normal Distribution\n\\[f(x) = \\frac{1}{\\sigma\\sqrt{2\\pi}} e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}\\]\n\n\n30.5.2 Z-score\n\\[z = \\frac{x - \\mu}{\\sigma}\\]\n\n\n30.5.3 Linear Regression Model\n\\[Y_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i\\]\nwhere \\(\\epsilon_i \\sim N(0, \\sigma^2)\\)\n\n\n30.5.4 Correlation Coefficient\n\\[\\rho = \\frac{\\text{Cov}(X, Y)}{\\sigma_X \\sigma_Y}\\]\n\n\n30.5.5 Chi-Square Test Statistic\n\\[\\chi^2 = \\sum \\frac{(O - E)^2}{E}\\]\n\n\n30.5.6 ANOVA F-ratio\n\\[F = \\frac{MS_{\\text{between}}}{MS_{\\text{within}}} = \\frac{\\sigma^2_{\\text{between}}}{\\sigma^2_{\\text{within}}}\\]",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Greek Letters in Mathematics and Statistics</span>"
    ]
  },
  {
    "objectID": "chapters/A4-greek-letters.html#summary",
    "href": "chapters/A4-greek-letters.html#summary",
    "title": "30  Greek Letters in Mathematics and Statistics",
    "section": "30.6 Summary",
    "text": "30.6 Summary\nMastering Greek letters is essential for reading and writing statistical notation. The most important letters to memorize are:\n\nμ, σ, ρ: Population mean, standard deviation, and correlation\nα, β: Significance level and Type II error (or regression coefficients)\nΣ: Summation\nχ²: Chi-square\n\nWith practice, these symbols become as natural as the Roman alphabet, and they provide a universal language for expressing statistical concepts precisely and concisely.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Greek Letters in Mathematics and Statistics</span>"
    ]
  },
  {
    "objectID": "chapters/A5-sampling-distributions.html",
    "href": "chapters/A5-sampling-distributions.html",
    "title": "31  Sampling Distributions in Hypothesis Testing",
    "section": "",
    "text": "31.1 Why Sampling Distributions Matter\nThis appendix provides a reference for the statistical distributions used in hypothesis testing. While Chapter 29 covers probability distributions for modeling data, this appendix focuses on sampling distributions—the theoretical distributions that test statistics follow under the null hypothesis.\nWhen we conduct a hypothesis test, we calculate a test statistic from our sample data. To determine whether this statistic is “unusual,” we need to know what values to expect if the null hypothesis were true. The sampling distribution tells us exactly this—it’s the distribution of the test statistic across all possible samples.\nCode\n# Demonstrate: sampling distribution of the mean\nset.seed(42)\npopulation &lt;- rnorm(100000, mean = 100, sd = 15)\n\n# Take many samples and compute means\nsample_means &lt;- replicate(5000, mean(sample(population, 30)))\n\nhist(sample_means, breaks = 40, col = \"lightblue\",\n     main = \"Sampling Distribution of the Mean\",\n     xlab = \"Sample Mean (n = 30)\",\n     probability = TRUE)\n\n# Overlay theoretical normal\nx &lt;- seq(min(sample_means), max(sample_means), length.out = 100)\nlines(x, dnorm(x, mean = 100, sd = 15/sqrt(30)), col = \"red\", lwd = 2)\n\nlegend(\"topright\",\n       legend = c(\"Simulated\", \"Theoretical\"),\n       fill = c(\"lightblue\", NA),\n       border = c(\"black\", NA),\n       lty = c(NA, 1), lwd = c(NA, 2),\n       col = c(NA, \"red\"))",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Sampling Distributions in Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "chapters/A5-sampling-distributions.html#the-standard-normal-z-distribution",
    "href": "chapters/A5-sampling-distributions.html#the-standard-normal-z-distribution",
    "title": "31  Sampling Distributions in Hypothesis Testing",
    "section": "31.2 The Standard Normal (Z) Distribution",
    "text": "31.2 The Standard Normal (Z) Distribution\n\n31.2.1 When It’s Used\nThe standard normal distribution is used when:\n\nTesting means with known population variance\nLarge samples (n &gt; 30) where CLT applies\nTesting proportions with large samples\n\n\n\n31.2.2 The Distribution\n\\[Z = \\frac{\\bar{X} - \\mu}{\\sigma / \\sqrt{n}}\\]\nUnder \\(H_0\\), \\(Z \\sim N(0, 1)\\)\n\n\nCode\nx &lt;- seq(-4, 4, length.out = 200)\ny &lt;- dnorm(x)\n\nplot(x, y, type = \"l\", lwd = 2,\n     xlab = \"z\", ylab = \"Density\",\n     main = \"Standard Normal Distribution\")\n\n# Shade rejection regions (two-tailed, α = 0.05)\nx_left &lt;- seq(-4, -1.96, length.out = 50)\nx_right &lt;- seq(1.96, 4, length.out = 50)\n\npolygon(c(-4, x_left, -1.96), c(0, dnorm(x_left), 0),\n        col = rgb(1, 0, 0, 0.3), border = NA)\npolygon(c(1.96, x_right, 4), c(0, dnorm(x_right), 0),\n        col = rgb(1, 0, 0, 0.3), border = NA)\n\nabline(v = c(-1.96, 1.96), lty = 2, col = \"red\")\ntext(0, 0.15, \"95%\\nAcceptance\\nRegion\", cex = 0.9)\ntext(-2.8, 0.05, \"2.5%\", col = \"red\")\ntext(2.8, 0.05, \"2.5%\", col = \"red\")\n\n\n\n\n\n\n\n\n\n\n\n31.2.3 Critical Values\n\n\n\nConfidence Level\nTwo-tailed α\nCritical Z\n\n\n\n\n90%\n0.10\n±1.645\n\n\n95%\n0.05\n±1.960\n\n\n99%\n0.01\n±2.576\n\n\n\n\n\nCode\n# R functions for Z distribution\nqnorm(0.975)  # 97.5th percentile (for two-tailed 95% CI)\n\n\n[1] 1.959964\n\n\nCode\npnorm(1.96)   # Probability below z = 1.96\n\n\n[1] 0.9750021",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Sampling Distributions in Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "chapters/A5-sampling-distributions.html#students-t-distribution",
    "href": "chapters/A5-sampling-distributions.html#students-t-distribution",
    "title": "31  Sampling Distributions in Hypothesis Testing",
    "section": "31.3 Student’s t-Distribution",
    "text": "31.3 Student’s t-Distribution\n\n31.3.1 When It’s Used\nThe t-distribution is used when:\n\nTesting means with unknown population variance (estimated from sample)\nComparing two means (two-sample t-test)\nTesting regression coefficients\nSmall to moderate sample sizes\n\n\n\n31.3.2 The Distribution\n\\[t = \\frac{\\bar{X} - \\mu}{s / \\sqrt{n}}\\]\nUnder \\(H_0\\), \\(t \\sim t_{df}\\) where \\(df = n - 1\\) for one-sample tests.\n\n\n31.3.3 Effect of Degrees of Freedom\nThe t-distribution has heavier tails than the normal, reflecting additional uncertainty from estimating variance. As df increases, t approaches normal:\n\n\nCode\nx &lt;- seq(-4, 4, length.out = 200)\n\nplot(x, dnorm(x), type = \"l\", lwd = 2, col = \"black\",\n     xlab = \"t\", ylab = \"Density\",\n     main = \"t-Distribution: Effect of Degrees of Freedom\")\nlines(x, dt(x, df = 3), lwd = 2, col = \"red\")\nlines(x, dt(x, df = 10), lwd = 2, col = \"blue\")\nlines(x, dt(x, df = 30), lwd = 2, col = \"darkgreen\")\n\nlegend(\"topright\",\n       legend = c(\"Normal (df = ∞)\", \"t (df = 3)\", \"t (df = 10)\", \"t (df = 30)\"),\n       col = c(\"black\", \"red\", \"blue\", \"darkgreen\"),\n       lwd = 2)\n\n\n\n\n\n\n\n\n\nNotice how df = 3 has much heavier tails (more extreme values expected), while df = 30 is nearly indistinguishable from the normal.\n\n\n31.3.4 Critical Values Change with df\n\n\nCode\n# Critical t-values for 95% CI (two-tailed)\ndfs &lt;- c(5, 10, 20, 30, 50, 100, Inf)\nt_crits &lt;- qt(0.975, df = dfs)\n\ndata.frame(\n  df = dfs,\n  critical_t = round(t_crits, 3)\n)\n\n\n   df critical_t\n1   5      2.571\n2  10      2.228\n3  20      2.086\n4  30      2.042\n5  50      2.009\n6 100      1.984\n7 Inf      1.960\n\n\n\n\n31.3.5 Practical Implications\n\n\nCode\n# How confidence interval width depends on sample size\nn_values &lt;- seq(5, 100, by = 5)\nci_multipliers &lt;- qt(0.975, df = n_values - 1)\n\nplot(n_values, ci_multipliers, type = \"b\", pch = 19, col = \"blue\",\n     xlab = \"Sample Size (n)\", ylab = \"t Critical Value (α = 0.05)\",\n     main = \"Why Larger Samples Give Narrower CIs\")\nabline(h = 1.96, lty = 2, col = \"red\")\ntext(80, 2.05, \"Z = 1.96 (infinite df)\", col = \"red\")\n\n\n\n\n\n\n\n\n\nWith small samples, we need a larger critical value to achieve the same confidence level, making confidence intervals wider.\n\n\n31.3.6 R Functions\n\n\nCode\n# t-distribution functions\nqt(0.975, df = 10)    # Critical value for 95% CI with df = 10\n\n\n[1] 2.228139\n\n\nCode\npt(2.228, df = 10)    # Probability below t = 2.228\n\n\n[1] 0.9749941\n\n\nCode\ndt(0, df = 10)        # Density at t = 0\n\n\n[1] 0.3891084",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Sampling Distributions in Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "chapters/A5-sampling-distributions.html#chi-square-χ²-distribution",
    "href": "chapters/A5-sampling-distributions.html#chi-square-χ²-distribution",
    "title": "31  Sampling Distributions in Hypothesis Testing",
    "section": "31.4 Chi-Square (χ²) Distribution",
    "text": "31.4 Chi-Square (χ²) Distribution\n\n31.4.1 When It’s Used\nThe chi-square distribution is used for:\n\nGoodness of fit tests (observed vs. expected frequencies)\nTests of independence (contingency tables)\nTesting variance (one population)\nModel fit in regression (deviance tests)\n\n\n\n31.4.2 The Distribution\nThe chi-square distribution is the sum of squared standard normal variables:\n\\[\\chi^2 = \\sum_{i=1}^{k} Z_i^2\\]\nThe distribution is always positive and right-skewed. As df increases, it becomes more symmetric and approaches normality.\n\n\nCode\nx &lt;- seq(0, 30, length.out = 200)\n\nplot(x, dchisq(x, df = 2), type = \"l\", lwd = 2, col = \"red\",\n     xlab = expression(chi^2), ylab = \"Density\",\n     main = expression(paste(chi^2, \" Distribution\")),\n     ylim = c(0, 0.3))\nlines(x, dchisq(x, df = 5), lwd = 2, col = \"blue\")\nlines(x, dchisq(x, df = 10), lwd = 2, col = \"darkgreen\")\nlines(x, dchisq(x, df = 20), lwd = 2, col = \"purple\")\n\nlegend(\"topright\",\n       legend = c(\"df = 2\", \"df = 5\", \"df = 10\", \"df = 20\"),\n       col = c(\"red\", \"blue\", \"darkgreen\", \"purple\"),\n       lwd = 2)\n\n\n\n\n\n\n\n\n\n\n\n31.4.3 Properties\n\nMean: \\(E[\\chi^2] = df\\)\nVariance: \\(Var(\\chi^2) = 2 \\times df\\)\nAlways positive (sums of squares)\nRight-skewed, especially for small df\n\n\n\n31.4.4 Critical Values for Common Tests\n\n\nCode\n# Chi-square critical values (right-tail, α = 0.05)\ndfs &lt;- c(1, 2, 3, 5, 10, 20)\nchi_crits &lt;- qchisq(0.95, df = dfs)\n\ndata.frame(\n  df = dfs,\n  critical_chi_sq = round(chi_crits, 3),\n  mean = dfs  # Note: critical value is close to df + 2*sqrt(2*df)\n)\n\n\n  df critical_chi_sq mean\n1  1           3.841    1\n2  2           5.991    2\n3  3           7.815    3\n4  5          11.070    5\n5 10          18.307   10\n6 20          31.410   20\n\n\n\n\n31.4.5 R Functions\n\n\nCode\n# Chi-square distribution functions\nqchisq(0.95, df = 5)      # Critical value (right-tail α = 0.05)\n\n\n[1] 11.0705\n\n\nCode\n1 - pchisq(11.07, df = 5) # p-value for chi-square = 11.07\n\n\n[1] 0.05000962\n\n\nCode\ndchisq(5, df = 5)         # Density at chi-square = 5\n\n\n[1] 0.1220415",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Sampling Distributions in Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "chapters/A5-sampling-distributions.html#f-distribution",
    "href": "chapters/A5-sampling-distributions.html#f-distribution",
    "title": "31  Sampling Distributions in Hypothesis Testing",
    "section": "31.5 F Distribution",
    "text": "31.5 F Distribution\n\n31.5.1 When It’s Used\nThe F distribution is used for:\n\nComparing two variances (F-test)\nANOVA (comparing means of multiple groups)\nTesting overall significance in regression\nComparing nested models\n\n\n\n31.5.2 The Distribution\nThe F distribution is the ratio of two chi-square distributions:\n\\[F = \\frac{\\chi^2_1 / df_1}{\\chi^2_2 / df_2}\\]\n\n\\(df_1\\): numerator degrees of freedom (between-groups)\n\\(df_2\\): denominator degrees of freedom (within-groups or error)\n\n\n\nCode\nx &lt;- seq(0, 5, length.out = 200)\n\nplot(x, df(x, df1 = 1, df2 = 10), type = \"l\", lwd = 2, col = \"red\",\n     xlab = \"F\", ylab = \"Density\",\n     main = \"F Distribution\",\n     ylim = c(0, 1))\nlines(x, df(x, df1 = 5, df2 = 10), lwd = 2, col = \"blue\")\nlines(x, df(x, df1 = 10, df2 = 10), lwd = 2, col = \"darkgreen\")\nlines(x, df(x, df1 = 10, df2 = 50), lwd = 2, col = \"purple\")\n\nlegend(\"topright\",\n       legend = c(\"F(1,10)\", \"F(5,10)\", \"F(10,10)\", \"F(10,50)\"),\n       col = c(\"red\", \"blue\", \"darkgreen\", \"purple\"),\n       lwd = 2)\n\n\n\n\n\n\n\n\n\n\n\n31.5.3 Understanding F in ANOVA\nIn ANOVA, F is the ratio of between-group variance to within-group variance:\n\\[F = \\frac{MS_{between}}{MS_{within}} = \\frac{\\text{Signal}}{\\text{Noise}}\\]\n\nLarge F: Groups differ more than expected from random variation\nF ≈ 1: Group differences are similar to within-group variation\n\n\n\nCode\n# Visualize rejection region for ANOVA\nx &lt;- seq(0, 6, length.out = 200)\ny &lt;- df(x, df1 = 3, df2 = 20)  # 4 groups, total n = 24\n\nplot(x, y, type = \"l\", lwd = 2,\n     xlab = \"F\", ylab = \"Density\",\n     main = \"F(3, 20) Distribution for One-Way ANOVA\")\n\n# Critical value and rejection region\nf_crit &lt;- qf(0.95, df1 = 3, df2 = 20)\nx_reject &lt;- seq(f_crit, 6, length.out = 50)\npolygon(c(f_crit, x_reject, 6), c(0, df(x_reject, 3, 20), 0),\n        col = rgb(1, 0, 0, 0.3), border = NA)\n\nabline(v = f_crit, lty = 2, col = \"red\")\ntext(f_crit + 0.3, 0.3, paste(\"F* =\", round(f_crit, 2)), col = \"red\")\ntext(4.5, 0.05, \"Rejection\\nRegion\\n(α = 0.05)\", col = \"red\")\n\n\n\n\n\n\n\n\n\n\n\n31.5.4 Critical Values Table\n\n\nCode\n# F critical values for α = 0.05 (common ANOVA scenarios)\n# Rows: numerator df (groups - 1)\n# Columns: denominator df (total n - groups)\n\ndf1_vals &lt;- c(1, 2, 3, 4, 5)\ndf2_vals &lt;- c(10, 20, 30, 60, 120)\n\nf_table &lt;- outer(df1_vals, df2_vals,\n                 function(d1, d2) round(qf(0.95, d1, d2), 2))\nrownames(f_table) &lt;- paste(\"df1 =\", df1_vals)\ncolnames(f_table) &lt;- paste(\"df2 =\", df2_vals)\n\ncat(\"F Critical Values (α = 0.05)\\n\")\n\n\nF Critical Values (α = 0.05)\n\n\nCode\nprint(f_table)\n\n\n        df2 = 10 df2 = 20 df2 = 30 df2 = 60 df2 = 120\ndf1 = 1     4.96     4.35     4.17     4.00      3.92\ndf1 = 2     4.10     3.49     3.32     3.15      3.07\ndf1 = 3     3.71     3.10     2.92     2.76      2.68\ndf1 = 4     3.48     2.87     2.69     2.53      2.45\ndf1 = 5     3.33     2.71     2.53     2.37      2.29\n\n\n\n\n31.5.5 R Functions\n\n\nCode\n# F distribution functions\nqf(0.95, df1 = 3, df2 = 20)  # Critical F for ANOVA\n\n\n[1] 3.098391\n\n\nCode\n1 - pf(3.5, df1 = 3, df2 = 20)  # p-value for F = 3.5\n\n\n[1] 0.0344931",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Sampling Distributions in Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "chapters/A5-sampling-distributions.html#relationships-between-distributions",
    "href": "chapters/A5-sampling-distributions.html#relationships-between-distributions",
    "title": "31  Sampling Distributions in Hypothesis Testing",
    "section": "31.6 Relationships Between Distributions",
    "text": "31.6 Relationships Between Distributions\nThese distributions are mathematically related:\n\n\n\n\n\n\n\n\n\nKey relationships:\n\nt² = F(1, df): A squared t-statistic follows an F distribution with 1 numerator df\nχ² → Normal: As df increases, chi-square approaches normality\nt → Z: As df → ∞, t-distribution becomes standard normal\nF(1, ∞) = χ²(1): Limiting case of F distribution",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Sampling Distributions in Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "chapters/A5-sampling-distributions.html#choosing-the-right-distribution",
    "href": "chapters/A5-sampling-distributions.html#choosing-the-right-distribution",
    "title": "31  Sampling Distributions in Hypothesis Testing",
    "section": "31.7 Choosing the Right Distribution",
    "text": "31.7 Choosing the Right Distribution\n\n\n\nTest\nDistribution\nDegrees of Freedom\n\n\n\n\nZ-test (known σ)\nNormal\nN/A\n\n\nOne-sample t-test\nt\nn - 1\n\n\nTwo-sample t-test\nt\nn₁ + n₂ - 2 (pooled)\n\n\nPaired t-test\nt\nn - 1\n\n\nChi-square GOF\nχ²\nk - 1\n\n\nChi-square independence\nχ²\n(r-1)(c-1)\n\n\nOne-way ANOVA\nF\nk-1, N-k\n\n\nRegression F-test\nF\np, n-p-1\n\n\nRegression coefficient\nt\nn - p - 1\n\n\n\nwhere k = number of groups/categories, n = sample size, p = number of predictors",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Sampling Distributions in Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "chapters/A5-sampling-distributions.html#degrees-of-freedom-intuition",
    "href": "chapters/A5-sampling-distributions.html#degrees-of-freedom-intuition",
    "title": "31  Sampling Distributions in Hypothesis Testing",
    "section": "31.8 Degrees of Freedom: Intuition",
    "text": "31.8 Degrees of Freedom: Intuition\nDegrees of freedom represent the number of independent pieces of information available for estimation. They decrease when we estimate parameters from the data:\n\nSample mean: Uses 1 df → leaves n-1 for variance estimation\nTwo groups: Estimate 2 means → lose 2 df from total\nRegression: Estimate p+1 coefficients → leaves n-p-1 error df\n\n\n\nCode\n# Demonstration: Why df matters\n# Sampling distribution of sample variance with different n\n\nset.seed(42)\ntrue_variance &lt;- 100\n\nsimulate_s2 &lt;- function(n, reps = 5000) {\n  replicate(reps, var(rnorm(n, mean = 0, sd = 10)))\n}\n\ns2_small &lt;- simulate_s2(5)    # df = 4\ns2_medium &lt;- simulate_s2(20)  # df = 19\ns2_large &lt;- simulate_s2(50)   # df = 49\n\npar(mfrow = c(1, 3))\nhist(s2_small, breaks = 30, main = \"n = 5 (df = 4)\",\n     xlab = \"Sample Variance\", col = \"lightblue\", xlim = c(0, 400))\nabline(v = 100, col = \"red\", lwd = 2)\n\nhist(s2_medium, breaks = 30, main = \"n = 20 (df = 19)\",\n     xlab = \"Sample Variance\", col = \"lightblue\", xlim = c(0, 400))\nabline(v = 100, col = \"red\", lwd = 2)\n\nhist(s2_large, breaks = 30, main = \"n = 50 (df = 49)\",\n     xlab = \"Sample Variance\", col = \"lightblue\", xlim = c(0, 400))\nabline(v = 100, col = \"red\", lwd = 2)\n\n\n\n\n\n\n\n\n\nWith more degrees of freedom: - Variance estimates are more precise (narrower distribution) - More likely to be close to the true value - Critical values move closer to their limiting values",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Sampling Distributions in Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "chapters/A5-sampling-distributions.html#summary",
    "href": "chapters/A5-sampling-distributions.html#summary",
    "title": "31  Sampling Distributions in Hypothesis Testing",
    "section": "31.9 Summary",
    "text": "31.9 Summary\n\n\n\nDistribution\nParameters\nMean\nUse For\n\n\n\n\nNormal (Z)\nNone\n0\nMeans (known σ), proportions\n\n\nt\ndf\n0\nMeans (unknown σ), regression\n\n\nChi-square\ndf\ndf\nFrequencies, variance, GOF\n\n\nF\ndf₁, df₂\ndf₂/(df₂-2)\nANOVA, comparing variances\n\n\n\nRemember: - More data (higher df) → distributions approach their limits - The t approaches Z, χ² becomes symmetric, F becomes more peaked - Heavier tails in t and F require larger critical values for small df - These distributions assume normality of underlying data (robustness varies)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Sampling Distributions in Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "chapters/A6-unix-reference.html",
    "href": "chapters/A6-unix-reference.html",
    "title": "32  Unix Command Reference",
    "section": "",
    "text": "32.1 Navigation Commands\nThis appendix provides a comprehensive reference for Unix/Linux commands covered throughout the book.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Unix Command Reference</span>"
    ]
  },
  {
    "objectID": "chapters/A6-unix-reference.html#sec-unix-nav",
    "href": "chapters/A6-unix-reference.html#sec-unix-nav",
    "title": "32  Unix Command Reference",
    "section": "",
    "text": "Command\nDescription\nExample\n\n\n\n\npwd\nPrint working directory\npwd\n\n\nls\nList directory contents\nls -la\n\n\nls -l\nLong format listing\nls -l *.txt\n\n\nls -a\nShow hidden files\nls -a\n\n\nls -h\nHuman-readable sizes\nls -lh\n\n\nls -R\nRecursive listing\nls -R projects/\n\n\nls -t\nSort by modification time\nls -lt\n\n\ncd\nChange directory\ncd ~/projects\n\n\ncd ..\nGo up one directory\ncd ..\n\n\ncd -\nGo to previous directory\ncd -\n\n\ncd ~\nGo to home directory\ncd ~\n\n\nmkdir\nCreate directory\nmkdir data\n\n\nmkdir -p\nCreate nested directories\nmkdir -p data/raw/2024\n\n\nrmdir\nRemove empty directory\nrmdir old_folder\n\n\ntree\nDisplay directory tree\ntree -L 2",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Unix Command Reference</span>"
    ]
  },
  {
    "objectID": "chapters/A6-unix-reference.html#sec-unix-files",
    "href": "chapters/A6-unix-reference.html#sec-unix-files",
    "title": "32  Unix Command Reference",
    "section": "32.2 File Operations",
    "text": "32.2 File Operations\n\n\n\n\n\n\n\n\nCommand\nDescription\nExample\n\n\n\n\ncp\nCopy files\ncp file.txt backup/\n\n\ncp -r\nCopy directories recursively\ncp -r data/ backup/\n\n\ncp -i\nInteractive (prompt before overwrite)\ncp -i *.txt dest/\n\n\ncp -v\nVerbose output\ncp -v file.txt backup/\n\n\nmv\nMove or rename files\nmv old.txt new.txt\n\n\nmv -i\nInteractive move\nmv -i *.txt archive/\n\n\nrm\nRemove files\nrm unwanted.txt\n\n\nrm -r\nRemove directories recursively\nrm -r old_directory/\n\n\nrm -i\nInteractive removal\nrm -i *.log\n\n\nrm -f\nForce removal (no prompts)\nrm -f temp*.txt\n\n\ntouch\nCreate empty file / update timestamp\ntouch notes.txt\n\n\nln -s\nCreate symbolic link\nln -s /path/to/file link_name\n\n\nfile\nDetermine file type\nfile mystery_file\n\n\nstat\nDisplay file status\nstat data.csv",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Unix Command Reference</span>"
    ]
  },
  {
    "objectID": "chapters/A6-unix-reference.html#sec-unix-view",
    "href": "chapters/A6-unix-reference.html#sec-unix-view",
    "title": "32  Unix Command Reference",
    "section": "32.3 Viewing File Contents",
    "text": "32.3 Viewing File Contents\n\n\n\nCommand\nDescription\nExample\n\n\n\n\ncat\nDisplay entire file\ncat data.csv\n\n\ncat -n\nDisplay with line numbers\ncat -n script.sh\n\n\nhead\nShow first lines (default 10)\nhead file.txt\n\n\nhead -n\nShow first n lines\nhead -n 20 file.txt\n\n\ntail\nShow last lines (default 10)\ntail file.txt\n\n\ntail -n\nShow last n lines\ntail -n 50 log.txt\n\n\ntail -f\nFollow file (live updates)\ntail -f server.log\n\n\nless\nPage through file\nless huge_file.txt\n\n\nmore\nSimple pager\nmore file.txt\n\n\ndiff\nCompare files\ndiff file1.txt file2.txt\n\n\ndiff -u\nUnified diff format\ndiff -u old.txt new.txt\n\n\ncmp\nCompare files byte by byte\ncmp file1 file2\n\n\nmd5sum\nCalculate MD5 checksum\nmd5sum file.txt\n\n\nsha256sum\nCalculate SHA256 checksum\nsha256sum file.txt",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Unix Command Reference</span>"
    ]
  },
  {
    "objectID": "chapters/A6-unix-reference.html#sec-unix-text",
    "href": "chapters/A6-unix-reference.html#sec-unix-text",
    "title": "32  Unix Command Reference",
    "section": "32.4 Text Processing",
    "text": "32.4 Text Processing\n\n\n\n\n\n\n\n\nCommand\nDescription\nExample\n\n\n\n\ngrep\nSearch for patterns\ngrep \"error\" log.txt\n\n\ngrep -E\nExtended regex\ngrep -E \"gene[0-9]+\" data.txt\n\n\ngrep -v\nInvert match (exclude)\ngrep -v \"^#\" data.txt\n\n\ngrep -c\nCount matches\ngrep -c \"&gt;\" sequences.fa\n\n\ngrep -i\nCase insensitive\ngrep -i \"warning\" log.txt\n\n\ngrep -n\nShow line numbers\ngrep -n \"TODO\" code.py\n\n\ngrep -l\nList matching files only\ngrep -l \"error\" *.log\n\n\ngrep -r\nRecursive search\ngrep -r \"function\" src/\n\n\ngrep -A n\nShow n lines after match\ngrep -A 3 \"error\" log.txt\n\n\ngrep -B n\nShow n lines before match\ngrep -B 2 \"error\" log.txt\n\n\nsort\nSort lines\nsort names.txt\n\n\nsort -n\nNumeric sort\nsort -n numbers.txt\n\n\nsort -r\nReverse sort\nsort -r names.txt\n\n\nsort -k\nSort by column\nsort -k2 data.tsv\n\n\nsort -u\nSort and remove duplicates\nsort -u list.txt\n\n\nuniq\nRemove adjacent duplicates\nsort file | uniq\n\n\nuniq -c\nCount occurrences\nsort file | uniq -c\n\n\nuniq -d\nShow only duplicates\nsort file | uniq -d\n\n\ncut -f\nExtract fields (tab-delimited)\ncut -f2 data.tsv\n\n\ncut -d\nSpecify delimiter\ncut -d',' -f1,3 data.csv\n\n\ncut -c\nExtract characters\ncut -c1-10 file.txt\n\n\ntr\nTranslate characters\ntr 'a-z' 'A-Z'\n\n\ntr -d\nDelete characters\ntr -d '\\r' &lt; file.txt\n\n\ntr -s\nSqueeze repeats\ntr -s ' '\n\n\nsed\nStream editor\nsed 's/old/new/g' file.txt\n\n\nsed -i\nEdit file in place\nsed -i 's/old/new/g' file.txt\n\n\nsed -n\nSuppress output\nsed -n '10,20p' file.txt\n\n\nawk\nPattern processing\nawk '{print $1}' file.txt\n\n\nawk -F\nSpecify field separator\nawk -F',' '{print $2}' data.csv\n\n\nwc\nCount lines, words, bytes\nwc file.txt\n\n\nwc -l\nCount lines only\nwc -l data.csv\n\n\nwc -w\nCount words only\nwc -w essay.txt\n\n\nwc -c\nCount bytes only\nwc -c file.bin\n\n\npaste\nMerge files line by line\npaste file1.txt file2.txt\n\n\njoin\nJoin files on common field\njoin file1.txt file2.txt\n\n\nsplit\nSplit file into pieces\nsplit -l 1000 large.txt",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Unix Command Reference</span>"
    ]
  },
  {
    "objectID": "chapters/A6-unix-reference.html#sec-unix-redirect",
    "href": "chapters/A6-unix-reference.html#sec-unix-redirect",
    "title": "32  Unix Command Reference",
    "section": "32.5 Redirection and Pipes",
    "text": "32.5 Redirection and Pipes\n\n\n\n\n\n\n\n\nOperator\nDescription\nExample\n\n\n\n\n&gt;\nRedirect output (overwrite)\nls &gt; files.txt\n\n\n&gt;&gt;\nRedirect output (append)\necho \"done\" &gt;&gt; log.txt\n\n\n&lt;\nRedirect input\nwc -l &lt; data.txt\n\n\n2&gt;\nRedirect stderr\ncmd 2&gt; errors.txt\n\n\n2&gt;&1\nRedirect stderr to stdout\ncmd &gt; out.txt 2&gt;&1\n\n\n&&gt;\nRedirect both stdout and stderr\ncmd &&gt; all.txt\n\n\n|\nPipe to next command\ncat file | sort | uniq\n\n\n|&\nPipe stdout and stderr\ncmd |& less\n\n\ntee\nWrite to file and stdout\ncmd | tee output.txt\n\n\nxargs\nBuild commands from input\nfind . -name \"*.txt\" | xargs grep \"pattern\"",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Unix Command Reference</span>"
    ]
  },
  {
    "objectID": "chapters/A6-unix-reference.html#sec-unix-compress",
    "href": "chapters/A6-unix-reference.html#sec-unix-compress",
    "title": "32  Unix Command Reference",
    "section": "32.6 File Compression",
    "text": "32.6 File Compression\n\n\n\nCommand\nDescription\nExample\n\n\n\n\ngzip\nCompress file\ngzip large_file.txt\n\n\ngzip -k\nKeep original file\ngzip -k file.txt\n\n\ngzip -d\nDecompress\ngzip -d file.txt.gz\n\n\ngunzip\nDecompress .gz file\ngunzip file.txt.gz\n\n\nzcat\nView compressed file\nzcat data.gz | head\n\n\nzgrep\nSearch compressed file\nzgrep \"pattern\" file.gz\n\n\nzless\nPage through compressed file\nzless data.gz\n\n\nbzip2\nCompress (better ratio)\nbzip2 large_file.txt\n\n\nbunzip2\nDecompress .bz2 file\nbunzip2 file.txt.bz2\n\n\ntar -c\nCreate archive\ntar -cvf archive.tar dir/\n\n\ntar -x\nExtract archive\ntar -xvf archive.tar\n\n\ntar -z\nUse gzip compression\ntar -czvf archive.tar.gz dir/\n\n\ntar -j\nUse bzip2 compression\ntar -cjvf archive.tar.bz2 dir/\n\n\ntar -t\nList archive contents\ntar -tvf archive.tar.gz\n\n\nzip\nCreate zip archive\nzip -r archive.zip dir/\n\n\nunzip\nExtract zip archive\nunzip archive.zip\n\n\nunzip -l\nList zip contents\nunzip -l archive.zip",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Unix Command Reference</span>"
    ]
  },
  {
    "objectID": "chapters/A6-unix-reference.html#sec-unix-permissions",
    "href": "chapters/A6-unix-reference.html#sec-unix-permissions",
    "title": "32  Unix Command Reference",
    "section": "32.7 File Permissions",
    "text": "32.7 File Permissions\n\n\n\n\n\n\n\n\nCommand\nDescription\nExample\n\n\n\n\nchmod\nChange file permissions\nchmod +x script.sh\n\n\nchmod u+x\nAdd execute for owner\nchmod u+x script.sh\n\n\nchmod go-w\nRemove write for group/others\nchmod go-w file.txt\n\n\nchmod 755\nSet rwxr-xr-x\nchmod 755 script.sh\n\n\nchmod 644\nSet rw-r–r–\nchmod 644 data.txt\n\n\nchmod -R\nRecursive permission change\nchmod -R 755 scripts/\n\n\nchown\nChange file owner\nchown user file.txt\n\n\nchown user:group\nChange owner and group\nchown user:group file.txt\n\n\nchown -R\nRecursive ownership change\nchown -R user:group dir/\n\n\nchgrp\nChange group ownership\nchgrp group file.txt\n\n\n\n\n32.7.1 Permission Codes\n\n\n\nMode\nMeaning\nNumeric\n\n\n\n\nr\nRead\n4\n\n\nw\nWrite\n2\n\n\nx\nExecute\n1\n\n\nrwx\nRead, write, execute\n7\n\n\nrw-\nRead, write\n6\n\n\nr-x\nRead, execute\n5\n\n\nr--\nRead only\n4\n\n\n\n\n\n32.7.2 Common Permission Settings\n\n\n\nSetting\nMeaning\nUse Case\n\n\n\n\n755\nrwxr-xr-x\nExecutable scripts\n\n\n644\nrw-r–r–\nRegular files\n\n\n700\nrwx——\nPrivate scripts\n\n\n600\nrw——-\nPrivate files\n\n\n777\nrwxrwxrwx\n(Avoid - too permissive)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Unix Command Reference</span>"
    ]
  },
  {
    "objectID": "chapters/A6-unix-reference.html#sec-unix-system",
    "href": "chapters/A6-unix-reference.html#sec-unix-system",
    "title": "32  Unix Command Reference",
    "section": "32.8 System Information",
    "text": "32.8 System Information\n\n\n\nCommand\nDescription\nExample\n\n\n\n\nwhoami\nCurrent username\nwhoami\n\n\nid\nUser and group IDs\nid\n\n\nhostname\nComputer name\nhostname\n\n\nuname -a\nSystem information\nuname -a\n\n\ndate\nCurrent date/time\ndate\n\n\ndate +%Y-%m-%d\nFormatted date\ndate +%Y-%m-%d\n\n\ncal\nDisplay calendar\ncal\n\n\nuptime\nSystem uptime\nuptime\n\n\ndf -h\nDisk space usage\ndf -h\n\n\ndu -sh\nDirectory size\ndu -sh folder/\n\n\ndu -h --max-depth=1\nSubdirectory sizes\ndu -h --max-depth=1\n\n\nfree -h\nMemory usage\nfree -h\n\n\ntop\nRunning processes (interactive)\ntop\n\n\nhtop\nEnhanced process viewer\nhtop\n\n\nps\nProcess status\nps aux\n\n\nps -ef\nAll processes\nps -ef\n\n\npgrep\nFind process by name\npgrep python\n\n\nkill\nTerminate process\nkill PID\n\n\nkill -9\nForce terminate\nkill -9 PID\n\n\nkillall\nKill by name\nkillall process_name\n\n\nnohup\nRun immune to hangups\nnohup script.sh &\n\n\nbg\nSend to background\nbg\n\n\nfg\nBring to foreground\nfg\n\n\njobs\nList background jobs\njobs\n\n\nwhich\nLocate command\nwhich python\n\n\nwhereis\nLocate binary and man page\nwhereis python\n\n\ntype\nDisplay command type\ntype ls",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Unix Command Reference</span>"
    ]
  },
  {
    "objectID": "chapters/A6-unix-reference.html#sec-unix-find",
    "href": "chapters/A6-unix-reference.html#sec-unix-find",
    "title": "32  Unix Command Reference",
    "section": "32.9 Finding Files",
    "text": "32.9 Finding Files\n\n\n\n\n\n\n\n\nCommand\nDescription\nExample\n\n\n\n\nfind\nFind files by criteria\nfind . -name \"*.txt\"\n\n\nfind -type f\nFind files only\nfind . -type f -name \"*.py\"\n\n\nfind -type d\nFind directories only\nfind . -type d -name \"data\"\n\n\nfind -mtime\nFind by modification time\nfind . -mtime -7 (last 7 days)\n\n\nfind -size\nFind by size\nfind . -size +100M\n\n\nfind -exec\nExecute command on results\nfind . -name \"*.tmp\" -exec rm {} \\;\n\n\nfind -delete\nDelete matching files\nfind . -name \"*.tmp\" -delete\n\n\nlocate\nFast file search (uses database)\nlocate filename\n\n\nupdatedb\nUpdate locate database\nsudo updatedb",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Unix Command Reference</span>"
    ]
  },
  {
    "objectID": "chapters/A6-unix-reference.html#sec-unix-network",
    "href": "chapters/A6-unix-reference.html#sec-unix-network",
    "title": "32  Unix Command Reference",
    "section": "32.10 Network and Remote",
    "text": "32.10 Network and Remote\n\n\n\n\n\n\n\n\nCommand\nDescription\nExample\n\n\n\n\nssh\nSecure shell connection\nssh user@host\n\n\nssh -p\nSpecify port\nssh -p 2222 user@host\n\n\nssh -i\nUse identity file\nssh -i key.pem user@host\n\n\nssh -L\nLocal port forwarding\nssh -L 8080:localhost:80 user@host\n\n\nscp\nSecure copy to remote\nscp file user@host:path/\n\n\nscp -r\nCopy directory recursively\nscp -r dir/ user@host:path/\n\n\nscp\nCopy from remote\nscp user@host:file local_path\n\n\nrsync\nSync files efficiently\nrsync -avz src/ dest/\n\n\nrsync --delete\nSync and delete extra files\nrsync -avz --delete src/ dest/\n\n\nwget\nDownload file\nwget URL\n\n\nwget -O\nDownload with custom name\nwget -O output.txt URL\n\n\nwget -c\nContinue interrupted download\nwget -c URL\n\n\ncurl\nTransfer data\ncurl URL\n\n\ncurl -O\nSave with remote filename\ncurl -O URL\n\n\ncurl -o\nSave with custom filename\ncurl -o output.txt URL\n\n\nping\nTest network connectivity\nping host\n\n\ntraceroute\nShow network path\ntraceroute host\n\n\nnetstat\nNetwork statistics\nnetstat -an\n\n\nifconfig\nNetwork interface config\nifconfig\n\n\nip addr\nShow IP addresses\nip addr",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Unix Command Reference</span>"
    ]
  },
  {
    "objectID": "chapters/A6-unix-reference.html#sec-unix-env",
    "href": "chapters/A6-unix-reference.html#sec-unix-env",
    "title": "32  Unix Command Reference",
    "section": "32.11 Environment Variables",
    "text": "32.11 Environment Variables\n\n\n\n\n\n\n\n\nCommand\nDescription\nExample\n\n\n\n\necho $VAR\nDisplay variable value\necho $PATH\n\n\nexport VAR=value\nSet environment variable\nexport PATH=$PATH:/new/path\n\n\nenv\nDisplay all environment variables\nenv\n\n\nprintenv\nPrint environment variables\nprintenv HOME\n\n\nunset VAR\nRemove variable\nunset MYVAR\n\n\nsource\nExecute script in current shell\nsource ~/.bashrc\n\n\n.\nSame as source\n. ~/.bashrc\n\n\n\n\n32.11.1 Common Environment Variables\n\n\n\nVariable\nDescription\n\n\n\n\n$HOME\nHome directory\n\n\n$USER\nCurrent username\n\n\n$PATH\nExecutable search path\n\n\n$PWD\nCurrent directory\n\n\n$SHELL\nCurrent shell\n\n\n$EDITOR\nDefault text editor\n\n\n$HOSTNAME\nComputer hostname\n\n\n$?\nExit status of last command\n\n\n$$\nCurrent process ID",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Unix Command Reference</span>"
    ]
  },
  {
    "objectID": "chapters/A7-latex-reference.html",
    "href": "chapters/A7-latex-reference.html",
    "title": "33  LaTeX Command Reference",
    "section": "",
    "text": "33.1 Greek Letters\nThis appendix provides a comprehensive reference for LaTeX mathematical notation commands commonly used in Quarto and R Markdown documents.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>LaTeX Command Reference</span>"
    ]
  },
  {
    "objectID": "chapters/A7-latex-reference.html#sec-latex-greek",
    "href": "chapters/A7-latex-reference.html#sec-latex-greek",
    "title": "33  LaTeX Command Reference",
    "section": "",
    "text": "33.1.1 Lowercase Greek Letters\n\n\n\n\n\n\n\n\n\n\n\nSymbol\nCommand\nSymbol\nCommand\nSymbol\nCommand\n\n\n\n\n\\(\\alpha\\)\n\\alpha\n\\(\\iota\\)\n\\iota\n\\(\\rho\\)\n\\rho\n\n\n\\(\\beta\\)\n\\beta\n\\(\\kappa\\)\n\\kappa\n\\(\\sigma\\)\n\\sigma\n\n\n\\(\\gamma\\)\n\\gamma\n\\(\\lambda\\)\n\\lambda\n\\(\\tau\\)\n\\tau\n\n\n\\(\\delta\\)\n\\delta\n\\(\\mu\\)\n\\mu\n\\(\\upsilon\\)\n\\upsilon\n\n\n\\(\\epsilon\\)\n\\epsilon\n\\(\\nu\\)\n\\nu\n\\(\\phi\\)\n\\phi\n\n\n\\(\\varepsilon\\)\n\\varepsilon\n\\(\\xi\\)\n\\xi\n\\(\\varphi\\)\n\\varphi\n\n\n\\(\\zeta\\)\n\\zeta\n\\(\\pi\\)\n\\pi\n\\(\\chi\\)\n\\chi\n\n\n\\(\\eta\\)\n\\eta\n\\(\\varpi\\)\n\\varpi\n\\(\\psi\\)\n\\psi\n\n\n\\(\\theta\\)\n\\theta\n\\(\\omega\\)\n\\omega\n\\(\\varsigma\\)\n\\varsigma\n\n\n\\(\\vartheta\\)\n\\vartheta\n\n\n\n\n\n\n\n\n\n33.1.2 Uppercase Greek Letters\n\n\n\nSymbol\nCommand\nSymbol\nCommand\nSymbol\nCommand\n\n\n\n\n\\(\\Gamma\\)\n\\Gamma\n\\(\\Xi\\)\n\\Xi\n\\(\\Phi\\)\n\\Phi\n\n\n\\(\\Delta\\)\n\\Delta\n\\(\\Pi\\)\n\\Pi\n\\(\\Psi\\)\n\\Psi\n\n\n\\(\\Theta\\)\n\\Theta\n\\(\\Sigma\\)\n\\Sigma\n\\(\\Omega\\)\n\\Omega\n\n\n\\(\\Lambda\\)\n\\Lambda\n\\(\\Upsilon\\)\n\\Upsilon",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>LaTeX Command Reference</span>"
    ]
  },
  {
    "objectID": "chapters/A7-latex-reference.html#sec-latex-arithmetic",
    "href": "chapters/A7-latex-reference.html#sec-latex-arithmetic",
    "title": "33  LaTeX Command Reference",
    "section": "33.2 Arithmetic Operations",
    "text": "33.2 Arithmetic Operations\n\n\n\nSymbol\nCommand\nDescription\nExample\n\n\n\n\n\\(+\\)\n+\nAddition\n$a + b$\n\n\n\\(-\\)\n-\nSubtraction\n$a - b$\n\n\n\\(\\times\\)\n\\times\nMultiplication\n$a \\times b$\n\n\n\\(\\cdot\\)\n\\cdot\nCentered dot\n$a \\cdot b$\n\n\n\\(\\div\\)\n\\div\nDivision\n$a \\div b$\n\n\n\\(\\frac{a}{b}\\)\n\\frac{a}{b}\nFraction\n$\\frac{x}{y}$\n\n\n\\(\\pm\\)\n\\pm\nPlus or minus\n$\\pm 5$\n\n\n\\(\\mp\\)\n\\mp\nMinus or plus\n$\\mp 5$",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>LaTeX Command Reference</span>"
    ]
  },
  {
    "objectID": "chapters/A7-latex-reference.html#sec-latex-powers",
    "href": "chapters/A7-latex-reference.html#sec-latex-powers",
    "title": "33  LaTeX Command Reference",
    "section": "33.3 Powers and Indices",
    "text": "33.3 Powers and Indices\n\n\n\nSymbol\nCommand\nDescription\nExample\n\n\n\n\n\\(x^n\\)\nx^n\nSuperscript\n$x^2$\n\n\n\\(x_n\\)\nx_n\nSubscript\n$x_1$\n\n\n\\(x^{n+1}\\)\nx^{n+1}\nMulti-char superscript\n$x^{ab}$\n\n\n\\(x_{i,j}\\)\nx_{i,j}\nMulti-char subscript\n$x_{ij}$\n\n\n\\(x_n^2\\)\nx_n^2\nCombined\n$x_i^2$\n\n\n\\(\\sqrt{x}\\)\n\\sqrt{x}\nSquare root\n$\\sqrt{16}$\n\n\n\\(\\sqrt[n]{x}\\)\n\\sqrt[n]{x}\nnth root\n$\\sqrt[3]{8}$",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>LaTeX Command Reference</span>"
    ]
  },
  {
    "objectID": "chapters/A7-latex-reference.html#sec-latex-relations",
    "href": "chapters/A7-latex-reference.html#sec-latex-relations",
    "title": "33  LaTeX Command Reference",
    "section": "33.4 Relation Operators",
    "text": "33.4 Relation Operators\n\n\n\nSymbol\nCommand\nDescription\n\n\n\n\n\\(=\\)\n=\nEqual to\n\n\n\\(\\neq\\)\n\\neq\nNot equal to\n\n\n\\(&lt;\\)\n&lt;\nLess than\n\n\n\\(&gt;\\)\n&gt;\nGreater than\n\n\n\\(\\leq\\)\n\\leq\nLess than or equal\n\n\n\\(\\geq\\)\n\\geq\nGreater than or equal\n\n\n\\(\\ll\\)\n\\ll\nMuch less than\n\n\n\\(\\gg\\)\n\\gg\nMuch greater than\n\n\n\\(\\approx\\)\n\\approx\nApproximately equal\n\n\n\\(\\sim\\)\n\\sim\nSimilar to / distributed as\n\n\n\\(\\simeq\\)\n\\simeq\nSimilar or equal\n\n\n\\(\\cong\\)\n\\cong\nCongruent\n\n\n\\(\\equiv\\)\n\\equiv\nEquivalent\n\n\n\\(\\propto\\)\n\\propto\nProportional to\n\n\n\\(\\doteq\\)\n\\doteq\nApproaches the limit",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>LaTeX Command Reference</span>"
    ]
  },
  {
    "objectID": "chapters/A7-latex-reference.html#sec-latex-sets",
    "href": "chapters/A7-latex-reference.html#sec-latex-sets",
    "title": "33  LaTeX Command Reference",
    "section": "33.5 Set Theory",
    "text": "33.5 Set Theory\n\n\n\nSymbol\nCommand\nDescription\n\n\n\n\n\\(\\in\\)\n\\in\nElement of\n\n\n\\(\\notin\\)\n\\notin\nNot element of\n\n\n\\(\\ni\\)\n\\ni\nContains as element\n\n\n\\(\\subset\\)\n\\subset\nProper subset\n\n\n\\(\\subseteq\\)\n\\subseteq\nSubset or equal\n\n\n\\(\\supset\\)\n\\supset\nProper superset\n\n\n\\(\\supseteq\\)\n\\supseteq\nSuperset or equal\n\n\n\\(\\cap\\)\n\\cap\nIntersection\n\n\n\\(\\cup\\)\n\\cup\nUnion\n\n\n\\(\\setminus\\)\n\\setminus\nSet difference\n\n\n\\(\\emptyset\\)\n\\emptyset\nEmpty set\n\n\n\\(\\varnothing\\)\n\\varnothing\nEmpty set (variant)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>LaTeX Command Reference</span>"
    ]
  },
  {
    "objectID": "chapters/A7-latex-reference.html#sec-latex-logic",
    "href": "chapters/A7-latex-reference.html#sec-latex-logic",
    "title": "33  LaTeX Command Reference",
    "section": "33.6 Logic Symbols",
    "text": "33.6 Logic Symbols\n\n\n\nSymbol\nCommand\nDescription\n\n\n\n\n\\(\\land\\)\n\\land\nLogical AND\n\n\n\\(\\lor\\)\n\\lor\nLogical OR\n\n\n\\(\\neg\\)\n\\neg\nLogical NOT\n\n\n\\(\\forall\\)\n\\forall\nFor all\n\n\n\\(\\exists\\)\n\\exists\nThere exists\n\n\n\\(\\nexists\\)\n\\nexists\nThere does not exist\n\n\n\\(\\Rightarrow\\)\n\\Rightarrow\nImplies\n\n\n\\(\\Leftarrow\\)\n\\Leftarrow\nIs implied by\n\n\n\\(\\Leftrightarrow\\)\n\\Leftrightarrow\nIf and only if\n\n\n\\(\\therefore\\)\n\\therefore\nTherefore\n\n\n\\(\\because\\)\n\\because\nBecause",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>LaTeX Command Reference</span>"
    ]
  },
  {
    "objectID": "chapters/A7-latex-reference.html#sec-latex-arrows",
    "href": "chapters/A7-latex-reference.html#sec-latex-arrows",
    "title": "33  LaTeX Command Reference",
    "section": "33.7 Arrows",
    "text": "33.7 Arrows\n\n\n\n\n\n\n\n\n\nSymbol\nCommand\nSymbol\nCommand\n\n\n\n\n\\(\\rightarrow\\)\n\\rightarrow\n\\(\\leftarrow\\)\n\\leftarrow\n\n\n\\(\\Rightarrow\\)\n\\Rightarrow\n\\(\\Leftarrow\\)\n\\Leftarrow\n\n\n\\(\\leftrightarrow\\)\n\\leftrightarrow\n\\(\\Leftrightarrow\\)\n\\Leftrightarrow\n\n\n\\(\\longrightarrow\\)\n\\longrightarrow\n\\(\\longleftarrow\\)\n\\longleftarrow\n\n\n\\(\\uparrow\\)\n\\uparrow\n\\(\\downarrow\\)\n\\downarrow\n\n\n\\(\\Uparrow\\)\n\\Uparrow\n\\(\\Downarrow\\)\n\\Downarrow\n\n\n\\(\\nearrow\\)\n\\nearrow\n\\(\\searrow\\)\n\\searrow\n\n\n\\(\\nwarrow\\)\n\\nwarrow\n\\(\\swarrow\\)\n\\swarrow\n\n\n\\(\\mapsto\\)\n\\mapsto\n\\(\\to\\)\n\\to",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>LaTeX Command Reference</span>"
    ]
  },
  {
    "objectID": "chapters/A7-latex-reference.html#sec-latex-big-operators",
    "href": "chapters/A7-latex-reference.html#sec-latex-big-operators",
    "title": "33  LaTeX Command Reference",
    "section": "33.8 Sums, Products, and Integrals",
    "text": "33.8 Sums, Products, and Integrals\n\n\n\nSymbol\nCommand\nExample\n\n\n\n\n\\(\\sum\\)\n\\sum\n$\\sum_{i=1}^{n} x_i$\n\n\n\\(\\prod\\)\n\\prod\n$\\prod_{i=1}^{n} x_i$\n\n\n\\(\\coprod\\)\n\\coprod\n$\\coprod_{i=1}^{n}$\n\n\n\\(\\int\\)\n\\int\n$\\int_a^b f(x) dx$\n\n\n\\(\\iint\\)\n\\iint\n$\\iint_D f(x,y) dA$\n\n\n\\(\\iiint\\)\n\\iiint\n$\\iiint_V f dV$\n\n\n\\(\\oint\\)\n\\oint\n$\\oint_C F \\cdot dr$\n\n\n\\(\\bigcup\\)\n\\bigcup\n$\\bigcup_{i=1}^{n} A_i$\n\n\n\\(\\bigcap\\)\n\\bigcap\n$\\bigcap_{i=1}^{n} A_i$\n\n\n\\(\\bigoplus\\)\n\\bigoplus\n$\\bigoplus_{i=1}^{n}$\n\n\n\\(\\bigotimes\\)\n\\bigotimes\n$\\bigotimes_{i=1}^{n}$\n\n\n\\(\\lim\\)\n\\lim\n$\\lim_{x \\to 0}$\n\n\n\\(\\limsup\\)\n\\limsup\n$\\limsup_{n \\to \\infty}$\n\n\n\\(\\liminf\\)\n\\liminf\n$\\liminf_{n \\to \\infty}$",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>LaTeX Command Reference</span>"
    ]
  },
  {
    "objectID": "chapters/A7-latex-reference.html#sec-latex-functions",
    "href": "chapters/A7-latex-reference.html#sec-latex-functions",
    "title": "33  LaTeX Command Reference",
    "section": "33.9 Mathematical Functions",
    "text": "33.9 Mathematical Functions\n\n\n\nSymbol\nCommand\nSymbol\nCommand\n\n\n\n\n\\(\\sin\\)\n\\sin\n\\(\\arcsin\\)\n\\arcsin\n\n\n\\(\\cos\\)\n\\cos\n\\(\\arccos\\)\n\\arccos\n\n\n\\(\\tan\\)\n\\tan\n\\(\\arctan\\)\n\\arctan\n\n\n\\(\\cot\\)\n\\cot\n\\(\\sec\\)\n\\sec\n\n\n\\(\\csc\\)\n\\csc\n\\(\\sinh\\)\n\\sinh\n\n\n\\(\\cosh\\)\n\\cosh\n\\(\\tanh\\)\n\\tanh\n\n\n\\(\\log\\)\n\\log\n\\(\\ln\\)\n\\ln\n\n\n\\(\\lg\\)\n\\lg\n\\(\\exp\\)\n\\exp\n\n\n\\(\\max\\)\n\\max\n\\(\\min\\)\n\\min\n\n\n\\(\\sup\\)\n\\sup\n\\(\\inf\\)\n\\inf\n\n\n\\(\\arg\\)\n\\arg\n\\(\\det\\)\n\\det\n\n\n\\(\\dim\\)\n\\dim\n\\(\\ker\\)\n\\ker\n\n\n\\(\\gcd\\)\n\\gcd\n\\(\\deg\\)\n\\deg\n\n\n\\(\\hom\\)\n\\hom\n\\(\\Pr\\)\n\\Pr",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>LaTeX Command Reference</span>"
    ]
  },
  {
    "objectID": "chapters/A7-latex-reference.html#sec-latex-accents",
    "href": "chapters/A7-latex-reference.html#sec-latex-accents",
    "title": "33  LaTeX Command Reference",
    "section": "33.10 Accents and Decorations",
    "text": "33.10 Accents and Decorations\n\n\n\nSymbol\nCommand\nDescription\n\n\n\n\n\\(\\hat{x}\\)\n\\hat{x}\nHat (estimator)\n\n\n\\(\\widehat{xyz}\\)\n\\widehat{xyz}\nWide hat\n\n\n\\(\\bar{x}\\)\n\\bar{x}\nBar (mean)\n\n\n\\(\\overline{xyz}\\)\n\\overline{xyz}\nOverline\n\n\n\\(\\tilde{x}\\)\n\\tilde{x}\nTilde\n\n\n\\(\\widetilde{xyz}\\)\n\\widetilde{xyz}\nWide tilde\n\n\n\\(\\vec{x}\\)\n\\vec{x}\nVector arrow\n\n\n\\(\\overrightarrow{AB}\\)\n\\overrightarrow{AB}\nVector from A to B\n\n\n\\(\\dot{x}\\)\n\\dot{x}\nDot (time derivative)\n\n\n\\(\\ddot{x}\\)\n\\ddot{x}\nDouble dot\n\n\n\\(\\acute{x}\\)\n\\acute{x}\nAcute accent\n\n\n\\(\\grave{x}\\)\n\\grave{x}\nGrave accent\n\n\n\\(\\breve{x}\\)\n\\breve{x}\nBreve\n\n\n\\(\\check{x}\\)\n\\check{x}\nCheck mark",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>LaTeX Command Reference</span>"
    ]
  },
  {
    "objectID": "chapters/A7-latex-reference.html#sec-latex-fonts",
    "href": "chapters/A7-latex-reference.html#sec-latex-fonts",
    "title": "33  LaTeX Command Reference",
    "section": "33.11 Font Styles",
    "text": "33.11 Font Styles\n\n\n\nSymbol\nCommand\nDescription\n\n\n\n\n\\(\\mathbf{x}\\)\n\\mathbf{x}\nBold (vectors/matrices)\n\n\n\\(\\mathit{x}\\)\n\\mathit{x}\nItalic\n\n\n\\(\\mathrm{x}\\)\n\\mathrm{x}\nRoman (upright)\n\n\n\\(\\mathsf{x}\\)\n\\mathsf{x}\nSans-serif\n\n\n\\(\\mathtt{x}\\)\n\\mathtt{x}\nTypewriter\n\n\n\\(\\mathcal{X}\\)\n\\mathcal{X}\nCalligraphic\n\n\n\\(\\mathbb{R}\\)\n\\mathbb{R}\nBlackboard bold\n\n\n\\(\\mathfrak{X}\\)\n\\mathfrak{X}\nFraktur\n\n\n\\(\\boldsymbol{\\alpha}\\)\n\\boldsymbol{\\alpha}\nBold symbol (Greek)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>LaTeX Command Reference</span>"
    ]
  },
  {
    "objectID": "chapters/A7-latex-reference.html#sec-latex-number-sets",
    "href": "chapters/A7-latex-reference.html#sec-latex-number-sets",
    "title": "33  LaTeX Command Reference",
    "section": "33.12 Common Number Sets",
    "text": "33.12 Common Number Sets\n\n\n\nSymbol\nCommand\nDescription\n\n\n\n\n\\(\\mathbb{N}\\)\n\\mathbb{N}\nNatural numbers\n\n\n\\(\\mathbb{Z}\\)\n\\mathbb{Z}\nIntegers\n\n\n\\(\\mathbb{Q}\\)\n\\mathbb{Q}\nRational numbers\n\n\n\\(\\mathbb{R}\\)\n\\mathbb{R}\nReal numbers\n\n\n\\(\\mathbb{C}\\)\n\\mathbb{C}\nComplex numbers\n\n\n\\(\\mathbb{P}\\)\n\\mathbb{P}\nPrime numbers",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>LaTeX Command Reference</span>"
    ]
  },
  {
    "objectID": "chapters/A7-latex-reference.html#sec-latex-brackets",
    "href": "chapters/A7-latex-reference.html#sec-latex-brackets",
    "title": "33  LaTeX Command Reference",
    "section": "33.13 Brackets and Delimiters",
    "text": "33.13 Brackets and Delimiters\n\n\n\nSymbol\nCommand\nDescription\n\n\n\n\n\\((x)\\)\n(x)\nParentheses\n\n\n\\([x]\\)\n[x]\nSquare brackets\n\n\n\\(\\{x\\}\\)\n\\{x\\}\nCurly braces\n\n\n\\(\\langle x \\rangle\\)\n\\langle x \\rangle\nAngle brackets\n\n\n\\(|x|\\)\n|x|\nVertical bars (absolute value)\n\n\n\\(\\|x\\|\\)\n\\|x\\|\nDouble bars (norm)\n\n\n\\(\\lfloor x \\rfloor\\)\n\\lfloor x \\rfloor\nFloor\n\n\n\\(\\lceil x \\rceil\\)\n\\lceil x \\rceil\nCeiling\n\n\n\n\n33.13.1 Auto-sizing Delimiters\nUse \\left and \\right for automatically sized delimiters:\n\n\n\n\n\n\n\nExample\nCode\n\n\n\n\n\\(\\left( \\frac{a}{b} \\right)\\)\n\\left( \\frac{a}{b} \\right)\n\n\n\\(\\left[ \\sum_{i=1}^{n} x_i \\right]\\)\n\\left[ \\sum_{i=1}^{n} x_i \\right]\n\n\n\\(\\left\\{ \\frac{a}{b} \\right\\}\\)\n\\left\\{ \\frac{a}{b} \\right\\}\n\n\n\\(\\left| \\frac{a}{b} \\right|\\)\n\\left| \\frac{a}{b} \\right|",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>LaTeX Command Reference</span>"
    ]
  },
  {
    "objectID": "chapters/A7-latex-reference.html#sec-latex-matrices",
    "href": "chapters/A7-latex-reference.html#sec-latex-matrices",
    "title": "33  LaTeX Command Reference",
    "section": "33.14 Matrix Environments",
    "text": "33.14 Matrix Environments\n\n\n\n\n\n\n\n\nEnvironment\nResult\nBrackets\n\n\n\n\nmatrix\n\\(\\begin{matrix} a & b \\\\ c & d \\end{matrix}\\)\nNone\n\n\npmatrix\n\\(\\begin{pmatrix} a & b \\\\ c & d \\end{pmatrix}\\)\nParentheses\n\n\nbmatrix\n\\(\\begin{bmatrix} a & b \\\\ c & d \\end{bmatrix}\\)\nSquare brackets\n\n\nBmatrix\n\\(\\begin{Bmatrix} a & b \\\\ c & d \\end{Bmatrix}\\)\nCurly braces\n\n\nvmatrix\n\\(\\begin{vmatrix} a & b \\\\ c & d \\end{vmatrix}\\)\nVertical bars\n\n\nVmatrix\n\\(\\begin{Vmatrix} a & b \\\\ c & d \\end{Vmatrix}\\)\nDouble vertical\n\n\n\nSyntax:\n$$\\begin{bmatrix}\na & b & c \\\\\nd & e & f\n\\end{bmatrix}$$",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>LaTeX Command Reference</span>"
    ]
  },
  {
    "objectID": "chapters/A7-latex-reference.html#sec-latex-spacing",
    "href": "chapters/A7-latex-reference.html#sec-latex-spacing",
    "title": "33  LaTeX Command Reference",
    "section": "33.15 Spacing",
    "text": "33.15 Spacing\n\n\n\nCommand\nWidth\nExample\n\n\n\n\n\\,\n3/18 em (thin)\n\\(a\\,b\\)\n\n\n\\:\n4/18 em (medium)\n\\(a\\:b\\)\n\n\n\\;\n5/18 em (thick)\n\\(a\\;b\\)\n\n\n\\quad\n1 em\n\\(a\\quad b\\)\n\n\n\\qquad\n2 em\n\\(a\\qquad b\\)\n\n\n\\!\n-3/18 em (negative)\n\\(a\\!b\\)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>LaTeX Command Reference</span>"
    ]
  },
  {
    "objectID": "chapters/A7-latex-reference.html#sec-latex-dots",
    "href": "chapters/A7-latex-reference.html#sec-latex-dots",
    "title": "33  LaTeX Command Reference",
    "section": "33.16 Dots and Ellipses",
    "text": "33.16 Dots and Ellipses\n\n\n\nSymbol\nCommand\nUsage\n\n\n\n\n\\(\\ldots\\)\n\\ldots\nBaseline dots\n\n\n\\(\\cdots\\)\n\\cdots\nCentered dots\n\n\n\\(\\vdots\\)\n\\vdots\nVertical dots\n\n\n\\(\\ddots\\)\n\\ddots\nDiagonal dots\n\n\n\\(\\cdot\\)\n\\cdot\nSingle centered dot",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>LaTeX Command Reference</span>"
    ]
  },
  {
    "objectID": "chapters/A7-latex-reference.html#sec-latex-misc",
    "href": "chapters/A7-latex-reference.html#sec-latex-misc",
    "title": "33  LaTeX Command Reference",
    "section": "33.17 Miscellaneous Symbols",
    "text": "33.17 Miscellaneous Symbols\n\n\n\nSymbol\nCommand\nDescription\n\n\n\n\n\\(\\infty\\)\n\\infty\nInfinity\n\n\n\\(\\partial\\)\n\\partial\nPartial derivative\n\n\n\\(\\nabla\\)\n\\nabla\nNabla / Del\n\n\n\\(\\prime\\)\n\\prime\nPrime\n\n\n\\(\\angle\\)\n\\angle\nAngle\n\n\n\\(\\triangle\\)\n\\triangle\nTriangle\n\n\n\\(\\square\\)\n\\square\nSquare\n\n\n\\(\\circ\\)\n\\circ\nCircle / composition\n\n\n\\(\\bullet\\)\n\\bullet\nBullet\n\n\n\\(\\star\\)\n\\star\nStar\n\n\n\\(\\dagger\\)\n\\dagger\nDagger\n\n\n\\(\\ddagger\\)\n\\ddagger\nDouble dagger\n\n\n\\(\\ell\\)\n\\ell\nScript l\n\n\n\\(\\hbar\\)\n\\hbar\nh-bar (Planck’s constant)\n\n\n\\(\\Re\\)\n\\Re\nReal part\n\n\n\\(\\Im\\)\n\\Im\nImaginary part\n\n\n\\(\\aleph\\)\n\\aleph\nAleph",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>LaTeX Command Reference</span>"
    ]
  },
  {
    "objectID": "chapters/A7-latex-reference.html#sec-latex-stats",
    "href": "chapters/A7-latex-reference.html#sec-latex-stats",
    "title": "33  LaTeX Command Reference",
    "section": "33.18 Statistical Notation",
    "text": "33.18 Statistical Notation\n\n\n\nExpression\nCode\nDescription\n\n\n\n\n\\(\\bar{x}\\)\n\\bar{x}\nSample mean\n\n\n\\(\\hat{\\mu}\\)\n\\hat{\\mu}\nEstimated parameter\n\n\n\\(\\tilde{x}\\)\n\\tilde{x}\nMedian\n\n\n\\(X \\sim N(\\mu, \\sigma^2)\\)\nX \\sim N(\\mu, \\sigma^2)\nDistribution\n\n\n\\({n \\choose k}\\)\n{n \\choose k}\nBinomial coefficient\n\n\n\\(\\binom{n}{k}\\)\n\\binom{n}{k}\nBinomial coefficient (alternate)\n\n\n\\(P(A \\mid B)\\)\nP(A \\mid B)\nConditional probability\n\n\n\\(\\mathbb{E}[X]\\)\n\\mathbb{E}[X]\nExpected value\n\n\n\\(\\text{Var}(X)\\)\n\\text{Var}(X)\nVariance\n\n\n\\(\\text{Cov}(X, Y)\\)\n\\text{Cov}(X, Y)\nCovariance",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>LaTeX Command Reference</span>"
    ]
  },
  {
    "objectID": "chapters/A7-latex-reference.html#sec-latex-stat-formulas",
    "href": "chapters/A7-latex-reference.html#sec-latex-stat-formulas",
    "title": "33  LaTeX Command Reference",
    "section": "33.19 Common Statistical Formulas",
    "text": "33.19 Common Statistical Formulas\n\n33.19.1 Sample Statistics\n\n\n\n\n\n\n\nStatistic\nFormula\n\n\n\n\nSample mean\n\\(\\bar{x} = \\frac{1}{n}\\sum_{i=1}^{n} x_i\\)\n\n\nSample variance\n\\(s^2 = \\frac{1}{n-1}\\sum_{i=1}^{n}(x_i - \\bar{x})^2\\)\n\n\nStandard error\n\\(SE = \\frac{s}{\\sqrt{n}}\\)\n\n\nZ-score\n\\(z = \\frac{x - \\mu}{\\sigma}\\)\n\n\n\n\n\n33.19.2 Probability Distributions\n\n\n\n\n\n\n\nDistribution\nProbability Function\n\n\n\n\nBinomial\n\\(P(X=k) = \\binom{n}{k} p^k (1-p)^{n-k}\\)\n\n\nPoisson\n\\(P(X=k) = \\frac{e^{-\\lambda}\\lambda^k}{k!}\\)\n\n\nNormal\n\\(f(x) = \\frac{1}{\\sigma\\sqrt{2\\pi}} e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}\\)\n\n\nExponential\n\\(f(x) = \\lambda e^{-\\lambda x}\\)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>LaTeX Command Reference</span>"
    ]
  },
  {
    "objectID": "chapters/A7-latex-reference.html#sec-latex-numbering",
    "href": "chapters/A7-latex-reference.html#sec-latex-numbering",
    "title": "33  LaTeX Command Reference",
    "section": "33.20 Equation Numbering in Quarto",
    "text": "33.20 Equation Numbering in Quarto\nAdd equation labels for cross-referencing:\n$$\nE = mc^2\n$$ {#eq-einstein}\n\nSee @eq-einstein for the mass-energy equivalence.\nMultiple equations with alignment:\n$$\n\\begin{aligned}\ny &= mx + b \\\\\n  &= 2x + 3\n\\end{aligned}\n$$ {#eq-line}",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>LaTeX Command Reference</span>"
    ]
  },
  {
    "objectID": "chapters/A8-keyboard-shortcuts.html",
    "href": "chapters/A8-keyboard-shortcuts.html",
    "title": "34  Keyboard Shortcuts Reference",
    "section": "",
    "text": "34.1 Bash/Terminal Shortcuts\nThis appendix provides essential keyboard shortcuts for the tools commonly used in biostatistics and data science workflows.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Keyboard Shortcuts Reference</span>"
    ]
  },
  {
    "objectID": "chapters/A8-keyboard-shortcuts.html#bashterminal-shortcuts",
    "href": "chapters/A8-keyboard-shortcuts.html#bashterminal-shortcuts",
    "title": "34  Keyboard Shortcuts Reference",
    "section": "",
    "text": "34.1.1 Navigation and Editing\n\n\n\nShortcut\nDescription\n\n\n\n\nCtrl + A\nMove cursor to beginning of line\n\n\nCtrl + E\nMove cursor to end of line\n\n\nCtrl + B\nMove cursor back one character\n\n\nCtrl + F\nMove cursor forward one character\n\n\nAlt + B\nMove cursor back one word\n\n\nAlt + F\nMove cursor forward one word\n\n\nCtrl + U\nDelete from cursor to beginning of line\n\n\nCtrl + K\nDelete from cursor to end of line\n\n\nCtrl + W\nDelete word before cursor\n\n\nAlt + D\nDelete word after cursor\n\n\nCtrl + Y\nPaste (yank) deleted text\n\n\nCtrl + _\nUndo last edit\n\n\n\n\n\n34.1.2 History and Search\n\n\n\nShortcut\nDescription\n\n\n\n\nCtrl + R\nSearch command history (reverse)\n\n\nCtrl + S\nSearch command history (forward)\n\n\nCtrl + G\nCancel history search\n\n\nCtrl + P or Up\nPrevious command in history\n\n\nCtrl + N or Down\nNext command in history\n\n\n!!\nRepeat last command\n\n\n!$\nLast argument of previous command\n\n\n!n\nExecute command number n from history\n\n\n!string\nExecute last command starting with string\n\n\n\n\n\n34.1.3 Process Control\n\n\n\nShortcut\nDescription\n\n\n\n\nCtrl + C\nInterrupt/kill current process\n\n\nCtrl + Z\nSuspend current process (use fg to resume)\n\n\nCtrl + D\nExit shell / End of input\n\n\nCtrl + L\nClear screen\n\n\nCtrl + S\nPause terminal output\n\n\nCtrl + Q\nResume terminal output",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Keyboard Shortcuts Reference</span>"
    ]
  },
  {
    "objectID": "chapters/A8-keyboard-shortcuts.html#vim-shortcuts",
    "href": "chapters/A8-keyboard-shortcuts.html#vim-shortcuts",
    "title": "34  Keyboard Shortcuts Reference",
    "section": "34.2 Vim Shortcuts",
    "text": "34.2 Vim Shortcuts\n\n34.2.1 Modes\n\n\n\nKey\nDescription\n\n\n\n\ni\nEnter Insert mode (before cursor)\n\n\na\nEnter Insert mode (after cursor)\n\n\nI\nInsert at beginning of line\n\n\nA\nInsert at end of line\n\n\no\nOpen new line below\n\n\nO\nOpen new line above\n\n\nv\nEnter Visual mode (character)\n\n\nV\nEnter Visual mode (line)\n\n\nCtrl + V\nEnter Visual Block mode\n\n\nEsc\nReturn to Normal mode\n\n\n:\nEnter Command mode\n\n\n\n\n\n34.2.2 Navigation (Normal Mode)\n\n\n\nKey\nDescription\n\n\n\n\nh, j, k, l\nLeft, down, up, right\n\n\nw\nMove to next word\n\n\nb\nMove to previous word\n\n\ne\nMove to end of word\n\n\n0\nMove to beginning of line\n\n\n$\nMove to end of line\n\n\n^\nMove to first non-blank character\n\n\ngg\nGo to first line\n\n\nG\nGo to last line\n\n\nnG or :n\nGo to line n\n\n\nCtrl + F\nPage down\n\n\nCtrl + B\nPage up\n\n\nCtrl + D\nHalf page down\n\n\nCtrl + U\nHalf page up\n\n\n%\nJump to matching bracket\n\n\n{\nJump to previous paragraph\n\n\n}\nJump to next paragraph\n\n\n\n\n\n34.2.3 Editing (Normal Mode)\n\n\n\nKey\nDescription\n\n\n\n\nx\nDelete character under cursor\n\n\nX\nDelete character before cursor\n\n\ndd\nDelete entire line\n\n\ndw\nDelete word\n\n\nd$ or D\nDelete to end of line\n\n\nd0\nDelete to beginning of line\n\n\nyy\nYank (copy) entire line\n\n\nyw\nYank word\n\n\np\nPaste after cursor\n\n\nP\nPaste before cursor\n\n\nr\nReplace single character\n\n\nR\nEnter Replace mode\n\n\ncc\nChange entire line\n\n\ncw\nChange word\n\n\nc$ or C\nChange to end of line\n\n\nu\nUndo\n\n\nCtrl + R\nRedo\n\n\n.\nRepeat last command\n\n\n~\nToggle case\n\n\n&gt;&gt;\nIndent line\n\n\n&lt;&lt;\nUnindent line\n\n\n\n\n\n34.2.4 Search and Replace\n\n\n\nCommand\nDescription\n\n\n\n\n/pattern\nSearch forward for pattern\n\n\n?pattern\nSearch backward for pattern\n\n\nn\nFind next occurrence\n\n\nN\nFind previous occurrence\n\n\n*\nSearch for word under cursor\n\n\n:%s/old/new/g\nReplace all occurrences in file\n\n\n:s/old/new/g\nReplace all occurrences in line\n\n\n:%s/old/new/gc\nReplace with confirmation\n\n\n\n\n\n34.2.5 File Operations\n\n\n\nCommand\nDescription\n\n\n\n\n:w\nSave file\n\n\n:w filename\nSave as filename\n\n\n:q\nQuit\n\n\n:q!\nQuit without saving\n\n\n:wq or :x\nSave and quit\n\n\nZZ\nSave and quit (Normal mode)\n\n\n:e filename\nOpen file\n\n\n:r filename\nInsert file contents",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Keyboard Shortcuts Reference</span>"
    ]
  },
  {
    "objectID": "chapters/A8-keyboard-shortcuts.html#rstudio-shortcuts",
    "href": "chapters/A8-keyboard-shortcuts.html#rstudio-shortcuts",
    "title": "34  Keyboard Shortcuts Reference",
    "section": "34.3 RStudio Shortcuts",
    "text": "34.3 RStudio Shortcuts\n\n34.3.1 Console\n\n\n\nShortcut\nDescription\n\n\n\n\nCtrl + Enter\nRun current line/selection\n\n\nCtrl + Shift + Enter\nRun entire script\n\n\nCtrl + Alt + R\nRun from beginning to current line\n\n\nCtrl + Alt + E\nRun from current line to end\n\n\nCtrl + Alt + B\nRun from beginning to cursor\n\n\nCtrl + L\nClear console\n\n\nEsc\nInterrupt R\n\n\nTab\nAutocomplete\n\n\nCtrl + Up\nCommand history popup\n\n\n\n\n\n34.3.2 Editor\n\n\n\nShortcut\nDescription\n\n\n\n\nCtrl + S\nSave current file\n\n\nCtrl + Shift + S\nSave all open files\n\n\nCtrl + Z\nUndo\n\n\nCtrl + Shift + Z\nRedo\n\n\nCtrl + F\nFind\n\n\nCtrl + H\nFind and replace\n\n\nCtrl + Shift + F\nFind in files\n\n\nCtrl + D\nDelete line\n\n\nCtrl + Shift + D\nDuplicate line\n\n\nAlt + Up/Down\nMove line up/down\n\n\nCtrl + Shift + C\nComment/uncomment line\n\n\nCtrl + Shift + /\nReflow comment\n\n\nCtrl + I\nReindent lines\n\n\nCtrl + Shift + A\nReformat code\n\n\nCtrl + Shift + M\nInsert pipe (%&gt;% or |&gt;)\n\n\nAlt + -\nInsert assignment (&lt;-)\n\n\nCtrl + Shift + R\nInsert code section\n\n\n\n\n\n34.3.3 Navigation\n\n\n\nShortcut\nDescription\n\n\n\n\nCtrl + .\nGo to file/function\n\n\nF1\nHelp on selected function\n\n\nF2\nGo to function definition\n\n\nCtrl + Shift + O\nDocument outline\n\n\nCtrl + Tab\nSwitch between open files\n\n\nCtrl + Shift + Tab\nSwitch to previous file\n\n\nCtrl + 1\nFocus on Source editor\n\n\nCtrl + 2\nFocus on Console\n\n\nCtrl + 3\nFocus on Help\n\n\nCtrl + 4\nFocus on History\n\n\nCtrl + 5\nFocus on Files\n\n\nCtrl + 6\nFocus on Plots\n\n\nCtrl + 7\nFocus on Packages\n\n\nCtrl + 8\nFocus on Environment\n\n\nCtrl + 9\nFocus on Viewer\n\n\n\n\n\n34.3.4 Projects and Sessions\n\n\n\nShortcut\nDescription\n\n\n\n\nCtrl + Shift + N\nNew file\n\n\nCtrl + O\nOpen file\n\n\nCtrl + Shift + P\nCommand palette\n\n\nCtrl + Shift + F10\nRestart R session\n\n\nCtrl + Q\nQuit RStudio",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Keyboard Shortcuts Reference</span>"
    ]
  },
  {
    "objectID": "chapters/A8-keyboard-shortcuts.html#jupyter-notebook-shortcuts",
    "href": "chapters/A8-keyboard-shortcuts.html#jupyter-notebook-shortcuts",
    "title": "34  Keyboard Shortcuts Reference",
    "section": "34.4 Jupyter Notebook Shortcuts",
    "text": "34.4 Jupyter Notebook Shortcuts\n\n34.4.1 Command Mode (Blue Border)\nPress Esc to enter Command mode.\n\n\n\nKey\nDescription\n\n\n\n\nEnter\nEnter Edit mode\n\n\nH\nShow keyboard shortcuts\n\n\nA\nInsert cell above\n\n\nB\nInsert cell below\n\n\nX\nCut selected cells\n\n\nC\nCopy selected cells\n\n\nV\nPaste cells below\n\n\nShift + V\nPaste cells above\n\n\nD, D\nDelete selected cells\n\n\nZ\nUndo cell deletion\n\n\nY\nChange cell to Code\n\n\nM\nChange cell to Markdown\n\n\nR\nChange cell to Raw\n\n\n1-6\nChange to heading level 1-6\n\n\nUp/K\nSelect cell above\n\n\nDown/J\nSelect cell below\n\n\nShift + Up/K\nExtend selection above\n\n\nShift + Down/J\nExtend selection below\n\n\nShift + M\nMerge selected cells\n\n\nO\nToggle output\n\n\nShift + O\nToggle output scrolling\n\n\nI, I\nInterrupt kernel\n\n\n0, 0\nRestart kernel\n\n\nS or Ctrl + S\nSave notebook\n\n\nL\nToggle line numbers\n\n\n\n\n\n34.4.2 Edit Mode (Green Border)\nPress Enter to enter Edit mode.\n\n\n\nShortcut\nDescription\n\n\n\n\nEsc\nEnter Command mode\n\n\nCtrl + Enter\nRun cell\n\n\nShift + Enter\nRun cell, select below\n\n\nAlt + Enter\nRun cell, insert below\n\n\nCtrl + Shift + -\nSplit cell at cursor\n\n\nTab\nCode completion\n\n\nShift + Tab\nTooltip/documentation\n\n\nCtrl + ]\nIndent\n\n\nCtrl + [\nDedent\n\n\nCtrl + A\nSelect all\n\n\nCtrl + Z\nUndo\n\n\nCtrl + Shift + Z\nRedo\n\n\nCtrl + D\nDelete whole line\n\n\nCtrl + /\nComment/uncomment\n\n\nCtrl + Home\nGo to cell start\n\n\nCtrl + End\nGo to cell end",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Keyboard Shortcuts Reference</span>"
    ]
  },
  {
    "objectID": "chapters/A8-keyboard-shortcuts.html#vs-code-shortcuts",
    "href": "chapters/A8-keyboard-shortcuts.html#vs-code-shortcuts",
    "title": "34  Keyboard Shortcuts Reference",
    "section": "34.5 VS Code Shortcuts",
    "text": "34.5 VS Code Shortcuts\n\n34.5.1 General\n\n\n\nShortcut\nDescription\n\n\n\n\nCtrl + Shift + P\nCommand palette\n\n\nCtrl + P\nQuick open file\n\n\nCtrl + Shift + N\nNew window\n\n\nCtrl + W\nClose editor\n\n\nCtrl + ,\nOpen settings\n\n\nCtrl + K Ctrl + S\nKeyboard shortcuts\n\n\nCtrl + ``\nToggle integrated terminal\n\n\nF11\nToggle full screen\n\n\nCtrl + B\nToggle sidebar\n\n\nCtrl + Shift + E\nExplorer focus\n\n\nCtrl + Shift + F\nSearch across files\n\n\nCtrl + Shift + G\nSource control\n\n\nCtrl + Shift + D\nDebug\n\n\nCtrl + Shift + X\nExtensions\n\n\n\n\n\n34.5.2 Editing\n\n\n\nShortcut\nDescription\n\n\n\n\nCtrl + X\nCut line (empty selection)\n\n\nCtrl + C\nCopy line (empty selection)\n\n\nCtrl + V\nPaste\n\n\nCtrl + Shift + K\nDelete line\n\n\nCtrl + Enter\nInsert line below\n\n\nCtrl + Shift + Enter\nInsert line above\n\n\nAlt + Up/Down\nMove line up/down\n\n\nShift + Alt + Up/Down\nCopy line up/down\n\n\nCtrl + D\nSelect word (repeat for next)\n\n\nCtrl + Shift + L\nSelect all occurrences\n\n\nCtrl + L\nSelect current line\n\n\nCtrl + /\nToggle line comment\n\n\nShift + Alt + A\nToggle block comment\n\n\nCtrl + ]\nIndent line\n\n\nCtrl + [\nOutdent line\n\n\nCtrl + Shift + \\\nJump to matching bracket\n\n\nCtrl + Z\nUndo\n\n\nCtrl + Shift + Z\nRedo\n\n\n\n\n\n34.5.3 Navigation\n\n\n\nShortcut\nDescription\n\n\n\n\nCtrl + G\nGo to line\n\n\nCtrl + P\nGo to file\n\n\nCtrl + Shift + O\nGo to symbol\n\n\nF12\nGo to definition\n\n\nAlt + F12\nPeek definition\n\n\nCtrl + Shift + \\\nGo to matching bracket\n\n\nCtrl + Home\nGo to beginning of file\n\n\nCtrl + End\nGo to end of file\n\n\nCtrl + Tab\nSwitch between open editors\n\n\nAlt + Left/Right\nGo back/forward\n\n\nCtrl + K Ctrl + Q\nGo to last edit location\n\n\n\n\n\n34.5.4 Multi-Cursor and Selection\n\n\n\nShortcut\nDescription\n\n\n\n\nAlt + Click\nInsert cursor\n\n\nCtrl + Alt + Up/Down\nInsert cursor above/below\n\n\nCtrl + U\nUndo cursor operation\n\n\nShift + Alt + I\nInsert cursor at end of each line\n\n\nCtrl + Shift + L\nSelect all occurrences of selection\n\n\nCtrl + F2\nSelect all occurrences of word\n\n\nShift + Alt + Drag\nColumn selection\n\n\nCtrl + Shift + Alt + Arrow\nColumn selection\n\n\n\n\n\n34.5.5 Search and Replace\n\n\n\nShortcut\nDescription\n\n\n\n\nCtrl + F\nFind\n\n\nCtrl + H\nReplace\n\n\nF3 / Shift + F3\nFind next/previous\n\n\nAlt + Enter\nSelect all occurrences of find match\n\n\nCtrl + D\nAdd selection to next find match\n\n\nCtrl + K Ctrl + D\nMove last selection to next find match\n\n\n\n\n\n34.5.6 Integrated Terminal\n\n\n\nShortcut\nDescription\n\n\n\n\nCtrl + ``\nToggle terminal\n\n\nCtrl + Shift + ``\nCreate new terminal\n\n\nCtrl + Shift + 5\nSplit terminal\n\n\nCtrl + PageUp/Down\nScroll terminal\n\n\nShift + PageUp/Down\nScroll terminal by page\n\n\n\n\n\n\n\n\n\nCustomizing Shortcuts\n\n\n\nMost applications allow you to customize keyboard shortcuts:\n\nRStudio: Tools → Modify Keyboard Shortcuts\nVS Code: File → Preferences → Keyboard Shortcuts (or Ctrl + K Ctrl + S)\nJupyter: Help → Edit Keyboard Shortcuts\nBash: Edit ~/.inputrc for readline customizations\n\nLearning the default shortcuts first helps when working on different machines, but customization can boost your personal productivity significantly.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Keyboard Shortcuts Reference</span>"
    ]
  },
  {
    "objectID": "chapters/A8-keyboard-shortcuts.html#platform-differences",
    "href": "chapters/A8-keyboard-shortcuts.html#platform-differences",
    "title": "34  Keyboard Shortcuts Reference",
    "section": "34.6 Platform Differences",
    "text": "34.6 Platform Differences\nMost shortcuts in this appendix use Ctrl as the modifier key (Windows/Linux). On macOS:\n\n\n\nWindows/Linux\nmacOS\n\n\n\n\nCtrl\nCmd (⌘)\n\n\nAlt\nOption (⌥)\n\n\nCtrl + Alt\nCmd + Option\n\n\n\nSome applications maintain the Ctrl key on macOS for terminal-related shortcuts (Bash, Vim).",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Keyboard Shortcuts Reference</span>"
    ]
  },
  {
    "objectID": "chapters/A9-exercises.html",
    "href": "chapters/A9-exercises.html",
    "title": "35  Practice Exercises",
    "section": "",
    "text": "35.1 Unix and Command Line Exercises\nThis appendix contains practice exercises organized by chapter. Working through these exercises will help reinforce the concepts covered in the main text and develop your practical skills in data analysis. Each section corresponds to a chapter in the book, with cross-references to help you review relevant material.\nThese exercises correspond to Chapter 2. Save a digital record of your work so that you can study it later if you need to.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>Practice Exercises</span>"
    ]
  },
  {
    "objectID": "chapters/A9-exercises.html#sec-ex-unix",
    "href": "chapters/A9-exercises.html#sec-ex-unix",
    "title": "35  Practice Exercises",
    "section": "",
    "text": "35.1.1 Exercise U.1: Basic Navigation and File Operations\nOpen up a terminal and execute the following using Unix commands:\n\nPrint your working directory using pwd\nNavigate to a directory somewhere below your home directory where you want to practice writing files\nMake 5 directories called dir_1, dir_2, dir_3, dir_4, and dir_5 using mkdir\nWithin each of those directories, create files called file_1.txt, file_2.txt, and file_3.txt using touch\nOpen file_1.txt in dir_1 using a plain text editor (such as nano or vim), type a few words, and save it\nPrint file_1.txt in dir_1 to the terminal using cat\nDelete all files in dir_3 using rm\nList all of the contents of your current directory line-by-line using ls -l\nDelete dir_3 using rmdir\n\n\n\n35.1.2 Exercise U.2: Working with Data Files\nFor this exercise, create a sample tab-separated file or download a GFF file from a genomics database.\n\nNavigate to dir_1\nCopy a data file (using its absolute path) to your current directory\nDelete the copy that is in your current directory, then copy it again using a relative path this time\nUse at least 3 different Unix commands to examine all or parts of your data file (try cat, head, tail, less, and wc)\nWhat is the file size? Use ls -lh to find out\nHow many lines does the file have? Use wc -l\nHow many lines begin with a specific pattern (like a chromosome name)? Use grep -c \"^pattern\"\nHow many unique entries are there in a specific column? Use cut and sort -u | wc -l\nSort the file based on reverse numeric order in a specific field using sort -k -nr\nCapture specific fields and write to a new file using cut and redirection\nReplace all instances of one string with another using sed 's/old/new/g'\n\n\n\n35.1.3 Exercise U.3: Building Pipelines\nPractice combining commands with pipes to answer questions about your data:\n\nCount the number of unique values in the third column of a tab-separated file:\ncut -f3 data.tsv | sort | uniq | wc -l\nFind all lines containing a pattern, extract specific columns, and sort the results:\ngrep \"pattern\" data.tsv | cut -f1,2,5 | sort -k3,3 -n\nCreate a pipeline that filters rows based on a condition, extracts columns, and saves to a new file\nUse awk to filter rows where a numeric column exceeds a threshold:\nawk '$5 &gt; 1000 {print $1, $2, $5}' data.tsv\n\n\n\n35.1.4 Exercise U.4: File Permissions and Scripts\n\nCreate a simple shell script that prints “Hello, World!” and the current date\nTry to run the script—what error do you get?\nMake the script executable using chmod +x\nRun the script and verify it works\nExamine the permissions of various files in your system using ls -l\nPractice changing permissions using both symbolic notation (chmod u+x) and octal notation (chmod 755)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>Practice Exercises</span>"
    ]
  },
  {
    "objectID": "chapters/A9-exercises.html#sec-ex-r",
    "href": "chapters/A9-exercises.html#sec-ex-r",
    "title": "35  Practice Exercises",
    "section": "35.2 R and RStudio Exercises",
    "text": "35.2 R and RStudio Exercises\nThese exercises correspond to Chapter 3.\n\n35.2.1 Exercise R.1: Exploring RStudio\nTake a few minutes to familiarize yourself with the RStudio environment:\n\nLocate the four main panes:\n\nThe code editor (top left)\nThe workspace and history (top right)\nThe plots and files window (bottom right)\nThe R console (bottom left)\n\nIn the plots and files window, click on the Packages and Help tabs to see what they offer\nSee what types of new files can be made in RStudio by clicking File → New File\nOpen a new R script and a new R Markdown file to see the difference\n\n\n\n35.2.2 Exercise R.2: Basic Mathematics in R\nInsert a code chunk and complete the following tasks:\n\nAdd and subtract numbers\nMultiply and divide numbers\nRaise a number to a power using the ^ symbol\nCreate a more complex equation involving all of these operations to convince yourself that R follows the normal priority of mathematical evaluation (PEMDAS)\n\n\n\nCode\n# Example:\n(4 + 3 * 2^2) / 5 - 1\n\n\n\n\n35.2.3 Exercise R.3: Assigning Variables and Functions\n\nAssign three variables using basic mathematical operations\nTake the log of your three variables using log()\nUse the print() function to display your most complex variable\nUse the c() (concatenate) function combined with paste() to create and print a sentence\n\n\n\nCode\n# Example:\nx &lt;- 10\ny &lt;- x * 2\nz &lt;- sqrt(x + y)\nprint(paste(\"The value of z is\", z))\n\n\n\n\n35.2.4 Exercise R.4: Vectors and Factors\n\nCreate a numeric vector using the c() function with at least 5 elements\nCreate a character vector and convert it to a factor using as.factor()\n\n\n\nCode\n# Example:\nvec1 &lt;- c(\"control\", \"treatment\", \"control\", \"treatment\", \"control\")\nfac1 &lt;- as.factor(vec1)\nprint(fac1)\n\n\n[1] control   treatment control   treatment control  \nLevels: control treatment\n\n\nCode\nlevels(fac1)\n\n\n[1] \"control\"   \"treatment\"\n\n\n\nUse str() and class() to evaluate your variables\nWhat is the difference between a character vector and a factor?\n\n\n\n35.2.5 Exercise R.5: Basic Statistics\n\nCreate a numeric vector with at least 10 elements\nCalculate the mean(), sd(), sum(), length(), and var() of your vector\nUse the log() and sqrt() functions on your vector\nWhat happens when you try to apply mean() to a factor? Try it and explain the result\n\n\n\nCode\n# Example:\nmy_vector &lt;- c(12, 15, 18, 22, 25, 28, 31, 35, 38, 42)\nmean(my_vector)\nsd(my_vector)\n\n\n\n\n35.2.6 Exercise R.6: Creating Sequences and Random Sampling\nSet the random seed for reproducibility, then:\n\n\nCode\nset.seed(42)\n\n\n\nCreate a vector with 100 elements using seq() and calculate the mean and standard deviation\nCreate a variable and sample() it with equal probability—experiment with the size and replace arguments\nCreate a normally distributed variable of 10000 elements using rnorm(), then sample that distribution with and without replacement\nUse hist() to plot your normally distributed variable\n\n\n\n35.2.7 Exercise R.7: Basic Visualization\nCreate visualizations with proper axis labels and colors:\n\nCreate a sequence variable using seq() and make two different plots by changing the type argument (\"p\" for points, \"l\" for lines, \"b\" for both)\nCreate a normally distributed variable using rnorm() and make histograms with different breaks values—what does breaks control?\nUse par(mfrow = c(2, 2)) to create a 2×2 grid of plots\n\n\n\nCode\npar(mfrow = c(2, 2))\nx &lt;- seq(1, 100, by = 1)\nplot(x, type = \"p\", main = \"Points\", col = \"blue\")\nplot(x, type = \"l\", main = \"Lines\", col = \"red\")\ny &lt;- rnorm(1000)\nhist(y, breaks = 10, main = \"10 Breaks\", col = \"lightblue\")\nhist(y, breaks = 50, main = \"50 Breaks\", col = \"lightgreen\")\n\n\n\n\n35.2.8 Exercise R.8: Creating Data Frames\n\nCreate a data frame with at least three columns: one character/factor, one numeric, and one logical\nAssign row names to your data frame using rownames()\nExamine your data frame structure using str()\nCalculate the mean of each numeric variable\nUse head() and tail() to view portions of your data frame\n\n\n\nCode\n# Example:\ntreatment &lt;- c(\"control\", \"low\", \"medium\", \"high\", \"control\", \"low\")\nresponse &lt;- c(12.3, 15.6, 18.9, 24.2, 11.8, 16.1)\nsignificant &lt;- c(FALSE, TRUE, TRUE, TRUE, FALSE, TRUE)\nmy_data &lt;- data.frame(treatment, response, significant)\nstr(my_data)\n\n\n\n\n35.2.9 Exercise R.9: Data Import and Indexing\n\nCreate a simple CSV file or use a built-in dataset like iris\nUse read.csv() to read in your file (or access iris directly)\nUse str() and head() to examine the data structure\nUse $ and [ ] operators to select different parts of the data frame\nCreate a plot of two numeric variables\nUse tapply() to calculate summary statistics grouped by a categorical variable\nExport your data frame using write.csv()\n\n\n\nCode\n# Example with iris:\ndata(iris)\nstr(iris)\nhead(iris)\niris$Sepal.Length[1:5]  # First 5 sepal lengths\niris[1:3, ]  # First 3 rows\nplot(iris$Sepal.Length, iris$Petal.Length, col = iris$Species)\ntapply(iris$Sepal.Length, iris$Species, mean)\n\n\n\n\n35.2.10 Exercise R.10: Understanding Object Types\nExplore how R handles different data types:\n\nCreate variables of different classes: numeric, character, logical, and factor\nWhat happens when you try to perform arithmetic on character data?\nExperiment with type coercion using as.numeric(), as.character(), and as.factor()\nWhat happens when you add a character element to a numeric vector?",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>Practice Exercises</span>"
    ]
  },
  {
    "objectID": "chapters/A9-exercises.html#sec-ex-markdown",
    "href": "chapters/A9-exercises.html#sec-ex-markdown",
    "title": "35  Practice Exercises",
    "section": "35.3 Markdown and LaTeX Exercises",
    "text": "35.3 Markdown and LaTeX Exercises\nThese exercises correspond to ?sec-markdown.\n\n35.3.1 Exercise M.1: R Markdown Basics\nCreate a new R Markdown document and practice:\n\nCreating headers at different levels using #, ##, and ###\nMaking text italic and bold\nCreating ordered and unordered lists\nInserting a hyperlink\nCreating a code chunk that generates output\nKnitting the document to HTML\n\n\n\n35.3.2 Exercise M.2: Code Chunk Options\nExperiment with code chunk options:\n\nCreate a code chunk with echo=TRUE and eval=TRUE (default behavior)\nCreate the same code chunk with echo=FALSE—what happens?\nTry eval=FALSE—what happens?\nUse fig.width and fig.height to control plot dimensions\n\n\n\nCode\n# Example chunk with options\nx &lt;- rnorm(100)\nhist(x, col = \"steelblue\", main = \"Random Normal Data\")\n\n\n\n\n\n\n\n\n\n\n\n35.3.3 Exercise M.3: LaTeX Mathematical Notation\nPractice writing equations in LaTeX:\n\nWrite the equation for the mean: \\(\\bar{x} = \\frac{1}{n}\\sum_{i=1}^{n} x_i\\)\nWrite the equation for variance\nWrite the normal distribution probability density function\nUse both inline math ($...$) and display math ($$...$$)\n\n\n\n35.3.4 Exercise M.4: Tables in Markdown\nCreate tables using:\n\nBasic Markdown table syntax\nThe kable() function from the knitr package\n\n\n\nCode\nlibrary(knitr)\nkable(head(iris, 5))",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>Practice Exercises</span>"
    ]
  },
  {
    "objectID": "chapters/A9-exercises.html#sec-ex-tidy",
    "href": "chapters/A9-exercises.html#sec-ex-tidy",
    "title": "35  Practice Exercises",
    "section": "35.4 Tidy Data and Data Wrangling Exercises",
    "text": "35.4 Tidy Data and Data Wrangling Exercises\nThese exercises correspond to Chapter 5.\n\n35.4.1 Exercise T.1: Tidyverse Basics\nLoad the tidyverse and practice with a dataset:\n\n\nCode\nlibrary(tidyverse)\ndata(mpg)\n\n\n\nConvert the mpg data frame to a tibble using as_tibble()\nWhat differences do you notice in how it prints?\nUse glimpse() to get an overview of the data\n\n\n\n35.4.2 Exercise T.2: The Five dplyr Verbs\nUsing the mpg dataset, practice each core verb:\n\nfilter(): Select only cars with highway mpg greater than 30\nselect(): Choose only the manufacturer, model, and highway mpg columns\narrange(): Sort by highway mpg in descending order\nmutate(): Create a new column that calculates the ratio of highway to city mpg\nsummarize(): Calculate the mean highway mpg for the entire dataset\n\n\n\nCode\n# Example solutions:\nmpg |&gt; filter(hwy &gt; 30)\nmpg |&gt; select(manufacturer, model, hwy)\nmpg |&gt; arrange(desc(hwy))\nmpg |&gt; mutate(hwy_city_ratio = hwy / cty)\nmpg |&gt; summarize(mean_hwy = mean(hwy))\n\n\n\n\n35.4.3 Exercise T.3: Grouping and Summarizing\n\nGroup the mpg data by manufacturer and calculate the mean highway mpg for each\nFind the manufacturer with the highest average highway mpg\nCount how many models each manufacturer has in the dataset\nCalculate both mean and standard deviation of highway mpg by vehicle class\n\n\n\nCode\nmpg |&gt;\n  group_by(manufacturer) |&gt;\n  summarize(\n    mean_hwy = mean(hwy),\n    n_models = n()\n  ) |&gt;\n  arrange(desc(mean_hwy))\n\n\n\n\n35.4.4 Exercise T.4: Data Wrangling Pipeline\nConstruct a pipeline that:\n\nFilters for a subset of manufacturers (e.g., “audi”, “toyota”, “honda”)\nSelects relevant columns\nCreates a new calculated column\nGroups by a categorical variable\nSummarizes with multiple statistics\n\n\n\nCode\nmpg |&gt;\n  filter(manufacturer %in% c(\"audi\", \"toyota\", \"honda\")) |&gt;\n  select(manufacturer, model, year, cty, hwy) |&gt;\n  mutate(avg_mpg = (cty + hwy) / 2) |&gt;\n  group_by(manufacturer) |&gt;\n  summarize(\n    mean_mpg = mean(avg_mpg),\n    sd_mpg = sd(avg_mpg),\n    n = n()\n  )\n\n\n\n\n35.4.5 Exercise T.5: Reshaping Data\nPractice with pivot_longer() and pivot_wider():\n\nCreate a wide dataset with measurements across multiple time points\nConvert it to long format using pivot_longer()\nConvert it back to wide format using pivot_wider()\n\n\n\nCode\n# Example wide data\nwide_data &lt;- tibble(\n  sample = c(\"A\", \"B\", \"C\"),\n  time_0 = c(10, 12, 8),\n  time_1 = c(15, 18, 12),\n  time_2 = c(22, 25, 18)\n)\n\n# Convert to long format\nlong_data &lt;- wide_data |&gt;\n  pivot_longer(\n    cols = starts_with(\"time\"),\n    names_to = \"timepoint\",\n    values_to = \"measurement\"\n  )\nprint(long_data)\n\n\n# A tibble: 9 × 3\n  sample timepoint measurement\n  &lt;chr&gt;  &lt;chr&gt;           &lt;dbl&gt;\n1 A      time_0             10\n2 A      time_1             15\n3 A      time_2             22\n4 B      time_0             12\n5 B      time_1             18\n6 B      time_2             25\n7 C      time_0              8\n8 C      time_1             12\n9 C      time_2             18\n\n\n\n\n35.4.6 Exercise T.6: Joining Data\nCreate two related tibbles and practice joins:\n\n\nCode\n# Sample data\nexperiments &lt;- tibble(\n  sample_id = c(\"S1\", \"S2\", \"S3\", \"S4\"),\n  treatment = c(\"control\", \"low\", \"medium\", \"high\")\n)\n\nmeasurements &lt;- tibble(\n  sample_id = c(\"S1\", \"S1\", \"S2\", \"S3\"),\n  replicate = c(1, 2, 1, 1),\n  value = c(10.2, 9.8, 15.3, 18.7)\n)\n\n\n\nPerform a left_join() to add treatment information to measurements\nUse anti_join() to find samples that have no measurements\nExplain the difference between inner_join() and full_join()",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>Practice Exercises</span>"
    ]
  },
  {
    "objectID": "chapters/A9-exercises.html#sec-ex-viz",
    "href": "chapters/A9-exercises.html#sec-ex-viz",
    "title": "35  Practice Exercises",
    "section": "35.5 Data Visualization Exercises",
    "text": "35.5 Data Visualization Exercises\nThese exercises correspond to Chapter 6.\n\n35.5.1 Exercise V.1: Basic ggplot2\nCreate your first ggplot visualizations:\n\n\nCode\nlibrary(ggplot2)\ndata(mpg)\n\n# Basic scatterplot\nggplot(mpg, aes(x = displ, y = hwy)) +\n  geom_point()\n\n\n\nCreate a scatterplot of engine displacement vs. highway mpg\nAdd color based on vehicle class\nAdd appropriate axis labels and a title\nTry different themes (theme_minimal(), theme_classic(), theme_bw())\n\n\n\n35.5.2 Exercise V.2: Geometric Objects\nPractice with different geoms:\n\nCreate a histogram of highway mpg using geom_histogram()\nCreate a boxplot of highway mpg by vehicle class using geom_boxplot()\nCreate a bar chart showing the count of vehicles by manufacturer using geom_bar()\nCreate a line plot (use a time series dataset or create synthetic data)\n\n\n\n35.5.3 Exercise V.3: Aesthetic Mappings\nExplore different aesthetic mappings:\n\nMap a continuous variable to color in a scatterplot\nMap a categorical variable to shape\nSet fixed aesthetics (like size = 3) outside of aes()\nWhat is the difference between mapping a variable to an aesthetic inside aes() versus setting a fixed value outside?\n\n\n\nCode\n# Mapped aesthetic (variable determines color)\nggplot(mpg, aes(x = displ, y = hwy, color = class)) +\n  geom_point(size = 3)\n\n# Fixed aesthetic (all points same color)\nggplot(mpg, aes(x = displ, y = hwy)) +\n  geom_point(color = \"steelblue\", size = 3)\n\n\n\n\n35.5.4 Exercise V.4: Faceting\nPractice creating small multiples:\n\nCreate a scatterplot faceted by vehicle class using facet_wrap()\nCreate a grid of plots using facet_grid() with two variables\nExperiment with the scales argument to allow different axis scales per facet\n\n\n\nCode\nggplot(mpg, aes(x = displ, y = hwy)) +\n  geom_point() +\n  facet_wrap(~ class, nrow = 2)\n\n\n\n\n35.5.5 Exercise V.5: Combining Layers\nBuild complex visualizations by layering:\n\nCreate a scatterplot with a smoothed trend line\nAdd both points and a regression line\nUse different colors for points and the trend line\n\n\n\nCode\nggplot(mpg, aes(x = displ, y = hwy)) +\n  geom_point(aes(color = class), alpha = 0.7) +\n  geom_smooth(method = \"lm\", color = \"black\", se = TRUE) +\n  labs(\n    title = \"Engine Size vs. Fuel Efficiency\",\n    x = \"Engine Displacement (L)\",\n    y = \"Highway MPG\",\n    color = \"Vehicle Class\"\n  ) +\n  theme_minimal()\n\n\n\n\n35.5.6 Exercise V.6: Publication-Quality Figures\nCreate a polished figure suitable for publication:\n\nChoose an appropriate chart type for your data\nAdd informative labels (title, subtitle, caption, axis labels)\nUse an appropriate color palette\nAdjust theme elements for clarity\nSave the figure using ggsave() with appropriate dimensions and resolution",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>Practice Exercises</span>"
    ]
  },
  {
    "objectID": "chapters/A9-exercises.html#sec-ex-prob",
    "href": "chapters/A9-exercises.html#sec-ex-prob",
    "title": "35  Practice Exercises",
    "section": "35.6 Probability Exercises",
    "text": "35.6 Probability Exercises\nThese exercises correspond to Chapter 7.\n\n35.6.1 Exercise P.1: Simulating Coin Flips\nUse R to simulate coin flips:\n\n\nCode\nset.seed(123)\n\n\n\nSimulate 100 fair coin flips using rbinom() or sample()\nCalculate the proportion of heads\nRepeat with 1000 and 10000 flips—how does the proportion change?\nCreate a histogram of results from many simulations\n\n\n\nCode\n# Simulate coin flips\nn_flips &lt;- 1000\nflips &lt;- rbinom(n_flips, size = 1, prob = 0.5)\nmean(flips)  # Proportion of heads (1s)\n\n# Or using sample\nflips &lt;- sample(c(\"H\", \"T\"), n_flips, replace = TRUE)\nmean(flips == \"H\")\n\n\n\n\n35.6.2 Exercise P.2: Binomial Distribution\nExplore the binomial distribution:\n\nUse rbinom() to simulate 1000 experiments, each with 20 coin flips\nCreate a histogram of the number of heads per experiment\nWhat is the most common outcome? Does this match your expectation?\nChange the probability to simulate an unfair coin\nHow does the distribution change with 200 or 2000 flips per experiment?\n\n\n\nCode\n# 1000 experiments, 20 flips each, fair coin\nset.seed(42)\nresults &lt;- rbinom(1000, size = 20, prob = 0.5)\nhist(results, breaks = 20, col = \"steelblue\",\n     main = \"Number of Heads in 20 Flips\",\n     xlab = \"Number of Heads\")\n\n\n\n\n\n\n\n\n\n\n\n35.6.3 Exercise P.3: The Birthday Problem\nUse Monte Carlo simulation to explore the birthday problem:\n\nWrite a function that simulates whether any two people in a group share a birthday\nEstimate the probability for groups of size 10, 23, and 50\nPlot the probability as a function of group size\nAt what group size does the probability exceed 50%?\n\n\n\nCode\n# Birthday simulation function\nsame_birthday &lt;- function(n, B = 10000) {\n  matches &lt;- replicate(B, {\n    birthdays &lt;- sample(1:365, n, replace = TRUE)\n    any(duplicated(birthdays))\n  })\n  mean(matches)\n}\n\n# Test for different group sizes\nsizes &lt;- 2:50\nprobs &lt;- sapply(sizes, same_birthday)\nplot(sizes, probs, type = \"l\",\n     xlab = \"Group Size\", ylab = \"Probability of Shared Birthday\")\nabline(h = 0.5, col = \"red\", lty = 2)\n\n\n\n\n35.6.4 Exercise P.4: Conditional Probability\nExplore conditional probability with card simulations:\n\nCreate a virtual deck of 52 cards\nCalculate the probability of drawing a King\nGiven that the first card drawn is a King, what is the probability the second card is also a King?\nUse simulation to verify your calculation\n\n\n\n35.6.5 Exercise P.5: The Monty Hall Problem\nSimulate the Monty Hall problem:\n\nWrite a function that simulates one round of the game\nCompare the win rate when you stick versus when you switch\nRun 10,000 simulations for each strategy\nDoes switching really double your chances?",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>Practice Exercises</span>"
    ]
  },
  {
    "objectID": "chapters/A9-exercises.html#sec-ex-discrete",
    "href": "chapters/A9-exercises.html#sec-ex-discrete",
    "title": "35  Practice Exercises",
    "section": "35.7 Discrete Distributions Exercises",
    "text": "35.7 Discrete Distributions Exercises\nThese exercises correspond to Chapter 8.\n\n35.7.1 Exercise D.1: Binomial Distribution Properties\n\nFor n = 20 and p = 0.3, calculate the probability of exactly 5 successes using dbinom()\nCalculate the probability of 5 or fewer successes using pbinom()\nGenerate 1000 random values from this distribution using rbinom()\nCompare your simulated mean and variance to the theoretical values (np and np(1-p))\n\n\n\nCode\nn &lt;- 20\np &lt;- 0.3\n\n# Exact probability of 5 successes\ndbinom(5, size = n, prob = p)\n\n# Cumulative probability (≤5)\npbinom(5, size = n, prob = p)\n\n# Simulation\nset.seed(42)\nsamples &lt;- rbinom(1000, size = n, prob = p)\nmean(samples)  # Compare to n*p\nvar(samples)   # Compare to n*p*(1-p)\n\n\n\n\n35.7.2 Exercise D.2: Poisson Distribution\nThe Poisson distribution models counts of rare events:\n\nIf a hospital averages 4 emergency admissions per hour, what is the probability of exactly 6 admissions in one hour?\nWhat is the probability of 10 or more admissions?\nSimulate 1000 hours and plot the distribution\nHow does the distribution change when λ is small (0.5) versus large (20)?\n\n\n\n35.7.3 Exercise D.3: Comparing Distributions\nGenerate samples from binomial and Poisson distributions with similar means and compare:\n\nCreate histograms side by side\nWhen does the Poisson approximate the binomial well?\nWhat happens as n increases and p decreases while np stays constant?",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>Practice Exercises</span>"
    ]
  },
  {
    "objectID": "chapters/A9-exercises.html#sec-ex-continuous",
    "href": "chapters/A9-exercises.html#sec-ex-continuous",
    "title": "35  Practice Exercises",
    "section": "35.8 Continuous Distributions Exercises",
    "text": "35.8 Continuous Distributions Exercises\nThese exercises correspond to Chapter 9.\n\n35.8.1 Exercise C.1: Normal Distribution\nExplore the normal distribution:\n\nGenerate 10,000 values from a normal distribution with μ = 100 and σ = 15\nCalculate the sample mean and standard deviation—how close are they to the parameters?\nWhat proportion of values fall within 1, 2, and 3 standard deviations of the mean?\nCompare to the theoretical values (68-95-99.7 rule)\n\n\n\nCode\nset.seed(42)\nx &lt;- rnorm(10000, mean = 100, sd = 15)\n\n# Sample statistics\nmean(x)\n\n\n[1] 99.83036\n\n\nCode\nsd(x)\n\n\n[1] 15.09202\n\n\nCode\n# Proportions within SDs\nmean(abs(x - mean(x)) &lt; 1*sd(x))  # Within 1 SD\n\n\n[1] 0.6829\n\n\nCode\nmean(abs(x - mean(x)) &lt; 2*sd(x))  # Within 2 SDs\n\n\n[1] 0.9553\n\n\nCode\nmean(abs(x - mean(x)) &lt; 3*sd(x))  # Within 3 SDs\n\n\n[1] 0.9973\n\n\nCode\n# Visualize\nhist(x, breaks = 50, freq = FALSE, col = \"lightblue\",\n     main = \"Normal Distribution (μ=100, σ=15)\")\ncurve(dnorm(x, mean = 100, sd = 15), add = TRUE, col = \"red\", lwd = 2)\n\n\n\n\n\n\n\n\n\n\n\n35.8.2 Exercise C.2: Standard Normal and Z-Scores\n\nConvert a set of raw scores to z-scores\nUse pnorm() to find the proportion of values below a given z-score\nUse qnorm() to find the z-score corresponding to a given percentile\nWhat z-score corresponds to the 95th percentile?\n\n\n\n35.8.3 Exercise C.3: Log-Normal Distribution\nMany biological measurements follow a log-normal distribution:\n\nGenerate data from a log-normal distribution using rlnorm()\nPlot the original data—notice the right skew\nTake the log and plot again—it should appear normal\nWhen might you encounter log-normal data in biology?",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>Practice Exercises</span>"
    ]
  },
  {
    "objectID": "chapters/A9-exercises.html#sec-ex-hypothesis",
    "href": "chapters/A9-exercises.html#sec-ex-hypothesis",
    "title": "35  Practice Exercises",
    "section": "35.9 Hypothesis Testing Exercises",
    "text": "35.9 Hypothesis Testing Exercises\nThese exercises correspond to Chapter 11 and Chapter 12.\n\n35.9.1 Exercise H.1: One-Sample t-test\n\nGenerate a sample of 30 observations from a normal distribution with mean 105 and SD 15\nTest whether the mean differs significantly from 100\nInterpret the p-value and confidence interval\nWhat happens to the p-value when you increase the sample size?\n\n\n\nCode\nset.seed(42)\nsample_data &lt;- rnorm(30, mean = 105, sd = 15)\nt.test(sample_data, mu = 100)\n\n\n\n\n35.9.2 Exercise H.2: Two-Sample t-test\nCreate a dummy dataset with one continuous and one categorical variable:\n\nDraw samples of 100 observations from two normal distributions with slightly different means but equal standard deviations\nPerform a two-sample t-test\nVisualize the data with a boxplot\nRepeat with sample sizes of 10, 100, and 1000—how does sample size affect the results?\nWhat happens when you make the means more different?\n\n\n\nCode\nset.seed(42)\ngroup_a &lt;- rnorm(100, mean = 10, sd = 2)\ngroup_b &lt;- rnorm(100, mean = 11, sd = 2)\n\n# Combine into data frame\ndata &lt;- data.frame(\n  value = c(group_a, group_b),\n  group = rep(c(\"A\", \"B\"), each = 100)\n)\n\n# t-test\nt.test(value ~ group, data = data)\n\n# Visualization\nboxplot(value ~ group, data = data)\n\n\n\n\n35.9.3 Exercise H.3: Chi-Square Test\nTest for Hardy-Weinberg equilibrium:\n\n\nCode\n# Observed genotype counts\nAA_counts &lt;- 50\nAa_counts &lt;- 40\naa_counts &lt;- 10\n\n# Calculate allele frequencies\ntotal &lt;- AA_counts + Aa_counts + aa_counts\np &lt;- (2*AA_counts + Aa_counts) / (2*total)\nq &lt;- 1 - p\n\n# Expected counts under HWE\nexpected &lt;- c(p^2, 2*p*q, q^2) * total\n\n# Chi-square test\nobserved &lt;- c(AA_counts, Aa_counts, aa_counts)\nchisq.test(observed, p = c(p^2, 2*p*q, q^2))\n\n\n\n    Chi-squared test for given probabilities\n\ndata:  observed\nX-squared = 0.22676, df = 2, p-value = 0.8928\n\n\n\nModify the observed counts and see how it affects the test result\nWhat genotype frequencies would indicate strong departure from HWE?",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>Practice Exercises</span>"
    ]
  },
  {
    "objectID": "chapters/A9-exercises.html#sec-ex-linear",
    "href": "chapters/A9-exercises.html#sec-ex-linear",
    "title": "35  Practice Exercises",
    "section": "35.10 Linear Models Exercises",
    "text": "35.10 Linear Models Exercises\nThese exercises correspond to Chapter 16, ?sec-regression, and Chapter 20.\n\n35.10.1 Exercise L.1: Correlation\n\nCreate two correlated variables using simulation\nCalculate the Pearson correlation coefficient\nCreate a scatterplot and add the correlation value\nWhat happens to the correlation when you add outliers?\n\n\n\nCode\nset.seed(42)\nx &lt;- rnorm(50)\ny &lt;- 0.7*x + rnorm(50, sd = 0.5)  # Correlated with x\n\ncor(x, y)\n\n\n[1] 0.8400841\n\n\nCode\nplot(x, y, main = paste(\"Correlation:\", round(cor(x, y), 2)))\nabline(lm(y ~ x), col = \"red\")\n\n\n\n\n\n\n\n\n\n\n\n35.10.2 Exercise L.2: Simple Linear Regression\n\nUsing a dataset of your choice, fit a linear model with lm()\nExamine the model summary\nCreate a scatterplot with the regression line\nPlot the residuals—do they appear randomly distributed?\n\n\n\nCode\n# Example with built-in data\ndata(mtcars)\nmodel &lt;- lm(mpg ~ wt, data = mtcars)\nsummary(model)\n\n# Regression plot\nplot(mpg ~ wt, data = mtcars)\nabline(model, col = \"red\")\n\n# Residual plot\nplot(model$fitted.values, model$residuals)\nabline(h = 0, col = \"red\", lty = 2)\n\n\n\n\n35.10.3 Exercise L.3: ANOVA\nPerform a one-way ANOVA:\n\nUsing the iris dataset, test whether sepal length differs among species\nExamine the ANOVA table\nIf significant, which pairs of species differ? Use a post-hoc test\nVisualize the differences with boxplots\n\n\n\nCode\n# One-way ANOVA\nmodel &lt;- aov(Sepal.Length ~ Species, data = iris)\nsummary(model)\n\n# Post-hoc test\nTukeyHSD(model)\n\n# Visualization\nboxplot(Sepal.Length ~ Species, data = iris)\n\n\n\n\n35.10.4 Exercise L.4: Two-Way Factorial ANOVA\nFor a dataset with two categorical predictors:\n\nFit a factorial model including the interaction\nInterpret the main effects and interaction\nCreate an interaction plot\nWhat does a significant interaction mean biologically?",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>Practice Exercises</span>"
    ]
  },
  {
    "objectID": "chapters/A9-exercises.html#sec-ex-advanced",
    "href": "chapters/A9-exercises.html#sec-ex-advanced",
    "title": "35  Practice Exercises",
    "section": "35.11 Advanced Topics Exercises",
    "text": "35.11 Advanced Topics Exercises\n\n35.11.1 Exercise A.1: Principal Component Analysis\nUsing a multivariate dataset:\n\nStandardize the variables\nPerform PCA using prcomp()\nExamine the proportion of variance explained by each component\nCreate a biplot showing samples and variable loadings\nHow many components would you retain?\n\n\n\nCode\n# PCA on iris data (excluding species)\niris_numeric &lt;- iris[, 1:4]\npca &lt;- prcomp(iris_numeric, scale. = TRUE)\n\n# Variance explained\nsummary(pca)\n\n# Biplot\nbiplot(pca)\n\n\n\n\n35.11.2 Exercise A.2: Logistic Regression\nFit a logistic regression model:\n\nCreate or load a dataset with a binary outcome\nFit a logistic regression model using glm() with family = binomial\nInterpret the coefficients in terms of odds ratios\nCalculate predicted probabilities for new observations\n\n\n\n35.11.3 Exercise A.3: Bootstrapping\nEstimate confidence intervals using bootstrapping:\n\nDraw a sample of 50 observations\nCreate 1000 bootstrap samples\nCalculate the mean for each bootstrap sample\nConstruct a 95% confidence interval from the bootstrap distribution\nCompare to the theoretical confidence interval\n\n\n\nCode\nset.seed(42)\noriginal_sample &lt;- rnorm(50, mean = 100, sd = 15)\n\n# Bootstrap\nn_bootstrap &lt;- 1000\nbootstrap_means &lt;- replicate(n_bootstrap, {\n  boot_sample &lt;- sample(original_sample, replace = TRUE)\n  mean(boot_sample)\n})\n\n# Confidence interval\nquantile(bootstrap_means, c(0.025, 0.975))\n\n\n    2.5%    97.5% \n 94.6165 103.9561 \n\n\nCode\n# Compare to theoretical\nt.test(original_sample)$conf.int\n\n\n[1]  94.55623 104.37361\nattr(,\"conf.level\")\n[1] 0.95\n\n\nCode\n# Visualize\nhist(bootstrap_means, breaks = 30, main = \"Bootstrap Distribution of Means\",\n     col = \"lightblue\")\nabline(v = mean(original_sample), col = \"red\", lwd = 2)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>Practice Exercises</span>"
    ]
  },
  {
    "objectID": "chapters/A9-exercises.html#sec-ex-git",
    "href": "chapters/A9-exercises.html#sec-ex-git",
    "title": "35  Practice Exercises",
    "section": "35.12 Version Control with Git",
    "text": "35.12 Version Control with Git\n\n35.12.1 Exercise G.1: Git Basics\nPractice basic Git operations:\n\nCreate a new directory and initialize a Git repository with git init\nCreate a new file and check the status with git status\nStage the file with git add\nCreate a commit with git commit -m \"message\"\nMake changes and commit again\nView the commit history with git log\n\n\n\n35.12.2 Exercise G.2: Working with GitHub\n\nCreate a GitHub account if you don’t have one\nCreate a new repository on GitHub\nClone it to your local machine with git clone\nMake changes, commit, and push with git push\nPractice the pull-commit-push workflow\n\n\n\n35.12.3 Exercise G.3: Collaboration\nIf working with a partner:\n\nClone your partner’s repository\nMake a change and push it\nHave your partner pull the changes\nPractice resolving a merge conflict",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>Practice Exercises</span>"
    ]
  },
  {
    "objectID": "chapters/A9-exercises.html#summary",
    "href": "chapters/A9-exercises.html#summary",
    "title": "35  Practice Exercises",
    "section": "35.13 Summary",
    "text": "35.13 Summary\nThese exercises cover the core topics in statistical analysis and data science. As you work through them, remember:\n\nPractice regularly: Skills improve with consistent practice\nExperiment: Try variations on the exercises to deepen understanding\nConsult documentation: Use ?function_name and online resources\nDebug systematically: When code doesn’t work, check each step\nCollaborate: Discuss problems with classmates and colleagues\n\nThe exercises are designed to build skills progressively. Return to earlier exercises as you learn new techniques—you may find more elegant solutions with your growing knowledge.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>Practice Exercises</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Cohen, Jacob. 1988. Statistical Power Analysis for the Behavioral\nSciences. 2nd ed. Lawrence Erlbaum Associates.\n\n\nCrawley, Michael J. 2007. The r Book. John Wiley & Sons.\n\n\nEfron, Bradley. 1979. “Bootstrap Methods: Another Look at the\nJackknife.” The Annals of Statistics 7 (1): 1–26.\n\n\nIrizarry, Rafael A. 2019. Introduction to Data Science.\nSelf-published. https://rafalab.github.io/dsbook/.\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani.\n2023. An Introduction to Statistical Learning with Applications in\nr. 2nd ed. Springer. https://www.statlearning.com.\n\n\nLogan, Murray. 2010. Biostatistical Design and Analysis Using\nr. Wiley-Blackwell.\n\n\nThulin, Måns. 2025. Modern Statistics with r. CRC Press. https://www.modernstatisticswithr.com.",
    "crumbs": [
      "References"
    ]
  }
]