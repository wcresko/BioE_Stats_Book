<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.30">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>35&nbsp; Support Vector Machines – Statistics for the Biosciences and Bioengineering</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../chapters/35-clustering.html" rel="next">
<link href="../chapters/33-trees-forests.html" rel="prev">
<script src="../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-de070a7b0ab54f8780927367ac907214.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap-485d01fc63b59abcd3ee1bf1e8e2748d.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar floating quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../chapters/28-intro-statistical-learning.html">Statistical Learning</a></li><li class="breadcrumb-item"><a href="../chapters/34-svm.html"><span class="chapter-number">35</span>&nbsp; <span class="chapter-title">Support Vector Machines</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../">Statistics for the Biosciences and Bioengineering</a> 
        <div class="sidebar-tools-main">
  <a href="" class="quarto-reader-toggle quarto-navigation-tool px-1" onclick="window.quartoToggleReader(); return false;" title="Toggle reader mode">
  <div class="quarto-reader-toggle-btn">
  <i class="bi"></i>
  </div>
</a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Why This Book?</span></span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">Core Data Science Tools</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/01-introduction.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Introduction</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/02-installing-tools.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Installing Core Tools</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/03-unix-command-line.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Unix and the Command Line</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/04-r-rstudio.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">R and RStudio</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/05-markdown-latex.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Markdown and LaTeX</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true">
 <span class="menu-text">Data Exploration</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/06-tidy-data.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Tidy Data</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/07-data-wrangling.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Data Wrangling with dplyr</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/08-exploratory-data-analysis.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Exploratory Data Analysis</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/09-data-visualization.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Data Visualization</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true">
 <span class="menu-text">Probability and Distributions</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/10-probability-foundations.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Foundations of Probability</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/11-discrete-distributions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Discrete Probability Distributions</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/12-continuous-distributions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Continuous Probability Distributions</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/13-sampling-estimation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Sampling and Parameter Estimation</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true">
 <span class="menu-text">Statistical Inference</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/14-experimental-design.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Experimental Design Principles</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/15-hypothesis-testing.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Hypothesis Testing</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/16-t-tests.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">T-Tests</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/17-nonparametric-tests.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Nonparametric Tests</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/18-bootstrapping.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">Resampling Methods</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="true">
 <span class="menu-text">Linear Models</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/19-what-are-models.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">What are Models?</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/20-correlation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">21</span>&nbsp; <span class="chapter-title">Correlation</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/21-simple-linear-regression.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">22</span>&nbsp; <span class="chapter-title">Simple Linear Regression</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/22-residual-analysis.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">23</span>&nbsp; <span class="chapter-title">Residual Analysis</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/23-statistical-power.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">24</span>&nbsp; <span class="chapter-title">Statistical Power</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/24-multiple-regression.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">25</span>&nbsp; <span class="chapter-title">Multiple Regression</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/25-single-factor-anova.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">26</span>&nbsp; <span class="chapter-title">Single Factor ANOVA</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/26-multifactor-anova.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">27</span>&nbsp; <span class="chapter-title">Multi-Factor ANOVA</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/27-glm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">28</span>&nbsp; <span class="chapter-title">Generalized Linear Models</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="true">
 <span class="menu-text">Statistical Learning</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-6" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/28-intro-statistical-learning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">29</span>&nbsp; <span class="chapter-title">Introduction to Statistical Learning</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/29-model-validation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">30</span>&nbsp; <span class="chapter-title">Model Validation and the Bias-Variance Tradeoff</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/30-regularization.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">31</span>&nbsp; <span class="chapter-title">Regularization Methods</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/31-smoothing.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">32</span>&nbsp; <span class="chapter-title">Smoothing and Non-Parametric Regression</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/32-classification-methods.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">33</span>&nbsp; <span class="chapter-title">Classification and Performance Metrics</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/33-trees-forests.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">34</span>&nbsp; <span class="chapter-title">Decision Trees and Random Forests</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/34-svm.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">35</span>&nbsp; <span class="chapter-title">Support Vector Machines</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/35-clustering.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">36</span>&nbsp; <span class="chapter-title">Clustering</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/36-dimensionality-reduction.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">37</span>&nbsp; <span class="chapter-title">Dimensionality Reduction and Multivariate Methods</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/37-tsne-umap.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">38</span>&nbsp; <span class="chapter-title">Non-Linear Dimensionality Reduction: t-SNE and UMAP</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/38-bayesian-statistics.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">39</span>&nbsp; <span class="chapter-title">Bayesian Statistics</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/39-deep-learning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">40</span>&nbsp; <span class="chapter-title">Introduction to Deep Learning</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/40-high-performance-computing.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">41</span>&nbsp; <span class="chapter-title">High Performance Computing</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="true">
 <span class="menu-text">Scientific Communication</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-7" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/41-presenting-results.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">42</span>&nbsp; <span class="chapter-title">Presenting Statistical Results</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-8" role="navigation" aria-expanded="true">
 <span class="menu-text">Historical Context</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-8" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-8" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/A1-eugenics-history.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">43</span>&nbsp; <span class="chapter-title">The Eugenics History of Statistics</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-9" role="navigation" aria-expanded="true">
 <span class="menu-text">Appendices</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-9" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-9" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/A2-keyboard-shortcuts.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">44</span>&nbsp; <span class="chapter-title">Keyboard Shortcuts Reference</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/A3-unix-reference.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">45</span>&nbsp; <span class="chapter-title">Unix Command Reference</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/A4-r-reference.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">46</span>&nbsp; <span class="chapter-title">R Command Reference</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/A5-quarto-reference.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">47</span>&nbsp; <span class="chapter-title">Quarto Markdown Reference</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/A6-latex-reference.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">48</span>&nbsp; <span class="chapter-title">LaTeX Command Reference</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/A7-greek-letters.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">49</span>&nbsp; <span class="chapter-title">Greek Letters in Mathematics and Statistics</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/A8-probability-distributions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">50</span>&nbsp; <span class="chapter-title">Common Probability Distributions</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/A9-sampling-distributions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">51</span>&nbsp; <span class="chapter-title">Sampling Distributions in Hypothesis Testing</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/A10-matrix-algebra.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">52</span>&nbsp; <span class="chapter-title">Matrix Algebra Fundamentals</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#introduction-to-support-vector-machines" id="toc-introduction-to-support-vector-machines" class="nav-link active" data-scroll-target="#introduction-to-support-vector-machines"><span class="header-section-number">35.1</span> Introduction to Support Vector Machines</a></li>
  <li><a href="#the-maximum-margin-classifier" id="toc-the-maximum-margin-classifier" class="nav-link" data-scroll-target="#the-maximum-margin-classifier"><span class="header-section-number">35.2</span> The Maximum Margin Classifier</a>
  <ul class="collapse">
  <li><a href="#separating-hyperplanes" id="toc-separating-hyperplanes" class="nav-link" data-scroll-target="#separating-hyperplanes">Separating Hyperplanes</a></li>
  <li><a href="#the-margin" id="toc-the-margin" class="nav-link" data-scroll-target="#the-margin">The Margin</a></li>
  <li><a href="#support-vectors" id="toc-support-vectors" class="nav-link" data-scroll-target="#support-vectors">Support Vectors</a></li>
  </ul></li>
  <li><a href="#the-support-vector-classifier-soft-margin" id="toc-the-support-vector-classifier-soft-margin" class="nav-link" data-scroll-target="#the-support-vector-classifier-soft-margin"><span class="header-section-number">35.3</span> The Support Vector Classifier (Soft Margin)</a>
  <ul class="collapse">
  <li><a href="#the-cost-parameter-c" id="toc-the-cost-parameter-c" class="nav-link" data-scroll-target="#the-cost-parameter-c">The Cost Parameter C</a></li>
  </ul></li>
  <li><a href="#the-kernel-trick-non-linear-svms" id="toc-the-kernel-trick-non-linear-svms" class="nav-link" data-scroll-target="#the-kernel-trick-non-linear-svms"><span class="header-section-number">35.4</span> The Kernel Trick: Non-Linear SVMs</a>
  <ul class="collapse">
  <li><a href="#why-kernels" id="toc-why-kernels" class="nav-link" data-scroll-target="#why-kernels">Why Kernels?</a></li>
  <li><a href="#common-kernels" id="toc-common-kernels" class="nav-link" data-scroll-target="#common-kernels">Common Kernels</a></li>
  <li><a href="#tuning-the-rbf-kernel" id="toc-tuning-the-rbf-kernel" class="nav-link" data-scroll-target="#tuning-the-rbf-kernel">Tuning the RBF Kernel</a></li>
  <li><a href="#cross-validation-for-parameter-selection" id="toc-cross-validation-for-parameter-selection" class="nav-link" data-scroll-target="#cross-validation-for-parameter-selection">Cross-Validation for Parameter Selection</a></li>
  </ul></li>
  <li><a href="#multi-class-svms" id="toc-multi-class-svms" class="nav-link" data-scroll-target="#multi-class-svms"><span class="header-section-number">35.5</span> Multi-Class SVMs</a></li>
  <li><a href="#support-vector-regression-svr" id="toc-support-vector-regression-svr" class="nav-link" data-scroll-target="#support-vector-regression-svr"><span class="header-section-number">35.6</span> Support Vector Regression (SVR)</a></li>
  <li><a href="#practical-considerations" id="toc-practical-considerations" class="nav-link" data-scroll-target="#practical-considerations"><span class="header-section-number">35.7</span> Practical Considerations</a>
  <ul class="collapse">
  <li><a href="#feature-scaling" id="toc-feature-scaling" class="nav-link" data-scroll-target="#feature-scaling">Feature Scaling</a></li>
  <li><a href="#probability-estimates" id="toc-probability-estimates" class="nav-link" data-scroll-target="#probability-estimates">Probability Estimates</a></li>
  <li><a href="#decision-values" id="toc-decision-values" class="nav-link" data-scroll-target="#decision-values">Decision Values</a></li>
  <li><a href="#choosing-the-kernel" id="toc-choosing-the-kernel" class="nav-link" data-scroll-target="#choosing-the-kernel">Choosing the Kernel</a></li>
  <li><a href="#kernel-selection-guidelines" id="toc-kernel-selection-guidelines" class="nav-link" data-scroll-target="#kernel-selection-guidelines">Kernel Selection Guidelines</a></li>
  <li><a href="#advantages-and-limitations" id="toc-advantages-and-limitations" class="nav-link" data-scroll-target="#advantages-and-limitations">Advantages and Limitations</a></li>
  <li><a href="#comparison-with-other-methods" id="toc-comparison-with-other-methods" class="nav-link" data-scroll-target="#comparison-with-other-methods">Comparison with Other Methods</a></li>
  <li><a href="#feature-importance" id="toc-feature-importance" class="nav-link" data-scroll-target="#feature-importance">Feature Importance</a></li>
  <li><a href="#scaling-for-large-datasets" id="toc-scaling-for-large-datasets" class="nav-link" data-scroll-target="#scaling-for-large-datasets">Scaling for Large Datasets</a></li>
  </ul></li>
  <li><a href="#exercises" id="toc-exercises" class="nav-link" data-scroll-target="#exercises"><span class="header-section-number">35.8</span> Exercises</a></li>
  <li><a href="#summary" id="toc-summary" class="nav-link" data-scroll-target="#summary"><span class="header-section-number">35.9</span> Summary</a></li>
  <li><a href="#additional-resources" id="toc-additional-resources" class="nav-link" data-scroll-target="#additional-resources"><span class="header-section-number">35.10</span> Additional Resources</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../chapters/28-intro-statistical-learning.html">Statistical Learning</a></li><li class="breadcrumb-item"><a href="../chapters/34-svm.html"><span class="chapter-number">35</span>&nbsp; <span class="chapter-title">Support Vector Machines</span></a></li></ol></nav>
<div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title"><span id="sec-svm" class="quarto-section-identifier"><span class="chapter-number">35</span>&nbsp; <span class="chapter-title">Support Vector Machines</span></span></h1><button type="button" class="btn code-tools-button dropdown-toggle" id="quarto-code-tools-menu" data-bs-toggle="dropdown" aria-expanded="false"><i class="bi"></i> Code</button><ul class="dropdown-menu dropdown-menu-end" aria-labelelledby="quarto-code-tools-menu"><li><a id="quarto-show-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Show All Code</a></li><li><a id="quarto-hide-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Hide All Code</a></li><li><hr class="dropdown-divider"></li><li><a id="quarto-view-source" class="dropdown-item" href="javascript:void(0)" role="button">View Source</a></li></ul></div></div>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<div class="cell">
<div class="cell-output cell-output-error">
<pre><code>Error in `library()`:
! there is no package called 'kernlab'</code></pre>
</div>
</div>
<section id="introduction-to-support-vector-machines" class="level2" data-number="35.1">
<h2 data-number="35.1" class="anchored" data-anchor-id="introduction-to-support-vector-machines"><span class="header-section-number">35.1</span> Introduction to Support Vector Machines</h2>
<p><strong>Support Vector Machines (SVMs)</strong> are powerful supervised learning algorithms that excel at classification tasks, particularly when decision boundaries are complex. SVMs work by finding the optimal <strong>hyperplane</strong> that separates classes with the maximum margin.</p>
<p>SVMs were developed by Vladimir Vapnik and colleagues in the 1990s and quickly became one of the most popular machine learning methods before the rise of deep learning. They remain valuable tools today, particularly for problems with many features relative to samples, where their maximum margin principle helps prevent overfitting. They work well on small to medium-sized datasets where deep learning would lack sufficient data, and they are often preferred in applications requiring robust generalization with limited tuning. In biology, SVMs have been particularly successful for sequence classification—predicting protein structure or function from amino acid sequences, identifying splice sites in genes, or classifying regulatory elements in DNA.</p>
</section>
<section id="the-maximum-margin-classifier" class="level2" data-number="35.2">
<h2 data-number="35.2" class="anchored" data-anchor-id="the-maximum-margin-classifier"><span class="header-section-number">35.2</span> The Maximum Margin Classifier</h2>
<section id="separating-hyperplanes" class="level3">
<h3 class="anchored" data-anchor-id="separating-hyperplanes">Separating Hyperplanes</h3>
<p>Consider a simple two-class classification problem with two features. If the classes are <strong>linearly separable</strong>—they can be perfectly separated by a straight line (or hyperplane in higher dimensions)—there are infinitely many possible separating lines.</p>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate linearly separable data</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">42</span>)</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">50</span></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>x1 <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fu">rnorm</span>(n<span class="sc">/</span><span class="dv">2</span>, <span class="sc">-</span><span class="dv">1</span>, <span class="fl">0.5</span>), <span class="fu">rnorm</span>(n<span class="sc">/</span><span class="dv">2</span>, <span class="dv">1</span>, <span class="fl">0.5</span>))</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>x2 <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fu">rnorm</span>(n<span class="sc">/</span><span class="dv">2</span>, <span class="sc">-</span><span class="dv">1</span>, <span class="fl">0.5</span>), <span class="fu">rnorm</span>(n<span class="sc">/</span><span class="dv">2</span>, <span class="dv">1</span>, <span class="fl">0.5</span>))</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> <span class="fu">factor</span>(<span class="fu">c</span>(<span class="fu">rep</span>(<span class="sc">-</span><span class="dv">1</span>, n<span class="sc">/</span><span class="dv">2</span>), <span class="fu">rep</span>(<span class="dv">1</span>, n<span class="sc">/</span><span class="dv">2</span>)))</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>data_sep <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">x1 =</span> x1, <span class="at">x2 =</span> x2, <span class="at">y =</span> y)</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(x1, x2, <span class="at">col =</span> <span class="fu">c</span>(<span class="st">"red"</span>, <span class="st">"blue"</span>)[<span class="fu">as.numeric</span>(y)], <span class="at">pch =</span> <span class="dv">19</span>,</span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>     <span class="at">main =</span> <span class="st">"Multiple Separating Hyperplanes"</span>)</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Several possible separating lines</span></span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="sc">-</span><span class="fl">0.1</span>, <span class="dv">1</span>, <span class="at">lty =</span> <span class="dv">2</span>, <span class="at">col =</span> <span class="st">"gray"</span>)</span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="fl">0.2</span>, <span class="fl">0.9</span>, <span class="at">lty =</span> <span class="dv">2</span>, <span class="at">col =</span> <span class="st">"gray"</span>)</span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="sc">-</span><span class="fl">0.3</span>, <span class="fl">1.1</span>, <span class="at">lty =</span> <span class="dv">2</span>, <span class="at">col =</span> <span class="st">"gray"</span>)</span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="at">lwd =</span> <span class="dv">2</span>, <span class="at">col =</span> <span class="st">"darkgreen"</span>)  <span class="co"># Best one</span></span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a><span class="fu">legend</span>(<span class="st">"topleft"</span>, <span class="fu">c</span>(<span class="st">"Class -1"</span>, <span class="st">"Class +1"</span>),</span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a>       <span class="at">col =</span> <span class="fu">c</span>(<span class="st">"red"</span>, <span class="st">"blue"</span>), <span class="at">pch =</span> <span class="dv">19</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div id="fig-separating-hyperplanes" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-separating-hyperplanes-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="34-svm_files/figure-html/fig-separating-hyperplanes-1.png" class="img-fluid figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-separating-hyperplanes-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;35.1: Multiple hyperplanes can separate linearly separable classes. Which one is best?
</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="the-margin" class="level3">
<h3 class="anchored" data-anchor-id="the-margin">The Margin</h3>
<p>The <strong>maximum margin classifier</strong> chooses the hyperplane that maximizes the distance to the nearest points from either class. This distance is called the <strong>margin</strong>.</p>
<p>A hyperplane in <span class="math inline">\(p\)</span> dimensions is defined by: <span class="math display">\[\beta_0 + \beta_1 X_1 + \beta_2 X_2 + \cdots + \beta_p X_p = 0\]</span></p>
<p>For a new observation <span class="math inline">\(x^*\)</span>, we classify based on which side of the hyperplane it falls: <span class="math display">\[\text{sign}(\beta_0 + \beta_1 x_1^* + \beta_2 x_2^* + \cdots + \beta_p x_p^*) = \pm 1\]</span></p>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit SVM (linear kernel)</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(e1071)</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>svm_fit <span class="ot">&lt;-</span> <span class="fu">svm</span>(y <span class="sc">~</span> x1 <span class="sc">+</span> x2, <span class="at">data =</span> data_sep, <span class="at">kernel =</span> <span class="st">"linear"</span>, <span class="at">scale =</span> <span class="cn">FALSE</span>)</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot</span></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(x1, x2, <span class="at">col =</span> <span class="fu">c</span>(<span class="st">"red"</span>, <span class="st">"blue"</span>)[<span class="fu">as.numeric</span>(y)], <span class="at">pch =</span> <span class="dv">19</span>,</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>     <span class="at">main =</span> <span class="st">"Maximum Margin Classifier"</span>)</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Extract coefficients</span></span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>w <span class="ot">&lt;-</span> <span class="fu">t</span>(svm_fit<span class="sc">$</span>coefs) <span class="sc">%*%</span> svm_fit<span class="sc">$</span>SV</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>b <span class="ot">&lt;-</span> <span class="sc">-</span>svm_fit<span class="sc">$</span>rho</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Decision boundary</span></span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="sc">-</span>b<span class="sc">/</span>w[<span class="dv">2</span>], <span class="sc">-</span>w[<span class="dv">1</span>]<span class="sc">/</span>w[<span class="dv">2</span>], <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Margins</span></span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>((<span class="sc">-</span>b<span class="dv">-1</span>)<span class="sc">/</span>w[<span class="dv">2</span>], <span class="sc">-</span>w[<span class="dv">1</span>]<span class="sc">/</span>w[<span class="dv">2</span>], <span class="at">lty =</span> <span class="dv">2</span>)</span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>((<span class="sc">-</span>b<span class="sc">+</span><span class="dv">1</span>)<span class="sc">/</span>w[<span class="dv">2</span>], <span class="sc">-</span>w[<span class="dv">1</span>]<span class="sc">/</span>w[<span class="dv">2</span>], <span class="at">lty =</span> <span class="dv">2</span>)</span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a><span class="co"># Highlight support vectors</span></span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span>(svm_fit<span class="sc">$</span>SV, <span class="at">col =</span> <span class="st">"black"</span>, <span class="at">cex =</span> <span class="dv">2</span>, <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb3-22"><a href="#cb3-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-23"><a href="#cb3-23" aria-hidden="true" tabindex="-1"></a><span class="fu">legend</span>(<span class="st">"topleft"</span>, <span class="fu">c</span>(<span class="st">"Class -1"</span>, <span class="st">"Class +1"</span>, <span class="st">"Support Vectors"</span>),</span>
<span id="cb3-24"><a href="#cb3-24" aria-hidden="true" tabindex="-1"></a>       <span class="at">col =</span> <span class="fu">c</span>(<span class="st">"red"</span>, <span class="st">"blue"</span>, <span class="st">"black"</span>), <span class="at">pch =</span> <span class="fu">c</span>(<span class="dv">19</span>, <span class="dv">19</span>, <span class="dv">1</span>), <span class="at">pt.cex =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">2</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div id="fig-margin" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-margin-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="34-svm_files/figure-html/fig-margin-1.png" class="img-fluid figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-margin-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;35.2: The maximum margin classifier finds the hyperplane that maximizes the margin (distance to nearest points). Support vectors are the points on the margin boundaries.
</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="support-vectors" class="level3">
<h3 class="anchored" data-anchor-id="support-vectors">Support Vectors</h3>
<p>The observations that lie exactly on the margin boundary are called <strong>support vectors</strong>. These points “support” the hyperplane—if they were moved, the optimal hyperplane would change. Points farther from the margin have no effect on the solution.</p>
<p>This property has important practical implications. SVMs are <strong>sparse</strong> in the sense that only support vectors matter for prediction—the model can discard all other training points without affecting its predictions. This makes SVMs <strong>efficient</strong> at prediction time, since classification depends only on distances to the support vectors rather than to all training observations. And it makes SVMs <strong>robust</strong> to outliers that are far from the decision boundary, since such points don’t influence the model at all.</p>
</section>
</section>
<section id="the-support-vector-classifier-soft-margin" class="level2" data-number="35.3">
<h2 data-number="35.3" class="anchored" data-anchor-id="the-support-vector-classifier-soft-margin"><span class="header-section-number">35.3</span> The Support Vector Classifier (Soft Margin)</h2>
<p>Real data is rarely perfectly separable. The <strong>support vector classifier</strong> (soft margin SVM) allows some violations of the margin:</p>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate non-separable data</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>x1_ns <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fu">rnorm</span>(n<span class="sc">/</span><span class="dv">2</span>, <span class="sc">-</span><span class="fl">0.5</span>, <span class="fl">0.7</span>), <span class="fu">rnorm</span>(n<span class="sc">/</span><span class="dv">2</span>, <span class="fl">0.5</span>, <span class="fl">0.7</span>))</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>x2_ns <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fu">rnorm</span>(n<span class="sc">/</span><span class="dv">2</span>, <span class="sc">-</span><span class="fl">0.5</span>, <span class="fl">0.7</span>), <span class="fu">rnorm</span>(n<span class="sc">/</span><span class="dv">2</span>, <span class="fl">0.5</span>, <span class="fl">0.7</span>))</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>data_ns <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">x1 =</span> x1_ns, <span class="at">x2 =</span> x2_ns, <span class="at">y =</span> y)</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(x1_ns, x2_ns, <span class="at">col =</span> <span class="fu">c</span>(<span class="st">"red"</span>, <span class="st">"blue"</span>)[<span class="fu">as.numeric</span>(y)], <span class="at">pch =</span> <span class="dv">19</span>,</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>     <span class="at">main =</span> <span class="st">"Non-Separable Classes"</span>)</span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a><span class="fu">legend</span>(<span class="st">"topleft"</span>, <span class="fu">c</span>(<span class="st">"Class -1"</span>, <span class="st">"Class +1"</span>), <span class="at">col =</span> <span class="fu">c</span>(<span class="st">"red"</span>, <span class="st">"blue"</span>), <span class="at">pch =</span> <span class="dv">19</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div id="fig-nonseparable" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-nonseparable-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="34-svm_files/figure-html/fig-nonseparable-1.png" class="img-fluid figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-nonseparable-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;35.3: When classes overlap, perfect separation is impossible. The soft margin SVM allows some points to be on the wrong side.
</figcaption>
</figure>
</div>
</div>
</div>
<p>The soft margin formulation introduces <strong>slack variables</strong> <span class="math inline">\(\xi_i\)</span> that allow points to violate the margin:</p>
<p><span class="math display">\[\text{Minimize: } \frac{1}{2}||\beta||^2 + C \sum_{i=1}^n \xi_i\]</span></p>
<p>subject to: <span class="math display">\[y_i(\beta_0 + \beta^T x_i) \geq 1 - \xi_i, \quad \xi_i \geq 0\]</span></p>
<section id="the-cost-parameter-c" class="level3">
<h3 class="anchored" data-anchor-id="the-cost-parameter-c">The Cost Parameter C</h3>
<p>The parameter <strong>C</strong> controls the tradeoff between margin width and the number of violations allowed.</p>
<p>When <strong>C is large</strong>, the penalty for violations is high, so the algorithm tries hard to classify all training points correctly. This typically produces a narrow margin with few violations, but risks overfitting to noise in the training data. When <strong>C is small</strong>, violations are tolerated more easily, resulting in a wider margin that may misclassify some training points. This trades off training accuracy for a simpler decision boundary that often generalizes better to new data.</p>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">3</span>))</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (C_val <span class="cf">in</span> <span class="fu">c</span>(<span class="fl">0.1</span>, <span class="dv">1</span>, <span class="dv">100</span>)) {</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>  svm_c <span class="ot">&lt;-</span> <span class="fu">svm</span>(y <span class="sc">~</span> x1 <span class="sc">+</span> x2, <span class="at">data =</span> data_ns, <span class="at">kernel =</span> <span class="st">"linear"</span>,</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>               <span class="at">cost =</span> C_val, <span class="at">scale =</span> <span class="cn">FALSE</span>)</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">plot</span>(x1_ns, x2_ns, <span class="at">col =</span> <span class="fu">c</span>(<span class="st">"red"</span>, <span class="st">"blue"</span>)[<span class="fu">as.numeric</span>(y)], <span class="at">pch =</span> <span class="dv">19</span>,</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>       <span class="at">main =</span> <span class="fu">paste</span>(<span class="st">"C ="</span>, C_val))</span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Decision boundary</span></span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>  w <span class="ot">&lt;-</span> <span class="fu">t</span>(svm_c<span class="sc">$</span>coefs) <span class="sc">%*%</span> svm_c<span class="sc">$</span>SV</span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a>  b <span class="ot">&lt;-</span> <span class="sc">-</span>svm_c<span class="sc">$</span>rho</span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a>  <span class="fu">abline</span>(<span class="sc">-</span>b<span class="sc">/</span>w[<span class="dv">2</span>], <span class="sc">-</span>w[<span class="dv">1</span>]<span class="sc">/</span>w[<span class="dv">2</span>], <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Support vectors</span></span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a>  <span class="fu">points</span>(svm_c<span class="sc">$</span>SV, <span class="at">col =</span> <span class="st">"black"</span>, <span class="at">cex =</span> <span class="fl">1.5</span>, <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a>  <span class="fu">text</span>(<span class="dv">0</span>, <span class="dv">2</span>, <span class="fu">paste</span>(<span class="fu">nrow</span>(svm_c<span class="sc">$</span>SV), <span class="st">"SVs"</span>), <span class="at">cex =</span> <span class="fl">0.8</span>)</span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div id="fig-svm-cost" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-svm-cost-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="34-svm_files/figure-html/fig-svm-cost-1.png" class="img-fluid figure-img" width="864">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-svm-cost-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;35.4: Effect of cost parameter C on the support vector classifier. Larger C allows fewer violations but may overfit.
</figcaption>
</figure>
</div>
</div>
</div>
<p>As C decreases, more support vectors are used and the margin widens. This is another manifestation of the bias-variance tradeoff.</p>
</section>
</section>
<section id="the-kernel-trick-non-linear-svms" class="level2" data-number="35.4">
<h2 data-number="35.4" class="anchored" data-anchor-id="the-kernel-trick-non-linear-svms"><span class="header-section-number">35.4</span> The Kernel Trick: Non-Linear SVMs</h2>
<p>Linear boundaries are often insufficient. The <strong>kernel trick</strong> allows SVMs to learn non-linear decision boundaries by implicitly mapping data to higher-dimensional space.</p>
<section id="why-kernels" class="level3">
<h3 class="anchored" data-anchor-id="why-kernels">Why Kernels?</h3>
<p>Consider data that’s not linearly separable:</p>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate data requiring non-linear boundary</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">42</span>)</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">200</span></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>r1 <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(n<span class="sc">/</span><span class="dv">2</span>, <span class="dv">1</span>, <span class="fl">0.3</span>)</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>r2 <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(n<span class="sc">/</span><span class="dv">2</span>, <span class="fl">2.5</span>, <span class="fl">0.3</span>)</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>theta <span class="ot">&lt;-</span> <span class="fu">runif</span>(n, <span class="dv">0</span>, <span class="dv">2</span><span class="sc">*</span>pi)</span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>x1_nl <span class="ot">&lt;-</span> <span class="fu">c</span>(r1 <span class="sc">*</span> <span class="fu">cos</span>(theta[<span class="dv">1</span><span class="sc">:</span>(n<span class="sc">/</span><span class="dv">2</span>)]), r2 <span class="sc">*</span> <span class="fu">cos</span>(theta[(n<span class="sc">/</span><span class="dv">2</span><span class="sc">+</span><span class="dv">1</span>)<span class="sc">:</span>n]))</span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>x2_nl <span class="ot">&lt;-</span> <span class="fu">c</span>(r1 <span class="sc">*</span> <span class="fu">sin</span>(theta[<span class="dv">1</span><span class="sc">:</span>(n<span class="sc">/</span><span class="dv">2</span>)]), r2 <span class="sc">*</span> <span class="fu">sin</span>(theta[(n<span class="sc">/</span><span class="dv">2</span><span class="sc">+</span><span class="dv">1</span>)<span class="sc">:</span>n]))</span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>y_nl <span class="ot">&lt;-</span> <span class="fu">factor</span>(<span class="fu">c</span>(<span class="fu">rep</span>(<span class="sc">-</span><span class="dv">1</span>, n<span class="sc">/</span><span class="dv">2</span>), <span class="fu">rep</span>(<span class="dv">1</span>, n<span class="sc">/</span><span class="dv">2</span>)))</span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a>data_nl <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">x1 =</span> x1_nl, <span class="at">x2 =</span> x2_nl, <span class="at">y =</span> y_nl)</span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(x1_nl, x2_nl, <span class="at">col =</span> <span class="fu">c</span>(<span class="st">"red"</span>, <span class="st">"blue"</span>)[<span class="fu">as.numeric</span>(y_nl)], <span class="at">pch =</span> <span class="dv">19</span>,</span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a>     <span class="at">main =</span> <span class="st">"Non-Linear Decision Boundary Needed"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div id="fig-nonlinear-data" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-nonlinear-data-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="34-svm_files/figure-html/fig-nonlinear-data-1.png" class="img-fluid figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-nonlinear-data-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;35.5: Data requiring a non-linear decision boundary. No straight line can separate these classes.
</figcaption>
</figure>
</div>
</div>
</div>
<p>A linear SVM fails on this data. But if we map to a higher-dimensional space using features like <span class="math inline">\(x_1^2\)</span>, <span class="math inline">\(x_2^2\)</span>, <span class="math inline">\(x_1 x_2\)</span>, the classes might become linearly separable.</p>
</section>
<section id="common-kernels" class="level3">
<h3 class="anchored" data-anchor-id="common-kernels">Common Kernels</h3>
<p>The <strong>Linear Kernel</strong>, <span class="math inline">\(K(x_i, x_j) = x_i^T x_j\)</span>, is equivalent to the standard support vector classifier with no transformation—it finds linear decision boundaries in the original feature space.</p>
<p>The <strong>Polynomial Kernel</strong>, <span class="math inline">\(K(x_i, x_j) = (1 + x_i^T x_j)^d\)</span>, creates polynomial decision boundaries of degree <span class="math inline">\(d\)</span>. This allows curved boundaries while still having a clear interpretation in terms of polynomial features.</p>
<p>The <strong>Radial Basis Function (RBF) Kernel</strong>, also called the Gaussian kernel, is defined as <span class="math inline">\(K(x_i, x_j) = \exp(-\gamma ||x_i - x_j||^2)\)</span>. It creates flexible, localized decision boundaries and is the most commonly used kernel for non-linear problems. The parameter <span class="math inline">\(\gamma\)</span> controls the influence radius of each training point—larger values of <span class="math inline">\(\gamma\)</span> mean that each point influences only its immediate neighborhood, while smaller values create smoother boundaries where each point influences a wider region.</p>
<p>The <strong>Sigmoid Kernel</strong>, <span class="math inline">\(K(x_i, x_j) = \tanh(\gamma x_i^T x_j + r)\)</span>, mimics neural network activation functions and is occasionally used when you want to relate SVMs to neural networks. However, it is not a valid kernel for all parameter values and is less commonly used than the others.</p>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create grid for visualization</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>x1_seq <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="sc">-</span><span class="dv">4</span>, <span class="dv">4</span>, <span class="at">length.out =</span> <span class="dv">100</span>)</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>x2_seq <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="sc">-</span><span class="dv">4</span>, <span class="dv">4</span>, <span class="at">length.out =</span> <span class="dv">100</span>)</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>grid <span class="ot">&lt;-</span> <span class="fu">expand.grid</span>(<span class="at">x1 =</span> x1_seq, <span class="at">x2 =</span> x2_seq)</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">3</span>))</span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Linear kernel</span></span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>svm_linear <span class="ot">&lt;-</span> <span class="fu">svm</span>(y <span class="sc">~</span> x1 <span class="sc">+</span> x2, <span class="at">data =</span> data_nl, <span class="at">kernel =</span> <span class="st">"linear"</span>)</span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>grid<span class="sc">$</span>pred <span class="ot">&lt;-</span> <span class="fu">predict</span>(svm_linear, grid)</span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(grid<span class="sc">$</span>x1, grid<span class="sc">$</span>x2,</span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a>     <span class="at">col =</span> <span class="fu">c</span>(<span class="fu">rgb</span>(<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="fl">0.1</span>), <span class="fu">rgb</span>(<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="fl">0.1</span>))[<span class="fu">as.numeric</span>(grid<span class="sc">$</span>pred)],</span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a>     <span class="at">pch =</span> <span class="dv">15</span>, <span class="at">cex =</span> <span class="fl">0.3</span>, <span class="at">main =</span> <span class="st">"Linear Kernel"</span>)</span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span>(x1_nl, x2_nl, <span class="at">col =</span> <span class="fu">c</span>(<span class="st">"red"</span>, <span class="st">"blue"</span>)[<span class="fu">as.numeric</span>(y_nl)], <span class="at">pch =</span> <span class="dv">19</span>)</span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Polynomial kernel</span></span>
<span id="cb7-17"><a href="#cb7-17" aria-hidden="true" tabindex="-1"></a>svm_poly <span class="ot">&lt;-</span> <span class="fu">svm</span>(y <span class="sc">~</span> x1 <span class="sc">+</span> x2, <span class="at">data =</span> data_nl, <span class="at">kernel =</span> <span class="st">"polynomial"</span>, <span class="at">degree =</span> <span class="dv">2</span>)</span>
<span id="cb7-18"><a href="#cb7-18" aria-hidden="true" tabindex="-1"></a>grid<span class="sc">$</span>pred <span class="ot">&lt;-</span> <span class="fu">predict</span>(svm_poly, grid)</span>
<span id="cb7-19"><a href="#cb7-19" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(grid<span class="sc">$</span>x1, grid<span class="sc">$</span>x2,</span>
<span id="cb7-20"><a href="#cb7-20" aria-hidden="true" tabindex="-1"></a>     <span class="at">col =</span> <span class="fu">c</span>(<span class="fu">rgb</span>(<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="fl">0.1</span>), <span class="fu">rgb</span>(<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="fl">0.1</span>))[<span class="fu">as.numeric</span>(grid<span class="sc">$</span>pred)],</span>
<span id="cb7-21"><a href="#cb7-21" aria-hidden="true" tabindex="-1"></a>     <span class="at">pch =</span> <span class="dv">15</span>, <span class="at">cex =</span> <span class="fl">0.3</span>, <span class="at">main =</span> <span class="st">"Polynomial Kernel (d=2)"</span>)</span>
<span id="cb7-22"><a href="#cb7-22" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span>(x1_nl, x2_nl, <span class="at">col =</span> <span class="fu">c</span>(<span class="st">"red"</span>, <span class="st">"blue"</span>)[<span class="fu">as.numeric</span>(y_nl)], <span class="at">pch =</span> <span class="dv">19</span>)</span>
<span id="cb7-23"><a href="#cb7-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-24"><a href="#cb7-24" aria-hidden="true" tabindex="-1"></a><span class="co"># RBF kernel</span></span>
<span id="cb7-25"><a href="#cb7-25" aria-hidden="true" tabindex="-1"></a>svm_rbf <span class="ot">&lt;-</span> <span class="fu">svm</span>(y <span class="sc">~</span> x1 <span class="sc">+</span> x2, <span class="at">data =</span> data_nl, <span class="at">kernel =</span> <span class="st">"radial"</span>, <span class="at">gamma =</span> <span class="fl">0.5</span>)</span>
<span id="cb7-26"><a href="#cb7-26" aria-hidden="true" tabindex="-1"></a>grid<span class="sc">$</span>pred <span class="ot">&lt;-</span> <span class="fu">predict</span>(svm_rbf, grid)</span>
<span id="cb7-27"><a href="#cb7-27" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(grid<span class="sc">$</span>x1, grid<span class="sc">$</span>x2,</span>
<span id="cb7-28"><a href="#cb7-28" aria-hidden="true" tabindex="-1"></a>     <span class="at">col =</span> <span class="fu">c</span>(<span class="fu">rgb</span>(<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="fl">0.1</span>), <span class="fu">rgb</span>(<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="fl">0.1</span>))[<span class="fu">as.numeric</span>(grid<span class="sc">$</span>pred)],</span>
<span id="cb7-29"><a href="#cb7-29" aria-hidden="true" tabindex="-1"></a>     <span class="at">pch =</span> <span class="dv">15</span>, <span class="at">cex =</span> <span class="fl">0.3</span>, <span class="at">main =</span> <span class="st">"RBF Kernel"</span>)</span>
<span id="cb7-30"><a href="#cb7-30" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span>(x1_nl, x2_nl, <span class="at">col =</span> <span class="fu">c</span>(<span class="st">"red"</span>, <span class="st">"blue"</span>)[<span class="fu">as.numeric</span>(y_nl)], <span class="at">pch =</span> <span class="dv">19</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div id="fig-kernel-comparison" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-kernel-comparison-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="34-svm_files/figure-html/fig-kernel-comparison-1.png" class="img-fluid figure-img" width="864">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-kernel-comparison-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;35.6: Comparison of different SVM kernels on non-linear data
</figcaption>
</figure>
</div>
</div>
</div>
<p>The RBF kernel successfully captures the circular decision boundary.</p>
</section>
<section id="tuning-the-rbf-kernel" class="level3">
<h3 class="anchored" data-anchor-id="tuning-the-rbf-kernel">Tuning the RBF Kernel</h3>
<p>The RBF kernel has two parameters that must be tuned together. The <strong>cost parameter C</strong> controls margin violations as before—how much the algorithm penalizes misclassifications. The <strong>gamma parameter</strong> (<span class="math inline">\(\gamma\)</span>) controls the kernel width, determining how far the influence of a single training example reaches.</p>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">3</span>))</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (gamma_val <span class="cf">in</span> <span class="fu">c</span>(<span class="fl">0.1</span>, <span class="dv">1</span>, <span class="dv">10</span>)) {</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>  svm_g <span class="ot">&lt;-</span> <span class="fu">svm</span>(y <span class="sc">~</span> x1 <span class="sc">+</span> x2, <span class="at">data =</span> data_nl, <span class="at">kernel =</span> <span class="st">"radial"</span>,</span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>               <span class="at">gamma =</span> gamma_val, <span class="at">cost =</span> <span class="dv">1</span>)</span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a>  grid<span class="sc">$</span>pred <span class="ot">&lt;-</span> <span class="fu">predict</span>(svm_g, grid)</span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">plot</span>(grid<span class="sc">$</span>x1, grid<span class="sc">$</span>x2,</span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a>       <span class="at">col =</span> <span class="fu">c</span>(<span class="fu">rgb</span>(<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="fl">0.1</span>), <span class="fu">rgb</span>(<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="fl">0.1</span>))[<span class="fu">as.numeric</span>(grid<span class="sc">$</span>pred)],</span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a>       <span class="at">pch =</span> <span class="dv">15</span>, <span class="at">cex =</span> <span class="fl">0.3</span>, <span class="at">main =</span> <span class="fu">paste</span>(<span class="st">"gamma ="</span>, gamma_val))</span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">points</span>(x1_nl, x2_nl, <span class="at">col =</span> <span class="fu">c</span>(<span class="st">"red"</span>, <span class="st">"blue"</span>)[<span class="fu">as.numeric</span>(y_nl)], <span class="at">pch =</span> <span class="dv">19</span>)</span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div id="fig-gamma-effect" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-gamma-effect-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="34-svm_files/figure-html/fig-gamma-effect-1.png" class="img-fluid figure-img" width="864">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-gamma-effect-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;35.7: Effect of gamma on RBF SVM: small gamma gives smoother boundaries, large gamma fits training data more closely
</figcaption>
</figure>
</div>
</div>
</div>
<p>With <strong>small gamma</strong>, each training point has a wide influence, creating smooth decision boundaries. This corresponds to high bias but low variance—the model may be too simple but is stable across different training samples. With <strong>large gamma</strong>, each training point only affects its immediate neighborhood, allowing complex, highly curved boundaries. This corresponds to low bias but high variance—the model can capture intricate patterns but may overfit to the specific training sample.</p>
</section>
<section id="cross-validation-for-parameter-selection" class="level3">
<h3 class="anchored" data-anchor-id="cross-validation-for-parameter-selection">Cross-Validation for Parameter Selection</h3>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Grid search with cross-validation</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">42</span>)</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>tune_result <span class="ot">&lt;-</span> <span class="fu">tune</span>(svm, y <span class="sc">~</span> x1 <span class="sc">+</span> x2, <span class="at">data =</span> data_nl,</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>                     <span class="at">kernel =</span> <span class="st">"radial"</span>,</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>                     <span class="at">ranges =</span> <span class="fu">list</span>(</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>                       <span class="at">cost =</span> <span class="fu">c</span>(<span class="fl">0.1</span>, <span class="dv">1</span>, <span class="dv">10</span>, <span class="dv">100</span>),</span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a>                       <span class="at">gamma =</span> <span class="fu">c</span>(<span class="fl">0.1</span>, <span class="fl">0.5</span>, <span class="dv">1</span>, <span class="dv">2</span>)</span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a>                     ))</span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Best parameters</span></span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(tune_result<span class="sc">$</span>best.parameters)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>  cost gamma
2    1   0.1</code></pre>
</div>
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"</span><span class="sc">\n</span><span class="st">Best cross-validation error:"</span>, <span class="fu">round</span>(tune_result<span class="sc">$</span>best.performance, <span class="dv">4</span>), <span class="st">"</span><span class="sc">\n</span><span class="st">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>
Best cross-validation error: 0 </code></pre>
</div>
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Summary of all models</span></span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(tune_result)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>
Parameter tuning of 'svm':

- sampling method: 10-fold cross validation 

- best parameters:
 cost gamma
    1   0.1

- best performance: 0 

- Detailed performance results:
    cost gamma error dispersion
1    0.1   0.1 0.345 0.15890249
2    1.0   0.1 0.000 0.00000000
3   10.0   0.1 0.000 0.00000000
4  100.0   0.1 0.000 0.00000000
5    0.1   0.5 0.005 0.01581139
6    1.0   0.5 0.000 0.00000000
7   10.0   0.5 0.000 0.00000000
8  100.0   0.5 0.000 0.00000000
9    0.1   1.0 0.005 0.01581139
10   1.0   1.0 0.000 0.00000000
11  10.0   1.0 0.000 0.00000000
12 100.0   1.0 0.000 0.00000000
13   0.1   2.0 0.010 0.02108185
14   1.0   2.0 0.000 0.00000000
15  10.0   2.0 0.000 0.00000000
16 100.0   2.0 0.000 0.00000000</code></pre>
</div>
</div>
</section>
</section>
<section id="multi-class-svms" class="level2" data-number="35.5">
<h2 data-number="35.5" class="anchored" data-anchor-id="multi-class-svms"><span class="header-section-number">35.5</span> Multi-Class SVMs</h2>
<p>SVMs are naturally binary classifiers. For multi-class problems, two strategies are common:</p>
<p><strong>One-vs-One (OvO)</strong>: Train <span class="math inline">\(\binom{K}{2}\)</span> classifiers for each pair of classes. Predict by majority voting.</p>
<p><strong>One-vs-All (OvA)</strong>: Train <span class="math inline">\(K\)</span> classifiers, each separating one class from all others. Predict based on highest confidence.</p>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Multi-class SVM on iris</span></span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a><span class="fu">data</span>(iris)</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a>svm_iris <span class="ot">&lt;-</span> <span class="fu">svm</span>(Species <span class="sc">~</span> Petal.Length <span class="sc">+</span> Petal.Width, <span class="at">data =</span> iris,</span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a>                 <span class="at">kernel =</span> <span class="st">"radial"</span>, <span class="at">gamma =</span> <span class="fl">0.5</span>, <span class="at">cost =</span> <span class="dv">1</span>)</span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Create grid</span></span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a>pl_seq <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="dv">0</span>, <span class="dv">7</span>, <span class="at">length.out =</span> <span class="dv">100</span>)</span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a>pw_seq <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="dv">0</span>, <span class="dv">3</span>, <span class="at">length.out =</span> <span class="dv">100</span>)</span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true" tabindex="-1"></a>grid_iris <span class="ot">&lt;-</span> <span class="fu">expand.grid</span>(<span class="at">Petal.Length =</span> pl_seq, <span class="at">Petal.Width =</span> pw_seq)</span>
<span id="cb15-10"><a href="#cb15-10" aria-hidden="true" tabindex="-1"></a>grid_iris<span class="sc">$</span>pred <span class="ot">&lt;-</span> <span class="fu">predict</span>(svm_iris, grid_iris)</span>
<span id="cb15-11"><a href="#cb15-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-12"><a href="#cb15-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot</span></span>
<span id="cb15-13"><a href="#cb15-13" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(grid_iris<span class="sc">$</span>Petal.Length, grid_iris<span class="sc">$</span>Petal.Width,</span>
<span id="cb15-14"><a href="#cb15-14" aria-hidden="true" tabindex="-1"></a>     <span class="at">col =</span> <span class="fu">c</span>(<span class="fu">rgb</span>(<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="fl">0.1</span>), <span class="fu">rgb</span>(<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="fl">0.1</span>), <span class="fu">rgb</span>(<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="fl">0.1</span>))[grid_iris<span class="sc">$</span>pred],</span>
<span id="cb15-15"><a href="#cb15-15" aria-hidden="true" tabindex="-1"></a>     <span class="at">pch =</span> <span class="dv">15</span>, <span class="at">cex =</span> <span class="fl">0.3</span>,</span>
<span id="cb15-16"><a href="#cb15-16" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">"Petal Length"</span>, <span class="at">ylab =</span> <span class="st">"Petal Width"</span>,</span>
<span id="cb15-17"><a href="#cb15-17" aria-hidden="true" tabindex="-1"></a>     <span class="at">main =</span> <span class="st">"Multi-Class SVM on Iris Data"</span>)</span>
<span id="cb15-18"><a href="#cb15-18" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span>(iris<span class="sc">$</span>Petal.Length, iris<span class="sc">$</span>Petal.Width,</span>
<span id="cb15-19"><a href="#cb15-19" aria-hidden="true" tabindex="-1"></a>       <span class="at">col =</span> <span class="fu">c</span>(<span class="st">"red"</span>, <span class="st">"green"</span>, <span class="st">"blue"</span>)[iris<span class="sc">$</span>Species], <span class="at">pch =</span> <span class="dv">19</span>)</span>
<span id="cb15-20"><a href="#cb15-20" aria-hidden="true" tabindex="-1"></a><span class="fu">legend</span>(<span class="st">"topleft"</span>, <span class="fu">levels</span>(iris<span class="sc">$</span>Species),</span>
<span id="cb15-21"><a href="#cb15-21" aria-hidden="true" tabindex="-1"></a>       <span class="at">col =</span> <span class="fu">c</span>(<span class="st">"red"</span>, <span class="st">"green"</span>, <span class="st">"blue"</span>), <span class="at">pch =</span> <span class="dv">19</span>)</span>
<span id="cb15-22"><a href="#cb15-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-23"><a href="#cb15-23" aria-hidden="true" tabindex="-1"></a><span class="co"># Confusion matrix</span></span>
<span id="cb15-24"><a href="#cb15-24" aria-hidden="true" tabindex="-1"></a>pred_iris <span class="ot">&lt;-</span> <span class="fu">predict</span>(svm_iris, iris)</span>
<span id="cb15-25"><a href="#cb15-25" aria-hidden="true" tabindex="-1"></a><span class="fu">table</span>(<span class="at">Predicted =</span> pred_iris, <span class="at">Actual =</span> iris<span class="sc">$</span>Species)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>            Actual
Predicted    setosa versicolor virginica
  setosa         50          0         0
  versicolor      0         47         3
  virginica       0          3        47</code></pre>
</div>
<div class="cell-output-display">
<div id="fig-multiclass-svm" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-multiclass-svm-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="34-svm_files/figure-html/fig-multiclass-svm-1.png" class="img-fluid figure-img" width="768">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-multiclass-svm-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;35.8: Multi-class SVM classification on the iris dataset
</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="support-vector-regression-svr" class="level2" data-number="35.6">
<h2 data-number="35.6" class="anchored" data-anchor-id="support-vector-regression-svr"><span class="header-section-number">35.6</span> Support Vector Regression (SVR)</h2>
<p>SVMs can be extended to regression problems. <strong>Support Vector Regression (SVR)</strong> fits a tube of width <span class="math inline">\(\epsilon\)</span> around the data, ignoring errors within the tube:</p>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate regression data</span></span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">42</span>)</span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">100</span></span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a>x_reg <span class="ot">&lt;-</span> <span class="fu">sort</span>(<span class="fu">runif</span>(n, <span class="dv">0</span>, <span class="dv">10</span>))</span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a>y_reg <span class="ot">&lt;-</span> <span class="fu">sin</span>(x_reg) <span class="sc">+</span> <span class="fu">rnorm</span>(n, <span class="at">sd =</span> <span class="fl">0.3</span>)</span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a>data_reg <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">x =</span> x_reg, <span class="at">y =</span> y_reg)</span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit SVR</span></span>
<span id="cb17-9"><a href="#cb17-9" aria-hidden="true" tabindex="-1"></a>svr_fit <span class="ot">&lt;-</span> <span class="fu">svm</span>(y <span class="sc">~</span> x, <span class="at">data =</span> data_reg, <span class="at">type =</span> <span class="st">"eps-regression"</span>,</span>
<span id="cb17-10"><a href="#cb17-10" aria-hidden="true" tabindex="-1"></a>                <span class="at">kernel =</span> <span class="st">"radial"</span>, <span class="at">gamma =</span> <span class="fl">0.5</span>, <span class="at">epsilon =</span> <span class="fl">0.3</span>)</span>
<span id="cb17-11"><a href="#cb17-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-12"><a href="#cb17-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Predictions</span></span>
<span id="cb17-13"><a href="#cb17-13" aria-hidden="true" tabindex="-1"></a>x_grid <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="dv">0</span>, <span class="dv">10</span>, <span class="at">length.out =</span> <span class="dv">200</span>)</span>
<span id="cb17-14"><a href="#cb17-14" aria-hidden="true" tabindex="-1"></a>y_pred <span class="ot">&lt;-</span> <span class="fu">predict</span>(svr_fit, <span class="at">newdata =</span> <span class="fu">data.frame</span>(<span class="at">x =</span> x_grid))</span>
<span id="cb17-15"><a href="#cb17-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-16"><a href="#cb17-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot</span></span>
<span id="cb17-17"><a href="#cb17-17" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(x_reg, y_reg, <span class="at">pch =</span> <span class="dv">16</span>, <span class="at">col =</span> <span class="st">"gray50"</span>,</span>
<span id="cb17-18"><a href="#cb17-18" aria-hidden="true" tabindex="-1"></a>     <span class="at">main =</span> <span class="st">"Support Vector Regression"</span>)</span>
<span id="cb17-19"><a href="#cb17-19" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(x_grid, y_pred, <span class="at">col =</span> <span class="st">"blue"</span>, <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb17-20"><a href="#cb17-20" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(x_grid, y_pred <span class="sc">+</span> <span class="fl">0.3</span>, <span class="at">col =</span> <span class="st">"blue"</span>, <span class="at">lty =</span> <span class="dv">2</span>)  <span class="co"># Epsilon tube</span></span>
<span id="cb17-21"><a href="#cb17-21" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(x_grid, y_pred <span class="sc">-</span> <span class="fl">0.3</span>, <span class="at">col =</span> <span class="st">"blue"</span>, <span class="at">lty =</span> <span class="dv">2</span>)</span>
<span id="cb17-22"><a href="#cb17-22" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(x_grid, <span class="fu">sin</span>(x_grid), <span class="at">col =</span> <span class="st">"red"</span>, <span class="at">lwd =</span> <span class="dv">2</span>, <span class="at">lty =</span> <span class="dv">2</span>)</span>
<span id="cb17-23"><a href="#cb17-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-24"><a href="#cb17-24" aria-hidden="true" tabindex="-1"></a><span class="co"># Highlight support vectors</span></span>
<span id="cb17-25"><a href="#cb17-25" aria-hidden="true" tabindex="-1"></a>sv_idx <span class="ot">&lt;-</span> svr_fit<span class="sc">$</span>index</span>
<span id="cb17-26"><a href="#cb17-26" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span>(x_reg[sv_idx], y_reg[sv_idx], <span class="at">col =</span> <span class="st">"black"</span>, <span class="at">cex =</span> <span class="fl">1.5</span>, <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb17-27"><a href="#cb17-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-28"><a href="#cb17-28" aria-hidden="true" tabindex="-1"></a><span class="fu">legend</span>(<span class="st">"topright"</span>, <span class="fu">c</span>(<span class="st">"Data"</span>, <span class="st">"SVR fit"</span>, <span class="st">"Epsilon tube"</span>, <span class="st">"True function"</span>, <span class="st">"Support vectors"</span>),</span>
<span id="cb17-29"><a href="#cb17-29" aria-hidden="true" tabindex="-1"></a>       <span class="at">col =</span> <span class="fu">c</span>(<span class="st">"gray50"</span>, <span class="st">"blue"</span>, <span class="st">"blue"</span>, <span class="st">"red"</span>, <span class="st">"black"</span>),</span>
<span id="cb17-30"><a href="#cb17-30" aria-hidden="true" tabindex="-1"></a>       <span class="at">pch =</span> <span class="fu">c</span>(<span class="dv">16</span>, <span class="cn">NA</span>, <span class="cn">NA</span>, <span class="cn">NA</span>, <span class="dv">1</span>), <span class="at">lty =</span> <span class="fu">c</span>(<span class="cn">NA</span>, <span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">2</span>, <span class="cn">NA</span>), <span class="at">lwd =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">2</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div id="fig-svr" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-svr-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="34-svm_files/figure-html/fig-svr-1.png" class="img-fluid figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-svr-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;35.9: Support Vector Regression fits a tube around the data. Only points outside the tube affect the model.
</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="practical-considerations" class="level2" data-number="35.7">
<h2 data-number="35.7" class="anchored" data-anchor-id="practical-considerations"><span class="header-section-number">35.7</span> Practical Considerations</h2>
<section id="feature-scaling" class="level3">
<h3 class="anchored" data-anchor-id="feature-scaling">Feature Scaling</h3>
<p>SVMs are sensitive to feature scales. Always standardize features before training:</p>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="co"># With scaling (default)</span></span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>svm_scaled <span class="ot">&lt;-</span> <span class="fu">svm</span>(Species <span class="sc">~</span> ., <span class="at">data =</span> iris, <span class="at">scale =</span> <span class="cn">TRUE</span>)</span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Without scaling (not recommended)</span></span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a><span class="co"># svm_unscaled &lt;- svm(Species ~ ., data = iris, scale = FALSE)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="probability-estimates" class="level3">
<h3 class="anchored" data-anchor-id="probability-estimates">Probability Estimates</h3>
<p>By default, SVMs output only class predictions. However, you can obtain probability estimates using <strong>Platt scaling</strong>, which fits a sigmoid function to the decision values:</p>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Train SVM with probability estimates enabled</span></span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>svm_prob <span class="ot">&lt;-</span> <span class="fu">svm</span>(Species <span class="sc">~</span> ., <span class="at">data =</span> iris,</span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a>                <span class="at">kernel =</span> <span class="st">"radial"</span>,</span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a>                <span class="at">probability =</span> <span class="cn">TRUE</span>)</span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Predict with probabilities</span></span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a>pred_prob <span class="ot">&lt;-</span> <span class="fu">predict</span>(svm_prob, iris, <span class="at">probability =</span> <span class="cn">TRUE</span>)</span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-9"><a href="#cb19-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Extract probability matrix</span></span>
<span id="cb19-10"><a href="#cb19-10" aria-hidden="true" tabindex="-1"></a>probs <span class="ot">&lt;-</span> <span class="fu">attr</span>(pred_prob, <span class="st">"probabilities"</span>)</span>
<span id="cb19-11"><a href="#cb19-11" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(probs)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>     setosa versicolor   virginica
1 0.9808043 0.01091925 0.008276452
2 0.9736294 0.01750091 0.008869655
3 0.9795448 0.01152417 0.008931067
4 0.9756104 0.01480310 0.009586477
5 0.9799921 0.01123496 0.008772975
6 0.9746892 0.01620451 0.009106251</code></pre>
</div>
</div>
<p>Platt scaling works by fitting a logistic regression model to the SVM’s decision values after training. While this provides calibrated probabilities, it adds computational overhead and may be less reliable when training data is limited. For applications requiring well-calibrated probabilities (such as medical diagnosis where you need to communicate uncertainty), consider whether logistic regression or other natively probabilistic methods might be more appropriate.</p>
</section>
<section id="decision-values" class="level3">
<h3 class="anchored" data-anchor-id="decision-values">Decision Values</h3>
<p>For more detailed analysis, you can extract the raw <strong>decision values</strong>—the signed distances from the decision boundary:</p>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Get decision values</span></span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>pred_dv <span class="ot">&lt;-</span> <span class="fu">predict</span>(svm_prob, iris, <span class="at">decision.values =</span> <span class="cn">TRUE</span>)</span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a>decision_vals <span class="ot">&lt;-</span> <span class="fu">attr</span>(pred_dv, <span class="st">"decision.values"</span>)</span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(decision_vals)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>  setosa/versicolor setosa/virginica versicolor/virginica
1          1.196203         1.091460            0.6709454
2          1.064664         1.056332            0.8485954
3          1.180892         1.074534            0.6441745
4          1.110746         1.053143            0.6784462
5          1.185382         1.093964            0.5587731
6          1.081216         1.094082            0.5537525</code></pre>
</div>
</div>
<p>For a two-class problem, the decision value represents how far and on which side of the hyperplane an observation lies. Larger absolute values indicate higher confidence in the classification. For multi-class problems using one-vs-one classification, you get decision values for each pair of classes.</p>
</section>
<section id="choosing-the-kernel" class="level3">
<h3 class="anchored" data-anchor-id="choosing-the-kernel">Choosing the Kernel</h3>
</section>
<section id="kernel-selection-guidelines" class="level3">
<h3 class="anchored" data-anchor-id="kernel-selection-guidelines">Kernel Selection Guidelines</h3>
<p>The choice of kernel depends on your data and prior knowledge about the problem.</p>
<p><strong>Start with the RBF kernel</strong> as a sensible default—it handles most non-linear problems well and has the flexibility to approximate a wide range of decision boundaries. However, <strong>try a linear kernel first</strong> if you have many features relative to samples, as the data may already be linearly separable in the high-dimensional feature space without needing additional transformation. Linear kernels are also faster to train and easier to interpret.</p>
<p><strong>Polynomial kernels</strong> are useful when you have domain knowledge suggesting polynomial relationships—for instance, when interactions between features are expected to matter. <strong>Custom kernels</strong> allow you to encode domain-specific knowledge; string kernels for biological sequences, for example, measure similarity based on shared subsequences rather than Euclidean distance.</p>
</section>
<section id="advantages-and-limitations" class="level3">
<h3 class="anchored" data-anchor-id="advantages-and-limitations">Advantages and Limitations</h3>
<p>SVMs have several important advantages. They are effective in high-dimensional spaces, often working well even when the number of features exceeds the number of samples. They are memory efficient because the final model depends only on the support vectors, not all training observations. They are versatile through the choice of kernel, allowing them to learn complex non-linear boundaries. And the maximum margin principle provides a form of regularization that often leads to good generalization.</p>
<p>However, SVMs also have significant limitations. Training time scales as <span class="math inline">\(O(n^2)\)</span> to <span class="math inline">\(O(n^3)\)</span> with the number of observations, making them impractical for very large datasets. They are sensitive to feature scaling—variables must be standardized before training. They do not naturally provide probability estimates (though these can be obtained through additional computation). And selecting the appropriate kernel and tuning the parameters requires care, often necessitating extensive cross-validation.</p>
</section>
<section id="comparison-with-other-methods" class="level3">
<h3 class="anchored" data-anchor-id="comparison-with-other-methods">Comparison with Other Methods</h3>
<p>How do SVMs compare to other classification methods? Each approach has different strengths:</p>
<table class="caption-top table">
<colgroup>
<col style="width: 12%">
<col style="width: 7%">
<col style="width: 22%">
<col style="width: 31%">
<col style="width: 25%">
</colgroup>
<thead>
<tr class="header">
<th>Aspect</th>
<th>SVM</th>
<th>Random Forest</th>
<th>Logistic Regression</th>
<th>Neural Networks</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Interpretability</strong></td>
<td>Low (except linear)</td>
<td>Medium (variable importance)</td>
<td>High (coefficients)</td>
<td>Low</td>
</tr>
<tr class="even">
<td><strong>Probability estimates</strong></td>
<td>Requires Platt scaling</td>
<td>Natural (vote proportions)</td>
<td>Natural</td>
<td>Natural (softmax)</td>
</tr>
<tr class="odd">
<td><strong>High dimensions</strong></td>
<td>Excellent</td>
<td>Good</td>
<td>Good with regularization</td>
<td>Needs many samples</td>
</tr>
<tr class="even">
<td><strong>Non-linear boundaries</strong></td>
<td>Via kernels</td>
<td>Natural</td>
<td>Needs feature engineering</td>
<td>Natural</td>
</tr>
<tr class="odd">
<td><strong>Training speed</strong></td>
<td>Slow for large n</td>
<td>Fast (parallelizable)</td>
<td>Very fast</td>
<td>Slow</td>
</tr>
<tr class="even">
<td><strong>Hyperparameters</strong></td>
<td>C, kernel params</td>
<td>Trees, depth, features</td>
<td>Regularization</td>
<td>Many (architecture)</td>
</tr>
<tr class="odd">
<td><strong>Robustness to noise</strong></td>
<td>Moderate</td>
<td>Excellent</td>
<td>Moderate</td>
<td>Sensitive</td>
</tr>
</tbody>
</table>
<p>Choose <strong>SVMs</strong> when you have limited samples, many features, and need good generalization. Choose <strong>Random Forests</strong> when you need variable importance, robustness, or have many observations. Choose <strong>Logistic Regression</strong> when interpretability is paramount and relationships are approximately linear. Choose <strong>Neural Networks</strong> when you have massive data and complex patterns to capture.</p>
</section>
<section id="feature-importance" class="level3">
<h3 class="anchored" data-anchor-id="feature-importance">Feature Importance</h3>
<p>Unlike random forests, SVMs don’t provide built-in variable importance measures. However, several approaches can help interpret which features matter:</p>
<p>For <strong>linear SVMs</strong>, the magnitude of the weight coefficients indicates feature importance:</p>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit linear SVM</span></span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a>svm_linear_iris <span class="ot">&lt;-</span> <span class="fu">svm</span>(Species <span class="sc">~</span> ., <span class="at">data =</span> iris, <span class="at">kernel =</span> <span class="st">"linear"</span>, <span class="at">scale =</span> <span class="cn">TRUE</span>)</span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Extract weights (for binary classification)</span></span>
<span id="cb23-5"><a href="#cb23-5" aria-hidden="true" tabindex="-1"></a><span class="co"># For multi-class, examine each one-vs-one classifier</span></span>
<span id="cb23-6"><a href="#cb23-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Here we use a simpler 2-class example</span></span>
<span id="cb23-7"><a href="#cb23-7" aria-hidden="true" tabindex="-1"></a>iris_binary <span class="ot">&lt;-</span> iris[iris<span class="sc">$</span>Species <span class="sc">!=</span> <span class="st">"setosa"</span>,]</span>
<span id="cb23-8"><a href="#cb23-8" aria-hidden="true" tabindex="-1"></a>iris_binary<span class="sc">$</span>Species <span class="ot">&lt;-</span> <span class="fu">droplevels</span>(iris_binary<span class="sc">$</span>Species)</span>
<span id="cb23-9"><a href="#cb23-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-10"><a href="#cb23-10" aria-hidden="true" tabindex="-1"></a>svm_binary <span class="ot">&lt;-</span> <span class="fu">svm</span>(Species <span class="sc">~</span> ., <span class="at">data =</span> iris_binary, <span class="at">kernel =</span> <span class="st">"linear"</span>, <span class="at">scale =</span> <span class="cn">TRUE</span>)</span>
<span id="cb23-11"><a href="#cb23-11" aria-hidden="true" tabindex="-1"></a>w <span class="ot">&lt;-</span> <span class="fu">t</span>(svm_binary<span class="sc">$</span>coefs) <span class="sc">%*%</span> svm_binary<span class="sc">$</span>SV</span>
<span id="cb23-12"><a href="#cb23-12" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"Feature weights (absolute values indicate importance):</span><span class="sc">\n</span><span class="st">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Feature weights (absolute values indicate importance):</code></pre>
</div>
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(<span class="fu">abs</span>(w))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>     Sepal.Length Sepal.Width Petal.Length Petal.Width
[1,]    0.3735423   0.5227155     1.599519    1.626847</code></pre>
</div>
</div>
<p>For <strong>non-linear SVMs</strong>, permutation importance provides a model-agnostic approach—shuffle each feature and measure the decrease in accuracy. Alternatively, you can examine which features have the most support vectors in high-density regions.</p>
</section>
<section id="scaling-for-large-datasets" class="level3">
<h3 class="anchored" data-anchor-id="scaling-for-large-datasets">Scaling for Large Datasets</h3>
<p>Standard SVM implementations struggle with large datasets due to their <span class="math inline">\(O(n^2)\)</span> to <span class="math inline">\(O(n^3)\)</span> training complexity. For datasets with many observations, consider these alternatives.</p>
<p>The <strong>LiblineaR</strong> package implements linear SVMs that scale to millions of observations. It uses a coordinate descent algorithm that is much faster than the standard quadratic programming approach:</p>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(LiblineaR)</span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Much faster for large datasets with linear kernel</span></span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a>model <span class="ot">&lt;-</span> <span class="fu">LiblineaR</span>(<span class="at">data =</span> X_train, <span class="at">target =</span> y_train, <span class="at">type =</span> <span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p><strong>Stochastic gradient descent</strong> approaches approximate the SVM solution by processing data in batches, making them suitable for online learning and very large datasets.</p>
<p>For kernel SVMs on large data, <strong>random Fourier features</strong> or <strong>Nyström approximation</strong> can approximate the kernel mapping, allowing linear methods to be applied to the transformed features.</p>
</section>
</section>
<section id="exercises" class="level2" data-number="35.8">
<h2 data-number="35.8" class="anchored" data-anchor-id="exercises"><span class="header-section-number">35.8</span> Exercises</h2>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Exercise SVM.1: Linear SVM
</div>
</div>
<div class="callout-body-container callout-body">
<ol type="1">
<li><p>Generate linearly separable data in two dimensions. Fit an SVM with a linear kernel.</p></li>
<li><p>Identify the support vectors. How many are there?</p></li>
<li><p>Add noise to make the classes overlap. How does the number of support vectors change with different values of C?</p></li>
</ol>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Exercise SVM.2: Non-Linear Classification
</div>
</div>
<div class="callout-body-container callout-body">
<ol start="4" type="1">
<li><p>Generate data that requires a non-linear decision boundary (e.g., circles or moons).</p></li>
<li><p>Fit SVMs with polynomial and RBF kernels. Compare performance.</p></li>
<li><p>Use cross-validation to select optimal parameters (C, gamma or degree).</p></li>
</ol>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Exercise SVM.3: Real Data
</div>
</div>
<div class="callout-body-container callout-body">
<ol start="7" type="1">
<li><p>Load the <code>tissue_gene_expression</code> dataset. Use SVM to classify tissue types.</p></li>
<li><p>Compare linear and RBF kernels. Which performs better?</p></li>
<li><p>How does SVM compare to random forests on this data?</p></li>
</ol>
</div>
</div>
</section>
<section id="summary" class="level2" data-number="35.9">
<h2 data-number="35.9" class="anchored" data-anchor-id="summary"><span class="header-section-number">35.9</span> Summary</h2>
<p>This chapter introduced <strong>Support Vector Machines</strong>, a powerful class of algorithms that find optimal separating hyperplanes with maximum margin. The fundamental idea is to choose the decision boundary that is as far as possible from the nearest training points of each class.</p>
<p><strong>Support vectors</strong> are the training points that lie on the margin boundary and determine the decision boundary. All other points are irrelevant to the model, making SVMs sparse and efficient at prediction time. The <strong>cost parameter C</strong> controls the tradeoff between margin width and the number of violations allowed—larger C means fewer violations but risks overfitting.</p>
<p>The <strong>kernel trick</strong> extends SVMs to non-linear decision boundaries by implicitly mapping data to higher-dimensional spaces. The linear kernel corresponds to the standard support vector classifier. The polynomial kernel creates polynomial boundaries of specified degree. The RBF (radial basis function) kernel is the most commonly used choice for non-linear problems, creating flexible, localized decision boundaries. The <strong>gamma parameter</strong> (<span class="math inline">\(\gamma\)</span>) controls the RBF kernel width—larger values produce more complex boundaries with higher variance.</p>
<p><strong>Cross-validation</strong> is essential for selecting the parameters C and gamma, since both substantially affect model performance. SVMs extend naturally to <strong>multi-class</strong> problems through one-vs-one or one-vs-all strategies, and to <strong>regression</strong> through Support Vector Regression (SVR). <strong>Feature scaling</strong> is required before training because SVMs are sensitive to the relative scales of different variables. While SVMs work remarkably well in high dimensions, their training time scales poorly with the number of observations, limiting their applicability to very large datasets.</p>
</section>
<section id="additional-resources" class="level2" data-number="35.10">
<h2 data-number="35.10" class="anchored" data-anchor-id="additional-resources"><span class="header-section-number">35.10</span> Additional Resources</h2>
<ul>
<li><span class="citation" data-cites="james2023islr">James et al. (<a href="../references.html#ref-james2023islr" role="doc-biblioref">2023</a>)</span> - Accessible introduction to SVMs</li>
<li><span class="citation" data-cites="hastie2009elements">Hastie, Tibshirani, and Friedman (<a href="../references.html#ref-hastie2009elements" role="doc-biblioref">2009</a>)</span> - Theoretical foundations</li>
<li>Cristianini &amp; Shawe-Taylor (2000). <em>An Introduction to Support Vector Machines</em> - Classic reference</li>
<li><code>e1071</code> and <code>kernlab</code> package documentation</li>
</ul>


<!-- -->

<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list" style="display: none">
<div id="ref-hastie2009elements" class="csl-entry" role="listitem">
Hastie, Trevor, Robert Tibshirani, and Jerome Friedman. 2009. <em>The Elements of Statistical Learning: Data Mining, Inference, and Prediction</em>. 2nd ed. New York: Springer.
</div>
<div id="ref-james2023islr" class="csl-entry" role="listitem">
James, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2023. <em>An Introduction to Statistical Learning with Applications in r</em>. 2nd ed. Springer. <a href="https://www.statlearning.com">https://www.statlearning.com</a>.
</div>
</div>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
    const viewSource = window.document.getElementById('quarto-view-source') ||
                       window.document.getElementById('quarto-code-tools-source');
    if (viewSource) {
      const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
      viewSource.addEventListener("click", function(e) {
        if (sourceUrl) {
          // rstudio viewer pane
          if (/\bcapabilities=\b/.test(window.location)) {
            window.open(sourceUrl);
          } else {
            window.location.href = sourceUrl;
          }
        } else {
          const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
          modal.show();
        }
        return false;
      });
    }
    function toggleCodeHandler(show) {
      return function(e) {
        const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
        for (let i=0; i<detailsSrc.length; i++) {
          const details = detailsSrc[i].parentElement;
          if (show) {
            details.open = true;
          } else {
            details.removeAttribute("open");
          }
        }
        const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
        const fromCls = show ? "hidden" : "unhidden";
        const toCls = show ? "unhidden" : "hidden";
        for (let i=0; i<cellCodeDivs.length; i++) {
          const codeDiv = cellCodeDivs[i];
          if (codeDiv.classList.contains(fromCls)) {
            codeDiv.classList.remove(fromCls);
            codeDiv.classList.add(toCls);
          } 
        }
        return false;
      }
    }
    const hideAllCode = window.document.getElementById("quarto-hide-all-code");
    if (hideAllCode) {
      hideAllCode.addEventListener("click", toggleCodeHandler(false));
    }
    const showAllCode = window.document.getElementById("quarto-show-all-code");
    if (showAllCode) {
      showAllCode.addEventListener("click", toggleCodeHandler(true));
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../chapters/33-trees-forests.html" class="pagination-link" aria-label="Decision Trees and Random Forests">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">34</span>&nbsp; <span class="chapter-title">Decision Trees and Random Forests</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../chapters/35-clustering.html" class="pagination-link" aria-label="Clustering">
        <span class="nav-page-text"><span class="chapter-number">36</span>&nbsp; <span class="chapter-title">Clustering</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb28" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a><span class="fu"># Support Vector Machines {#sec-svm}</span></span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-5"><a href="#cb28-5" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb28-6"><a href="#cb28-6" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: false</span></span>
<span id="cb28-7"><a href="#cb28-7" aria-hidden="true" tabindex="-1"></a><span class="co">#| message: false</span></span>
<span id="cb28-8"><a href="#cb28-8" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tidyverse)</span>
<span id="cb28-9"><a href="#cb28-9" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(e1071)</span>
<span id="cb28-10"><a href="#cb28-10" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(kernlab)</span>
<span id="cb28-11"><a href="#cb28-11" aria-hidden="true" tabindex="-1"></a><span class="fu">theme_set</span>(<span class="fu">theme_minimal</span>())</span>
<span id="cb28-12"><a href="#cb28-12" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb28-13"><a href="#cb28-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-14"><a href="#cb28-14" aria-hidden="true" tabindex="-1"></a><span class="fu">## Introduction to Support Vector Machines</span></span>
<span id="cb28-15"><a href="#cb28-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-16"><a href="#cb28-16" aria-hidden="true" tabindex="-1"></a>**Support Vector Machines (SVMs)** are powerful supervised learning algorithms that excel at classification tasks, particularly when decision boundaries are complex. SVMs work by finding the optimal **hyperplane** that separates classes with the maximum margin.</span>
<span id="cb28-17"><a href="#cb28-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-18"><a href="#cb28-18" aria-hidden="true" tabindex="-1"></a>SVMs were developed by Vladimir Vapnik and colleagues in the 1990s and quickly became one of the most popular machine learning methods before the rise of deep learning. They remain valuable tools today, particularly for problems with many features relative to samples, where their maximum margin principle helps prevent overfitting. They work well on small to medium-sized datasets where deep learning would lack sufficient data, and they are often preferred in applications requiring robust generalization with limited tuning. In biology, SVMs have been particularly successful for sequence classification—predicting protein structure or function from amino acid sequences, identifying splice sites in genes, or classifying regulatory elements in DNA.</span>
<span id="cb28-19"><a href="#cb28-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-20"><a href="#cb28-20" aria-hidden="true" tabindex="-1"></a><span class="fu">## The Maximum Margin Classifier</span></span>
<span id="cb28-21"><a href="#cb28-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-22"><a href="#cb28-22" aria-hidden="true" tabindex="-1"></a><span class="fu">### Separating Hyperplanes</span></span>
<span id="cb28-23"><a href="#cb28-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-24"><a href="#cb28-24" aria-hidden="true" tabindex="-1"></a>Consider a simple two-class classification problem with two features. If the classes are **linearly separable**—they can be perfectly separated by a straight line (or hyperplane in higher dimensions)—there are infinitely many possible separating lines.</span>
<span id="cb28-25"><a href="#cb28-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-28"><a href="#cb28-28" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb28-29"><a href="#cb28-29" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-separating-hyperplanes</span></span>
<span id="cb28-30"><a href="#cb28-30" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "Multiple hyperplanes can separate linearly separable classes. Which one is best?"</span></span>
<span id="cb28-31"><a href="#cb28-31" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-width: 7</span></span>
<span id="cb28-32"><a href="#cb28-32" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-height: 6</span></span>
<span id="cb28-33"><a href="#cb28-33" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate linearly separable data</span></span>
<span id="cb28-34"><a href="#cb28-34" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">42</span>)</span>
<span id="cb28-35"><a href="#cb28-35" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">50</span></span>
<span id="cb28-36"><a href="#cb28-36" aria-hidden="true" tabindex="-1"></a>x1 <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fu">rnorm</span>(n<span class="sc">/</span><span class="dv">2</span>, <span class="sc">-</span><span class="dv">1</span>, <span class="fl">0.5</span>), <span class="fu">rnorm</span>(n<span class="sc">/</span><span class="dv">2</span>, <span class="dv">1</span>, <span class="fl">0.5</span>))</span>
<span id="cb28-37"><a href="#cb28-37" aria-hidden="true" tabindex="-1"></a>x2 <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fu">rnorm</span>(n<span class="sc">/</span><span class="dv">2</span>, <span class="sc">-</span><span class="dv">1</span>, <span class="fl">0.5</span>), <span class="fu">rnorm</span>(n<span class="sc">/</span><span class="dv">2</span>, <span class="dv">1</span>, <span class="fl">0.5</span>))</span>
<span id="cb28-38"><a href="#cb28-38" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> <span class="fu">factor</span>(<span class="fu">c</span>(<span class="fu">rep</span>(<span class="sc">-</span><span class="dv">1</span>, n<span class="sc">/</span><span class="dv">2</span>), <span class="fu">rep</span>(<span class="dv">1</span>, n<span class="sc">/</span><span class="dv">2</span>)))</span>
<span id="cb28-39"><a href="#cb28-39" aria-hidden="true" tabindex="-1"></a>data_sep <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">x1 =</span> x1, <span class="at">x2 =</span> x2, <span class="at">y =</span> y)</span>
<span id="cb28-40"><a href="#cb28-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-41"><a href="#cb28-41" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(x1, x2, <span class="at">col =</span> <span class="fu">c</span>(<span class="st">"red"</span>, <span class="st">"blue"</span>)[<span class="fu">as.numeric</span>(y)], <span class="at">pch =</span> <span class="dv">19</span>,</span>
<span id="cb28-42"><a href="#cb28-42" aria-hidden="true" tabindex="-1"></a>     <span class="at">main =</span> <span class="st">"Multiple Separating Hyperplanes"</span>)</span>
<span id="cb28-43"><a href="#cb28-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-44"><a href="#cb28-44" aria-hidden="true" tabindex="-1"></a><span class="co"># Several possible separating lines</span></span>
<span id="cb28-45"><a href="#cb28-45" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="sc">-</span><span class="fl">0.1</span>, <span class="dv">1</span>, <span class="at">lty =</span> <span class="dv">2</span>, <span class="at">col =</span> <span class="st">"gray"</span>)</span>
<span id="cb28-46"><a href="#cb28-46" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="fl">0.2</span>, <span class="fl">0.9</span>, <span class="at">lty =</span> <span class="dv">2</span>, <span class="at">col =</span> <span class="st">"gray"</span>)</span>
<span id="cb28-47"><a href="#cb28-47" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="sc">-</span><span class="fl">0.3</span>, <span class="fl">1.1</span>, <span class="at">lty =</span> <span class="dv">2</span>, <span class="at">col =</span> <span class="st">"gray"</span>)</span>
<span id="cb28-48"><a href="#cb28-48" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="at">lwd =</span> <span class="dv">2</span>, <span class="at">col =</span> <span class="st">"darkgreen"</span>)  <span class="co"># Best one</span></span>
<span id="cb28-49"><a href="#cb28-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-50"><a href="#cb28-50" aria-hidden="true" tabindex="-1"></a><span class="fu">legend</span>(<span class="st">"topleft"</span>, <span class="fu">c</span>(<span class="st">"Class -1"</span>, <span class="st">"Class +1"</span>),</span>
<span id="cb28-51"><a href="#cb28-51" aria-hidden="true" tabindex="-1"></a>       <span class="at">col =</span> <span class="fu">c</span>(<span class="st">"red"</span>, <span class="st">"blue"</span>), <span class="at">pch =</span> <span class="dv">19</span>)</span>
<span id="cb28-52"><a href="#cb28-52" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb28-53"><a href="#cb28-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-54"><a href="#cb28-54" aria-hidden="true" tabindex="-1"></a><span class="fu">### The Margin</span></span>
<span id="cb28-55"><a href="#cb28-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-56"><a href="#cb28-56" aria-hidden="true" tabindex="-1"></a>The **maximum margin classifier** chooses the hyperplane that maximizes the distance to the nearest points from either class. This distance is called the **margin**.</span>
<span id="cb28-57"><a href="#cb28-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-58"><a href="#cb28-58" aria-hidden="true" tabindex="-1"></a>A hyperplane in $p$ dimensions is defined by:</span>
<span id="cb28-59"><a href="#cb28-59" aria-hidden="true" tabindex="-1"></a>$$\beta_0 + \beta_1 X_1 + \beta_2 X_2 + \cdots + \beta_p X_p = 0$$</span>
<span id="cb28-60"><a href="#cb28-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-61"><a href="#cb28-61" aria-hidden="true" tabindex="-1"></a>For a new observation $x^*$, we classify based on which side of the hyperplane it falls:</span>
<span id="cb28-62"><a href="#cb28-62" aria-hidden="true" tabindex="-1"></a>$$\text{sign}(\beta_0 + \beta_1 x_1^* + \beta_2 x_2^* + \cdots + \beta_p x_p^*) = \pm 1$$</span>
<span id="cb28-63"><a href="#cb28-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-66"><a href="#cb28-66" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb28-67"><a href="#cb28-67" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-margin</span></span>
<span id="cb28-68"><a href="#cb28-68" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "The maximum margin classifier finds the hyperplane that maximizes the margin (distance to nearest points). Support vectors are the points on the margin boundaries."</span></span>
<span id="cb28-69"><a href="#cb28-69" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-width: 7</span></span>
<span id="cb28-70"><a href="#cb28-70" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-height: 6</span></span>
<span id="cb28-71"><a href="#cb28-71" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit SVM (linear kernel)</span></span>
<span id="cb28-72"><a href="#cb28-72" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(e1071)</span>
<span id="cb28-73"><a href="#cb28-73" aria-hidden="true" tabindex="-1"></a>svm_fit <span class="ot">&lt;-</span> <span class="fu">svm</span>(y <span class="sc">~</span> x1 <span class="sc">+</span> x2, <span class="at">data =</span> data_sep, <span class="at">kernel =</span> <span class="st">"linear"</span>, <span class="at">scale =</span> <span class="cn">FALSE</span>)</span>
<span id="cb28-74"><a href="#cb28-74" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-75"><a href="#cb28-75" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot</span></span>
<span id="cb28-76"><a href="#cb28-76" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(x1, x2, <span class="at">col =</span> <span class="fu">c</span>(<span class="st">"red"</span>, <span class="st">"blue"</span>)[<span class="fu">as.numeric</span>(y)], <span class="at">pch =</span> <span class="dv">19</span>,</span>
<span id="cb28-77"><a href="#cb28-77" aria-hidden="true" tabindex="-1"></a>     <span class="at">main =</span> <span class="st">"Maximum Margin Classifier"</span>)</span>
<span id="cb28-78"><a href="#cb28-78" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-79"><a href="#cb28-79" aria-hidden="true" tabindex="-1"></a><span class="co"># Extract coefficients</span></span>
<span id="cb28-80"><a href="#cb28-80" aria-hidden="true" tabindex="-1"></a>w <span class="ot">&lt;-</span> <span class="fu">t</span>(svm_fit<span class="sc">$</span>coefs) <span class="sc">%*%</span> svm_fit<span class="sc">$</span>SV</span>
<span id="cb28-81"><a href="#cb28-81" aria-hidden="true" tabindex="-1"></a>b <span class="ot">&lt;-</span> <span class="sc">-</span>svm_fit<span class="sc">$</span>rho</span>
<span id="cb28-82"><a href="#cb28-82" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-83"><a href="#cb28-83" aria-hidden="true" tabindex="-1"></a><span class="co"># Decision boundary</span></span>
<span id="cb28-84"><a href="#cb28-84" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="sc">-</span>b<span class="sc">/</span>w[<span class="dv">2</span>], <span class="sc">-</span>w[<span class="dv">1</span>]<span class="sc">/</span>w[<span class="dv">2</span>], <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb28-85"><a href="#cb28-85" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-86"><a href="#cb28-86" aria-hidden="true" tabindex="-1"></a><span class="co"># Margins</span></span>
<span id="cb28-87"><a href="#cb28-87" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>((<span class="sc">-</span>b<span class="dv">-1</span>)<span class="sc">/</span>w[<span class="dv">2</span>], <span class="sc">-</span>w[<span class="dv">1</span>]<span class="sc">/</span>w[<span class="dv">2</span>], <span class="at">lty =</span> <span class="dv">2</span>)</span>
<span id="cb28-88"><a href="#cb28-88" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>((<span class="sc">-</span>b<span class="sc">+</span><span class="dv">1</span>)<span class="sc">/</span>w[<span class="dv">2</span>], <span class="sc">-</span>w[<span class="dv">1</span>]<span class="sc">/</span>w[<span class="dv">2</span>], <span class="at">lty =</span> <span class="dv">2</span>)</span>
<span id="cb28-89"><a href="#cb28-89" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-90"><a href="#cb28-90" aria-hidden="true" tabindex="-1"></a><span class="co"># Highlight support vectors</span></span>
<span id="cb28-91"><a href="#cb28-91" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span>(svm_fit<span class="sc">$</span>SV, <span class="at">col =</span> <span class="st">"black"</span>, <span class="at">cex =</span> <span class="dv">2</span>, <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb28-92"><a href="#cb28-92" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-93"><a href="#cb28-93" aria-hidden="true" tabindex="-1"></a><span class="fu">legend</span>(<span class="st">"topleft"</span>, <span class="fu">c</span>(<span class="st">"Class -1"</span>, <span class="st">"Class +1"</span>, <span class="st">"Support Vectors"</span>),</span>
<span id="cb28-94"><a href="#cb28-94" aria-hidden="true" tabindex="-1"></a>       <span class="at">col =</span> <span class="fu">c</span>(<span class="st">"red"</span>, <span class="st">"blue"</span>, <span class="st">"black"</span>), <span class="at">pch =</span> <span class="fu">c</span>(<span class="dv">19</span>, <span class="dv">19</span>, <span class="dv">1</span>), <span class="at">pt.cex =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">2</span>))</span>
<span id="cb28-95"><a href="#cb28-95" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb28-96"><a href="#cb28-96" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-97"><a href="#cb28-97" aria-hidden="true" tabindex="-1"></a><span class="fu">### Support Vectors</span></span>
<span id="cb28-98"><a href="#cb28-98" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-99"><a href="#cb28-99" aria-hidden="true" tabindex="-1"></a>The observations that lie exactly on the margin boundary are called **support vectors**. These points "support" the hyperplane—if they were moved, the optimal hyperplane would change. Points farther from the margin have no effect on the solution.</span>
<span id="cb28-100"><a href="#cb28-100" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-101"><a href="#cb28-101" aria-hidden="true" tabindex="-1"></a>This property has important practical implications. SVMs are **sparse** in the sense that only support vectors matter for prediction—the model can discard all other training points without affecting its predictions. This makes SVMs **efficient** at prediction time, since classification depends only on distances to the support vectors rather than to all training observations. And it makes SVMs **robust** to outliers that are far from the decision boundary, since such points don't influence the model at all.</span>
<span id="cb28-102"><a href="#cb28-102" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-103"><a href="#cb28-103" aria-hidden="true" tabindex="-1"></a><span class="fu">## The Support Vector Classifier (Soft Margin)</span></span>
<span id="cb28-104"><a href="#cb28-104" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-105"><a href="#cb28-105" aria-hidden="true" tabindex="-1"></a>Real data is rarely perfectly separable. The **support vector classifier** (soft margin SVM) allows some violations of the margin:</span>
<span id="cb28-106"><a href="#cb28-106" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-109"><a href="#cb28-109" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb28-110"><a href="#cb28-110" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-nonseparable</span></span>
<span id="cb28-111"><a href="#cb28-111" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "When classes overlap, perfect separation is impossible. The soft margin SVM allows some points to be on the wrong side."</span></span>
<span id="cb28-112"><a href="#cb28-112" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-width: 7</span></span>
<span id="cb28-113"><a href="#cb28-113" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-height: 6</span></span>
<span id="cb28-114"><a href="#cb28-114" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate non-separable data</span></span>
<span id="cb28-115"><a href="#cb28-115" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb28-116"><a href="#cb28-116" aria-hidden="true" tabindex="-1"></a>x1_ns <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fu">rnorm</span>(n<span class="sc">/</span><span class="dv">2</span>, <span class="sc">-</span><span class="fl">0.5</span>, <span class="fl">0.7</span>), <span class="fu">rnorm</span>(n<span class="sc">/</span><span class="dv">2</span>, <span class="fl">0.5</span>, <span class="fl">0.7</span>))</span>
<span id="cb28-117"><a href="#cb28-117" aria-hidden="true" tabindex="-1"></a>x2_ns <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fu">rnorm</span>(n<span class="sc">/</span><span class="dv">2</span>, <span class="sc">-</span><span class="fl">0.5</span>, <span class="fl">0.7</span>), <span class="fu">rnorm</span>(n<span class="sc">/</span><span class="dv">2</span>, <span class="fl">0.5</span>, <span class="fl">0.7</span>))</span>
<span id="cb28-118"><a href="#cb28-118" aria-hidden="true" tabindex="-1"></a>data_ns <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">x1 =</span> x1_ns, <span class="at">x2 =</span> x2_ns, <span class="at">y =</span> y)</span>
<span id="cb28-119"><a href="#cb28-119" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-120"><a href="#cb28-120" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(x1_ns, x2_ns, <span class="at">col =</span> <span class="fu">c</span>(<span class="st">"red"</span>, <span class="st">"blue"</span>)[<span class="fu">as.numeric</span>(y)], <span class="at">pch =</span> <span class="dv">19</span>,</span>
<span id="cb28-121"><a href="#cb28-121" aria-hidden="true" tabindex="-1"></a>     <span class="at">main =</span> <span class="st">"Non-Separable Classes"</span>)</span>
<span id="cb28-122"><a href="#cb28-122" aria-hidden="true" tabindex="-1"></a><span class="fu">legend</span>(<span class="st">"topleft"</span>, <span class="fu">c</span>(<span class="st">"Class -1"</span>, <span class="st">"Class +1"</span>), <span class="at">col =</span> <span class="fu">c</span>(<span class="st">"red"</span>, <span class="st">"blue"</span>), <span class="at">pch =</span> <span class="dv">19</span>)</span>
<span id="cb28-123"><a href="#cb28-123" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb28-124"><a href="#cb28-124" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-125"><a href="#cb28-125" aria-hidden="true" tabindex="-1"></a>The soft margin formulation introduces **slack variables** $\xi_i$ that allow points to violate the margin:</span>
<span id="cb28-126"><a href="#cb28-126" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-127"><a href="#cb28-127" aria-hidden="true" tabindex="-1"></a>$$\text{Minimize: } \frac{1}{2}||\beta||^2 + C \sum_{i=1}^n \xi_i$$</span>
<span id="cb28-128"><a href="#cb28-128" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-129"><a href="#cb28-129" aria-hidden="true" tabindex="-1"></a>subject to:</span>
<span id="cb28-130"><a href="#cb28-130" aria-hidden="true" tabindex="-1"></a>$$y_i(\beta_0 + \beta^T x_i) \geq 1 - \xi_i, \quad \xi_i \geq 0$$</span>
<span id="cb28-131"><a href="#cb28-131" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-132"><a href="#cb28-132" aria-hidden="true" tabindex="-1"></a><span class="fu">### The Cost Parameter C</span></span>
<span id="cb28-133"><a href="#cb28-133" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-134"><a href="#cb28-134" aria-hidden="true" tabindex="-1"></a>The parameter **C** controls the tradeoff between margin width and the number of violations allowed.</span>
<span id="cb28-135"><a href="#cb28-135" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-136"><a href="#cb28-136" aria-hidden="true" tabindex="-1"></a>When **C is large**, the penalty for violations is high, so the algorithm tries hard to classify all training points correctly. This typically produces a narrow margin with few violations, but risks overfitting to noise in the training data. When **C is small**, violations are tolerated more easily, resulting in a wider margin that may misclassify some training points. This trades off training accuracy for a simpler decision boundary that often generalizes better to new data.</span>
<span id="cb28-137"><a href="#cb28-137" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-140"><a href="#cb28-140" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb28-141"><a href="#cb28-141" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-svm-cost</span></span>
<span id="cb28-142"><a href="#cb28-142" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "Effect of cost parameter C on the support vector classifier. Larger C allows fewer violations but may overfit."</span></span>
<span id="cb28-143"><a href="#cb28-143" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-width: 9</span></span>
<span id="cb28-144"><a href="#cb28-144" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-height: 4</span></span>
<span id="cb28-145"><a href="#cb28-145" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">3</span>))</span>
<span id="cb28-146"><a href="#cb28-146" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-147"><a href="#cb28-147" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (C_val <span class="cf">in</span> <span class="fu">c</span>(<span class="fl">0.1</span>, <span class="dv">1</span>, <span class="dv">100</span>)) {</span>
<span id="cb28-148"><a href="#cb28-148" aria-hidden="true" tabindex="-1"></a>  svm_c <span class="ot">&lt;-</span> <span class="fu">svm</span>(y <span class="sc">~</span> x1 <span class="sc">+</span> x2, <span class="at">data =</span> data_ns, <span class="at">kernel =</span> <span class="st">"linear"</span>,</span>
<span id="cb28-149"><a href="#cb28-149" aria-hidden="true" tabindex="-1"></a>               <span class="at">cost =</span> C_val, <span class="at">scale =</span> <span class="cn">FALSE</span>)</span>
<span id="cb28-150"><a href="#cb28-150" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-151"><a href="#cb28-151" aria-hidden="true" tabindex="-1"></a>  <span class="fu">plot</span>(x1_ns, x2_ns, <span class="at">col =</span> <span class="fu">c</span>(<span class="st">"red"</span>, <span class="st">"blue"</span>)[<span class="fu">as.numeric</span>(y)], <span class="at">pch =</span> <span class="dv">19</span>,</span>
<span id="cb28-152"><a href="#cb28-152" aria-hidden="true" tabindex="-1"></a>       <span class="at">main =</span> <span class="fu">paste</span>(<span class="st">"C ="</span>, C_val))</span>
<span id="cb28-153"><a href="#cb28-153" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-154"><a href="#cb28-154" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Decision boundary</span></span>
<span id="cb28-155"><a href="#cb28-155" aria-hidden="true" tabindex="-1"></a>  w <span class="ot">&lt;-</span> <span class="fu">t</span>(svm_c<span class="sc">$</span>coefs) <span class="sc">%*%</span> svm_c<span class="sc">$</span>SV</span>
<span id="cb28-156"><a href="#cb28-156" aria-hidden="true" tabindex="-1"></a>  b <span class="ot">&lt;-</span> <span class="sc">-</span>svm_c<span class="sc">$</span>rho</span>
<span id="cb28-157"><a href="#cb28-157" aria-hidden="true" tabindex="-1"></a>  <span class="fu">abline</span>(<span class="sc">-</span>b<span class="sc">/</span>w[<span class="dv">2</span>], <span class="sc">-</span>w[<span class="dv">1</span>]<span class="sc">/</span>w[<span class="dv">2</span>], <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb28-158"><a href="#cb28-158" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-159"><a href="#cb28-159" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Support vectors</span></span>
<span id="cb28-160"><a href="#cb28-160" aria-hidden="true" tabindex="-1"></a>  <span class="fu">points</span>(svm_c<span class="sc">$</span>SV, <span class="at">col =</span> <span class="st">"black"</span>, <span class="at">cex =</span> <span class="fl">1.5</span>, <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb28-161"><a href="#cb28-161" aria-hidden="true" tabindex="-1"></a>  <span class="fu">text</span>(<span class="dv">0</span>, <span class="dv">2</span>, <span class="fu">paste</span>(<span class="fu">nrow</span>(svm_c<span class="sc">$</span>SV), <span class="st">"SVs"</span>), <span class="at">cex =</span> <span class="fl">0.8</span>)</span>
<span id="cb28-162"><a href="#cb28-162" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb28-163"><a href="#cb28-163" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb28-164"><a href="#cb28-164" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-165"><a href="#cb28-165" aria-hidden="true" tabindex="-1"></a>As C decreases, more support vectors are used and the margin widens. This is another manifestation of the bias-variance tradeoff.</span>
<span id="cb28-166"><a href="#cb28-166" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-167"><a href="#cb28-167" aria-hidden="true" tabindex="-1"></a><span class="fu">## The Kernel Trick: Non-Linear SVMs</span></span>
<span id="cb28-168"><a href="#cb28-168" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-169"><a href="#cb28-169" aria-hidden="true" tabindex="-1"></a>Linear boundaries are often insufficient. The **kernel trick** allows SVMs to learn non-linear decision boundaries by implicitly mapping data to higher-dimensional space.</span>
<span id="cb28-170"><a href="#cb28-170" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-171"><a href="#cb28-171" aria-hidden="true" tabindex="-1"></a><span class="fu">### Why Kernels?</span></span>
<span id="cb28-172"><a href="#cb28-172" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-173"><a href="#cb28-173" aria-hidden="true" tabindex="-1"></a>Consider data that's not linearly separable:</span>
<span id="cb28-174"><a href="#cb28-174" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-177"><a href="#cb28-177" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb28-178"><a href="#cb28-178" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-nonlinear-data</span></span>
<span id="cb28-179"><a href="#cb28-179" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "Data requiring a non-linear decision boundary. No straight line can separate these classes."</span></span>
<span id="cb28-180"><a href="#cb28-180" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-width: 7</span></span>
<span id="cb28-181"><a href="#cb28-181" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-height: 6</span></span>
<span id="cb28-182"><a href="#cb28-182" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate data requiring non-linear boundary</span></span>
<span id="cb28-183"><a href="#cb28-183" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">42</span>)</span>
<span id="cb28-184"><a href="#cb28-184" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">200</span></span>
<span id="cb28-185"><a href="#cb28-185" aria-hidden="true" tabindex="-1"></a>r1 <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(n<span class="sc">/</span><span class="dv">2</span>, <span class="dv">1</span>, <span class="fl">0.3</span>)</span>
<span id="cb28-186"><a href="#cb28-186" aria-hidden="true" tabindex="-1"></a>r2 <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(n<span class="sc">/</span><span class="dv">2</span>, <span class="fl">2.5</span>, <span class="fl">0.3</span>)</span>
<span id="cb28-187"><a href="#cb28-187" aria-hidden="true" tabindex="-1"></a>theta <span class="ot">&lt;-</span> <span class="fu">runif</span>(n, <span class="dv">0</span>, <span class="dv">2</span><span class="sc">*</span>pi)</span>
<span id="cb28-188"><a href="#cb28-188" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-189"><a href="#cb28-189" aria-hidden="true" tabindex="-1"></a>x1_nl <span class="ot">&lt;-</span> <span class="fu">c</span>(r1 <span class="sc">*</span> <span class="fu">cos</span>(theta[<span class="dv">1</span><span class="sc">:</span>(n<span class="sc">/</span><span class="dv">2</span>)]), r2 <span class="sc">*</span> <span class="fu">cos</span>(theta[(n<span class="sc">/</span><span class="dv">2</span><span class="sc">+</span><span class="dv">1</span>)<span class="sc">:</span>n]))</span>
<span id="cb28-190"><a href="#cb28-190" aria-hidden="true" tabindex="-1"></a>x2_nl <span class="ot">&lt;-</span> <span class="fu">c</span>(r1 <span class="sc">*</span> <span class="fu">sin</span>(theta[<span class="dv">1</span><span class="sc">:</span>(n<span class="sc">/</span><span class="dv">2</span>)]), r2 <span class="sc">*</span> <span class="fu">sin</span>(theta[(n<span class="sc">/</span><span class="dv">2</span><span class="sc">+</span><span class="dv">1</span>)<span class="sc">:</span>n]))</span>
<span id="cb28-191"><a href="#cb28-191" aria-hidden="true" tabindex="-1"></a>y_nl <span class="ot">&lt;-</span> <span class="fu">factor</span>(<span class="fu">c</span>(<span class="fu">rep</span>(<span class="sc">-</span><span class="dv">1</span>, n<span class="sc">/</span><span class="dv">2</span>), <span class="fu">rep</span>(<span class="dv">1</span>, n<span class="sc">/</span><span class="dv">2</span>)))</span>
<span id="cb28-192"><a href="#cb28-192" aria-hidden="true" tabindex="-1"></a>data_nl <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">x1 =</span> x1_nl, <span class="at">x2 =</span> x2_nl, <span class="at">y =</span> y_nl)</span>
<span id="cb28-193"><a href="#cb28-193" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-194"><a href="#cb28-194" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(x1_nl, x2_nl, <span class="at">col =</span> <span class="fu">c</span>(<span class="st">"red"</span>, <span class="st">"blue"</span>)[<span class="fu">as.numeric</span>(y_nl)], <span class="at">pch =</span> <span class="dv">19</span>,</span>
<span id="cb28-195"><a href="#cb28-195" aria-hidden="true" tabindex="-1"></a>     <span class="at">main =</span> <span class="st">"Non-Linear Decision Boundary Needed"</span>)</span>
<span id="cb28-196"><a href="#cb28-196" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb28-197"><a href="#cb28-197" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-198"><a href="#cb28-198" aria-hidden="true" tabindex="-1"></a>A linear SVM fails on this data. But if we map to a higher-dimensional space using features like $x_1^2$, $x_2^2$, $x_1 x_2$, the classes might become linearly separable.</span>
<span id="cb28-199"><a href="#cb28-199" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-200"><a href="#cb28-200" aria-hidden="true" tabindex="-1"></a><span class="fu">### Common Kernels</span></span>
<span id="cb28-201"><a href="#cb28-201" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-202"><a href="#cb28-202" aria-hidden="true" tabindex="-1"></a>The **Linear Kernel**, $K(x_i, x_j) = x_i^T x_j$, is equivalent to the standard support vector classifier with no transformation—it finds linear decision boundaries in the original feature space.</span>
<span id="cb28-203"><a href="#cb28-203" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-204"><a href="#cb28-204" aria-hidden="true" tabindex="-1"></a>The **Polynomial Kernel**, $K(x_i, x_j) = (1 + x_i^T x_j)^d$, creates polynomial decision boundaries of degree $d$. This allows curved boundaries while still having a clear interpretation in terms of polynomial features.</span>
<span id="cb28-205"><a href="#cb28-205" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-206"><a href="#cb28-206" aria-hidden="true" tabindex="-1"></a>The **Radial Basis Function (RBF) Kernel**, also called the Gaussian kernel, is defined as $K(x_i, x_j) = \exp(-\gamma ||x_i - x_j||^2)$. It creates flexible, localized decision boundaries and is the most commonly used kernel for non-linear problems. The parameter $\gamma$ controls the influence radius of each training point—larger values of $\gamma$ mean that each point influences only its immediate neighborhood, while smaller values create smoother boundaries where each point influences a wider region.</span>
<span id="cb28-207"><a href="#cb28-207" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-208"><a href="#cb28-208" aria-hidden="true" tabindex="-1"></a>The **Sigmoid Kernel**, $K(x_i, x_j) = \tanh(\gamma x_i^T x_j + r)$, mimics neural network activation functions and is occasionally used when you want to relate SVMs to neural networks. However, it is not a valid kernel for all parameter values and is less commonly used than the others.</span>
<span id="cb28-209"><a href="#cb28-209" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-212"><a href="#cb28-212" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb28-213"><a href="#cb28-213" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-kernel-comparison</span></span>
<span id="cb28-214"><a href="#cb28-214" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "Comparison of different SVM kernels on non-linear data"</span></span>
<span id="cb28-215"><a href="#cb28-215" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-width: 9</span></span>
<span id="cb28-216"><a href="#cb28-216" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-height: 4</span></span>
<span id="cb28-217"><a href="#cb28-217" aria-hidden="true" tabindex="-1"></a><span class="co"># Create grid for visualization</span></span>
<span id="cb28-218"><a href="#cb28-218" aria-hidden="true" tabindex="-1"></a>x1_seq <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="sc">-</span><span class="dv">4</span>, <span class="dv">4</span>, <span class="at">length.out =</span> <span class="dv">100</span>)</span>
<span id="cb28-219"><a href="#cb28-219" aria-hidden="true" tabindex="-1"></a>x2_seq <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="sc">-</span><span class="dv">4</span>, <span class="dv">4</span>, <span class="at">length.out =</span> <span class="dv">100</span>)</span>
<span id="cb28-220"><a href="#cb28-220" aria-hidden="true" tabindex="-1"></a>grid <span class="ot">&lt;-</span> <span class="fu">expand.grid</span>(<span class="at">x1 =</span> x1_seq, <span class="at">x2 =</span> x2_seq)</span>
<span id="cb28-221"><a href="#cb28-221" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-222"><a href="#cb28-222" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">3</span>))</span>
<span id="cb28-223"><a href="#cb28-223" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-224"><a href="#cb28-224" aria-hidden="true" tabindex="-1"></a><span class="co"># Linear kernel</span></span>
<span id="cb28-225"><a href="#cb28-225" aria-hidden="true" tabindex="-1"></a>svm_linear <span class="ot">&lt;-</span> <span class="fu">svm</span>(y <span class="sc">~</span> x1 <span class="sc">+</span> x2, <span class="at">data =</span> data_nl, <span class="at">kernel =</span> <span class="st">"linear"</span>)</span>
<span id="cb28-226"><a href="#cb28-226" aria-hidden="true" tabindex="-1"></a>grid<span class="sc">$</span>pred <span class="ot">&lt;-</span> <span class="fu">predict</span>(svm_linear, grid)</span>
<span id="cb28-227"><a href="#cb28-227" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(grid<span class="sc">$</span>x1, grid<span class="sc">$</span>x2,</span>
<span id="cb28-228"><a href="#cb28-228" aria-hidden="true" tabindex="-1"></a>     <span class="at">col =</span> <span class="fu">c</span>(<span class="fu">rgb</span>(<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="fl">0.1</span>), <span class="fu">rgb</span>(<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="fl">0.1</span>))[<span class="fu">as.numeric</span>(grid<span class="sc">$</span>pred)],</span>
<span id="cb28-229"><a href="#cb28-229" aria-hidden="true" tabindex="-1"></a>     <span class="at">pch =</span> <span class="dv">15</span>, <span class="at">cex =</span> <span class="fl">0.3</span>, <span class="at">main =</span> <span class="st">"Linear Kernel"</span>)</span>
<span id="cb28-230"><a href="#cb28-230" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span>(x1_nl, x2_nl, <span class="at">col =</span> <span class="fu">c</span>(<span class="st">"red"</span>, <span class="st">"blue"</span>)[<span class="fu">as.numeric</span>(y_nl)], <span class="at">pch =</span> <span class="dv">19</span>)</span>
<span id="cb28-231"><a href="#cb28-231" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-232"><a href="#cb28-232" aria-hidden="true" tabindex="-1"></a><span class="co"># Polynomial kernel</span></span>
<span id="cb28-233"><a href="#cb28-233" aria-hidden="true" tabindex="-1"></a>svm_poly <span class="ot">&lt;-</span> <span class="fu">svm</span>(y <span class="sc">~</span> x1 <span class="sc">+</span> x2, <span class="at">data =</span> data_nl, <span class="at">kernel =</span> <span class="st">"polynomial"</span>, <span class="at">degree =</span> <span class="dv">2</span>)</span>
<span id="cb28-234"><a href="#cb28-234" aria-hidden="true" tabindex="-1"></a>grid<span class="sc">$</span>pred <span class="ot">&lt;-</span> <span class="fu">predict</span>(svm_poly, grid)</span>
<span id="cb28-235"><a href="#cb28-235" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(grid<span class="sc">$</span>x1, grid<span class="sc">$</span>x2,</span>
<span id="cb28-236"><a href="#cb28-236" aria-hidden="true" tabindex="-1"></a>     <span class="at">col =</span> <span class="fu">c</span>(<span class="fu">rgb</span>(<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="fl">0.1</span>), <span class="fu">rgb</span>(<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="fl">0.1</span>))[<span class="fu">as.numeric</span>(grid<span class="sc">$</span>pred)],</span>
<span id="cb28-237"><a href="#cb28-237" aria-hidden="true" tabindex="-1"></a>     <span class="at">pch =</span> <span class="dv">15</span>, <span class="at">cex =</span> <span class="fl">0.3</span>, <span class="at">main =</span> <span class="st">"Polynomial Kernel (d=2)"</span>)</span>
<span id="cb28-238"><a href="#cb28-238" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span>(x1_nl, x2_nl, <span class="at">col =</span> <span class="fu">c</span>(<span class="st">"red"</span>, <span class="st">"blue"</span>)[<span class="fu">as.numeric</span>(y_nl)], <span class="at">pch =</span> <span class="dv">19</span>)</span>
<span id="cb28-239"><a href="#cb28-239" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-240"><a href="#cb28-240" aria-hidden="true" tabindex="-1"></a><span class="co"># RBF kernel</span></span>
<span id="cb28-241"><a href="#cb28-241" aria-hidden="true" tabindex="-1"></a>svm_rbf <span class="ot">&lt;-</span> <span class="fu">svm</span>(y <span class="sc">~</span> x1 <span class="sc">+</span> x2, <span class="at">data =</span> data_nl, <span class="at">kernel =</span> <span class="st">"radial"</span>, <span class="at">gamma =</span> <span class="fl">0.5</span>)</span>
<span id="cb28-242"><a href="#cb28-242" aria-hidden="true" tabindex="-1"></a>grid<span class="sc">$</span>pred <span class="ot">&lt;-</span> <span class="fu">predict</span>(svm_rbf, grid)</span>
<span id="cb28-243"><a href="#cb28-243" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(grid<span class="sc">$</span>x1, grid<span class="sc">$</span>x2,</span>
<span id="cb28-244"><a href="#cb28-244" aria-hidden="true" tabindex="-1"></a>     <span class="at">col =</span> <span class="fu">c</span>(<span class="fu">rgb</span>(<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="fl">0.1</span>), <span class="fu">rgb</span>(<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="fl">0.1</span>))[<span class="fu">as.numeric</span>(grid<span class="sc">$</span>pred)],</span>
<span id="cb28-245"><a href="#cb28-245" aria-hidden="true" tabindex="-1"></a>     <span class="at">pch =</span> <span class="dv">15</span>, <span class="at">cex =</span> <span class="fl">0.3</span>, <span class="at">main =</span> <span class="st">"RBF Kernel"</span>)</span>
<span id="cb28-246"><a href="#cb28-246" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span>(x1_nl, x2_nl, <span class="at">col =</span> <span class="fu">c</span>(<span class="st">"red"</span>, <span class="st">"blue"</span>)[<span class="fu">as.numeric</span>(y_nl)], <span class="at">pch =</span> <span class="dv">19</span>)</span>
<span id="cb28-247"><a href="#cb28-247" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb28-248"><a href="#cb28-248" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-249"><a href="#cb28-249" aria-hidden="true" tabindex="-1"></a>The RBF kernel successfully captures the circular decision boundary.</span>
<span id="cb28-250"><a href="#cb28-250" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-251"><a href="#cb28-251" aria-hidden="true" tabindex="-1"></a><span class="fu">### Tuning the RBF Kernel</span></span>
<span id="cb28-252"><a href="#cb28-252" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-253"><a href="#cb28-253" aria-hidden="true" tabindex="-1"></a>The RBF kernel has two parameters that must be tuned together. The **cost parameter C** controls margin violations as before—how much the algorithm penalizes misclassifications. The **gamma parameter** ($\gamma$) controls the kernel width, determining how far the influence of a single training example reaches.</span>
<span id="cb28-254"><a href="#cb28-254" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-257"><a href="#cb28-257" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb28-258"><a href="#cb28-258" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-gamma-effect</span></span>
<span id="cb28-259"><a href="#cb28-259" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "Effect of gamma on RBF SVM: small gamma gives smoother boundaries, large gamma fits training data more closely"</span></span>
<span id="cb28-260"><a href="#cb28-260" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-width: 9</span></span>
<span id="cb28-261"><a href="#cb28-261" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-height: 4</span></span>
<span id="cb28-262"><a href="#cb28-262" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">3</span>))</span>
<span id="cb28-263"><a href="#cb28-263" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-264"><a href="#cb28-264" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (gamma_val <span class="cf">in</span> <span class="fu">c</span>(<span class="fl">0.1</span>, <span class="dv">1</span>, <span class="dv">10</span>)) {</span>
<span id="cb28-265"><a href="#cb28-265" aria-hidden="true" tabindex="-1"></a>  svm_g <span class="ot">&lt;-</span> <span class="fu">svm</span>(y <span class="sc">~</span> x1 <span class="sc">+</span> x2, <span class="at">data =</span> data_nl, <span class="at">kernel =</span> <span class="st">"radial"</span>,</span>
<span id="cb28-266"><a href="#cb28-266" aria-hidden="true" tabindex="-1"></a>               <span class="at">gamma =</span> gamma_val, <span class="at">cost =</span> <span class="dv">1</span>)</span>
<span id="cb28-267"><a href="#cb28-267" aria-hidden="true" tabindex="-1"></a>  grid<span class="sc">$</span>pred <span class="ot">&lt;-</span> <span class="fu">predict</span>(svm_g, grid)</span>
<span id="cb28-268"><a href="#cb28-268" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-269"><a href="#cb28-269" aria-hidden="true" tabindex="-1"></a>  <span class="fu">plot</span>(grid<span class="sc">$</span>x1, grid<span class="sc">$</span>x2,</span>
<span id="cb28-270"><a href="#cb28-270" aria-hidden="true" tabindex="-1"></a>       <span class="at">col =</span> <span class="fu">c</span>(<span class="fu">rgb</span>(<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="fl">0.1</span>), <span class="fu">rgb</span>(<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="fl">0.1</span>))[<span class="fu">as.numeric</span>(grid<span class="sc">$</span>pred)],</span>
<span id="cb28-271"><a href="#cb28-271" aria-hidden="true" tabindex="-1"></a>       <span class="at">pch =</span> <span class="dv">15</span>, <span class="at">cex =</span> <span class="fl">0.3</span>, <span class="at">main =</span> <span class="fu">paste</span>(<span class="st">"gamma ="</span>, gamma_val))</span>
<span id="cb28-272"><a href="#cb28-272" aria-hidden="true" tabindex="-1"></a>  <span class="fu">points</span>(x1_nl, x2_nl, <span class="at">col =</span> <span class="fu">c</span>(<span class="st">"red"</span>, <span class="st">"blue"</span>)[<span class="fu">as.numeric</span>(y_nl)], <span class="at">pch =</span> <span class="dv">19</span>)</span>
<span id="cb28-273"><a href="#cb28-273" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb28-274"><a href="#cb28-274" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb28-275"><a href="#cb28-275" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-276"><a href="#cb28-276" aria-hidden="true" tabindex="-1"></a>With **small gamma**, each training point has a wide influence, creating smooth decision boundaries. This corresponds to high bias but low variance—the model may be too simple but is stable across different training samples. With **large gamma**, each training point only affects its immediate neighborhood, allowing complex, highly curved boundaries. This corresponds to low bias but high variance—the model can capture intricate patterns but may overfit to the specific training sample.</span>
<span id="cb28-277"><a href="#cb28-277" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-278"><a href="#cb28-278" aria-hidden="true" tabindex="-1"></a><span class="fu">### Cross-Validation for Parameter Selection</span></span>
<span id="cb28-279"><a href="#cb28-279" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-282"><a href="#cb28-282" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb28-283"><a href="#cb28-283" aria-hidden="true" tabindex="-1"></a><span class="co"># Grid search with cross-validation</span></span>
<span id="cb28-284"><a href="#cb28-284" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">42</span>)</span>
<span id="cb28-285"><a href="#cb28-285" aria-hidden="true" tabindex="-1"></a>tune_result <span class="ot">&lt;-</span> <span class="fu">tune</span>(svm, y <span class="sc">~</span> x1 <span class="sc">+</span> x2, <span class="at">data =</span> data_nl,</span>
<span id="cb28-286"><a href="#cb28-286" aria-hidden="true" tabindex="-1"></a>                     <span class="at">kernel =</span> <span class="st">"radial"</span>,</span>
<span id="cb28-287"><a href="#cb28-287" aria-hidden="true" tabindex="-1"></a>                     <span class="at">ranges =</span> <span class="fu">list</span>(</span>
<span id="cb28-288"><a href="#cb28-288" aria-hidden="true" tabindex="-1"></a>                       <span class="at">cost =</span> <span class="fu">c</span>(<span class="fl">0.1</span>, <span class="dv">1</span>, <span class="dv">10</span>, <span class="dv">100</span>),</span>
<span id="cb28-289"><a href="#cb28-289" aria-hidden="true" tabindex="-1"></a>                       <span class="at">gamma =</span> <span class="fu">c</span>(<span class="fl">0.1</span>, <span class="fl">0.5</span>, <span class="dv">1</span>, <span class="dv">2</span>)</span>
<span id="cb28-290"><a href="#cb28-290" aria-hidden="true" tabindex="-1"></a>                     ))</span>
<span id="cb28-291"><a href="#cb28-291" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-292"><a href="#cb28-292" aria-hidden="true" tabindex="-1"></a><span class="co"># Best parameters</span></span>
<span id="cb28-293"><a href="#cb28-293" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(tune_result<span class="sc">$</span>best.parameters)</span>
<span id="cb28-294"><a href="#cb28-294" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"</span><span class="sc">\n</span><span class="st">Best cross-validation error:"</span>, <span class="fu">round</span>(tune_result<span class="sc">$</span>best.performance, <span class="dv">4</span>), <span class="st">"</span><span class="sc">\n</span><span class="st">"</span>)</span>
<span id="cb28-295"><a href="#cb28-295" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-296"><a href="#cb28-296" aria-hidden="true" tabindex="-1"></a><span class="co"># Summary of all models</span></span>
<span id="cb28-297"><a href="#cb28-297" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(tune_result)</span>
<span id="cb28-298"><a href="#cb28-298" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb28-299"><a href="#cb28-299" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-300"><a href="#cb28-300" aria-hidden="true" tabindex="-1"></a><span class="fu">## Multi-Class SVMs</span></span>
<span id="cb28-301"><a href="#cb28-301" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-302"><a href="#cb28-302" aria-hidden="true" tabindex="-1"></a>SVMs are naturally binary classifiers. For multi-class problems, two strategies are common:</span>
<span id="cb28-303"><a href="#cb28-303" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-304"><a href="#cb28-304" aria-hidden="true" tabindex="-1"></a>**One-vs-One (OvO)**: Train $\binom{K}{2}$ classifiers for each pair of classes. Predict by majority voting.</span>
<span id="cb28-305"><a href="#cb28-305" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-306"><a href="#cb28-306" aria-hidden="true" tabindex="-1"></a>**One-vs-All (OvA)**: Train $K$ classifiers, each separating one class from all others. Predict based on highest confidence.</span>
<span id="cb28-307"><a href="#cb28-307" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-310"><a href="#cb28-310" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb28-311"><a href="#cb28-311" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-multiclass-svm</span></span>
<span id="cb28-312"><a href="#cb28-312" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "Multi-class SVM classification on the iris dataset"</span></span>
<span id="cb28-313"><a href="#cb28-313" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-width: 8</span></span>
<span id="cb28-314"><a href="#cb28-314" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-height: 6</span></span>
<span id="cb28-315"><a href="#cb28-315" aria-hidden="true" tabindex="-1"></a><span class="co"># Multi-class SVM on iris</span></span>
<span id="cb28-316"><a href="#cb28-316" aria-hidden="true" tabindex="-1"></a><span class="fu">data</span>(iris)</span>
<span id="cb28-317"><a href="#cb28-317" aria-hidden="true" tabindex="-1"></a>svm_iris <span class="ot">&lt;-</span> <span class="fu">svm</span>(Species <span class="sc">~</span> Petal.Length <span class="sc">+</span> Petal.Width, <span class="at">data =</span> iris,</span>
<span id="cb28-318"><a href="#cb28-318" aria-hidden="true" tabindex="-1"></a>                 <span class="at">kernel =</span> <span class="st">"radial"</span>, <span class="at">gamma =</span> <span class="fl">0.5</span>, <span class="at">cost =</span> <span class="dv">1</span>)</span>
<span id="cb28-319"><a href="#cb28-319" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-320"><a href="#cb28-320" aria-hidden="true" tabindex="-1"></a><span class="co"># Create grid</span></span>
<span id="cb28-321"><a href="#cb28-321" aria-hidden="true" tabindex="-1"></a>pl_seq <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="dv">0</span>, <span class="dv">7</span>, <span class="at">length.out =</span> <span class="dv">100</span>)</span>
<span id="cb28-322"><a href="#cb28-322" aria-hidden="true" tabindex="-1"></a>pw_seq <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="dv">0</span>, <span class="dv">3</span>, <span class="at">length.out =</span> <span class="dv">100</span>)</span>
<span id="cb28-323"><a href="#cb28-323" aria-hidden="true" tabindex="-1"></a>grid_iris <span class="ot">&lt;-</span> <span class="fu">expand.grid</span>(<span class="at">Petal.Length =</span> pl_seq, <span class="at">Petal.Width =</span> pw_seq)</span>
<span id="cb28-324"><a href="#cb28-324" aria-hidden="true" tabindex="-1"></a>grid_iris<span class="sc">$</span>pred <span class="ot">&lt;-</span> <span class="fu">predict</span>(svm_iris, grid_iris)</span>
<span id="cb28-325"><a href="#cb28-325" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-326"><a href="#cb28-326" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot</span></span>
<span id="cb28-327"><a href="#cb28-327" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(grid_iris<span class="sc">$</span>Petal.Length, grid_iris<span class="sc">$</span>Petal.Width,</span>
<span id="cb28-328"><a href="#cb28-328" aria-hidden="true" tabindex="-1"></a>     <span class="at">col =</span> <span class="fu">c</span>(<span class="fu">rgb</span>(<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="fl">0.1</span>), <span class="fu">rgb</span>(<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="fl">0.1</span>), <span class="fu">rgb</span>(<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="fl">0.1</span>))[grid_iris<span class="sc">$</span>pred],</span>
<span id="cb28-329"><a href="#cb28-329" aria-hidden="true" tabindex="-1"></a>     <span class="at">pch =</span> <span class="dv">15</span>, <span class="at">cex =</span> <span class="fl">0.3</span>,</span>
<span id="cb28-330"><a href="#cb28-330" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">"Petal Length"</span>, <span class="at">ylab =</span> <span class="st">"Petal Width"</span>,</span>
<span id="cb28-331"><a href="#cb28-331" aria-hidden="true" tabindex="-1"></a>     <span class="at">main =</span> <span class="st">"Multi-Class SVM on Iris Data"</span>)</span>
<span id="cb28-332"><a href="#cb28-332" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span>(iris<span class="sc">$</span>Petal.Length, iris<span class="sc">$</span>Petal.Width,</span>
<span id="cb28-333"><a href="#cb28-333" aria-hidden="true" tabindex="-1"></a>       <span class="at">col =</span> <span class="fu">c</span>(<span class="st">"red"</span>, <span class="st">"green"</span>, <span class="st">"blue"</span>)[iris<span class="sc">$</span>Species], <span class="at">pch =</span> <span class="dv">19</span>)</span>
<span id="cb28-334"><a href="#cb28-334" aria-hidden="true" tabindex="-1"></a><span class="fu">legend</span>(<span class="st">"topleft"</span>, <span class="fu">levels</span>(iris<span class="sc">$</span>Species),</span>
<span id="cb28-335"><a href="#cb28-335" aria-hidden="true" tabindex="-1"></a>       <span class="at">col =</span> <span class="fu">c</span>(<span class="st">"red"</span>, <span class="st">"green"</span>, <span class="st">"blue"</span>), <span class="at">pch =</span> <span class="dv">19</span>)</span>
<span id="cb28-336"><a href="#cb28-336" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-337"><a href="#cb28-337" aria-hidden="true" tabindex="-1"></a><span class="co"># Confusion matrix</span></span>
<span id="cb28-338"><a href="#cb28-338" aria-hidden="true" tabindex="-1"></a>pred_iris <span class="ot">&lt;-</span> <span class="fu">predict</span>(svm_iris, iris)</span>
<span id="cb28-339"><a href="#cb28-339" aria-hidden="true" tabindex="-1"></a><span class="fu">table</span>(<span class="at">Predicted =</span> pred_iris, <span class="at">Actual =</span> iris<span class="sc">$</span>Species)</span>
<span id="cb28-340"><a href="#cb28-340" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb28-341"><a href="#cb28-341" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-342"><a href="#cb28-342" aria-hidden="true" tabindex="-1"></a><span class="fu">## Support Vector Regression (SVR)</span></span>
<span id="cb28-343"><a href="#cb28-343" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-344"><a href="#cb28-344" aria-hidden="true" tabindex="-1"></a>SVMs can be extended to regression problems. **Support Vector Regression (SVR)** fits a tube of width $\epsilon$ around the data, ignoring errors within the tube:</span>
<span id="cb28-345"><a href="#cb28-345" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-348"><a href="#cb28-348" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb28-349"><a href="#cb28-349" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-svr</span></span>
<span id="cb28-350"><a href="#cb28-350" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "Support Vector Regression fits a tube around the data. Only points outside the tube affect the model."</span></span>
<span id="cb28-351"><a href="#cb28-351" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-width: 7</span></span>
<span id="cb28-352"><a href="#cb28-352" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-height: 5</span></span>
<span id="cb28-353"><a href="#cb28-353" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate regression data</span></span>
<span id="cb28-354"><a href="#cb28-354" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">42</span>)</span>
<span id="cb28-355"><a href="#cb28-355" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">100</span></span>
<span id="cb28-356"><a href="#cb28-356" aria-hidden="true" tabindex="-1"></a>x_reg <span class="ot">&lt;-</span> <span class="fu">sort</span>(<span class="fu">runif</span>(n, <span class="dv">0</span>, <span class="dv">10</span>))</span>
<span id="cb28-357"><a href="#cb28-357" aria-hidden="true" tabindex="-1"></a>y_reg <span class="ot">&lt;-</span> <span class="fu">sin</span>(x_reg) <span class="sc">+</span> <span class="fu">rnorm</span>(n, <span class="at">sd =</span> <span class="fl">0.3</span>)</span>
<span id="cb28-358"><a href="#cb28-358" aria-hidden="true" tabindex="-1"></a>data_reg <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">x =</span> x_reg, <span class="at">y =</span> y_reg)</span>
<span id="cb28-359"><a href="#cb28-359" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-360"><a href="#cb28-360" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit SVR</span></span>
<span id="cb28-361"><a href="#cb28-361" aria-hidden="true" tabindex="-1"></a>svr_fit <span class="ot">&lt;-</span> <span class="fu">svm</span>(y <span class="sc">~</span> x, <span class="at">data =</span> data_reg, <span class="at">type =</span> <span class="st">"eps-regression"</span>,</span>
<span id="cb28-362"><a href="#cb28-362" aria-hidden="true" tabindex="-1"></a>                <span class="at">kernel =</span> <span class="st">"radial"</span>, <span class="at">gamma =</span> <span class="fl">0.5</span>, <span class="at">epsilon =</span> <span class="fl">0.3</span>)</span>
<span id="cb28-363"><a href="#cb28-363" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-364"><a href="#cb28-364" aria-hidden="true" tabindex="-1"></a><span class="co"># Predictions</span></span>
<span id="cb28-365"><a href="#cb28-365" aria-hidden="true" tabindex="-1"></a>x_grid <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="dv">0</span>, <span class="dv">10</span>, <span class="at">length.out =</span> <span class="dv">200</span>)</span>
<span id="cb28-366"><a href="#cb28-366" aria-hidden="true" tabindex="-1"></a>y_pred <span class="ot">&lt;-</span> <span class="fu">predict</span>(svr_fit, <span class="at">newdata =</span> <span class="fu">data.frame</span>(<span class="at">x =</span> x_grid))</span>
<span id="cb28-367"><a href="#cb28-367" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-368"><a href="#cb28-368" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot</span></span>
<span id="cb28-369"><a href="#cb28-369" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(x_reg, y_reg, <span class="at">pch =</span> <span class="dv">16</span>, <span class="at">col =</span> <span class="st">"gray50"</span>,</span>
<span id="cb28-370"><a href="#cb28-370" aria-hidden="true" tabindex="-1"></a>     <span class="at">main =</span> <span class="st">"Support Vector Regression"</span>)</span>
<span id="cb28-371"><a href="#cb28-371" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(x_grid, y_pred, <span class="at">col =</span> <span class="st">"blue"</span>, <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb28-372"><a href="#cb28-372" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(x_grid, y_pred <span class="sc">+</span> <span class="fl">0.3</span>, <span class="at">col =</span> <span class="st">"blue"</span>, <span class="at">lty =</span> <span class="dv">2</span>)  <span class="co"># Epsilon tube</span></span>
<span id="cb28-373"><a href="#cb28-373" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(x_grid, y_pred <span class="sc">-</span> <span class="fl">0.3</span>, <span class="at">col =</span> <span class="st">"blue"</span>, <span class="at">lty =</span> <span class="dv">2</span>)</span>
<span id="cb28-374"><a href="#cb28-374" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(x_grid, <span class="fu">sin</span>(x_grid), <span class="at">col =</span> <span class="st">"red"</span>, <span class="at">lwd =</span> <span class="dv">2</span>, <span class="at">lty =</span> <span class="dv">2</span>)</span>
<span id="cb28-375"><a href="#cb28-375" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-376"><a href="#cb28-376" aria-hidden="true" tabindex="-1"></a><span class="co"># Highlight support vectors</span></span>
<span id="cb28-377"><a href="#cb28-377" aria-hidden="true" tabindex="-1"></a>sv_idx <span class="ot">&lt;-</span> svr_fit<span class="sc">$</span>index</span>
<span id="cb28-378"><a href="#cb28-378" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span>(x_reg[sv_idx], y_reg[sv_idx], <span class="at">col =</span> <span class="st">"black"</span>, <span class="at">cex =</span> <span class="fl">1.5</span>, <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb28-379"><a href="#cb28-379" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-380"><a href="#cb28-380" aria-hidden="true" tabindex="-1"></a><span class="fu">legend</span>(<span class="st">"topright"</span>, <span class="fu">c</span>(<span class="st">"Data"</span>, <span class="st">"SVR fit"</span>, <span class="st">"Epsilon tube"</span>, <span class="st">"True function"</span>, <span class="st">"Support vectors"</span>),</span>
<span id="cb28-381"><a href="#cb28-381" aria-hidden="true" tabindex="-1"></a>       <span class="at">col =</span> <span class="fu">c</span>(<span class="st">"gray50"</span>, <span class="st">"blue"</span>, <span class="st">"blue"</span>, <span class="st">"red"</span>, <span class="st">"black"</span>),</span>
<span id="cb28-382"><a href="#cb28-382" aria-hidden="true" tabindex="-1"></a>       <span class="at">pch =</span> <span class="fu">c</span>(<span class="dv">16</span>, <span class="cn">NA</span>, <span class="cn">NA</span>, <span class="cn">NA</span>, <span class="dv">1</span>), <span class="at">lty =</span> <span class="fu">c</span>(<span class="cn">NA</span>, <span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">2</span>, <span class="cn">NA</span>), <span class="at">lwd =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">2</span>))</span>
<span id="cb28-383"><a href="#cb28-383" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb28-384"><a href="#cb28-384" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-385"><a href="#cb28-385" aria-hidden="true" tabindex="-1"></a><span class="fu">## Practical Considerations</span></span>
<span id="cb28-386"><a href="#cb28-386" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-387"><a href="#cb28-387" aria-hidden="true" tabindex="-1"></a><span class="fu">### Feature Scaling</span></span>
<span id="cb28-388"><a href="#cb28-388" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-389"><a href="#cb28-389" aria-hidden="true" tabindex="-1"></a>SVMs are sensitive to feature scales. Always standardize features before training:</span>
<span id="cb28-390"><a href="#cb28-390" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-393"><a href="#cb28-393" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb28-394"><a href="#cb28-394" aria-hidden="true" tabindex="-1"></a><span class="co"># With scaling (default)</span></span>
<span id="cb28-395"><a href="#cb28-395" aria-hidden="true" tabindex="-1"></a>svm_scaled <span class="ot">&lt;-</span> <span class="fu">svm</span>(Species <span class="sc">~</span> ., <span class="at">data =</span> iris, <span class="at">scale =</span> <span class="cn">TRUE</span>)</span>
<span id="cb28-396"><a href="#cb28-396" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-397"><a href="#cb28-397" aria-hidden="true" tabindex="-1"></a><span class="co"># Without scaling (not recommended)</span></span>
<span id="cb28-398"><a href="#cb28-398" aria-hidden="true" tabindex="-1"></a><span class="co"># svm_unscaled &lt;- svm(Species ~ ., data = iris, scale = FALSE)</span></span>
<span id="cb28-399"><a href="#cb28-399" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb28-400"><a href="#cb28-400" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-401"><a href="#cb28-401" aria-hidden="true" tabindex="-1"></a><span class="fu">### Probability Estimates</span></span>
<span id="cb28-402"><a href="#cb28-402" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-403"><a href="#cb28-403" aria-hidden="true" tabindex="-1"></a>By default, SVMs output only class predictions. However, you can obtain probability estimates using **Platt scaling**, which fits a sigmoid function to the decision values:</span>
<span id="cb28-404"><a href="#cb28-404" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-407"><a href="#cb28-407" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb28-408"><a href="#cb28-408" aria-hidden="true" tabindex="-1"></a><span class="co"># Train SVM with probability estimates enabled</span></span>
<span id="cb28-409"><a href="#cb28-409" aria-hidden="true" tabindex="-1"></a>svm_prob <span class="ot">&lt;-</span> <span class="fu">svm</span>(Species <span class="sc">~</span> ., <span class="at">data =</span> iris,</span>
<span id="cb28-410"><a href="#cb28-410" aria-hidden="true" tabindex="-1"></a>                <span class="at">kernel =</span> <span class="st">"radial"</span>,</span>
<span id="cb28-411"><a href="#cb28-411" aria-hidden="true" tabindex="-1"></a>                <span class="at">probability =</span> <span class="cn">TRUE</span>)</span>
<span id="cb28-412"><a href="#cb28-412" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-413"><a href="#cb28-413" aria-hidden="true" tabindex="-1"></a><span class="co"># Predict with probabilities</span></span>
<span id="cb28-414"><a href="#cb28-414" aria-hidden="true" tabindex="-1"></a>pred_prob <span class="ot">&lt;-</span> <span class="fu">predict</span>(svm_prob, iris, <span class="at">probability =</span> <span class="cn">TRUE</span>)</span>
<span id="cb28-415"><a href="#cb28-415" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-416"><a href="#cb28-416" aria-hidden="true" tabindex="-1"></a><span class="co"># Extract probability matrix</span></span>
<span id="cb28-417"><a href="#cb28-417" aria-hidden="true" tabindex="-1"></a>probs <span class="ot">&lt;-</span> <span class="fu">attr</span>(pred_prob, <span class="st">"probabilities"</span>)</span>
<span id="cb28-418"><a href="#cb28-418" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(probs)</span>
<span id="cb28-419"><a href="#cb28-419" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb28-420"><a href="#cb28-420" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-421"><a href="#cb28-421" aria-hidden="true" tabindex="-1"></a>Platt scaling works by fitting a logistic regression model to the SVM's decision values after training. While this provides calibrated probabilities, it adds computational overhead and may be less reliable when training data is limited. For applications requiring well-calibrated probabilities (such as medical diagnosis where you need to communicate uncertainty), consider whether logistic regression or other natively probabilistic methods might be more appropriate.</span>
<span id="cb28-422"><a href="#cb28-422" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-423"><a href="#cb28-423" aria-hidden="true" tabindex="-1"></a><span class="fu">### Decision Values</span></span>
<span id="cb28-424"><a href="#cb28-424" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-425"><a href="#cb28-425" aria-hidden="true" tabindex="-1"></a>For more detailed analysis, you can extract the raw **decision values**—the signed distances from the decision boundary:</span>
<span id="cb28-426"><a href="#cb28-426" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-429"><a href="#cb28-429" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb28-430"><a href="#cb28-430" aria-hidden="true" tabindex="-1"></a><span class="co"># Get decision values</span></span>
<span id="cb28-431"><a href="#cb28-431" aria-hidden="true" tabindex="-1"></a>pred_dv <span class="ot">&lt;-</span> <span class="fu">predict</span>(svm_prob, iris, <span class="at">decision.values =</span> <span class="cn">TRUE</span>)</span>
<span id="cb28-432"><a href="#cb28-432" aria-hidden="true" tabindex="-1"></a>decision_vals <span class="ot">&lt;-</span> <span class="fu">attr</span>(pred_dv, <span class="st">"decision.values"</span>)</span>
<span id="cb28-433"><a href="#cb28-433" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(decision_vals)</span>
<span id="cb28-434"><a href="#cb28-434" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb28-435"><a href="#cb28-435" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-436"><a href="#cb28-436" aria-hidden="true" tabindex="-1"></a>For a two-class problem, the decision value represents how far and on which side of the hyperplane an observation lies. Larger absolute values indicate higher confidence in the classification. For multi-class problems using one-vs-one classification, you get decision values for each pair of classes.</span>
<span id="cb28-437"><a href="#cb28-437" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-438"><a href="#cb28-438" aria-hidden="true" tabindex="-1"></a><span class="fu">### Choosing the Kernel</span></span>
<span id="cb28-439"><a href="#cb28-439" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-440"><a href="#cb28-440" aria-hidden="true" tabindex="-1"></a><span class="fu">### Kernel Selection Guidelines</span></span>
<span id="cb28-441"><a href="#cb28-441" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-442"><a href="#cb28-442" aria-hidden="true" tabindex="-1"></a>The choice of kernel depends on your data and prior knowledge about the problem.</span>
<span id="cb28-443"><a href="#cb28-443" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-444"><a href="#cb28-444" aria-hidden="true" tabindex="-1"></a>**Start with the RBF kernel** as a sensible default—it handles most non-linear problems well and has the flexibility to approximate a wide range of decision boundaries. However, **try a linear kernel first** if you have many features relative to samples, as the data may already be linearly separable in the high-dimensional feature space without needing additional transformation. Linear kernels are also faster to train and easier to interpret.</span>
<span id="cb28-445"><a href="#cb28-445" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-446"><a href="#cb28-446" aria-hidden="true" tabindex="-1"></a>**Polynomial kernels** are useful when you have domain knowledge suggesting polynomial relationships—for instance, when interactions between features are expected to matter. **Custom kernels** allow you to encode domain-specific knowledge; string kernels for biological sequences, for example, measure similarity based on shared subsequences rather than Euclidean distance.</span>
<span id="cb28-447"><a href="#cb28-447" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-448"><a href="#cb28-448" aria-hidden="true" tabindex="-1"></a><span class="fu">### Advantages and Limitations</span></span>
<span id="cb28-449"><a href="#cb28-449" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-450"><a href="#cb28-450" aria-hidden="true" tabindex="-1"></a>SVMs have several important advantages. They are effective in high-dimensional spaces, often working well even when the number of features exceeds the number of samples. They are memory efficient because the final model depends only on the support vectors, not all training observations. They are versatile through the choice of kernel, allowing them to learn complex non-linear boundaries. And the maximum margin principle provides a form of regularization that often leads to good generalization.</span>
<span id="cb28-451"><a href="#cb28-451" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-452"><a href="#cb28-452" aria-hidden="true" tabindex="-1"></a>However, SVMs also have significant limitations. Training time scales as $O(n^2)$ to $O(n^3)$ with the number of observations, making them impractical for very large datasets. They are sensitive to feature scaling—variables must be standardized before training. They do not naturally provide probability estimates (though these can be obtained through additional computation). And selecting the appropriate kernel and tuning the parameters requires care, often necessitating extensive cross-validation.</span>
<span id="cb28-453"><a href="#cb28-453" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-454"><a href="#cb28-454" aria-hidden="true" tabindex="-1"></a><span class="fu">### Comparison with Other Methods</span></span>
<span id="cb28-455"><a href="#cb28-455" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-456"><a href="#cb28-456" aria-hidden="true" tabindex="-1"></a>How do SVMs compare to other classification methods? Each approach has different strengths:</span>
<span id="cb28-457"><a href="#cb28-457" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-458"><a href="#cb28-458" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> Aspect <span class="pp">|</span> SVM <span class="pp">|</span> Random Forest <span class="pp">|</span> Logistic Regression <span class="pp">|</span> Neural Networks <span class="pp">|</span></span>
<span id="cb28-459"><a href="#cb28-459" aria-hidden="true" tabindex="-1"></a><span class="pp">|--------|-----|---------------|---------------------|-----------------|</span></span>
<span id="cb28-460"><a href="#cb28-460" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **Interpretability** <span class="pp">|</span> Low (except linear) <span class="pp">|</span> Medium (variable importance) <span class="pp">|</span> High (coefficients) <span class="pp">|</span> Low <span class="pp">|</span></span>
<span id="cb28-461"><a href="#cb28-461" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **Probability estimates** <span class="pp">|</span> Requires Platt scaling <span class="pp">|</span> Natural (vote proportions) <span class="pp">|</span> Natural <span class="pp">|</span> Natural (softmax) <span class="pp">|</span></span>
<span id="cb28-462"><a href="#cb28-462" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **High dimensions** <span class="pp">|</span> Excellent <span class="pp">|</span> Good <span class="pp">|</span> Good with regularization <span class="pp">|</span> Needs many samples <span class="pp">|</span></span>
<span id="cb28-463"><a href="#cb28-463" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **Non-linear boundaries** <span class="pp">|</span> Via kernels <span class="pp">|</span> Natural <span class="pp">|</span> Needs feature engineering <span class="pp">|</span> Natural <span class="pp">|</span></span>
<span id="cb28-464"><a href="#cb28-464" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **Training speed** <span class="pp">|</span> Slow for large n <span class="pp">|</span> Fast (parallelizable) <span class="pp">|</span> Very fast <span class="pp">|</span> Slow <span class="pp">|</span></span>
<span id="cb28-465"><a href="#cb28-465" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **Hyperparameters** <span class="pp">|</span> C, kernel params <span class="pp">|</span> Trees, depth, features <span class="pp">|</span> Regularization <span class="pp">|</span> Many (architecture) <span class="pp">|</span></span>
<span id="cb28-466"><a href="#cb28-466" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **Robustness to noise** <span class="pp">|</span> Moderate <span class="pp">|</span> Excellent <span class="pp">|</span> Moderate <span class="pp">|</span> Sensitive <span class="pp">|</span></span>
<span id="cb28-467"><a href="#cb28-467" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-468"><a href="#cb28-468" aria-hidden="true" tabindex="-1"></a>Choose **SVMs** when you have limited samples, many features, and need good generalization. Choose **Random Forests** when you need variable importance, robustness, or have many observations. Choose **Logistic Regression** when interpretability is paramount and relationships are approximately linear. Choose **Neural Networks** when you have massive data and complex patterns to capture.</span>
<span id="cb28-469"><a href="#cb28-469" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-470"><a href="#cb28-470" aria-hidden="true" tabindex="-1"></a><span class="fu">### Feature Importance</span></span>
<span id="cb28-471"><a href="#cb28-471" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-472"><a href="#cb28-472" aria-hidden="true" tabindex="-1"></a>Unlike random forests, SVMs don't provide built-in variable importance measures. However, several approaches can help interpret which features matter:</span>
<span id="cb28-473"><a href="#cb28-473" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-474"><a href="#cb28-474" aria-hidden="true" tabindex="-1"></a>For **linear SVMs**, the magnitude of the weight coefficients indicates feature importance:</span>
<span id="cb28-475"><a href="#cb28-475" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-478"><a href="#cb28-478" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb28-479"><a href="#cb28-479" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit linear SVM</span></span>
<span id="cb28-480"><a href="#cb28-480" aria-hidden="true" tabindex="-1"></a>svm_linear_iris <span class="ot">&lt;-</span> <span class="fu">svm</span>(Species <span class="sc">~</span> ., <span class="at">data =</span> iris, <span class="at">kernel =</span> <span class="st">"linear"</span>, <span class="at">scale =</span> <span class="cn">TRUE</span>)</span>
<span id="cb28-481"><a href="#cb28-481" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-482"><a href="#cb28-482" aria-hidden="true" tabindex="-1"></a><span class="co"># Extract weights (for binary classification)</span></span>
<span id="cb28-483"><a href="#cb28-483" aria-hidden="true" tabindex="-1"></a><span class="co"># For multi-class, examine each one-vs-one classifier</span></span>
<span id="cb28-484"><a href="#cb28-484" aria-hidden="true" tabindex="-1"></a><span class="co"># Here we use a simpler 2-class example</span></span>
<span id="cb28-485"><a href="#cb28-485" aria-hidden="true" tabindex="-1"></a>iris_binary <span class="ot">&lt;-</span> iris[iris<span class="sc">$</span>Species <span class="sc">!=</span> <span class="st">"setosa"</span>,]</span>
<span id="cb28-486"><a href="#cb28-486" aria-hidden="true" tabindex="-1"></a>iris_binary<span class="sc">$</span>Species <span class="ot">&lt;-</span> <span class="fu">droplevels</span>(iris_binary<span class="sc">$</span>Species)</span>
<span id="cb28-487"><a href="#cb28-487" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-488"><a href="#cb28-488" aria-hidden="true" tabindex="-1"></a>svm_binary <span class="ot">&lt;-</span> <span class="fu">svm</span>(Species <span class="sc">~</span> ., <span class="at">data =</span> iris_binary, <span class="at">kernel =</span> <span class="st">"linear"</span>, <span class="at">scale =</span> <span class="cn">TRUE</span>)</span>
<span id="cb28-489"><a href="#cb28-489" aria-hidden="true" tabindex="-1"></a>w <span class="ot">&lt;-</span> <span class="fu">t</span>(svm_binary<span class="sc">$</span>coefs) <span class="sc">%*%</span> svm_binary<span class="sc">$</span>SV</span>
<span id="cb28-490"><a href="#cb28-490" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"Feature weights (absolute values indicate importance):</span><span class="sc">\n</span><span class="st">"</span>)</span>
<span id="cb28-491"><a href="#cb28-491" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(<span class="fu">abs</span>(w))</span>
<span id="cb28-492"><a href="#cb28-492" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb28-493"><a href="#cb28-493" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-494"><a href="#cb28-494" aria-hidden="true" tabindex="-1"></a>For **non-linear SVMs**, permutation importance provides a model-agnostic approach—shuffle each feature and measure the decrease in accuracy. Alternatively, you can examine which features have the most support vectors in high-density regions.</span>
<span id="cb28-495"><a href="#cb28-495" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-496"><a href="#cb28-496" aria-hidden="true" tabindex="-1"></a><span class="fu">### Scaling for Large Datasets</span></span>
<span id="cb28-497"><a href="#cb28-497" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-498"><a href="#cb28-498" aria-hidden="true" tabindex="-1"></a>Standard SVM implementations struggle with large datasets due to their $O(n^2)$ to $O(n^3)$ training complexity. For datasets with many observations, consider these alternatives.</span>
<span id="cb28-499"><a href="#cb28-499" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-500"><a href="#cb28-500" aria-hidden="true" tabindex="-1"></a>The **LiblineaR** package implements linear SVMs that scale to millions of observations. It uses a coordinate descent algorithm that is much faster than the standard quadratic programming approach:</span>
<span id="cb28-503"><a href="#cb28-503" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb28-504"><a href="#cb28-504" aria-hidden="true" tabindex="-1"></a><span class="co">#| eval: false</span></span>
<span id="cb28-505"><a href="#cb28-505" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(LiblineaR)</span>
<span id="cb28-506"><a href="#cb28-506" aria-hidden="true" tabindex="-1"></a><span class="co"># Much faster for large datasets with linear kernel</span></span>
<span id="cb28-507"><a href="#cb28-507" aria-hidden="true" tabindex="-1"></a>model <span class="ot">&lt;-</span> <span class="fu">LiblineaR</span>(<span class="at">data =</span> X_train, <span class="at">target =</span> y_train, <span class="at">type =</span> <span class="dv">1</span>)</span>
<span id="cb28-508"><a href="#cb28-508" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb28-509"><a href="#cb28-509" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-510"><a href="#cb28-510" aria-hidden="true" tabindex="-1"></a>**Stochastic gradient descent** approaches approximate the SVM solution by processing data in batches, making them suitable for online learning and very large datasets.</span>
<span id="cb28-511"><a href="#cb28-511" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-512"><a href="#cb28-512" aria-hidden="true" tabindex="-1"></a>For kernel SVMs on large data, **random Fourier features** or **Nyström approximation** can approximate the kernel mapping, allowing linear methods to be applied to the transformed features.</span>
<span id="cb28-513"><a href="#cb28-513" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-514"><a href="#cb28-514" aria-hidden="true" tabindex="-1"></a><span class="fu">## Exercises</span></span>
<span id="cb28-515"><a href="#cb28-515" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-516"><a href="#cb28-516" aria-hidden="true" tabindex="-1"></a>::: {.callout-note}</span>
<span id="cb28-517"><a href="#cb28-517" aria-hidden="true" tabindex="-1"></a><span class="fu">### Exercise SVM.1: Linear SVM</span></span>
<span id="cb28-518"><a href="#cb28-518" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-519"><a href="#cb28-519" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>Generate linearly separable data in two dimensions. Fit an SVM with a linear kernel.</span>
<span id="cb28-520"><a href="#cb28-520" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-521"><a href="#cb28-521" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>Identify the support vectors. How many are there?</span>
<span id="cb28-522"><a href="#cb28-522" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-523"><a href="#cb28-523" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>Add noise to make the classes overlap. How does the number of support vectors change with different values of C?</span>
<span id="cb28-524"><a href="#cb28-524" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb28-525"><a href="#cb28-525" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-526"><a href="#cb28-526" aria-hidden="true" tabindex="-1"></a>::: {.callout-note}</span>
<span id="cb28-527"><a href="#cb28-527" aria-hidden="true" tabindex="-1"></a><span class="fu">### Exercise SVM.2: Non-Linear Classification</span></span>
<span id="cb28-528"><a href="#cb28-528" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-529"><a href="#cb28-529" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>Generate data that requires a non-linear decision boundary (e.g., circles or moons).</span>
<span id="cb28-530"><a href="#cb28-530" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-531"><a href="#cb28-531" aria-hidden="true" tabindex="-1"></a><span class="ss">5. </span>Fit SVMs with polynomial and RBF kernels. Compare performance.</span>
<span id="cb28-532"><a href="#cb28-532" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-533"><a href="#cb28-533" aria-hidden="true" tabindex="-1"></a><span class="ss">6. </span>Use cross-validation to select optimal parameters (C, gamma or degree).</span>
<span id="cb28-534"><a href="#cb28-534" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb28-535"><a href="#cb28-535" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-536"><a href="#cb28-536" aria-hidden="true" tabindex="-1"></a>::: {.callout-note}</span>
<span id="cb28-537"><a href="#cb28-537" aria-hidden="true" tabindex="-1"></a><span class="fu">### Exercise SVM.3: Real Data</span></span>
<span id="cb28-538"><a href="#cb28-538" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-539"><a href="#cb28-539" aria-hidden="true" tabindex="-1"></a><span class="ss">7. </span>Load the <span class="in">`tissue_gene_expression`</span> dataset. Use SVM to classify tissue types.</span>
<span id="cb28-540"><a href="#cb28-540" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-541"><a href="#cb28-541" aria-hidden="true" tabindex="-1"></a><span class="ss">8. </span>Compare linear and RBF kernels. Which performs better?</span>
<span id="cb28-542"><a href="#cb28-542" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-543"><a href="#cb28-543" aria-hidden="true" tabindex="-1"></a><span class="ss">9. </span>How does SVM compare to random forests on this data?</span>
<span id="cb28-544"><a href="#cb28-544" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb28-545"><a href="#cb28-545" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-546"><a href="#cb28-546" aria-hidden="true" tabindex="-1"></a><span class="fu">## Summary</span></span>
<span id="cb28-547"><a href="#cb28-547" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-548"><a href="#cb28-548" aria-hidden="true" tabindex="-1"></a>This chapter introduced **Support Vector Machines**, a powerful class of algorithms that find optimal separating hyperplanes with maximum margin. The fundamental idea is to choose the decision boundary that is as far as possible from the nearest training points of each class.</span>
<span id="cb28-549"><a href="#cb28-549" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-550"><a href="#cb28-550" aria-hidden="true" tabindex="-1"></a>**Support vectors** are the training points that lie on the margin boundary and determine the decision boundary. All other points are irrelevant to the model, making SVMs sparse and efficient at prediction time. The **cost parameter C** controls the tradeoff between margin width and the number of violations allowed—larger C means fewer violations but risks overfitting.</span>
<span id="cb28-551"><a href="#cb28-551" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-552"><a href="#cb28-552" aria-hidden="true" tabindex="-1"></a>The **kernel trick** extends SVMs to non-linear decision boundaries by implicitly mapping data to higher-dimensional spaces. The linear kernel corresponds to the standard support vector classifier. The polynomial kernel creates polynomial boundaries of specified degree. The RBF (radial basis function) kernel is the most commonly used choice for non-linear problems, creating flexible, localized decision boundaries. The **gamma parameter** ($\gamma$) controls the RBF kernel width—larger values produce more complex boundaries with higher variance.</span>
<span id="cb28-553"><a href="#cb28-553" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-554"><a href="#cb28-554" aria-hidden="true" tabindex="-1"></a>**Cross-validation** is essential for selecting the parameters C and gamma, since both substantially affect model performance. SVMs extend naturally to **multi-class** problems through one-vs-one or one-vs-all strategies, and to **regression** through Support Vector Regression (SVR). **Feature scaling** is required before training because SVMs are sensitive to the relative scales of different variables. While SVMs work remarkably well in high dimensions, their training time scales poorly with the number of observations, limiting their applicability to very large datasets.</span>
<span id="cb28-555"><a href="#cb28-555" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-556"><a href="#cb28-556" aria-hidden="true" tabindex="-1"></a><span class="fu">## Additional Resources</span></span>
<span id="cb28-557"><a href="#cb28-557" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-558"><a href="#cb28-558" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>@james2023islr - Accessible introduction to SVMs</span>
<span id="cb28-559"><a href="#cb28-559" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>@hastie2009elements - Theoretical foundations</span>
<span id="cb28-560"><a href="#cb28-560" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Cristianini &amp; Shawe-Taylor (2000). *An Introduction to Support Vector Machines* - Classic reference</span>
<span id="cb28-561"><a href="#cb28-561" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="in">`e1071`</span> and <span class="in">`kernlab`</span> package documentation</span>
</code><button title="Copy to Clipboard" class="code-copy-button" data-in-quarto-modal=""><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p>Statistics for Biosciences and Bioengineering</p>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    </div>
    <div class="nav-footer-right">
<p>Built with <a href="https://quarto.org/">Quarto</a></p>
</div>
  </div>
</footer>




</body></html>