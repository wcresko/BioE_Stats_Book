<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.30">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>30&nbsp; Model Validation and the Bias-Variance Tradeoff – Statistics for the Biosciences and Bioengineering</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../chapters/30-regularization.html" rel="next">
<link href="../chapters/28-intro-statistical-learning.html" rel="prev">
<script src="../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-de070a7b0ab54f8780927367ac907214.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap-485d01fc63b59abcd3ee1bf1e8e2748d.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar floating quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../chapters/28-intro-statistical-learning.html">Statistical Learning</a></li><li class="breadcrumb-item"><a href="../chapters/29-model-validation.html"><span class="chapter-number">30</span>&nbsp; <span class="chapter-title">Model Validation and the Bias-Variance Tradeoff</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../">Statistics for the Biosciences and Bioengineering</a> 
        <div class="sidebar-tools-main">
  <a href="" class="quarto-reader-toggle quarto-navigation-tool px-1" onclick="window.quartoToggleReader(); return false;" title="Toggle reader mode">
  <div class="quarto-reader-toggle-btn">
  <i class="bi"></i>
  </div>
</a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Why This Book?</span></span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">Core Data Science Tools</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/01-introduction.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Introduction</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/02-installing-tools.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Installing Core Tools</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/03-unix-command-line.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Unix and the Command Line</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/04-r-rstudio.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">R and RStudio</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/05-markdown-latex.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Markdown and LaTeX</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true">
 <span class="menu-text">Data Exploration</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/06-tidy-data.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Tidy Data</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/07-data-wrangling.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Data Wrangling with dplyr</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/08-exploratory-data-analysis.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Exploratory Data Analysis</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/09-data-visualization.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Data Visualization</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true">
 <span class="menu-text">Probability and Distributions</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/10-probability-foundations.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Foundations of Probability</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/11-discrete-distributions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Discrete Probability Distributions</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/12-continuous-distributions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Continuous Probability Distributions</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/13-sampling-estimation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Sampling and Parameter Estimation</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true">
 <span class="menu-text">Statistical Inference</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/14-experimental-design.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Experimental Design Principles</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/15-hypothesis-testing.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Hypothesis Testing</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/16-t-tests.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">T-Tests</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/17-nonparametric-tests.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Nonparametric Tests</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/18-bootstrapping.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">Resampling Methods</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="true">
 <span class="menu-text">Linear Models</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/19-what-are-models.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">What are Models?</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/20-correlation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">21</span>&nbsp; <span class="chapter-title">Correlation</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/21-simple-linear-regression.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">22</span>&nbsp; <span class="chapter-title">Simple Linear Regression</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/22-residual-analysis.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">23</span>&nbsp; <span class="chapter-title">Residual Analysis</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/23-statistical-power.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">24</span>&nbsp; <span class="chapter-title">Statistical Power</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/24-multiple-regression.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">25</span>&nbsp; <span class="chapter-title">Multiple Regression</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/25-single-factor-anova.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">26</span>&nbsp; <span class="chapter-title">Single Factor ANOVA</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/26-multifactor-anova.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">27</span>&nbsp; <span class="chapter-title">Multi-Factor ANOVA</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/27-glm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">28</span>&nbsp; <span class="chapter-title">Generalized Linear Models</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="true">
 <span class="menu-text">Statistical Learning</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-6" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/28-intro-statistical-learning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">29</span>&nbsp; <span class="chapter-title">Introduction to Statistical Learning</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/29-model-validation.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">30</span>&nbsp; <span class="chapter-title">Model Validation and the Bias-Variance Tradeoff</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/30-regularization.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">31</span>&nbsp; <span class="chapter-title">Regularization Methods</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/31-smoothing.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">32</span>&nbsp; <span class="chapter-title">Smoothing and Non-Parametric Regression</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/32-classification-methods.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">33</span>&nbsp; <span class="chapter-title">Classification and Performance Metrics</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/33-trees-forests.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">34</span>&nbsp; <span class="chapter-title">Decision Trees and Random Forests</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/34-svm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">35</span>&nbsp; <span class="chapter-title">Support Vector Machines</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/35-clustering.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">36</span>&nbsp; <span class="chapter-title">Clustering</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/36-dimensionality-reduction.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">37</span>&nbsp; <span class="chapter-title">Dimensionality Reduction and Multivariate Methods</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/37-tsne-umap.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">38</span>&nbsp; <span class="chapter-title">Non-Linear Dimensionality Reduction: t-SNE and UMAP</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/38-bayesian-statistics.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">39</span>&nbsp; <span class="chapter-title">Bayesian Statistics</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/39-deep-learning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">40</span>&nbsp; <span class="chapter-title">Introduction to Deep Learning</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/40-high-performance-computing.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">41</span>&nbsp; <span class="chapter-title">High Performance Computing</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="true">
 <span class="menu-text">Scientific Communication</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-7" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/41-presenting-results.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">42</span>&nbsp; <span class="chapter-title">Presenting Statistical Results</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-8" role="navigation" aria-expanded="true">
 <span class="menu-text">Historical Context</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-8" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-8" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/A1-eugenics-history.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">43</span>&nbsp; <span class="chapter-title">The Eugenics History of Statistics</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-9" role="navigation" aria-expanded="true">
 <span class="menu-text">Appendices</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-9" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-9" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/A2-keyboard-shortcuts.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">44</span>&nbsp; <span class="chapter-title">Keyboard Shortcuts Reference</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/A3-unix-reference.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">45</span>&nbsp; <span class="chapter-title">Unix Command Reference</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/A4-r-reference.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">46</span>&nbsp; <span class="chapter-title">R Command Reference</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/A5-quarto-reference.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">47</span>&nbsp; <span class="chapter-title">Quarto Markdown Reference</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/A6-latex-reference.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">48</span>&nbsp; <span class="chapter-title">LaTeX Command Reference</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/A7-greek-letters.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">49</span>&nbsp; <span class="chapter-title">Greek Letters in Mathematics and Statistics</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/A8-probability-distributions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">50</span>&nbsp; <span class="chapter-title">Common Probability Distributions</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/A9-sampling-distributions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">51</span>&nbsp; <span class="chapter-title">Sampling Distributions in Hypothesis Testing</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/A10-matrix-algebra.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">52</span>&nbsp; <span class="chapter-title">Matrix Algebra Fundamentals</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#the-overfitting-problem" id="toc-the-overfitting-problem" class="nav-link active" data-scroll-target="#the-overfitting-problem"><span class="header-section-number">30.1</span> The Overfitting Problem</a></li>
  <li><a href="#loss-functions-quantifying-prediction-error" id="toc-loss-functions-quantifying-prediction-error" class="nav-link" data-scroll-target="#loss-functions-quantifying-prediction-error"><span class="header-section-number">30.2</span> Loss Functions: Quantifying Prediction Error</a>
  <ul class="collapse">
  <li><a href="#common-loss-functions-for-regression" id="toc-common-loss-functions-for-regression" class="nav-link" data-scroll-target="#common-loss-functions-for-regression">Common Loss Functions for Regression</a></li>
  <li><a href="#common-loss-functions-for-classification" id="toc-common-loss-functions-for-classification" class="nav-link" data-scroll-target="#common-loss-functions-for-classification">Common Loss Functions for Classification</a></li>
  <li><a href="#why-loss-functions-matter" id="toc-why-loss-functions-matter" class="nav-link" data-scroll-target="#why-loss-functions-matter">Why Loss Functions Matter</a></li>
  </ul></li>
  <li><a href="#training-error-vs.-test-error" id="toc-training-error-vs.-test-error" class="nav-link" data-scroll-target="#training-error-vs.-test-error"><span class="header-section-number">30.3</span> Training Error vs.&nbsp;Test Error</a></li>
  <li><a href="#the-bias-variance-tradeoff" id="toc-the-bias-variance-tradeoff" class="nav-link" data-scroll-target="#the-bias-variance-tradeoff"><span class="header-section-number">30.4</span> The Bias-Variance Tradeoff</a></li>
  <li><a href="#cross-validation" id="toc-cross-validation" class="nav-link" data-scroll-target="#cross-validation"><span class="header-section-number">30.5</span> Cross-Validation</a>
  <ul class="collapse">
  <li><a href="#k-fold-cross-validation" id="toc-k-fold-cross-validation" class="nav-link" data-scroll-target="#k-fold-cross-validation">K-Fold Cross-Validation</a></li>
  <li><a href="#leave-one-out-cross-validation-loocv" id="toc-leave-one-out-cross-validation-loocv" class="nav-link" data-scroll-target="#leave-one-out-cross-validation-loocv">Leave-One-Out Cross-Validation (LOOCV)</a></li>
  <li><a href="#bootstrap-for-error-estimation" id="toc-bootstrap-for-error-estimation" class="nav-link" data-scroll-target="#bootstrap-for-error-estimation">Bootstrap for Error Estimation</a></li>
  </ul></li>
  <li><a href="#using-cross-validation-for-model-selection" id="toc-using-cross-validation-for-model-selection" class="nav-link" data-scroll-target="#using-cross-validation-for-model-selection"><span class="header-section-number">30.6</span> Using Cross-Validation for Model Selection</a>
  <ul class="collapse">
  <li><a href="#the-one-standard-error-rule" id="toc-the-one-standard-error-rule" class="nav-link" data-scroll-target="#the-one-standard-error-rule">The One Standard Error Rule</a></li>
  </ul></li>
  <li><a href="#information-criteria" id="toc-information-criteria" class="nav-link" data-scroll-target="#information-criteria"><span class="header-section-number">30.7</span> Information Criteria</a></li>
  <li><a href="#practical-workflow" id="toc-practical-workflow" class="nav-link" data-scroll-target="#practical-workflow"><span class="header-section-number">30.8</span> Practical Workflow</a></li>
  <li><a href="#exercises" id="toc-exercises" class="nav-link" data-scroll-target="#exercises"><span class="header-section-number">30.9</span> Exercises</a></li>
  <li><a href="#summary" id="toc-summary" class="nav-link" data-scroll-target="#summary"><span class="header-section-number">30.10</span> Summary</a></li>
  <li><a href="#additional-resources" id="toc-additional-resources" class="nav-link" data-scroll-target="#additional-resources"><span class="header-section-number">30.11</span> Additional Resources</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../chapters/28-intro-statistical-learning.html">Statistical Learning</a></li><li class="breadcrumb-item"><a href="../chapters/29-model-validation.html"><span class="chapter-number">30</span>&nbsp; <span class="chapter-title">Model Validation and the Bias-Variance Tradeoff</span></a></li></ol></nav>
<div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title"><span id="sec-model-validation" class="quarto-section-identifier"><span class="chapter-number">30</span>&nbsp; <span class="chapter-title">Model Validation and the Bias-Variance Tradeoff</span></span></h1><button type="button" class="btn code-tools-button dropdown-toggle" id="quarto-code-tools-menu" data-bs-toggle="dropdown" aria-expanded="false"><i class="bi"></i> Code</button><ul class="dropdown-menu dropdown-menu-end" aria-labelelledby="quarto-code-tools-menu"><li><a id="quarto-show-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Show All Code</a></li><li><a id="quarto-hide-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Hide All Code</a></li><li><hr class="dropdown-divider"></li><li><a id="quarto-view-source" class="dropdown-item" href="javascript:void(0)" role="button">View Source</a></li></ul></div></div>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<section id="the-overfitting-problem" class="level2" data-number="30.1">
<h2 data-number="30.1" class="anchored" data-anchor-id="the-overfitting-problem"><span class="header-section-number">30.1</span> The Overfitting Problem</h2>
<p>Models are built to fit training data as closely as possible. A linear regression minimizes squared errors; a logistic regression maximizes likelihood. But models that fit training data too well often predict poorly on new data.</p>
<p><strong>Overfitting</strong> occurs when a model captures noise specific to the training data rather than the true underlying pattern. Complex models with many parameters are especially susceptible.</p>
<p>The solution is to evaluate models on data they have not seen—held-out test data or through cross-validation.</p>
</section>
<section id="loss-functions-quantifying-prediction-error" class="level2" data-number="30.2">
<h2 data-number="30.2" class="anchored" data-anchor-id="loss-functions-quantifying-prediction-error"><span class="header-section-number">30.2</span> Loss Functions: Quantifying Prediction Error</h2>
<p>A <strong>loss function</strong> (or <strong>cost function</strong>) measures how wrong a prediction is. It quantifies the penalty for predicting <span class="math inline">\(\hat{y}\)</span> when the true value is <span class="math inline">\(y\)</span>.</p>
<section id="common-loss-functions-for-regression" class="level3">
<h3 class="anchored" data-anchor-id="common-loss-functions-for-regression">Common Loss Functions for Regression</h3>
<p><strong>Squared Error Loss</strong> (L2): The most common loss for continuous outcomes: <span class="math display">\[L(y, \hat{y}) = (y - \hat{y})^2\]</span></p>
<p>Squaring penalizes large errors more heavily than small ones. Linear regression minimizes the sum of squared errors (SSE or RSS).</p>
<p><strong>Absolute Error Loss</strong> (L1): Less sensitive to outliers: <span class="math display">\[L(y, \hat{y}) = |y - \hat{y}|\]</span></p>
<p><strong>Mean Squared Error (MSE)</strong> and <strong>Root Mean Squared Error (RMSE)</strong> are averages across all predictions: <span class="math display">\[\text{MSE} = \frac{1}{n}\sum_{i=1}^n (y_i - \hat{y}_i)^2, \quad \text{RMSE} = \sqrt{\text{MSE}}\]</span></p>
</section>
<section id="common-loss-functions-for-classification" class="level3">
<h3 class="anchored" data-anchor-id="common-loss-functions-for-classification">Common Loss Functions for Classification</h3>
<p><strong>0-1 Loss</strong>: The simplest classification loss—1 if wrong, 0 if correct: <span class="math display">\[L(y, \hat{y}) = \mathbb{I}(y \neq \hat{y})\]</span></p>
<p>The average 0-1 loss is the <strong>error rate</strong>; one minus the error rate is <strong>accuracy</strong>.</p>
<p><strong>Log Loss</strong> (Cross-Entropy): Used when we have predicted probabilities <span class="math inline">\(\hat{p}\)</span>: <span class="math display">\[L(y, \hat{p}) = -[y \log(\hat{p}) + (1-y) \log(1-\hat{p})]\]</span></p>
<p>Log loss penalizes confident wrong predictions severely—predicting probability 0.99 for the wrong class incurs much larger loss than predicting 0.6.</p>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">2</span>))</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Regression loss functions</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>errors <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="sc">-</span><span class="dv">3</span>, <span class="dv">3</span>, <span class="at">length.out =</span> <span class="dv">100</span>)</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(errors, errors<span class="sc">^</span><span class="dv">2</span>, <span class="at">type =</span> <span class="st">"l"</span>, <span class="at">col =</span> <span class="st">"blue"</span>, <span class="at">lwd =</span> <span class="dv">2</span>,</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">"Prediction Error (y - ŷ)"</span>, <span class="at">ylab =</span> <span class="st">"Loss"</span>,</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>     <span class="at">main =</span> <span class="st">"Regression Loss Functions"</span>)</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(errors, <span class="fu">abs</span>(errors), <span class="at">col =</span> <span class="st">"red"</span>, <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="fu">legend</span>(<span class="st">"top"</span>, <span class="fu">c</span>(<span class="st">"Squared (L2)"</span>, <span class="st">"Absolute (L1)"</span>),</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>       <span class="at">col =</span> <span class="fu">c</span>(<span class="st">"blue"</span>, <span class="st">"red"</span>), <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Classification log loss</span></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>p <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="fl">0.01</span>, <span class="fl">0.99</span>, <span class="at">length.out =</span> <span class="dv">100</span>)</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(p, <span class="sc">-</span><span class="fu">log</span>(p), <span class="at">type =</span> <span class="st">"l"</span>, <span class="at">col =</span> <span class="st">"blue"</span>, <span class="at">lwd =</span> <span class="dv">2</span>,</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">"Predicted Probability for True Class"</span>, <span class="at">ylab =</span> <span class="st">"Log Loss"</span>,</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>     <span class="at">main =</span> <span class="st">"Classification Log Loss"</span>)</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">v =</span> <span class="fl">0.5</span>, <span class="at">lty =</span> <span class="dv">2</span>, <span class="at">col =</span> <span class="st">"gray"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div id="fig-loss-functions" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-loss-functions-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="29-model-validation_files/figure-html/fig-loss-functions-1.png" class="img-fluid figure-img" width="768">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-loss-functions-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;30.1: Comparison of squared loss (penalizes large errors heavily) versus absolute loss (more robust to outliers)
</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="why-loss-functions-matter" class="level3">
<h3 class="anchored" data-anchor-id="why-loss-functions-matter">Why Loss Functions Matter</h3>
<p>Different loss functions lead to different optimal predictions, which has profound implications for how we should model data. Under squared loss, the optimal prediction at any point is the <strong>mean</strong> of the outcome distribution—this is why ordinary least squares produces predictions that minimize average squared error. Under absolute loss, the optimal prediction is the <strong>median</strong>, which is more robust to outliers. For classification under 0-1 loss, the optimal prediction is simply the <strong>mode</strong>—the most frequent class.</p>
<p>The choice of loss function should reflect how errors affect your application. In medical diagnosis, the consequences of different types of errors are typically asymmetric: missing a disease (false negative) may be far more costly than a false alarm (false positive). A thoughtful analysis would weight these errors differently rather than treating all misclassifications equally.</p>
</section>
</section>
<section id="training-error-vs.-test-error" class="level2" data-number="30.3">
<h2 data-number="30.3" class="anchored" data-anchor-id="training-error-vs.-test-error"><span class="header-section-number">30.3</span> Training Error vs.&nbsp;Test Error</h2>
<p>A fundamental insight of statistical learning is that <strong>training error</strong> (how well we fit the data used to build the model) is an overly optimistic estimate of <strong>test error</strong> (how well we predict new data).</p>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Demonstrate training vs test error</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">42</span>)</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">100</span></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">sort</span>(<span class="fu">runif</span>(n, <span class="dv">0</span>, <span class="dv">10</span>))</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>y_true <span class="ot">&lt;-</span> <span class="fu">sin</span>(x) <span class="sc">+</span> <span class="fl">0.5</span> <span class="sc">*</span> <span class="fu">cos</span>(<span class="fl">0.5</span> <span class="sc">*</span> x)</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> y_true <span class="sc">+</span> <span class="fu">rnorm</span>(n, <span class="at">sd =</span> <span class="fl">0.3</span>)</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Split into training and test</span></span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>train_idx <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="dv">1</span><span class="sc">:</span>n, <span class="dv">70</span>)</span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>train_data <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">x =</span> x[train_idx], <span class="at">y =</span> y[train_idx])</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>test_data <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">x =</span> x[<span class="sc">-</span>train_idx], <span class="at">y =</span> y[<span class="sc">-</span>train_idx])</span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit polynomials of increasing degree</span></span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(splines)</span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a>degrees <span class="ot">&lt;-</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">15</span></span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a>train_error <span class="ot">&lt;-</span> test_error <span class="ot">&lt;-</span> <span class="fu">numeric</span>(<span class="fu">length</span>(degrees))</span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="fu">seq_along</span>(degrees)) {</span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a>  d <span class="ot">&lt;-</span> degrees[i]</span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a>  fit <span class="ot">&lt;-</span> <span class="fu">lm</span>(y <span class="sc">~</span> <span class="fu">poly</span>(x, d), <span class="at">data =</span> train_data)</span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a>  train_error[i] <span class="ot">&lt;-</span> <span class="fu">mean</span>((train_data<span class="sc">$</span>y <span class="sc">-</span> <span class="fu">predict</span>(fit, train_data))<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a>  test_error[i] <span class="ot">&lt;-</span> <span class="fu">mean</span>((test_data<span class="sc">$</span>y <span class="sc">-</span> <span class="fu">predict</span>(fit, test_data))<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb2-24"><a href="#cb2-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-25"><a href="#cb2-25" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot</span></span>
<span id="cb2-26"><a href="#cb2-26" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(degrees, train_error, <span class="at">type =</span> <span class="st">"b"</span>, <span class="at">pch =</span> <span class="dv">19</span>, <span class="at">col =</span> <span class="st">"blue"</span>,</span>
<span id="cb2-27"><a href="#cb2-27" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">"Model Complexity (Polynomial Degree)"</span>,</span>
<span id="cb2-28"><a href="#cb2-28" aria-hidden="true" tabindex="-1"></a>     <span class="at">ylab =</span> <span class="st">"Mean Squared Error"</span>,</span>
<span id="cb2-29"><a href="#cb2-29" aria-hidden="true" tabindex="-1"></a>     <span class="at">main =</span> <span class="st">"Training vs Test Error"</span>, <span class="at">ylim =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="fu">max</span>(test_error)))</span>
<span id="cb2-30"><a href="#cb2-30" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(degrees, test_error, <span class="at">type =</span> <span class="st">"b"</span>, <span class="at">pch =</span> <span class="dv">19</span>, <span class="at">col =</span> <span class="st">"red"</span>)</span>
<span id="cb2-31"><a href="#cb2-31" aria-hidden="true" tabindex="-1"></a><span class="fu">legend</span>(<span class="st">"topright"</span>, <span class="fu">c</span>(<span class="st">"Training Error"</span>, <span class="st">"Test Error"</span>),</span>
<span id="cb2-32"><a href="#cb2-32" aria-hidden="true" tabindex="-1"></a>       <span class="at">col =</span> <span class="fu">c</span>(<span class="st">"blue"</span>, <span class="st">"red"</span>), <span class="at">pch =</span> <span class="dv">19</span>, <span class="at">lty =</span> <span class="dv">1</span>)</span>
<span id="cb2-33"><a href="#cb2-33" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">v =</span> degrees[<span class="fu">which.min</span>(test_error)], <span class="at">lty =</span> <span class="dv">2</span>, <span class="at">col =</span> <span class="st">"gray"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div id="fig-train-test-error" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-train-test-error-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="29-model-validation_files/figure-html/fig-train-test-error-1.png" class="img-fluid figure-img" width="768">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-train-test-error-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;30.2: Training error always decreases with model complexity, but test error eventually increases due to overfitting. The optimal model minimizes test error.
</figcaption>
</figure>
</div>
</div>
</div>
<p>Notice that training error keeps decreasing as complexity increases, eventually reaching near zero. But test error follows a U-shape—it decreases initially as the model captures true patterns, then increases as the model starts fitting noise.</p>
</section>
<section id="the-bias-variance-tradeoff" class="level2" data-number="30.4">
<h2 data-number="30.4" class="anchored" data-anchor-id="the-bias-variance-tradeoff"><span class="header-section-number">30.4</span> The Bias-Variance Tradeoff</h2>
<p>Prediction error has two main components:</p>
<p><strong>Bias</strong>: Error from approximating a complex reality with a simpler model. Simple models have high bias—they may miss important patterns.</p>
<p><strong>Variance</strong>: Error from sensitivity to training data. Complex models have high variance—they change substantially with different training samples.</p>
<p><a href="#fig-bias-variance-concept" class="quarto-xref">Figure&nbsp;<span>30.3</span></a> illustrates this tradeoff visually. On the left, an underfit model (high bias) misses the pattern in the data. On the right, an overfit model (high variance) captures noise rather than signal. The optimal model complexity, shown in the middle, balances these competing concerns to minimize total error.</p>
<div id="fig-bias-variance-concept" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-bias-variance-concept-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../images/ch29/ch29_bias_variance.jpeg" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:80.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-bias-variance-concept-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;30.3: The bias-variance tradeoff visualized. Simple models (left) underfit the data, showing high bias. Complex models (right) overfit, showing high variance. The optimal model complexity (center) minimizes total error by balancing bias and variance. The U-shaped total error curve emerges because decreasing bias eventually comes at the cost of increasing variance.
</figcaption>
</figure>
</div>
<p>Mathematically, for a given test point <span class="math inline">\(x_0\)</span>, the expected prediction error can be decomposed as:</p>
<p><span class="math display">\[E[(y_0 - \hat{f}(x_0))^2] = \text{Var}(\hat{f}(x_0)) + [\text{Bias}(\hat{f}(x_0))]^2 + \text{Var}(\epsilon)\]</span></p>
<p>where: - <span class="math inline">\(\text{Var}(\hat{f}(x_0))\)</span> is the variance of the model predictions - <span class="math inline">\(\text{Bias}(\hat{f}(x_0))\)</span> is the bias (systematic error) - <span class="math inline">\(\text{Var}(\epsilon)\)</span> is the irreducible error (noise in the data)</p>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Illustrate bias-variance tradeoff conceptually</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>complexity <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="fl">0.1</span>, <span class="dv">10</span>, <span class="at">length.out =</span> <span class="dv">100</span>)</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>bias_sq <span class="ot">&lt;-</span> <span class="dv">5</span> <span class="sc">/</span> complexity</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>variance <span class="ot">&lt;-</span> <span class="fl">0.1</span> <span class="sc">*</span> complexity<span class="sc">^</span><span class="fl">1.5</span></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>irreducible <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="fl">0.5</span>, <span class="fu">length</span>(complexity))</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>total <span class="ot">&lt;-</span> bias_sq <span class="sc">+</span> variance <span class="sc">+</span> irreducible</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(complexity, total, <span class="at">type =</span> <span class="st">"l"</span>, <span class="at">lwd =</span> <span class="dv">3</span>, <span class="at">col =</span> <span class="st">"black"</span>,</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">"Model Complexity"</span>, <span class="at">ylab =</span> <span class="st">"Error"</span>,</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>     <span class="at">main =</span> <span class="st">"Bias-Variance Tradeoff"</span>, <span class="at">ylim =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="fu">max</span>(total)))</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(complexity, bias_sq, <span class="at">lwd =</span> <span class="dv">2</span>, <span class="at">col =</span> <span class="st">"blue"</span>, <span class="at">lty =</span> <span class="dv">2</span>)</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(complexity, variance, <span class="at">lwd =</span> <span class="dv">2</span>, <span class="at">col =</span> <span class="st">"red"</span>, <span class="at">lty =</span> <span class="dv">2</span>)</span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(complexity, irreducible, <span class="at">lwd =</span> <span class="dv">2</span>, <span class="at">col =</span> <span class="st">"gray"</span>, <span class="at">lty =</span> <span class="dv">3</span>)</span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Mark optimal</span></span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a>opt_idx <span class="ot">&lt;-</span> <span class="fu">which.min</span>(total)</span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">v =</span> complexity[opt_idx], <span class="at">lty =</span> <span class="dv">2</span>, <span class="at">col =</span> <span class="st">"darkgreen"</span>)</span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span>(complexity[opt_idx], total[opt_idx], <span class="at">pch =</span> <span class="dv">19</span>, <span class="at">cex =</span> <span class="dv">2</span>, <span class="at">col =</span> <span class="st">"darkgreen"</span>)</span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a><span class="fu">legend</span>(<span class="st">"topright"</span>, <span class="fu">c</span>(<span class="st">"Total Error"</span>, <span class="st">"Bias²"</span>, <span class="st">"Variance"</span>, <span class="st">"Irreducible Error"</span>, <span class="st">"Optimal"</span>),</span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a>       <span class="at">col =</span> <span class="fu">c</span>(<span class="st">"black"</span>, <span class="st">"blue"</span>, <span class="st">"red"</span>, <span class="st">"gray"</span>, <span class="st">"darkgreen"</span>),</span>
<span id="cb3-22"><a href="#cb3-22" aria-hidden="true" tabindex="-1"></a>       <span class="at">lty =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">2</span>), <span class="at">lwd =</span> <span class="fu">c</span>(<span class="dv">3</span>, <span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">1</span>), <span class="at">pch =</span> <span class="fu">c</span>(<span class="cn">NA</span>, <span class="cn">NA</span>, <span class="cn">NA</span>, <span class="cn">NA</span>, <span class="dv">19</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div id="fig-bias-variance" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-bias-variance-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="29-model-validation_files/figure-html/fig-bias-variance-1.png" class="img-fluid figure-img" width="768">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-bias-variance-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;30.4: The bias-variance tradeoff: as model complexity increases, bias decreases but variance increases. The optimal model minimizes total error.
</figcaption>
</figure>
</div>
</div>
</div>
<p>The best predictions come from models that balance bias and variance. As model complexity increases, bias decreases because more complex models can fit more patterns—they have the flexibility to capture structure that simpler models miss. But simultaneously, variance increases because more complex models are more sensitive to the particular training data used—with different training samples, they would produce substantially different predictions.</p>
<p>The total prediction error reflects both components. Initially, as we add complexity, the reduction in bias dominates and total error decreases. But eventually, further complexity provides diminishing bias reduction while variance continues to grow. At this point, adding complexity hurts more than it helps, and total error begins to increase. The optimal model sits at the bottom of this U-shaped curve, complex enough to capture the true patterns but not so complex that it fits noise.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Understanding Bias and Variance
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>High bias</strong> (underfitting): - Model is too simple - Consistently wrong predictions - Predictions don’t change much with different training data - Example: Fitting a straight line to curved data</p>
<p><strong>High variance</strong> (overfitting): - Model is too complex - Predictions vary wildly with different training samples - Fits training data well but generalizes poorly - Example: High-degree polynomial fitting noise</p>
</div>
</div>
</section>
<section id="cross-validation" class="level2" data-number="30.5">
<h2 data-number="30.5" class="anchored" data-anchor-id="cross-validation"><span class="header-section-number">30.5</span> Cross-Validation</h2>
<p>Cross-validation estimates how well a model will generalize to new data without requiring a separate test set.</p>
<section id="k-fold-cross-validation" class="level3">
<h3 class="anchored" data-anchor-id="k-fold-cross-validation">K-Fold Cross-Validation</h3>
<p><strong>K-fold cross-validation</strong> works by splitting the data into k roughly equal parts called folds. The algorithm then trains the model k times, each time holding out one fold as a test set and training on the remaining k-1 folds. Each observation ends up in the test set exactly once. The final performance estimate is the average across all k iterations, giving us a robust estimate of how the model will perform on new data.</p>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualize 5-fold CV</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mar =</span> <span class="fu">c</span>(<span class="dv">2</span>, <span class="dv">4</span>, <span class="dv">2</span>, <span class="dv">2</span>))</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(<span class="dv">1</span>, <span class="at">type =</span> <span class="st">"n"</span>, <span class="at">xlim =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">5</span>), <span class="at">ylim =</span> <span class="fu">c</span>(<span class="fl">0.5</span>, <span class="fl">5.5</span>),</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">""</span>, <span class="at">ylab =</span> <span class="st">"Fold"</span>, <span class="at">xaxt =</span> <span class="st">"n"</span>, <span class="at">yaxt =</span> <span class="st">"n"</span>,</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>     <span class="at">main =</span> <span class="st">"5-Fold Cross-Validation"</span>)</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a><span class="fu">axis</span>(<span class="dv">2</span>, <span class="at">at =</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">5</span>, <span class="at">labels =</span> <span class="fu">paste</span>(<span class="st">"Iteration"</span>, <span class="dv">5</span><span class="sc">:</span><span class="dv">1</span>), <span class="at">las =</span> <span class="dv">1</span>)</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>colors <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">"lightcoral"</span>, <span class="fu">rep</span>(<span class="st">"lightblue"</span>, <span class="dv">4</span>))</span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">5</span>) {</span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> (j <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">5</span>) {</span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>    col <span class="ot">&lt;-</span> <span class="fu">ifelse</span>(j <span class="sc">==</span> i, <span class="st">"lightcoral"</span>, <span class="st">"lightblue"</span>)</span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>    <span class="fu">rect</span>(j <span class="sc">-</span> <span class="dv">1</span>, <span class="dv">6</span> <span class="sc">-</span> i <span class="sc">-</span> <span class="fl">0.4</span>, j, <span class="dv">6</span> <span class="sc">-</span> i <span class="sc">+</span> <span class="fl">0.4</span>, <span class="at">col =</span> col, <span class="at">border =</span> <span class="st">"white"</span>)</span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a><span class="fu">legend</span>(<span class="st">"bottom"</span>, <span class="fu">c</span>(<span class="st">"Test Fold"</span>, <span class="st">"Training Folds"</span>), <span class="at">fill =</span> <span class="fu">c</span>(<span class="st">"lightcoral"</span>, <span class="st">"lightblue"</span>),</span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a>       <span class="at">horiz =</span> <span class="cn">TRUE</span>, <span class="at">bty =</span> <span class="st">"n"</span>)</span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a><span class="fu">axis</span>(<span class="dv">1</span>, <span class="at">at =</span> <span class="fl">0.5</span><span class="sc">:</span><span class="fl">4.5</span>, <span class="at">labels =</span> <span class="fu">paste</span>(<span class="st">"Fold"</span>, <span class="dv">1</span><span class="sc">:</span><span class="dv">5</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div id="fig-cross-validation-diagram" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-cross-validation-diagram-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="29-model-validation_files/figure-html/fig-cross-validation-diagram-1.png" class="img-fluid figure-img" width="768">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-cross-validation-diagram-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;30.5: K-fold cross-validation: each fold takes turns being the test set
</figcaption>
</figure>
</div>
</div>
</div>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Simple CV example with linear regression</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">42</span>)</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(<span class="dv">100</span>)</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> <span class="dv">2</span> <span class="sc">+</span> <span class="dv">3</span><span class="sc">*</span>x <span class="sc">+</span> <span class="fu">rnorm</span>(<span class="dv">100</span>)</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>data <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(x, y)</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit model and perform CV</span></span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>model <span class="ot">&lt;-</span> <span class="fu">glm</span>(y <span class="sc">~</span> x, <span class="at">data =</span> data)</span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a><span class="co"># 10-fold cross-validation</span></span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>cv_result <span class="ot">&lt;-</span> <span class="fu">cv.glm</span>(data, model, <span class="at">K =</span> <span class="dv">10</span>)</span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"CV estimate of prediction error:"</span>, <span class="fu">round</span>(cv_result<span class="sc">$</span>delta[<span class="dv">1</span>], <span class="dv">3</span>), <span class="st">"</span><span class="sc">\n</span><span class="st">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>CV estimate of prediction error: 0.846 </code></pre>
</div>
</div>
</section>
<section id="leave-one-out-cross-validation-loocv" class="level3">
<h3 class="anchored" data-anchor-id="leave-one-out-cross-validation-loocv">Leave-One-Out Cross-Validation (LOOCV)</h3>
<p><strong>Leave-one-out cross-validation (LOOCV)</strong> is k-fold with k = n: each observation is held out once.</p>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="co"># LOOCV</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>cv_loocv <span class="ot">&lt;-</span> <span class="fu">cv.glm</span>(data, model, <span class="at">K =</span> <span class="fu">nrow</span>(data))</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"LOOCV estimate of prediction error:"</span>, <span class="fu">round</span>(cv_loocv<span class="sc">$</span>delta[<span class="dv">1</span>], <span class="dv">3</span>), <span class="st">"</span><span class="sc">\n</span><span class="st">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>LOOCV estimate of prediction error: 0.851 </code></pre>
</div>
</div>
<p>Like all statistical methods, cross-validation involves tradeoffs. LOOCV has lower bias than k-fold CV because nearly all the data (n-1 observations) is used for training in each iteration—the training sets closely resemble the full dataset. However, LOOCV has higher variance because the n training sets are almost identical to each other, so the n fitted models are highly correlated. In practice, 5-fold or 10-fold CV provides a good balance of bias and variance, and empirical studies suggest these choices often give the best estimates of test error. LOOCV is also computationally expensive for large datasets, requiring n separate model fits rather than just k.</p>
</section>
<section id="bootstrap-for-error-estimation" class="level3">
<h3 class="anchored" data-anchor-id="bootstrap-for-error-estimation">Bootstrap for Error Estimation</h3>
<p>The <strong>bootstrap</strong> provides another approach to estimating prediction error. For each bootstrap iteration, we draw a sample of n observations with replacement from the training data—some observations appear multiple times, others not at all. We fit the model on this bootstrap sample and evaluate it on the observations that were not selected, called the “out-of-bag” (OOB) observations. On average, about 37% of observations are left out of each bootstrap sample. By repeating this process many times and averaging the OOB errors, we obtain an estimate of prediction error.</p>
<p>The bootstrap approach is similar in spirit to cross-validation but uses the natural randomness of sampling with replacement rather than deterministic fold assignments. It’s particularly useful when you also want confidence intervals for model parameters or predictions.</p>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Bootstrap estimate of prediction error</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>n_boot <span class="ot">&lt;-</span> <span class="dv">100</span></span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>boot_errors <span class="ot">&lt;-</span> <span class="fu">numeric</span>(n_boot)</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (b <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>n_boot) {</span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Bootstrap sample</span></span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a>  boot_idx <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="dv">1</span><span class="sc">:</span><span class="fu">nrow</span>(data), <span class="at">replace =</span> <span class="cn">TRUE</span>)</span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a>  oob_idx <span class="ot">&lt;-</span> <span class="fu">setdiff</span>(<span class="dv">1</span><span class="sc">:</span><span class="fu">nrow</span>(data), <span class="fu">unique</span>(boot_idx))</span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span> (<span class="fu">length</span>(oob_idx) <span class="sc">&gt;</span> <span class="dv">0</span>) {</span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a>    fit <span class="ot">&lt;-</span> <span class="fu">lm</span>(y <span class="sc">~</span> x, <span class="at">data =</span> data[boot_idx, ])</span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a>    boot_errors[b] <span class="ot">&lt;-</span> <span class="fu">mean</span>((data<span class="sc">$</span>y[oob_idx] <span class="sc">-</span> <span class="fu">predict</span>(fit, data[oob_idx, ]))<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb9-15"><a href="#cb9-15" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb9-16"><a href="#cb9-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-17"><a href="#cb9-17" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"Bootstrap estimate of prediction error:"</span>, <span class="fu">round</span>(<span class="fu">mean</span>(boot_errors), <span class="dv">3</span>), <span class="st">"</span><span class="sc">\n</span><span class="st">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Bootstrap estimate of prediction error: 0.867 </code></pre>
</div>
</div>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Choosing a CV Strategy
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><strong>k = 5 or k = 10</strong>: Standard choices that balance bias and variance</li>
<li><strong>LOOCV (k = n)</strong>: Low bias but high variance; expensive for large n</li>
<li><strong>Bootstrap</strong>: Useful when you also want confidence intervals</li>
<li><strong>Repeated CV</strong>: Run k-fold multiple times with different splits for more stable estimates</li>
</ul>
</div>
</div>
</section>
</section>
<section id="using-cross-validation-for-model-selection" class="level2" data-number="30.6">
<h2 data-number="30.6" class="anchored" data-anchor-id="using-cross-validation-for-model-selection"><span class="header-section-number">30.6</span> Using Cross-Validation for Model Selection</h2>
<p>Cross-validation is essential for tuning hyperparameters—values that control model complexity but are not learned from data.</p>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate data</span></span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">42</span>)</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">100</span></span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">runif</span>(n, <span class="dv">0</span>, <span class="dv">10</span>)</span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> <span class="fu">sin</span>(x) <span class="sc">+</span> <span class="fu">rnorm</span>(n, <span class="at">sd =</span> <span class="fl">0.3</span>)</span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a>cv_data <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">x =</span> x, <span class="at">y =</span> y)</span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Cross-validation for polynomial degree selection</span></span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a>degrees <span class="ot">&lt;-</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">15</span></span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a>cv_errors <span class="ot">&lt;-</span> <span class="fu">numeric</span>(<span class="fu">length</span>(degrees))</span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-12"><a href="#cb11-12" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (d <span class="cf">in</span> degrees) {</span>
<span id="cb11-13"><a href="#cb11-13" aria-hidden="true" tabindex="-1"></a>  model <span class="ot">&lt;-</span> <span class="fu">glm</span>(y <span class="sc">~</span> <span class="fu">poly</span>(x, d), <span class="at">data =</span> cv_data)</span>
<span id="cb11-14"><a href="#cb11-14" aria-hidden="true" tabindex="-1"></a>  cv_result <span class="ot">&lt;-</span> <span class="fu">cv.glm</span>(cv_data, model, <span class="at">K =</span> <span class="dv">10</span>)</span>
<span id="cb11-15"><a href="#cb11-15" aria-hidden="true" tabindex="-1"></a>  cv_errors[d] <span class="ot">&lt;-</span> cv_result<span class="sc">$</span>delta[<span class="dv">1</span>]</span>
<span id="cb11-16"><a href="#cb11-16" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb11-17"><a href="#cb11-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-18"><a href="#cb11-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot</span></span>
<span id="cb11-19"><a href="#cb11-19" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(degrees, cv_errors, <span class="at">type =</span> <span class="st">"b"</span>, <span class="at">pch =</span> <span class="dv">19</span>, <span class="at">col =</span> <span class="st">"blue"</span>,</span>
<span id="cb11-20"><a href="#cb11-20" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">"Polynomial Degree"</span>, <span class="at">ylab =</span> <span class="st">"CV Error (MSE)"</span>,</span>
<span id="cb11-21"><a href="#cb11-21" aria-hidden="true" tabindex="-1"></a>     <span class="at">main =</span> <span class="st">"Cross-Validation for Model Selection"</span>)</span>
<span id="cb11-22"><a href="#cb11-22" aria-hidden="true" tabindex="-1"></a>best_degree <span class="ot">&lt;-</span> <span class="fu">which.min</span>(cv_errors)</span>
<span id="cb11-23"><a href="#cb11-23" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span>(best_degree, cv_errors[best_degree], <span class="at">pch =</span> <span class="dv">19</span>, <span class="at">col =</span> <span class="st">"red"</span>, <span class="at">cex =</span> <span class="dv">2</span>)</span>
<span id="cb11-24"><a href="#cb11-24" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">v =</span> best_degree, <span class="at">lty =</span> <span class="dv">2</span>, <span class="at">col =</span> <span class="st">"red"</span>)</span>
<span id="cb11-25"><a href="#cb11-25" aria-hidden="true" tabindex="-1"></a><span class="fu">text</span>(best_degree <span class="sc">+</span> <span class="dv">1</span>, cv_errors[best_degree], <span class="fu">paste</span>(<span class="st">"Optimal:"</span>, best_degree), <span class="at">col =</span> <span class="st">"red"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div id="fig-cv-model-selection" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-cv-model-selection-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="29-model-validation_files/figure-html/fig-cv-model-selection-1.png" class="img-fluid figure-img" width="768">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-cv-model-selection-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;30.6: Using cross-validation to select the optimal polynomial degree. The optimal model minimizes CV error.
</figcaption>
</figure>
</div>
</div>
</div>
<section id="the-one-standard-error-rule" class="level3">
<h3 class="anchored" data-anchor-id="the-one-standard-error-rule">The One Standard Error Rule</h3>
<p>When selecting among models with similar CV error, we often choose the simplest model within one standard error of the minimum. This guards against overfitting to the CV procedure itself.</p>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate SE of CV errors (simplified - would need repeated CV for proper SE)</span></span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">42</span>)</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Repeated 10-fold CV for better SE estimates</span></span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a>n_repeats <span class="ot">&lt;-</span> <span class="dv">20</span></span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a>cv_matrix <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="cn">NA</span>, n_repeats, <span class="fu">length</span>(degrees))</span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (rep <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>n_repeats) {</span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> (d <span class="cf">in</span> degrees) {</span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a>    model <span class="ot">&lt;-</span> <span class="fu">glm</span>(y <span class="sc">~</span> <span class="fu">poly</span>(x, d), <span class="at">data =</span> cv_data)</span>
<span id="cb12-11"><a href="#cb12-11" aria-hidden="true" tabindex="-1"></a>    cv_result <span class="ot">&lt;-</span> <span class="fu">cv.glm</span>(cv_data, model, <span class="at">K =</span> <span class="dv">10</span>)</span>
<span id="cb12-12"><a href="#cb12-12" aria-hidden="true" tabindex="-1"></a>    cv_matrix[rep, d] <span class="ot">&lt;-</span> cv_result<span class="sc">$</span>delta[<span class="dv">1</span>]</span>
<span id="cb12-13"><a href="#cb12-13" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb12-14"><a href="#cb12-14" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb12-15"><a href="#cb12-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-16"><a href="#cb12-16" aria-hidden="true" tabindex="-1"></a>cv_means <span class="ot">&lt;-</span> <span class="fu">colMeans</span>(cv_matrix)</span>
<span id="cb12-17"><a href="#cb12-17" aria-hidden="true" tabindex="-1"></a>cv_se <span class="ot">&lt;-</span> <span class="fu">apply</span>(cv_matrix, <span class="dv">2</span>, sd) <span class="sc">/</span> <span class="fu">sqrt</span>(n_repeats)</span>
<span id="cb12-18"><a href="#cb12-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-19"><a href="#cb12-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot with error bars</span></span>
<span id="cb12-20"><a href="#cb12-20" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(degrees, cv_means, <span class="at">type =</span> <span class="st">"b"</span>, <span class="at">pch =</span> <span class="dv">19</span>, <span class="at">col =</span> <span class="st">"blue"</span>,</span>
<span id="cb12-21"><a href="#cb12-21" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">"Polynomial Degree"</span>, <span class="at">ylab =</span> <span class="st">"CV Error (MSE)"</span>,</span>
<span id="cb12-22"><a href="#cb12-22" aria-hidden="true" tabindex="-1"></a>     <span class="at">main =</span> <span class="st">"One Standard Error Rule"</span>,</span>
<span id="cb12-23"><a href="#cb12-23" aria-hidden="true" tabindex="-1"></a>     <span class="at">ylim =</span> <span class="fu">c</span>(<span class="fu">min</span>(cv_means <span class="sc">-</span> cv_se), <span class="fu">max</span>(cv_means <span class="sc">+</span> cv_se)))</span>
<span id="cb12-24"><a href="#cb12-24" aria-hidden="true" tabindex="-1"></a><span class="fu">arrows</span>(degrees, cv_means <span class="sc">-</span> cv_se, degrees, cv_means <span class="sc">+</span> cv_se,</span>
<span id="cb12-25"><a href="#cb12-25" aria-hidden="true" tabindex="-1"></a>       <span class="at">code =</span> <span class="dv">3</span>, <span class="at">angle =</span> <span class="dv">90</span>, <span class="at">length =</span> <span class="fl">0.05</span>, <span class="at">col =</span> <span class="st">"blue"</span>)</span>
<span id="cb12-26"><a href="#cb12-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-27"><a href="#cb12-27" aria-hidden="true" tabindex="-1"></a><span class="co"># Find minimum and 1-SE model</span></span>
<span id="cb12-28"><a href="#cb12-28" aria-hidden="true" tabindex="-1"></a>min_idx <span class="ot">&lt;-</span> <span class="fu">which.min</span>(cv_means)</span>
<span id="cb12-29"><a href="#cb12-29" aria-hidden="true" tabindex="-1"></a>threshold <span class="ot">&lt;-</span> cv_means[min_idx] <span class="sc">+</span> cv_se[min_idx]</span>
<span id="cb12-30"><a href="#cb12-30" aria-hidden="true" tabindex="-1"></a>one_se_idx <span class="ot">&lt;-</span> <span class="fu">min</span>(<span class="fu">which</span>(cv_means <span class="sc">&lt;=</span> threshold))</span>
<span id="cb12-31"><a href="#cb12-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-32"><a href="#cb12-32" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">h =</span> threshold, <span class="at">lty =</span> <span class="dv">2</span>, <span class="at">col =</span> <span class="st">"red"</span>)</span>
<span id="cb12-33"><a href="#cb12-33" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span>(min_idx, cv_means[min_idx], <span class="at">pch =</span> <span class="dv">19</span>, <span class="at">col =</span> <span class="st">"red"</span>, <span class="at">cex =</span> <span class="dv">2</span>)</span>
<span id="cb12-34"><a href="#cb12-34" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span>(one_se_idx, cv_means[one_se_idx], <span class="at">pch =</span> <span class="dv">17</span>, <span class="at">col =</span> <span class="st">"darkgreen"</span>, <span class="at">cex =</span> <span class="dv">2</span>)</span>
<span id="cb12-35"><a href="#cb12-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-36"><a href="#cb12-36" aria-hidden="true" tabindex="-1"></a><span class="fu">legend</span>(<span class="st">"topright"</span>, <span class="fu">c</span>(<span class="st">"Minimum CV error"</span>, <span class="st">"1-SE rule selection"</span>, <span class="st">"1-SE threshold"</span>),</span>
<span id="cb12-37"><a href="#cb12-37" aria-hidden="true" tabindex="-1"></a>       <span class="at">pch =</span> <span class="fu">c</span>(<span class="dv">19</span>, <span class="dv">17</span>, <span class="cn">NA</span>), <span class="at">lty =</span> <span class="fu">c</span>(<span class="cn">NA</span>, <span class="cn">NA</span>, <span class="dv">2</span>), <span class="at">col =</span> <span class="fu">c</span>(<span class="st">"red"</span>, <span class="st">"darkgreen"</span>, <span class="st">"red"</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div id="fig-one-se-rule" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-one-se-rule-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="29-model-validation_files/figure-html/fig-one-se-rule-1.png" class="img-fluid figure-img" width="768">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-one-se-rule-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;30.7: The one standard error rule: select the simplest model within one SE of the minimum CV error
</figcaption>
</figure>
</div>
</div>
</div>
</section>
</section>
<section id="information-criteria" class="level2" data-number="30.7">
<h2 data-number="30.7" class="anchored" data-anchor-id="information-criteria"><span class="header-section-number">30.7</span> Information Criteria</h2>
<p>An alternative to cross-validation is using information criteria, which balance fit quality against model complexity:</p>
<p><strong>Akaike Information Criterion (AIC)</strong>: <span class="math display">\[\text{AIC} = -2 \log L + 2k\]</span></p>
<p><strong>Bayesian Information Criterion (BIC)</strong>: <span class="math display">\[\text{BIC} = -2 \log L + k \log n\]</span></p>
<p>where <span class="math inline">\(L\)</span> is the likelihood, <span class="math inline">\(k\)</span> is the number of parameters, and <span class="math inline">\(n\)</span> is the sample size. Both criteria balance fit quality (the likelihood term) against model complexity (the penalty term), but they weight these differently.</p>
<p>Lower values indicate better models for both criteria. BIC penalizes complexity more heavily than AIC because its penalty term grows with <span class="math inline">\(\log n\)</span>, whereas AIC’s penalty is fixed at 2 per parameter. As a result, BIC tends to select simpler models, especially with larger sample sizes. AIC, being more lenient toward complexity, tends to select more complex models. Neither is universally better—the appropriate choice depends on whether you prioritize prediction (favor AIC) or identification of the true model structure (favor BIC).</p>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Compare AIC and BIC for polynomial models</span></span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>aic_values <span class="ot">&lt;-</span> bic_values <span class="ot">&lt;-</span> <span class="fu">numeric</span>(<span class="fu">length</span>(degrees))</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (d <span class="cf">in</span> degrees) {</span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>  model <span class="ot">&lt;-</span> <span class="fu">lm</span>(y <span class="sc">~</span> <span class="fu">poly</span>(x, d), <span class="at">data =</span> cv_data)</span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a>  aic_values[d] <span class="ot">&lt;-</span> <span class="fu">AIC</span>(model)</span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a>  bic_values[d] <span class="ot">&lt;-</span> <span class="fu">BIC</span>(model)</span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"Best degree by AIC:"</span>, <span class="fu">which.min</span>(aic_values), <span class="st">"</span><span class="sc">\n</span><span class="st">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Best degree by AIC: 6 </code></pre>
</div>
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"Best degree by BIC:"</span>, <span class="fu">which.min</span>(bic_values), <span class="st">"</span><span class="sc">\n</span><span class="st">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Best degree by BIC: 6 </code></pre>
</div>
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"Best degree by CV:"</span>, best_degree, <span class="st">"</span><span class="sc">\n</span><span class="st">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Best degree by CV: 6 </code></pre>
</div>
</div>
</section>
<section id="practical-workflow" class="level2" data-number="30.8">
<h2 data-number="30.8" class="anchored" data-anchor-id="practical-workflow"><span class="header-section-number">30.8</span> Practical Workflow</h2>
<p>A disciplined approach to model validation requires separating the data you use for development from the data you use for final evaluation. Begin by splitting your data into a training set (typically 70-80% of observations) and a test set (the remainder). The test set should be locked away—never look at it during model development.</p>
<p>Work exclusively with the training data during the model development phase. Explore the data to understand distributions and relationships. Use cross-validation on the training set to compare different model types, tune hyperparameters, and estimate expected performance. This iterative process of trying models, evaluating them with CV, and refining your approach may involve many cycles.</p>
<p>Once you have settled on a final model based on cross-validation performance, refit that model on the full training data to use all available information for parameter estimation. Only then—when all modeling decisions are complete—evaluate this final model on the held-out test set. This single evaluation provides an honest estimate of how your model will perform on truly new data. Report this test error as your final performance metric, resisting any temptation to go back and revise the model based on test set results.</p>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Common Mistakes
</div>
</div>
<div class="callout-body-container callout-body">
<ol type="1">
<li><strong>Using test data during model development</strong>: The test set must be locked away until final evaluation</li>
<li><strong>Reporting training error</strong>: Training error is optimistic and misleading</li>
<li><strong>Selecting models based on test error, then reporting test error</strong>: This invalidates the test error estimate</li>
<li><strong>Not setting random seeds</strong>: Results should be reproducible</li>
<li><strong>Ignoring variance in CV estimates</strong>: Single CV runs can be noisy</li>
</ol>
</div>
</div>
</section>
<section id="exercises" class="level2" data-number="30.9">
<h2 data-number="30.9" class="anchored" data-anchor-id="exercises"><span class="header-section-number">30.9</span> Exercises</h2>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Exercise MV.1: Training vs.&nbsp;Test Error
</div>
</div>
<div class="callout-body-container callout-body">
<ol type="1">
<li><p>Generate data from the model <span class="math inline">\(y = \sin(x) + \epsilon\)</span> where <span class="math inline">\(\epsilon \sim N(0, 0.25)\)</span> and <span class="math inline">\(x\)</span> ranges from 0 to <span class="math inline">\(2\pi\)</span>. Split into 80% training and 20% test.</p></li>
<li><p>Fit polynomial models of degrees 1 through 15. Plot both training and test error. At what degree does overfitting begin?</p></li>
</ol>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Exercise MV.2: Cross-Validation Practice
</div>
</div>
<div class="callout-body-container callout-body">
<ol start="3" type="1">
<li><p>Using the same data, implement 5-fold and 10-fold cross-validation. Compare the selected optimal degree between the two approaches. Which is more variable across different random seeds?</p></li>
<li><p>Implement LOOCV and compare to k-fold CV. How do the computational costs compare?</p></li>
</ol>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Exercise MV.3: The One Standard Error Rule
</div>
</div>
<div class="callout-body-container callout-body">
<ol start="5" type="1">
<li><p>Generate 100 random training/test splits. For each, use 10-fold CV to select the optimal polynomial degree. Plot the distribution of selected degrees.</p></li>
<li><p>Repeat using the one standard error rule. How does the distribution change?</p></li>
</ol>
</div>
</div>
</section>
<section id="summary" class="level2" data-number="30.10">
<h2 data-number="30.10" class="anchored" data-anchor-id="summary"><span class="header-section-number">30.10</span> Summary</h2>
<p>This chapter addressed one of the central challenges in statistical learning: how do we build models that generalize well to new data rather than merely memorizing the training set? <strong>Overfitting</strong> occurs when models capture noise specific to the training data rather than true underlying patterns, and avoiding this trap requires careful attention to model validation.</p>
<p>We quantify prediction quality through <strong>loss functions</strong>—squared error for regression, log loss or 0-1 loss for classification—and the choice should reflect how errors affect your particular application. A key insight is that <strong>training error</strong> systematically underestimates true prediction error; models always look better on the data used to fit them than on genuinely new observations.</p>
<p>The <strong>bias-variance tradeoff</strong> provides the conceptual framework for understanding model complexity. Simple models have high bias (they may miss important patterns) but low variance (they’re stable across different training samples). Complex models have the opposite problem: low bias but high variance. The optimal model balances these competing concerns, achieving the lowest total error.</p>
<p><strong>Cross-validation</strong> provides a practical tool for estimating generalization error without wasting data on a held-out test set. K-fold cross-validation with k=5 or 10 typically offers a good balance of bias and variance. LOOCV reduces bias but increases variance and computational cost. The bootstrap provides an alternative approach, particularly useful when confidence intervals are also needed.</p>
<p>When multiple models have similar cross-validation error, the <strong>one standard error rule</strong> suggests choosing the simplest model within one standard error of the minimum, guarding against overfitting to the CV procedure itself. <strong>Information criteria</strong> like AIC and BIC offer alternatives that balance fit against complexity without requiring repeated model fitting.</p>
<p>Finally, a disciplined workflow that strictly separates training, validation, and testing phases is essential for honest performance assessment. The test set must remain untouched until final evaluation—peeking at it during development invalidates the entire exercise.</p>
</section>
<section id="additional-resources" class="level2" data-number="30.11">
<h2 data-number="30.11" class="anchored" data-anchor-id="additional-resources"><span class="header-section-number">30.11</span> Additional Resources</h2>
<ul>
<li><span class="citation" data-cites="james2023islr">James et al. (<a href="../references.html#ref-james2023islr" role="doc-biblioref">2023</a>)</span> - Comprehensive treatment of resampling methods</li>
<li><span class="citation" data-cites="hastie2009elements">Hastie, Tibshirani, and Friedman (<a href="../references.html#ref-hastie2009elements" role="doc-biblioref">2009</a>)</span> - Theoretical foundations of bias-variance tradeoff</li>
<li><span class="citation" data-cites="thulin2025msr">Thulin (<a href="../references.html#ref-thulin2025msr" role="doc-biblioref">2025</a>)</span> - Practical model validation in R</li>
</ul>


<!-- -->

<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list" style="display: none">
<div id="ref-hastie2009elements" class="csl-entry" role="listitem">
Hastie, Trevor, Robert Tibshirani, and Jerome Friedman. 2009. <em>The Elements of Statistical Learning: Data Mining, Inference, and Prediction</em>. 2nd ed. New York: Springer.
</div>
<div id="ref-james2023islr" class="csl-entry" role="listitem">
James, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2023. <em>An Introduction to Statistical Learning with Applications in r</em>. 2nd ed. Springer. <a href="https://www.statlearning.com">https://www.statlearning.com</a>.
</div>
<div id="ref-thulin2025msr" class="csl-entry" role="listitem">
Thulin, Måns. 2025. <em>Modern Statistics with r</em>. CRC Press. <a href="https://www.modernstatisticswithr.com">https://www.modernstatisticswithr.com</a>.
</div>
</div>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
    const viewSource = window.document.getElementById('quarto-view-source') ||
                       window.document.getElementById('quarto-code-tools-source');
    if (viewSource) {
      const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
      viewSource.addEventListener("click", function(e) {
        if (sourceUrl) {
          // rstudio viewer pane
          if (/\bcapabilities=\b/.test(window.location)) {
            window.open(sourceUrl);
          } else {
            window.location.href = sourceUrl;
          }
        } else {
          const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
          modal.show();
        }
        return false;
      });
    }
    function toggleCodeHandler(show) {
      return function(e) {
        const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
        for (let i=0; i<detailsSrc.length; i++) {
          const details = detailsSrc[i].parentElement;
          if (show) {
            details.open = true;
          } else {
            details.removeAttribute("open");
          }
        }
        const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
        const fromCls = show ? "hidden" : "unhidden";
        const toCls = show ? "unhidden" : "hidden";
        for (let i=0; i<cellCodeDivs.length; i++) {
          const codeDiv = cellCodeDivs[i];
          if (codeDiv.classList.contains(fromCls)) {
            codeDiv.classList.remove(fromCls);
            codeDiv.classList.add(toCls);
          } 
        }
        return false;
      }
    }
    const hideAllCode = window.document.getElementById("quarto-hide-all-code");
    if (hideAllCode) {
      hideAllCode.addEventListener("click", toggleCodeHandler(false));
    }
    const showAllCode = window.document.getElementById("quarto-show-all-code");
    if (showAllCode) {
      showAllCode.addEventListener("click", toggleCodeHandler(true));
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../chapters/28-intro-statistical-learning.html" class="pagination-link" aria-label="Introduction to Statistical Learning">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">29</span>&nbsp; <span class="chapter-title">Introduction to Statistical Learning</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../chapters/30-regularization.html" class="pagination-link" aria-label="Regularization Methods">
        <span class="nav-page-text"><span class="chapter-number">31</span>&nbsp; <span class="chapter-title">Regularization Methods</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb19" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="fu"># Model Validation and the Bias-Variance Tradeoff {#sec-model-validation}</span></span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: false</span></span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a><span class="co">#| message: false</span></span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tidyverse)</span>
<span id="cb19-9"><a href="#cb19-9" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(boot)</span>
<span id="cb19-10"><a href="#cb19-10" aria-hidden="true" tabindex="-1"></a><span class="fu">theme_set</span>(<span class="fu">theme_minimal</span>())</span>
<span id="cb19-11"><a href="#cb19-11" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb19-12"><a href="#cb19-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-13"><a href="#cb19-13" aria-hidden="true" tabindex="-1"></a><span class="fu">## The Overfitting Problem</span></span>
<span id="cb19-14"><a href="#cb19-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-15"><a href="#cb19-15" aria-hidden="true" tabindex="-1"></a>Models are built to fit training data as closely as possible. A linear regression minimizes squared errors; a logistic regression maximizes likelihood. But models that fit training data too well often predict poorly on new data.</span>
<span id="cb19-16"><a href="#cb19-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-17"><a href="#cb19-17" aria-hidden="true" tabindex="-1"></a>**Overfitting** occurs when a model captures noise specific to the training data rather than the true underlying pattern. Complex models with many parameters are especially susceptible.</span>
<span id="cb19-18"><a href="#cb19-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-19"><a href="#cb19-19" aria-hidden="true" tabindex="-1"></a>The solution is to evaluate models on data they have not seen—held-out test data or through cross-validation.</span>
<span id="cb19-20"><a href="#cb19-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-21"><a href="#cb19-21" aria-hidden="true" tabindex="-1"></a><span class="fu">## Loss Functions: Quantifying Prediction Error</span></span>
<span id="cb19-22"><a href="#cb19-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-23"><a href="#cb19-23" aria-hidden="true" tabindex="-1"></a>A **loss function** (or **cost function**) measures how wrong a prediction is. It quantifies the penalty for predicting $\hat{y}$ when the true value is $y$.</span>
<span id="cb19-24"><a href="#cb19-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-25"><a href="#cb19-25" aria-hidden="true" tabindex="-1"></a><span class="fu">### Common Loss Functions for Regression</span></span>
<span id="cb19-26"><a href="#cb19-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-27"><a href="#cb19-27" aria-hidden="true" tabindex="-1"></a>**Squared Error Loss** (L2): The most common loss for continuous outcomes:</span>
<span id="cb19-28"><a href="#cb19-28" aria-hidden="true" tabindex="-1"></a>$$L(y, \hat{y}) = (y - \hat{y})^2$$</span>
<span id="cb19-29"><a href="#cb19-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-30"><a href="#cb19-30" aria-hidden="true" tabindex="-1"></a>Squaring penalizes large errors more heavily than small ones. Linear regression minimizes the sum of squared errors (SSE or RSS).</span>
<span id="cb19-31"><a href="#cb19-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-32"><a href="#cb19-32" aria-hidden="true" tabindex="-1"></a>**Absolute Error Loss** (L1): Less sensitive to outliers:</span>
<span id="cb19-33"><a href="#cb19-33" aria-hidden="true" tabindex="-1"></a>$$L(y, \hat{y}) = |y - \hat{y}|$$</span>
<span id="cb19-34"><a href="#cb19-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-35"><a href="#cb19-35" aria-hidden="true" tabindex="-1"></a>**Mean Squared Error (MSE)** and **Root Mean Squared Error (RMSE)** are averages across all predictions:</span>
<span id="cb19-36"><a href="#cb19-36" aria-hidden="true" tabindex="-1"></a>$$\text{MSE} = \frac{1}{n}\sum_{i=1}^n (y_i - \hat{y}_i)^2, \quad \text{RMSE} = \sqrt{\text{MSE}}$$</span>
<span id="cb19-37"><a href="#cb19-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-38"><a href="#cb19-38" aria-hidden="true" tabindex="-1"></a><span class="fu">### Common Loss Functions for Classification</span></span>
<span id="cb19-39"><a href="#cb19-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-40"><a href="#cb19-40" aria-hidden="true" tabindex="-1"></a>**0-1 Loss**: The simplest classification loss—1 if wrong, 0 if correct:</span>
<span id="cb19-41"><a href="#cb19-41" aria-hidden="true" tabindex="-1"></a>$$L(y, \hat{y}) = \mathbb{I}(y \neq \hat{y})$$</span>
<span id="cb19-42"><a href="#cb19-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-43"><a href="#cb19-43" aria-hidden="true" tabindex="-1"></a>The average 0-1 loss is the **error rate**; one minus the error rate is **accuracy**.</span>
<span id="cb19-44"><a href="#cb19-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-45"><a href="#cb19-45" aria-hidden="true" tabindex="-1"></a>**Log Loss** (Cross-Entropy): Used when we have predicted probabilities $\hat{p}$:</span>
<span id="cb19-46"><a href="#cb19-46" aria-hidden="true" tabindex="-1"></a>$$L(y, \hat{p}) = -<span class="co">[</span><span class="ot">y \log(\hat{p}) + (1-y) \log(1-\hat{p})</span><span class="co">]</span>$$</span>
<span id="cb19-47"><a href="#cb19-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-48"><a href="#cb19-48" aria-hidden="true" tabindex="-1"></a>Log loss penalizes confident wrong predictions severely—predicting probability 0.99 for the wrong class incurs much larger loss than predicting 0.6.</span>
<span id="cb19-49"><a href="#cb19-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-52"><a href="#cb19-52" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb19-53"><a href="#cb19-53" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-loss-functions</span></span>
<span id="cb19-54"><a href="#cb19-54" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "Comparison of squared loss (penalizes large errors heavily) versus absolute loss (more robust to outliers)"</span></span>
<span id="cb19-55"><a href="#cb19-55" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-width: 8</span></span>
<span id="cb19-56"><a href="#cb19-56" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-height: 4</span></span>
<span id="cb19-57"><a href="#cb19-57" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">2</span>))</span>
<span id="cb19-58"><a href="#cb19-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-59"><a href="#cb19-59" aria-hidden="true" tabindex="-1"></a><span class="co"># Regression loss functions</span></span>
<span id="cb19-60"><a href="#cb19-60" aria-hidden="true" tabindex="-1"></a>errors <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="sc">-</span><span class="dv">3</span>, <span class="dv">3</span>, <span class="at">length.out =</span> <span class="dv">100</span>)</span>
<span id="cb19-61"><a href="#cb19-61" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(errors, errors<span class="sc">^</span><span class="dv">2</span>, <span class="at">type =</span> <span class="st">"l"</span>, <span class="at">col =</span> <span class="st">"blue"</span>, <span class="at">lwd =</span> <span class="dv">2</span>,</span>
<span id="cb19-62"><a href="#cb19-62" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">"Prediction Error (y - ŷ)"</span>, <span class="at">ylab =</span> <span class="st">"Loss"</span>,</span>
<span id="cb19-63"><a href="#cb19-63" aria-hidden="true" tabindex="-1"></a>     <span class="at">main =</span> <span class="st">"Regression Loss Functions"</span>)</span>
<span id="cb19-64"><a href="#cb19-64" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(errors, <span class="fu">abs</span>(errors), <span class="at">col =</span> <span class="st">"red"</span>, <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb19-65"><a href="#cb19-65" aria-hidden="true" tabindex="-1"></a><span class="fu">legend</span>(<span class="st">"top"</span>, <span class="fu">c</span>(<span class="st">"Squared (L2)"</span>, <span class="st">"Absolute (L1)"</span>),</span>
<span id="cb19-66"><a href="#cb19-66" aria-hidden="true" tabindex="-1"></a>       <span class="at">col =</span> <span class="fu">c</span>(<span class="st">"blue"</span>, <span class="st">"red"</span>), <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb19-67"><a href="#cb19-67" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-68"><a href="#cb19-68" aria-hidden="true" tabindex="-1"></a><span class="co"># Classification log loss</span></span>
<span id="cb19-69"><a href="#cb19-69" aria-hidden="true" tabindex="-1"></a>p <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="fl">0.01</span>, <span class="fl">0.99</span>, <span class="at">length.out =</span> <span class="dv">100</span>)</span>
<span id="cb19-70"><a href="#cb19-70" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(p, <span class="sc">-</span><span class="fu">log</span>(p), <span class="at">type =</span> <span class="st">"l"</span>, <span class="at">col =</span> <span class="st">"blue"</span>, <span class="at">lwd =</span> <span class="dv">2</span>,</span>
<span id="cb19-71"><a href="#cb19-71" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">"Predicted Probability for True Class"</span>, <span class="at">ylab =</span> <span class="st">"Log Loss"</span>,</span>
<span id="cb19-72"><a href="#cb19-72" aria-hidden="true" tabindex="-1"></a>     <span class="at">main =</span> <span class="st">"Classification Log Loss"</span>)</span>
<span id="cb19-73"><a href="#cb19-73" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">v =</span> <span class="fl">0.5</span>, <span class="at">lty =</span> <span class="dv">2</span>, <span class="at">col =</span> <span class="st">"gray"</span>)</span>
<span id="cb19-74"><a href="#cb19-74" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb19-75"><a href="#cb19-75" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-76"><a href="#cb19-76" aria-hidden="true" tabindex="-1"></a><span class="fu">### Why Loss Functions Matter</span></span>
<span id="cb19-77"><a href="#cb19-77" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-78"><a href="#cb19-78" aria-hidden="true" tabindex="-1"></a>Different loss functions lead to different optimal predictions, which has profound implications for how we should model data. Under squared loss, the optimal prediction at any point is the **mean** of the outcome distribution—this is why ordinary least squares produces predictions that minimize average squared error. Under absolute loss, the optimal prediction is the **median**, which is more robust to outliers. For classification under 0-1 loss, the optimal prediction is simply the **mode**—the most frequent class.</span>
<span id="cb19-79"><a href="#cb19-79" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-80"><a href="#cb19-80" aria-hidden="true" tabindex="-1"></a>The choice of loss function should reflect how errors affect your application. In medical diagnosis, the consequences of different types of errors are typically asymmetric: missing a disease (false negative) may be far more costly than a false alarm (false positive). A thoughtful analysis would weight these errors differently rather than treating all misclassifications equally.</span>
<span id="cb19-81"><a href="#cb19-81" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-82"><a href="#cb19-82" aria-hidden="true" tabindex="-1"></a><span class="fu">## Training Error vs. Test Error</span></span>
<span id="cb19-83"><a href="#cb19-83" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-84"><a href="#cb19-84" aria-hidden="true" tabindex="-1"></a>A fundamental insight of statistical learning is that **training error** (how well we fit the data used to build the model) is an overly optimistic estimate of **test error** (how well we predict new data).</span>
<span id="cb19-85"><a href="#cb19-85" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-88"><a href="#cb19-88" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb19-89"><a href="#cb19-89" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-train-test-error</span></span>
<span id="cb19-90"><a href="#cb19-90" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "Training error always decreases with model complexity, but test error eventually increases due to overfitting. The optimal model minimizes test error."</span></span>
<span id="cb19-91"><a href="#cb19-91" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-width: 8</span></span>
<span id="cb19-92"><a href="#cb19-92" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-height: 5</span></span>
<span id="cb19-93"><a href="#cb19-93" aria-hidden="true" tabindex="-1"></a><span class="co"># Demonstrate training vs test error</span></span>
<span id="cb19-94"><a href="#cb19-94" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">42</span>)</span>
<span id="cb19-95"><a href="#cb19-95" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">100</span></span>
<span id="cb19-96"><a href="#cb19-96" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">sort</span>(<span class="fu">runif</span>(n, <span class="dv">0</span>, <span class="dv">10</span>))</span>
<span id="cb19-97"><a href="#cb19-97" aria-hidden="true" tabindex="-1"></a>y_true <span class="ot">&lt;-</span> <span class="fu">sin</span>(x) <span class="sc">+</span> <span class="fl">0.5</span> <span class="sc">*</span> <span class="fu">cos</span>(<span class="fl">0.5</span> <span class="sc">*</span> x)</span>
<span id="cb19-98"><a href="#cb19-98" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> y_true <span class="sc">+</span> <span class="fu">rnorm</span>(n, <span class="at">sd =</span> <span class="fl">0.3</span>)</span>
<span id="cb19-99"><a href="#cb19-99" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-100"><a href="#cb19-100" aria-hidden="true" tabindex="-1"></a><span class="co"># Split into training and test</span></span>
<span id="cb19-101"><a href="#cb19-101" aria-hidden="true" tabindex="-1"></a>train_idx <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="dv">1</span><span class="sc">:</span>n, <span class="dv">70</span>)</span>
<span id="cb19-102"><a href="#cb19-102" aria-hidden="true" tabindex="-1"></a>train_data <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">x =</span> x[train_idx], <span class="at">y =</span> y[train_idx])</span>
<span id="cb19-103"><a href="#cb19-103" aria-hidden="true" tabindex="-1"></a>test_data <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">x =</span> x[<span class="sc">-</span>train_idx], <span class="at">y =</span> y[<span class="sc">-</span>train_idx])</span>
<span id="cb19-104"><a href="#cb19-104" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-105"><a href="#cb19-105" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit polynomials of increasing degree</span></span>
<span id="cb19-106"><a href="#cb19-106" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(splines)</span>
<span id="cb19-107"><a href="#cb19-107" aria-hidden="true" tabindex="-1"></a>degrees <span class="ot">&lt;-</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">15</span></span>
<span id="cb19-108"><a href="#cb19-108" aria-hidden="true" tabindex="-1"></a>train_error <span class="ot">&lt;-</span> test_error <span class="ot">&lt;-</span> <span class="fu">numeric</span>(<span class="fu">length</span>(degrees))</span>
<span id="cb19-109"><a href="#cb19-109" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-110"><a href="#cb19-110" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="fu">seq_along</span>(degrees)) {</span>
<span id="cb19-111"><a href="#cb19-111" aria-hidden="true" tabindex="-1"></a>  d <span class="ot">&lt;-</span> degrees[i]</span>
<span id="cb19-112"><a href="#cb19-112" aria-hidden="true" tabindex="-1"></a>  fit <span class="ot">&lt;-</span> <span class="fu">lm</span>(y <span class="sc">~</span> <span class="fu">poly</span>(x, d), <span class="at">data =</span> train_data)</span>
<span id="cb19-113"><a href="#cb19-113" aria-hidden="true" tabindex="-1"></a>  train_error[i] <span class="ot">&lt;-</span> <span class="fu">mean</span>((train_data<span class="sc">$</span>y <span class="sc">-</span> <span class="fu">predict</span>(fit, train_data))<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb19-114"><a href="#cb19-114" aria-hidden="true" tabindex="-1"></a>  test_error[i] <span class="ot">&lt;-</span> <span class="fu">mean</span>((test_data<span class="sc">$</span>y <span class="sc">-</span> <span class="fu">predict</span>(fit, test_data))<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb19-115"><a href="#cb19-115" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb19-116"><a href="#cb19-116" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-117"><a href="#cb19-117" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot</span></span>
<span id="cb19-118"><a href="#cb19-118" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(degrees, train_error, <span class="at">type =</span> <span class="st">"b"</span>, <span class="at">pch =</span> <span class="dv">19</span>, <span class="at">col =</span> <span class="st">"blue"</span>,</span>
<span id="cb19-119"><a href="#cb19-119" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">"Model Complexity (Polynomial Degree)"</span>,</span>
<span id="cb19-120"><a href="#cb19-120" aria-hidden="true" tabindex="-1"></a>     <span class="at">ylab =</span> <span class="st">"Mean Squared Error"</span>,</span>
<span id="cb19-121"><a href="#cb19-121" aria-hidden="true" tabindex="-1"></a>     <span class="at">main =</span> <span class="st">"Training vs Test Error"</span>, <span class="at">ylim =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="fu">max</span>(test_error)))</span>
<span id="cb19-122"><a href="#cb19-122" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(degrees, test_error, <span class="at">type =</span> <span class="st">"b"</span>, <span class="at">pch =</span> <span class="dv">19</span>, <span class="at">col =</span> <span class="st">"red"</span>)</span>
<span id="cb19-123"><a href="#cb19-123" aria-hidden="true" tabindex="-1"></a><span class="fu">legend</span>(<span class="st">"topright"</span>, <span class="fu">c</span>(<span class="st">"Training Error"</span>, <span class="st">"Test Error"</span>),</span>
<span id="cb19-124"><a href="#cb19-124" aria-hidden="true" tabindex="-1"></a>       <span class="at">col =</span> <span class="fu">c</span>(<span class="st">"blue"</span>, <span class="st">"red"</span>), <span class="at">pch =</span> <span class="dv">19</span>, <span class="at">lty =</span> <span class="dv">1</span>)</span>
<span id="cb19-125"><a href="#cb19-125" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">v =</span> degrees[<span class="fu">which.min</span>(test_error)], <span class="at">lty =</span> <span class="dv">2</span>, <span class="at">col =</span> <span class="st">"gray"</span>)</span>
<span id="cb19-126"><a href="#cb19-126" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb19-127"><a href="#cb19-127" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-128"><a href="#cb19-128" aria-hidden="true" tabindex="-1"></a>Notice that training error keeps decreasing as complexity increases, eventually reaching near zero. But test error follows a U-shape—it decreases initially as the model captures true patterns, then increases as the model starts fitting noise.</span>
<span id="cb19-129"><a href="#cb19-129" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-130"><a href="#cb19-130" aria-hidden="true" tabindex="-1"></a><span class="fu">## The Bias-Variance Tradeoff</span></span>
<span id="cb19-131"><a href="#cb19-131" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-132"><a href="#cb19-132" aria-hidden="true" tabindex="-1"></a>Prediction error has two main components:</span>
<span id="cb19-133"><a href="#cb19-133" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-134"><a href="#cb19-134" aria-hidden="true" tabindex="-1"></a>**Bias**: Error from approximating a complex reality with a simpler model. Simple models have high bias—they may miss important patterns.</span>
<span id="cb19-135"><a href="#cb19-135" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-136"><a href="#cb19-136" aria-hidden="true" tabindex="-1"></a>**Variance**: Error from sensitivity to training data. Complex models have high variance—they change substantially with different training samples.</span>
<span id="cb19-137"><a href="#cb19-137" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-138"><a href="#cb19-138" aria-hidden="true" tabindex="-1"></a>@fig-bias-variance-concept illustrates this tradeoff visually. On the left, an underfit model (high bias) misses the pattern in the data. On the right, an overfit model (high variance) captures noise rather than signal. The optimal model complexity, shown in the middle, balances these competing concerns to minimize total error.</span>
<span id="cb19-139"><a href="#cb19-139" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-140"><a href="#cb19-140" aria-hidden="true" tabindex="-1"></a><span class="al">![The bias-variance tradeoff visualized. Simple models (left) underfit the data, showing high bias. Complex models (right) overfit, showing high variance. The optimal model complexity (center) minimizes total error by balancing bias and variance. The U-shaped total error curve emerges because decreasing bias eventually comes at the cost of increasing variance.](../images/ch29/ch29_bias_variance.jpeg)</span>{#fig-bias-variance-concept fig-align="center" width="80%"}</span>
<span id="cb19-141"><a href="#cb19-141" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-142"><a href="#cb19-142" aria-hidden="true" tabindex="-1"></a>Mathematically, for a given test point $x_0$, the expected prediction error can be decomposed as:</span>
<span id="cb19-143"><a href="#cb19-143" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-144"><a href="#cb19-144" aria-hidden="true" tabindex="-1"></a>$$E<span class="co">[</span><span class="ot">(y_0 - \hat{f}(x_0))^2</span><span class="co">]</span> = \text{Var}(\hat{f}(x_0)) + <span class="co">[</span><span class="ot">\text{Bias}(\hat{f}(x_0))</span><span class="co">]</span>^2 + \text{Var}(\epsilon)$$</span>
<span id="cb19-145"><a href="#cb19-145" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-146"><a href="#cb19-146" aria-hidden="true" tabindex="-1"></a>where:</span>
<span id="cb19-147"><a href="#cb19-147" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\text{Var}(\hat{f}(x_0))$ is the variance of the model predictions</span>
<span id="cb19-148"><a href="#cb19-148" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\text{Bias}(\hat{f}(x_0))$ is the bias (systematic error)</span>
<span id="cb19-149"><a href="#cb19-149" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\text{Var}(\epsilon)$ is the irreducible error (noise in the data)</span>
<span id="cb19-150"><a href="#cb19-150" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-153"><a href="#cb19-153" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb19-154"><a href="#cb19-154" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-bias-variance</span></span>
<span id="cb19-155"><a href="#cb19-155" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "The bias-variance tradeoff: as model complexity increases, bias decreases but variance increases. The optimal model minimizes total error."</span></span>
<span id="cb19-156"><a href="#cb19-156" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-width: 8</span></span>
<span id="cb19-157"><a href="#cb19-157" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-height: 5</span></span>
<span id="cb19-158"><a href="#cb19-158" aria-hidden="true" tabindex="-1"></a><span class="co"># Illustrate bias-variance tradeoff conceptually</span></span>
<span id="cb19-159"><a href="#cb19-159" aria-hidden="true" tabindex="-1"></a>complexity <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="fl">0.1</span>, <span class="dv">10</span>, <span class="at">length.out =</span> <span class="dv">100</span>)</span>
<span id="cb19-160"><a href="#cb19-160" aria-hidden="true" tabindex="-1"></a>bias_sq <span class="ot">&lt;-</span> <span class="dv">5</span> <span class="sc">/</span> complexity</span>
<span id="cb19-161"><a href="#cb19-161" aria-hidden="true" tabindex="-1"></a>variance <span class="ot">&lt;-</span> <span class="fl">0.1</span> <span class="sc">*</span> complexity<span class="sc">^</span><span class="fl">1.5</span></span>
<span id="cb19-162"><a href="#cb19-162" aria-hidden="true" tabindex="-1"></a>irreducible <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="fl">0.5</span>, <span class="fu">length</span>(complexity))</span>
<span id="cb19-163"><a href="#cb19-163" aria-hidden="true" tabindex="-1"></a>total <span class="ot">&lt;-</span> bias_sq <span class="sc">+</span> variance <span class="sc">+</span> irreducible</span>
<span id="cb19-164"><a href="#cb19-164" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-165"><a href="#cb19-165" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(complexity, total, <span class="at">type =</span> <span class="st">"l"</span>, <span class="at">lwd =</span> <span class="dv">3</span>, <span class="at">col =</span> <span class="st">"black"</span>,</span>
<span id="cb19-166"><a href="#cb19-166" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">"Model Complexity"</span>, <span class="at">ylab =</span> <span class="st">"Error"</span>,</span>
<span id="cb19-167"><a href="#cb19-167" aria-hidden="true" tabindex="-1"></a>     <span class="at">main =</span> <span class="st">"Bias-Variance Tradeoff"</span>, <span class="at">ylim =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="fu">max</span>(total)))</span>
<span id="cb19-168"><a href="#cb19-168" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(complexity, bias_sq, <span class="at">lwd =</span> <span class="dv">2</span>, <span class="at">col =</span> <span class="st">"blue"</span>, <span class="at">lty =</span> <span class="dv">2</span>)</span>
<span id="cb19-169"><a href="#cb19-169" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(complexity, variance, <span class="at">lwd =</span> <span class="dv">2</span>, <span class="at">col =</span> <span class="st">"red"</span>, <span class="at">lty =</span> <span class="dv">2</span>)</span>
<span id="cb19-170"><a href="#cb19-170" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(complexity, irreducible, <span class="at">lwd =</span> <span class="dv">2</span>, <span class="at">col =</span> <span class="st">"gray"</span>, <span class="at">lty =</span> <span class="dv">3</span>)</span>
<span id="cb19-171"><a href="#cb19-171" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-172"><a href="#cb19-172" aria-hidden="true" tabindex="-1"></a><span class="co"># Mark optimal</span></span>
<span id="cb19-173"><a href="#cb19-173" aria-hidden="true" tabindex="-1"></a>opt_idx <span class="ot">&lt;-</span> <span class="fu">which.min</span>(total)</span>
<span id="cb19-174"><a href="#cb19-174" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">v =</span> complexity[opt_idx], <span class="at">lty =</span> <span class="dv">2</span>, <span class="at">col =</span> <span class="st">"darkgreen"</span>)</span>
<span id="cb19-175"><a href="#cb19-175" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span>(complexity[opt_idx], total[opt_idx], <span class="at">pch =</span> <span class="dv">19</span>, <span class="at">cex =</span> <span class="dv">2</span>, <span class="at">col =</span> <span class="st">"darkgreen"</span>)</span>
<span id="cb19-176"><a href="#cb19-176" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-177"><a href="#cb19-177" aria-hidden="true" tabindex="-1"></a><span class="fu">legend</span>(<span class="st">"topright"</span>, <span class="fu">c</span>(<span class="st">"Total Error"</span>, <span class="st">"Bias²"</span>, <span class="st">"Variance"</span>, <span class="st">"Irreducible Error"</span>, <span class="st">"Optimal"</span>),</span>
<span id="cb19-178"><a href="#cb19-178" aria-hidden="true" tabindex="-1"></a>       <span class="at">col =</span> <span class="fu">c</span>(<span class="st">"black"</span>, <span class="st">"blue"</span>, <span class="st">"red"</span>, <span class="st">"gray"</span>, <span class="st">"darkgreen"</span>),</span>
<span id="cb19-179"><a href="#cb19-179" aria-hidden="true" tabindex="-1"></a>       <span class="at">lty =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">2</span>), <span class="at">lwd =</span> <span class="fu">c</span>(<span class="dv">3</span>, <span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">1</span>), <span class="at">pch =</span> <span class="fu">c</span>(<span class="cn">NA</span>, <span class="cn">NA</span>, <span class="cn">NA</span>, <span class="cn">NA</span>, <span class="dv">19</span>))</span>
<span id="cb19-180"><a href="#cb19-180" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb19-181"><a href="#cb19-181" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-182"><a href="#cb19-182" aria-hidden="true" tabindex="-1"></a>The best predictions come from models that balance bias and variance. As model complexity increases, bias decreases because more complex models can fit more patterns—they have the flexibility to capture structure that simpler models miss. But simultaneously, variance increases because more complex models are more sensitive to the particular training data used—with different training samples, they would produce substantially different predictions.</span>
<span id="cb19-183"><a href="#cb19-183" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-184"><a href="#cb19-184" aria-hidden="true" tabindex="-1"></a>The total prediction error reflects both components. Initially, as we add complexity, the reduction in bias dominates and total error decreases. But eventually, further complexity provides diminishing bias reduction while variance continues to grow. At this point, adding complexity hurts more than it helps, and total error begins to increase. The optimal model sits at the bottom of this U-shaped curve, complex enough to capture the true patterns but not so complex that it fits noise.</span>
<span id="cb19-185"><a href="#cb19-185" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-186"><a href="#cb19-186" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip}</span>
<span id="cb19-187"><a href="#cb19-187" aria-hidden="true" tabindex="-1"></a><span class="fu">## Understanding Bias and Variance</span></span>
<span id="cb19-188"><a href="#cb19-188" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-189"><a href="#cb19-189" aria-hidden="true" tabindex="-1"></a>**High bias** (underfitting):</span>
<span id="cb19-190"><a href="#cb19-190" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Model is too simple</span>
<span id="cb19-191"><a href="#cb19-191" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Consistently wrong predictions</span>
<span id="cb19-192"><a href="#cb19-192" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Predictions don't change much with different training data</span>
<span id="cb19-193"><a href="#cb19-193" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Example: Fitting a straight line to curved data</span>
<span id="cb19-194"><a href="#cb19-194" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-195"><a href="#cb19-195" aria-hidden="true" tabindex="-1"></a>**High variance** (overfitting):</span>
<span id="cb19-196"><a href="#cb19-196" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Model is too complex</span>
<span id="cb19-197"><a href="#cb19-197" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Predictions vary wildly with different training samples</span>
<span id="cb19-198"><a href="#cb19-198" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Fits training data well but generalizes poorly</span>
<span id="cb19-199"><a href="#cb19-199" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Example: High-degree polynomial fitting noise</span>
<span id="cb19-200"><a href="#cb19-200" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb19-201"><a href="#cb19-201" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-202"><a href="#cb19-202" aria-hidden="true" tabindex="-1"></a><span class="fu">## Cross-Validation</span></span>
<span id="cb19-203"><a href="#cb19-203" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-204"><a href="#cb19-204" aria-hidden="true" tabindex="-1"></a>Cross-validation estimates how well a model will generalize to new data without requiring a separate test set.</span>
<span id="cb19-205"><a href="#cb19-205" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-206"><a href="#cb19-206" aria-hidden="true" tabindex="-1"></a><span class="fu">### K-Fold Cross-Validation</span></span>
<span id="cb19-207"><a href="#cb19-207" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-208"><a href="#cb19-208" aria-hidden="true" tabindex="-1"></a>**K-fold cross-validation** works by splitting the data into k roughly equal parts called folds. The algorithm then trains the model k times, each time holding out one fold as a test set and training on the remaining k-1 folds. Each observation ends up in the test set exactly once. The final performance estimate is the average across all k iterations, giving us a robust estimate of how the model will perform on new data.</span>
<span id="cb19-209"><a href="#cb19-209" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-212"><a href="#cb19-212" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb19-213"><a href="#cb19-213" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-cross-validation-diagram</span></span>
<span id="cb19-214"><a href="#cb19-214" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "K-fold cross-validation: each fold takes turns being the test set"</span></span>
<span id="cb19-215"><a href="#cb19-215" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-width: 8</span></span>
<span id="cb19-216"><a href="#cb19-216" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-height: 4</span></span>
<span id="cb19-217"><a href="#cb19-217" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualize 5-fold CV</span></span>
<span id="cb19-218"><a href="#cb19-218" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mar =</span> <span class="fu">c</span>(<span class="dv">2</span>, <span class="dv">4</span>, <span class="dv">2</span>, <span class="dv">2</span>))</span>
<span id="cb19-219"><a href="#cb19-219" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(<span class="dv">1</span>, <span class="at">type =</span> <span class="st">"n"</span>, <span class="at">xlim =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">5</span>), <span class="at">ylim =</span> <span class="fu">c</span>(<span class="fl">0.5</span>, <span class="fl">5.5</span>),</span>
<span id="cb19-220"><a href="#cb19-220" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">""</span>, <span class="at">ylab =</span> <span class="st">"Fold"</span>, <span class="at">xaxt =</span> <span class="st">"n"</span>, <span class="at">yaxt =</span> <span class="st">"n"</span>,</span>
<span id="cb19-221"><a href="#cb19-221" aria-hidden="true" tabindex="-1"></a>     <span class="at">main =</span> <span class="st">"5-Fold Cross-Validation"</span>)</span>
<span id="cb19-222"><a href="#cb19-222" aria-hidden="true" tabindex="-1"></a><span class="fu">axis</span>(<span class="dv">2</span>, <span class="at">at =</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">5</span>, <span class="at">labels =</span> <span class="fu">paste</span>(<span class="st">"Iteration"</span>, <span class="dv">5</span><span class="sc">:</span><span class="dv">1</span>), <span class="at">las =</span> <span class="dv">1</span>)</span>
<span id="cb19-223"><a href="#cb19-223" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-224"><a href="#cb19-224" aria-hidden="true" tabindex="-1"></a>colors <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">"lightcoral"</span>, <span class="fu">rep</span>(<span class="st">"lightblue"</span>, <span class="dv">4</span>))</span>
<span id="cb19-225"><a href="#cb19-225" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">5</span>) {</span>
<span id="cb19-226"><a href="#cb19-226" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> (j <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">5</span>) {</span>
<span id="cb19-227"><a href="#cb19-227" aria-hidden="true" tabindex="-1"></a>    col <span class="ot">&lt;-</span> <span class="fu">ifelse</span>(j <span class="sc">==</span> i, <span class="st">"lightcoral"</span>, <span class="st">"lightblue"</span>)</span>
<span id="cb19-228"><a href="#cb19-228" aria-hidden="true" tabindex="-1"></a>    <span class="fu">rect</span>(j <span class="sc">-</span> <span class="dv">1</span>, <span class="dv">6</span> <span class="sc">-</span> i <span class="sc">-</span> <span class="fl">0.4</span>, j, <span class="dv">6</span> <span class="sc">-</span> i <span class="sc">+</span> <span class="fl">0.4</span>, <span class="at">col =</span> col, <span class="at">border =</span> <span class="st">"white"</span>)</span>
<span id="cb19-229"><a href="#cb19-229" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb19-230"><a href="#cb19-230" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb19-231"><a href="#cb19-231" aria-hidden="true" tabindex="-1"></a><span class="fu">legend</span>(<span class="st">"bottom"</span>, <span class="fu">c</span>(<span class="st">"Test Fold"</span>, <span class="st">"Training Folds"</span>), <span class="at">fill =</span> <span class="fu">c</span>(<span class="st">"lightcoral"</span>, <span class="st">"lightblue"</span>),</span>
<span id="cb19-232"><a href="#cb19-232" aria-hidden="true" tabindex="-1"></a>       <span class="at">horiz =</span> <span class="cn">TRUE</span>, <span class="at">bty =</span> <span class="st">"n"</span>)</span>
<span id="cb19-233"><a href="#cb19-233" aria-hidden="true" tabindex="-1"></a><span class="fu">axis</span>(<span class="dv">1</span>, <span class="at">at =</span> <span class="fl">0.5</span><span class="sc">:</span><span class="fl">4.5</span>, <span class="at">labels =</span> <span class="fu">paste</span>(<span class="st">"Fold"</span>, <span class="dv">1</span><span class="sc">:</span><span class="dv">5</span>))</span>
<span id="cb19-234"><a href="#cb19-234" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb19-235"><a href="#cb19-235" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-238"><a href="#cb19-238" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb19-239"><a href="#cb19-239" aria-hidden="true" tabindex="-1"></a><span class="co"># Simple CV example with linear regression</span></span>
<span id="cb19-240"><a href="#cb19-240" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">42</span>)</span>
<span id="cb19-241"><a href="#cb19-241" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(<span class="dv">100</span>)</span>
<span id="cb19-242"><a href="#cb19-242" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> <span class="dv">2</span> <span class="sc">+</span> <span class="dv">3</span><span class="sc">*</span>x <span class="sc">+</span> <span class="fu">rnorm</span>(<span class="dv">100</span>)</span>
<span id="cb19-243"><a href="#cb19-243" aria-hidden="true" tabindex="-1"></a>data <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(x, y)</span>
<span id="cb19-244"><a href="#cb19-244" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-245"><a href="#cb19-245" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit model and perform CV</span></span>
<span id="cb19-246"><a href="#cb19-246" aria-hidden="true" tabindex="-1"></a>model <span class="ot">&lt;-</span> <span class="fu">glm</span>(y <span class="sc">~</span> x, <span class="at">data =</span> data)</span>
<span id="cb19-247"><a href="#cb19-247" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-248"><a href="#cb19-248" aria-hidden="true" tabindex="-1"></a><span class="co"># 10-fold cross-validation</span></span>
<span id="cb19-249"><a href="#cb19-249" aria-hidden="true" tabindex="-1"></a>cv_result <span class="ot">&lt;-</span> <span class="fu">cv.glm</span>(data, model, <span class="at">K =</span> <span class="dv">10</span>)</span>
<span id="cb19-250"><a href="#cb19-250" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"CV estimate of prediction error:"</span>, <span class="fu">round</span>(cv_result<span class="sc">$</span>delta[<span class="dv">1</span>], <span class="dv">3</span>), <span class="st">"</span><span class="sc">\n</span><span class="st">"</span>)</span>
<span id="cb19-251"><a href="#cb19-251" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb19-252"><a href="#cb19-252" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-253"><a href="#cb19-253" aria-hidden="true" tabindex="-1"></a><span class="fu">### Leave-One-Out Cross-Validation (LOOCV)</span></span>
<span id="cb19-254"><a href="#cb19-254" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-255"><a href="#cb19-255" aria-hidden="true" tabindex="-1"></a>**Leave-one-out cross-validation (LOOCV)** is k-fold with k = n: each observation is held out once.</span>
<span id="cb19-256"><a href="#cb19-256" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-259"><a href="#cb19-259" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb19-260"><a href="#cb19-260" aria-hidden="true" tabindex="-1"></a><span class="co"># LOOCV</span></span>
<span id="cb19-261"><a href="#cb19-261" aria-hidden="true" tabindex="-1"></a>cv_loocv <span class="ot">&lt;-</span> <span class="fu">cv.glm</span>(data, model, <span class="at">K =</span> <span class="fu">nrow</span>(data))</span>
<span id="cb19-262"><a href="#cb19-262" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"LOOCV estimate of prediction error:"</span>, <span class="fu">round</span>(cv_loocv<span class="sc">$</span>delta[<span class="dv">1</span>], <span class="dv">3</span>), <span class="st">"</span><span class="sc">\n</span><span class="st">"</span>)</span>
<span id="cb19-263"><a href="#cb19-263" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb19-264"><a href="#cb19-264" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-265"><a href="#cb19-265" aria-hidden="true" tabindex="-1"></a>Like all statistical methods, cross-validation involves tradeoffs. LOOCV has lower bias than k-fold CV because nearly all the data (n-1 observations) is used for training in each iteration—the training sets closely resemble the full dataset. However, LOOCV has higher variance because the n training sets are almost identical to each other, so the n fitted models are highly correlated. In practice, 5-fold or 10-fold CV provides a good balance of bias and variance, and empirical studies suggest these choices often give the best estimates of test error. LOOCV is also computationally expensive for large datasets, requiring n separate model fits rather than just k.</span>
<span id="cb19-266"><a href="#cb19-266" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-267"><a href="#cb19-267" aria-hidden="true" tabindex="-1"></a><span class="fu">### Bootstrap for Error Estimation</span></span>
<span id="cb19-268"><a href="#cb19-268" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-269"><a href="#cb19-269" aria-hidden="true" tabindex="-1"></a>The **bootstrap** provides another approach to estimating prediction error. For each bootstrap iteration, we draw a sample of n observations with replacement from the training data—some observations appear multiple times, others not at all. We fit the model on this bootstrap sample and evaluate it on the observations that were not selected, called the "out-of-bag" (OOB) observations. On average, about 37% of observations are left out of each bootstrap sample. By repeating this process many times and averaging the OOB errors, we obtain an estimate of prediction error.</span>
<span id="cb19-270"><a href="#cb19-270" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-271"><a href="#cb19-271" aria-hidden="true" tabindex="-1"></a>The bootstrap approach is similar in spirit to cross-validation but uses the natural randomness of sampling with replacement rather than deterministic fold assignments. It's particularly useful when you also want confidence intervals for model parameters or predictions.</span>
<span id="cb19-272"><a href="#cb19-272" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-275"><a href="#cb19-275" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb19-276"><a href="#cb19-276" aria-hidden="true" tabindex="-1"></a><span class="co"># Bootstrap estimate of prediction error</span></span>
<span id="cb19-277"><a href="#cb19-277" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb19-278"><a href="#cb19-278" aria-hidden="true" tabindex="-1"></a>n_boot <span class="ot">&lt;-</span> <span class="dv">100</span></span>
<span id="cb19-279"><a href="#cb19-279" aria-hidden="true" tabindex="-1"></a>boot_errors <span class="ot">&lt;-</span> <span class="fu">numeric</span>(n_boot)</span>
<span id="cb19-280"><a href="#cb19-280" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-281"><a href="#cb19-281" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (b <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>n_boot) {</span>
<span id="cb19-282"><a href="#cb19-282" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Bootstrap sample</span></span>
<span id="cb19-283"><a href="#cb19-283" aria-hidden="true" tabindex="-1"></a>  boot_idx <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="dv">1</span><span class="sc">:</span><span class="fu">nrow</span>(data), <span class="at">replace =</span> <span class="cn">TRUE</span>)</span>
<span id="cb19-284"><a href="#cb19-284" aria-hidden="true" tabindex="-1"></a>  oob_idx <span class="ot">&lt;-</span> <span class="fu">setdiff</span>(<span class="dv">1</span><span class="sc">:</span><span class="fu">nrow</span>(data), <span class="fu">unique</span>(boot_idx))</span>
<span id="cb19-285"><a href="#cb19-285" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-286"><a href="#cb19-286" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span> (<span class="fu">length</span>(oob_idx) <span class="sc">&gt;</span> <span class="dv">0</span>) {</span>
<span id="cb19-287"><a href="#cb19-287" aria-hidden="true" tabindex="-1"></a>    fit <span class="ot">&lt;-</span> <span class="fu">lm</span>(y <span class="sc">~</span> x, <span class="at">data =</span> data[boot_idx, ])</span>
<span id="cb19-288"><a href="#cb19-288" aria-hidden="true" tabindex="-1"></a>    boot_errors[b] <span class="ot">&lt;-</span> <span class="fu">mean</span>((data<span class="sc">$</span>y[oob_idx] <span class="sc">-</span> <span class="fu">predict</span>(fit, data[oob_idx, ]))<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb19-289"><a href="#cb19-289" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb19-290"><a href="#cb19-290" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb19-291"><a href="#cb19-291" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-292"><a href="#cb19-292" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"Bootstrap estimate of prediction error:"</span>, <span class="fu">round</span>(<span class="fu">mean</span>(boot_errors), <span class="dv">3</span>), <span class="st">"</span><span class="sc">\n</span><span class="st">"</span>)</span>
<span id="cb19-293"><a href="#cb19-293" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb19-294"><a href="#cb19-294" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-295"><a href="#cb19-295" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip}</span>
<span id="cb19-296"><a href="#cb19-296" aria-hidden="true" tabindex="-1"></a><span class="fu">## Choosing a CV Strategy</span></span>
<span id="cb19-297"><a href="#cb19-297" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-298"><a href="#cb19-298" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**k = 5 or k = 10**: Standard choices that balance bias and variance</span>
<span id="cb19-299"><a href="#cb19-299" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**LOOCV (k = n)**: Low bias but high variance; expensive for large n</span>
<span id="cb19-300"><a href="#cb19-300" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Bootstrap**: Useful when you also want confidence intervals</span>
<span id="cb19-301"><a href="#cb19-301" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Repeated CV**: Run k-fold multiple times with different splits for more stable estimates</span>
<span id="cb19-302"><a href="#cb19-302" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb19-303"><a href="#cb19-303" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-304"><a href="#cb19-304" aria-hidden="true" tabindex="-1"></a><span class="fu">## Using Cross-Validation for Model Selection</span></span>
<span id="cb19-305"><a href="#cb19-305" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-306"><a href="#cb19-306" aria-hidden="true" tabindex="-1"></a>Cross-validation is essential for tuning hyperparameters—values that control model complexity but are not learned from data.</span>
<span id="cb19-307"><a href="#cb19-307" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-310"><a href="#cb19-310" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb19-311"><a href="#cb19-311" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-cv-model-selection</span></span>
<span id="cb19-312"><a href="#cb19-312" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "Using cross-validation to select the optimal polynomial degree. The optimal model minimizes CV error."</span></span>
<span id="cb19-313"><a href="#cb19-313" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-width: 8</span></span>
<span id="cb19-314"><a href="#cb19-314" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-height: 5</span></span>
<span id="cb19-315"><a href="#cb19-315" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate data</span></span>
<span id="cb19-316"><a href="#cb19-316" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">42</span>)</span>
<span id="cb19-317"><a href="#cb19-317" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">100</span></span>
<span id="cb19-318"><a href="#cb19-318" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">runif</span>(n, <span class="dv">0</span>, <span class="dv">10</span>)</span>
<span id="cb19-319"><a href="#cb19-319" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> <span class="fu">sin</span>(x) <span class="sc">+</span> <span class="fu">rnorm</span>(n, <span class="at">sd =</span> <span class="fl">0.3</span>)</span>
<span id="cb19-320"><a href="#cb19-320" aria-hidden="true" tabindex="-1"></a>cv_data <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">x =</span> x, <span class="at">y =</span> y)</span>
<span id="cb19-321"><a href="#cb19-321" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-322"><a href="#cb19-322" aria-hidden="true" tabindex="-1"></a><span class="co"># Cross-validation for polynomial degree selection</span></span>
<span id="cb19-323"><a href="#cb19-323" aria-hidden="true" tabindex="-1"></a>degrees <span class="ot">&lt;-</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">15</span></span>
<span id="cb19-324"><a href="#cb19-324" aria-hidden="true" tabindex="-1"></a>cv_errors <span class="ot">&lt;-</span> <span class="fu">numeric</span>(<span class="fu">length</span>(degrees))</span>
<span id="cb19-325"><a href="#cb19-325" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-326"><a href="#cb19-326" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (d <span class="cf">in</span> degrees) {</span>
<span id="cb19-327"><a href="#cb19-327" aria-hidden="true" tabindex="-1"></a>  model <span class="ot">&lt;-</span> <span class="fu">glm</span>(y <span class="sc">~</span> <span class="fu">poly</span>(x, d), <span class="at">data =</span> cv_data)</span>
<span id="cb19-328"><a href="#cb19-328" aria-hidden="true" tabindex="-1"></a>  cv_result <span class="ot">&lt;-</span> <span class="fu">cv.glm</span>(cv_data, model, <span class="at">K =</span> <span class="dv">10</span>)</span>
<span id="cb19-329"><a href="#cb19-329" aria-hidden="true" tabindex="-1"></a>  cv_errors[d] <span class="ot">&lt;-</span> cv_result<span class="sc">$</span>delta[<span class="dv">1</span>]</span>
<span id="cb19-330"><a href="#cb19-330" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb19-331"><a href="#cb19-331" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-332"><a href="#cb19-332" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot</span></span>
<span id="cb19-333"><a href="#cb19-333" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(degrees, cv_errors, <span class="at">type =</span> <span class="st">"b"</span>, <span class="at">pch =</span> <span class="dv">19</span>, <span class="at">col =</span> <span class="st">"blue"</span>,</span>
<span id="cb19-334"><a href="#cb19-334" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">"Polynomial Degree"</span>, <span class="at">ylab =</span> <span class="st">"CV Error (MSE)"</span>,</span>
<span id="cb19-335"><a href="#cb19-335" aria-hidden="true" tabindex="-1"></a>     <span class="at">main =</span> <span class="st">"Cross-Validation for Model Selection"</span>)</span>
<span id="cb19-336"><a href="#cb19-336" aria-hidden="true" tabindex="-1"></a>best_degree <span class="ot">&lt;-</span> <span class="fu">which.min</span>(cv_errors)</span>
<span id="cb19-337"><a href="#cb19-337" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span>(best_degree, cv_errors[best_degree], <span class="at">pch =</span> <span class="dv">19</span>, <span class="at">col =</span> <span class="st">"red"</span>, <span class="at">cex =</span> <span class="dv">2</span>)</span>
<span id="cb19-338"><a href="#cb19-338" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">v =</span> best_degree, <span class="at">lty =</span> <span class="dv">2</span>, <span class="at">col =</span> <span class="st">"red"</span>)</span>
<span id="cb19-339"><a href="#cb19-339" aria-hidden="true" tabindex="-1"></a><span class="fu">text</span>(best_degree <span class="sc">+</span> <span class="dv">1</span>, cv_errors[best_degree], <span class="fu">paste</span>(<span class="st">"Optimal:"</span>, best_degree), <span class="at">col =</span> <span class="st">"red"</span>)</span>
<span id="cb19-340"><a href="#cb19-340" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb19-341"><a href="#cb19-341" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-342"><a href="#cb19-342" aria-hidden="true" tabindex="-1"></a><span class="fu">### The One Standard Error Rule</span></span>
<span id="cb19-343"><a href="#cb19-343" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-344"><a href="#cb19-344" aria-hidden="true" tabindex="-1"></a>When selecting among models with similar CV error, we often choose the simplest model within one standard error of the minimum. This guards against overfitting to the CV procedure itself.</span>
<span id="cb19-345"><a href="#cb19-345" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-348"><a href="#cb19-348" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb19-349"><a href="#cb19-349" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-one-se-rule</span></span>
<span id="cb19-350"><a href="#cb19-350" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "The one standard error rule: select the simplest model within one SE of the minimum CV error"</span></span>
<span id="cb19-351"><a href="#cb19-351" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-width: 8</span></span>
<span id="cb19-352"><a href="#cb19-352" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-height: 5</span></span>
<span id="cb19-353"><a href="#cb19-353" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate SE of CV errors (simplified - would need repeated CV for proper SE)</span></span>
<span id="cb19-354"><a href="#cb19-354" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">42</span>)</span>
<span id="cb19-355"><a href="#cb19-355" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-356"><a href="#cb19-356" aria-hidden="true" tabindex="-1"></a><span class="co"># Repeated 10-fold CV for better SE estimates</span></span>
<span id="cb19-357"><a href="#cb19-357" aria-hidden="true" tabindex="-1"></a>n_repeats <span class="ot">&lt;-</span> <span class="dv">20</span></span>
<span id="cb19-358"><a href="#cb19-358" aria-hidden="true" tabindex="-1"></a>cv_matrix <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="cn">NA</span>, n_repeats, <span class="fu">length</span>(degrees))</span>
<span id="cb19-359"><a href="#cb19-359" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-360"><a href="#cb19-360" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (rep <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>n_repeats) {</span>
<span id="cb19-361"><a href="#cb19-361" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> (d <span class="cf">in</span> degrees) {</span>
<span id="cb19-362"><a href="#cb19-362" aria-hidden="true" tabindex="-1"></a>    model <span class="ot">&lt;-</span> <span class="fu">glm</span>(y <span class="sc">~</span> <span class="fu">poly</span>(x, d), <span class="at">data =</span> cv_data)</span>
<span id="cb19-363"><a href="#cb19-363" aria-hidden="true" tabindex="-1"></a>    cv_result <span class="ot">&lt;-</span> <span class="fu">cv.glm</span>(cv_data, model, <span class="at">K =</span> <span class="dv">10</span>)</span>
<span id="cb19-364"><a href="#cb19-364" aria-hidden="true" tabindex="-1"></a>    cv_matrix[rep, d] <span class="ot">&lt;-</span> cv_result<span class="sc">$</span>delta[<span class="dv">1</span>]</span>
<span id="cb19-365"><a href="#cb19-365" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb19-366"><a href="#cb19-366" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb19-367"><a href="#cb19-367" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-368"><a href="#cb19-368" aria-hidden="true" tabindex="-1"></a>cv_means <span class="ot">&lt;-</span> <span class="fu">colMeans</span>(cv_matrix)</span>
<span id="cb19-369"><a href="#cb19-369" aria-hidden="true" tabindex="-1"></a>cv_se <span class="ot">&lt;-</span> <span class="fu">apply</span>(cv_matrix, <span class="dv">2</span>, sd) <span class="sc">/</span> <span class="fu">sqrt</span>(n_repeats)</span>
<span id="cb19-370"><a href="#cb19-370" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-371"><a href="#cb19-371" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot with error bars</span></span>
<span id="cb19-372"><a href="#cb19-372" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(degrees, cv_means, <span class="at">type =</span> <span class="st">"b"</span>, <span class="at">pch =</span> <span class="dv">19</span>, <span class="at">col =</span> <span class="st">"blue"</span>,</span>
<span id="cb19-373"><a href="#cb19-373" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">"Polynomial Degree"</span>, <span class="at">ylab =</span> <span class="st">"CV Error (MSE)"</span>,</span>
<span id="cb19-374"><a href="#cb19-374" aria-hidden="true" tabindex="-1"></a>     <span class="at">main =</span> <span class="st">"One Standard Error Rule"</span>,</span>
<span id="cb19-375"><a href="#cb19-375" aria-hidden="true" tabindex="-1"></a>     <span class="at">ylim =</span> <span class="fu">c</span>(<span class="fu">min</span>(cv_means <span class="sc">-</span> cv_se), <span class="fu">max</span>(cv_means <span class="sc">+</span> cv_se)))</span>
<span id="cb19-376"><a href="#cb19-376" aria-hidden="true" tabindex="-1"></a><span class="fu">arrows</span>(degrees, cv_means <span class="sc">-</span> cv_se, degrees, cv_means <span class="sc">+</span> cv_se,</span>
<span id="cb19-377"><a href="#cb19-377" aria-hidden="true" tabindex="-1"></a>       <span class="at">code =</span> <span class="dv">3</span>, <span class="at">angle =</span> <span class="dv">90</span>, <span class="at">length =</span> <span class="fl">0.05</span>, <span class="at">col =</span> <span class="st">"blue"</span>)</span>
<span id="cb19-378"><a href="#cb19-378" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-379"><a href="#cb19-379" aria-hidden="true" tabindex="-1"></a><span class="co"># Find minimum and 1-SE model</span></span>
<span id="cb19-380"><a href="#cb19-380" aria-hidden="true" tabindex="-1"></a>min_idx <span class="ot">&lt;-</span> <span class="fu">which.min</span>(cv_means)</span>
<span id="cb19-381"><a href="#cb19-381" aria-hidden="true" tabindex="-1"></a>threshold <span class="ot">&lt;-</span> cv_means[min_idx] <span class="sc">+</span> cv_se[min_idx]</span>
<span id="cb19-382"><a href="#cb19-382" aria-hidden="true" tabindex="-1"></a>one_se_idx <span class="ot">&lt;-</span> <span class="fu">min</span>(<span class="fu">which</span>(cv_means <span class="sc">&lt;=</span> threshold))</span>
<span id="cb19-383"><a href="#cb19-383" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-384"><a href="#cb19-384" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">h =</span> threshold, <span class="at">lty =</span> <span class="dv">2</span>, <span class="at">col =</span> <span class="st">"red"</span>)</span>
<span id="cb19-385"><a href="#cb19-385" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span>(min_idx, cv_means[min_idx], <span class="at">pch =</span> <span class="dv">19</span>, <span class="at">col =</span> <span class="st">"red"</span>, <span class="at">cex =</span> <span class="dv">2</span>)</span>
<span id="cb19-386"><a href="#cb19-386" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span>(one_se_idx, cv_means[one_se_idx], <span class="at">pch =</span> <span class="dv">17</span>, <span class="at">col =</span> <span class="st">"darkgreen"</span>, <span class="at">cex =</span> <span class="dv">2</span>)</span>
<span id="cb19-387"><a href="#cb19-387" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-388"><a href="#cb19-388" aria-hidden="true" tabindex="-1"></a><span class="fu">legend</span>(<span class="st">"topright"</span>, <span class="fu">c</span>(<span class="st">"Minimum CV error"</span>, <span class="st">"1-SE rule selection"</span>, <span class="st">"1-SE threshold"</span>),</span>
<span id="cb19-389"><a href="#cb19-389" aria-hidden="true" tabindex="-1"></a>       <span class="at">pch =</span> <span class="fu">c</span>(<span class="dv">19</span>, <span class="dv">17</span>, <span class="cn">NA</span>), <span class="at">lty =</span> <span class="fu">c</span>(<span class="cn">NA</span>, <span class="cn">NA</span>, <span class="dv">2</span>), <span class="at">col =</span> <span class="fu">c</span>(<span class="st">"red"</span>, <span class="st">"darkgreen"</span>, <span class="st">"red"</span>))</span>
<span id="cb19-390"><a href="#cb19-390" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb19-391"><a href="#cb19-391" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-392"><a href="#cb19-392" aria-hidden="true" tabindex="-1"></a><span class="fu">## Information Criteria</span></span>
<span id="cb19-393"><a href="#cb19-393" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-394"><a href="#cb19-394" aria-hidden="true" tabindex="-1"></a>An alternative to cross-validation is using information criteria, which balance fit quality against model complexity:</span>
<span id="cb19-395"><a href="#cb19-395" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-396"><a href="#cb19-396" aria-hidden="true" tabindex="-1"></a>**Akaike Information Criterion (AIC)**:</span>
<span id="cb19-397"><a href="#cb19-397" aria-hidden="true" tabindex="-1"></a>$$\text{AIC} = -2 \log L + 2k$$</span>
<span id="cb19-398"><a href="#cb19-398" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-399"><a href="#cb19-399" aria-hidden="true" tabindex="-1"></a>**Bayesian Information Criterion (BIC)**:</span>
<span id="cb19-400"><a href="#cb19-400" aria-hidden="true" tabindex="-1"></a>$$\text{BIC} = -2 \log L + k \log n$$</span>
<span id="cb19-401"><a href="#cb19-401" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-402"><a href="#cb19-402" aria-hidden="true" tabindex="-1"></a>where $L$ is the likelihood, $k$ is the number of parameters, and $n$ is the sample size. Both criteria balance fit quality (the likelihood term) against model complexity (the penalty term), but they weight these differently.</span>
<span id="cb19-403"><a href="#cb19-403" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-404"><a href="#cb19-404" aria-hidden="true" tabindex="-1"></a>Lower values indicate better models for both criteria. BIC penalizes complexity more heavily than AIC because its penalty term grows with $\log n$, whereas AIC's penalty is fixed at 2 per parameter. As a result, BIC tends to select simpler models, especially with larger sample sizes. AIC, being more lenient toward complexity, tends to select more complex models. Neither is universally better—the appropriate choice depends on whether you prioritize prediction (favor AIC) or identification of the true model structure (favor BIC).</span>
<span id="cb19-405"><a href="#cb19-405" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-408"><a href="#cb19-408" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb19-409"><a href="#cb19-409" aria-hidden="true" tabindex="-1"></a><span class="co"># Compare AIC and BIC for polynomial models</span></span>
<span id="cb19-410"><a href="#cb19-410" aria-hidden="true" tabindex="-1"></a>aic_values <span class="ot">&lt;-</span> bic_values <span class="ot">&lt;-</span> <span class="fu">numeric</span>(<span class="fu">length</span>(degrees))</span>
<span id="cb19-411"><a href="#cb19-411" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-412"><a href="#cb19-412" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (d <span class="cf">in</span> degrees) {</span>
<span id="cb19-413"><a href="#cb19-413" aria-hidden="true" tabindex="-1"></a>  model <span class="ot">&lt;-</span> <span class="fu">lm</span>(y <span class="sc">~</span> <span class="fu">poly</span>(x, d), <span class="at">data =</span> cv_data)</span>
<span id="cb19-414"><a href="#cb19-414" aria-hidden="true" tabindex="-1"></a>  aic_values[d] <span class="ot">&lt;-</span> <span class="fu">AIC</span>(model)</span>
<span id="cb19-415"><a href="#cb19-415" aria-hidden="true" tabindex="-1"></a>  bic_values[d] <span class="ot">&lt;-</span> <span class="fu">BIC</span>(model)</span>
<span id="cb19-416"><a href="#cb19-416" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb19-417"><a href="#cb19-417" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-418"><a href="#cb19-418" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"Best degree by AIC:"</span>, <span class="fu">which.min</span>(aic_values), <span class="st">"</span><span class="sc">\n</span><span class="st">"</span>)</span>
<span id="cb19-419"><a href="#cb19-419" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"Best degree by BIC:"</span>, <span class="fu">which.min</span>(bic_values), <span class="st">"</span><span class="sc">\n</span><span class="st">"</span>)</span>
<span id="cb19-420"><a href="#cb19-420" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"Best degree by CV:"</span>, best_degree, <span class="st">"</span><span class="sc">\n</span><span class="st">"</span>)</span>
<span id="cb19-421"><a href="#cb19-421" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb19-422"><a href="#cb19-422" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-423"><a href="#cb19-423" aria-hidden="true" tabindex="-1"></a><span class="fu">## Practical Workflow</span></span>
<span id="cb19-424"><a href="#cb19-424" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-425"><a href="#cb19-425" aria-hidden="true" tabindex="-1"></a>A disciplined approach to model validation requires separating the data you use for development from the data you use for final evaluation. Begin by splitting your data into a training set (typically 70-80% of observations) and a test set (the remainder). The test set should be locked away—never look at it during model development.</span>
<span id="cb19-426"><a href="#cb19-426" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-427"><a href="#cb19-427" aria-hidden="true" tabindex="-1"></a>Work exclusively with the training data during the model development phase. Explore the data to understand distributions and relationships. Use cross-validation on the training set to compare different model types, tune hyperparameters, and estimate expected performance. This iterative process of trying models, evaluating them with CV, and refining your approach may involve many cycles.</span>
<span id="cb19-428"><a href="#cb19-428" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-429"><a href="#cb19-429" aria-hidden="true" tabindex="-1"></a>Once you have settled on a final model based on cross-validation performance, refit that model on the full training data to use all available information for parameter estimation. Only then—when all modeling decisions are complete—evaluate this final model on the held-out test set. This single evaluation provides an honest estimate of how your model will perform on truly new data. Report this test error as your final performance metric, resisting any temptation to go back and revise the model based on test set results.</span>
<span id="cb19-430"><a href="#cb19-430" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-431"><a href="#cb19-431" aria-hidden="true" tabindex="-1"></a>::: {.callout-warning}</span>
<span id="cb19-432"><a href="#cb19-432" aria-hidden="true" tabindex="-1"></a><span class="fu">## Common Mistakes</span></span>
<span id="cb19-433"><a href="#cb19-433" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-434"><a href="#cb19-434" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**Using test data during model development**: The test set must be locked away until final evaluation</span>
<span id="cb19-435"><a href="#cb19-435" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**Reporting training error**: Training error is optimistic and misleading</span>
<span id="cb19-436"><a href="#cb19-436" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**Selecting models based on test error, then reporting test error**: This invalidates the test error estimate</span>
<span id="cb19-437"><a href="#cb19-437" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>**Not setting random seeds**: Results should be reproducible</span>
<span id="cb19-438"><a href="#cb19-438" aria-hidden="true" tabindex="-1"></a><span class="ss">5. </span>**Ignoring variance in CV estimates**: Single CV runs can be noisy</span>
<span id="cb19-439"><a href="#cb19-439" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb19-440"><a href="#cb19-440" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-441"><a href="#cb19-441" aria-hidden="true" tabindex="-1"></a><span class="fu">## Exercises</span></span>
<span id="cb19-442"><a href="#cb19-442" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-443"><a href="#cb19-443" aria-hidden="true" tabindex="-1"></a>::: {.callout-note}</span>
<span id="cb19-444"><a href="#cb19-444" aria-hidden="true" tabindex="-1"></a><span class="fu">### Exercise MV.1: Training vs. Test Error</span></span>
<span id="cb19-445"><a href="#cb19-445" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-446"><a href="#cb19-446" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>Generate data from the model $y = \sin(x) + \epsilon$ where $\epsilon \sim N(0, 0.25)$ and $x$ ranges from 0 to $2\pi$. Split into 80% training and 20% test.</span>
<span id="cb19-447"><a href="#cb19-447" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-448"><a href="#cb19-448" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>Fit polynomial models of degrees 1 through 15. Plot both training and test error. At what degree does overfitting begin?</span>
<span id="cb19-449"><a href="#cb19-449" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb19-450"><a href="#cb19-450" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-451"><a href="#cb19-451" aria-hidden="true" tabindex="-1"></a>::: {.callout-note}</span>
<span id="cb19-452"><a href="#cb19-452" aria-hidden="true" tabindex="-1"></a><span class="fu">### Exercise MV.2: Cross-Validation Practice</span></span>
<span id="cb19-453"><a href="#cb19-453" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-454"><a href="#cb19-454" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>Using the same data, implement 5-fold and 10-fold cross-validation. Compare the selected optimal degree between the two approaches. Which is more variable across different random seeds?</span>
<span id="cb19-455"><a href="#cb19-455" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-456"><a href="#cb19-456" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>Implement LOOCV and compare to k-fold CV. How do the computational costs compare?</span>
<span id="cb19-457"><a href="#cb19-457" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb19-458"><a href="#cb19-458" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-459"><a href="#cb19-459" aria-hidden="true" tabindex="-1"></a>::: {.callout-note}</span>
<span id="cb19-460"><a href="#cb19-460" aria-hidden="true" tabindex="-1"></a><span class="fu">### Exercise MV.3: The One Standard Error Rule</span></span>
<span id="cb19-461"><a href="#cb19-461" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-462"><a href="#cb19-462" aria-hidden="true" tabindex="-1"></a><span class="ss">5. </span>Generate 100 random training/test splits. For each, use 10-fold CV to select the optimal polynomial degree. Plot the distribution of selected degrees.</span>
<span id="cb19-463"><a href="#cb19-463" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-464"><a href="#cb19-464" aria-hidden="true" tabindex="-1"></a><span class="ss">6. </span>Repeat using the one standard error rule. How does the distribution change?</span>
<span id="cb19-465"><a href="#cb19-465" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb19-466"><a href="#cb19-466" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-467"><a href="#cb19-467" aria-hidden="true" tabindex="-1"></a><span class="fu">## Summary</span></span>
<span id="cb19-468"><a href="#cb19-468" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-469"><a href="#cb19-469" aria-hidden="true" tabindex="-1"></a>This chapter addressed one of the central challenges in statistical learning: how do we build models that generalize well to new data rather than merely memorizing the training set? **Overfitting** occurs when models capture noise specific to the training data rather than true underlying patterns, and avoiding this trap requires careful attention to model validation.</span>
<span id="cb19-470"><a href="#cb19-470" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-471"><a href="#cb19-471" aria-hidden="true" tabindex="-1"></a>We quantify prediction quality through **loss functions**—squared error for regression, log loss or 0-1 loss for classification—and the choice should reflect how errors affect your particular application. A key insight is that **training error** systematically underestimates true prediction error; models always look better on the data used to fit them than on genuinely new observations.</span>
<span id="cb19-472"><a href="#cb19-472" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-473"><a href="#cb19-473" aria-hidden="true" tabindex="-1"></a>The **bias-variance tradeoff** provides the conceptual framework for understanding model complexity. Simple models have high bias (they may miss important patterns) but low variance (they're stable across different training samples). Complex models have the opposite problem: low bias but high variance. The optimal model balances these competing concerns, achieving the lowest total error.</span>
<span id="cb19-474"><a href="#cb19-474" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-475"><a href="#cb19-475" aria-hidden="true" tabindex="-1"></a>**Cross-validation** provides a practical tool for estimating generalization error without wasting data on a held-out test set. K-fold cross-validation with k=5 or 10 typically offers a good balance of bias and variance. LOOCV reduces bias but increases variance and computational cost. The bootstrap provides an alternative approach, particularly useful when confidence intervals are also needed.</span>
<span id="cb19-476"><a href="#cb19-476" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-477"><a href="#cb19-477" aria-hidden="true" tabindex="-1"></a>When multiple models have similar cross-validation error, the **one standard error rule** suggests choosing the simplest model within one standard error of the minimum, guarding against overfitting to the CV procedure itself. **Information criteria** like AIC and BIC offer alternatives that balance fit against complexity without requiring repeated model fitting.</span>
<span id="cb19-478"><a href="#cb19-478" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-479"><a href="#cb19-479" aria-hidden="true" tabindex="-1"></a>Finally, a disciplined workflow that strictly separates training, validation, and testing phases is essential for honest performance assessment. The test set must remain untouched until final evaluation—peeking at it during development invalidates the entire exercise.</span>
<span id="cb19-480"><a href="#cb19-480" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-481"><a href="#cb19-481" aria-hidden="true" tabindex="-1"></a><span class="fu">## Additional Resources</span></span>
<span id="cb19-482"><a href="#cb19-482" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-483"><a href="#cb19-483" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>@james2023islr - Comprehensive treatment of resampling methods</span>
<span id="cb19-484"><a href="#cb19-484" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>@hastie2009elements - Theoretical foundations of bias-variance tradeoff</span>
<span id="cb19-485"><a href="#cb19-485" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>@thulin2025msr - Practical model validation in R</span>
</code><button title="Copy to Clipboard" class="code-copy-button" data-in-quarto-modal=""><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p>Statistics for Biosciences and Bioengineering</p>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    </div>
    <div class="nav-footer-right">
<p>Built with <a href="https://quarto.org/">Quarto</a></p>
</div>
  </div>
</footer>




</body></html>