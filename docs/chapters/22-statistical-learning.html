<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.30">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>29&nbsp; Core Concepts in Statistical Learning – Statistics for the Biosciences and Bioengineering</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../chapters/31-bayesian-statistics.html" rel="next">
<link href="../chapters/21-glm.html" rel="prev">
<script src="../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-de070a7b0ab54f8780927367ac907214.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap-485d01fc63b59abcd3ee1bf1e8e2748d.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar floating quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../chapters/22-statistical-learning.html">Statistical Learning</a></li><li class="breadcrumb-item"><a href="../chapters/22-statistical-learning.html"><span class="chapter-number">29</span>&nbsp; <span class="chapter-title">Core Concepts in Statistical Learning</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../">Statistics for the Biosciences and Bioengineering</a> 
        <div class="sidebar-tools-main">
  <a href="" class="quarto-reader-toggle quarto-navigation-tool px-1" onclick="window.quartoToggleReader(); return false;" title="Toggle reader mode">
  <div class="quarto-reader-toggle-btn">
  <i class="bi"></i>
  </div>
</a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Why This Book?</span></span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">Core Data Science Tools</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/01-introduction.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Introduction</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/02-installing-tools.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Installing Core Tools</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/02-unix-command-line.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Unix and the Command Line</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/03-r-rstudio.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">R and RStudio</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/04-markdown-latex.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Markdown and LaTeX</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true">
 <span class="menu-text">Data Exploration</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/07-tidy-data.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Tidy Data</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/08-data-wrangling.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Data Wrangling with dplyr</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/09-exploratory-data-analysis.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Exploratory Data Analysis</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/06-data-visualization.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Data Visualization</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true">
 <span class="menu-text">Probability and Distributions</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/07-probability-foundations.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Foundations of Probability</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/08-discrete-distributions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Discrete Probability Distributions</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/09-continuous-distributions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Continuous Probability Distributions</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/10-sampling-estimation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Sampling and Parameter Estimation</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true">
 <span class="menu-text">Statistical Inference</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/11-experimental-design.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Experimental Design Principles</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/11-hypothesis-testing.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Hypothesis Testing</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/12-t-tests.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">T-Tests</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/13-nonparametric-tests.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Nonparametric Tests</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/14-bootstrapping.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">Resampling Methods</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="true">
 <span class="menu-text">Linear Models</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/30-what-are-models.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">What are Models?</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/15-correlation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">21</span>&nbsp; <span class="chapter-title">Correlation</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/16-simple-linear-regression.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">22</span>&nbsp; <span class="chapter-title">Simple Linear Regression</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/17-residual-analysis.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">23</span>&nbsp; <span class="chapter-title">Residual Analysis</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/18-statistical-power.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">24</span>&nbsp; <span class="chapter-title">Statistical Power</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/20-multiple-regression.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">25</span>&nbsp; <span class="chapter-title">Multiple Regression</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/19-single-factor-anova.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">26</span>&nbsp; <span class="chapter-title">Single Factor ANOVA</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/20-multifactor-anova.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">27</span>&nbsp; <span class="chapter-title">Multi-Factor ANOVA</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/21-glm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">28</span>&nbsp; <span class="chapter-title">Generalized Linear Models</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="true">
 <span class="menu-text">Statistical Learning</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-6" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/22-statistical-learning.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">29</span>&nbsp; <span class="chapter-title">Core Concepts in Statistical Learning</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/31-bayesian-statistics.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">30</span>&nbsp; <span class="chapter-title">Bayesian Statistics</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/32-classification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">31</span>&nbsp; <span class="chapter-title">Classification Methods</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/33-clustering.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">32</span>&nbsp; <span class="chapter-title">Clustering</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/23-dimensionality-reduction.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">33</span>&nbsp; <span class="chapter-title">Dimensionality Reduction and Multivariate Methods</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/34-tsne-umap.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">34</span>&nbsp; <span class="chapter-title">Non-Linear Dimensionality Reduction: t-SNE and UMAP</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/24-high-performance-computing.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">35</span>&nbsp; <span class="chapter-title">High Performance Computing</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="true">
 <span class="menu-text">Scientific Communication</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-7" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/25-presenting-results.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">36</span>&nbsp; <span class="chapter-title">Presenting Statistical Results</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-8" role="navigation" aria-expanded="true">
 <span class="menu-text">Historical Context</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-8" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-8" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/A1-eugenics-history.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">37</span>&nbsp; <span class="chapter-title">The Eugenics History of Statistics</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-9" role="navigation" aria-expanded="true">
 <span class="menu-text">Appendices</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-9" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-9" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/A8-keyboard-shortcuts.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">38</span>&nbsp; <span class="chapter-title">Keyboard Shortcuts Reference</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/A6-unix-reference.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">39</span>&nbsp; <span class="chapter-title">Unix Command Reference</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/A2-r-reference.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">40</span>&nbsp; <span class="chapter-title">R Command Reference</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/A9-quarto-reference.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">41</span>&nbsp; <span class="chapter-title">Quarto Markdown Reference</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/A7-latex-reference.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">42</span>&nbsp; <span class="chapter-title">LaTeX Command Reference</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/A4-greek-letters.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">43</span>&nbsp; <span class="chapter-title">Greek Letters in Mathematics and Statistics</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/A3-probability-distributions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">44</span>&nbsp; <span class="chapter-title">Common Probability Distributions</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/A5-sampling-distributions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">45</span>&nbsp; <span class="chapter-title">Sampling Distributions in Hypothesis Testing</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/A10-matrix-algebra.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">46</span>&nbsp; <span class="chapter-title">Matrix Algebra Fundamentals</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#from-inference-to-prediction" id="toc-from-inference-to-prediction" class="nav-link active" data-scroll-target="#from-inference-to-prediction"><span class="header-section-number">29.1</span> From Inference to Prediction</a></li>
  <li><a href="#the-overfitting-problem" id="toc-the-overfitting-problem" class="nav-link" data-scroll-target="#the-overfitting-problem"><span class="header-section-number">29.2</span> The Overfitting Problem</a></li>
  <li><a href="#loss-functions-quantifying-prediction-error" id="toc-loss-functions-quantifying-prediction-error" class="nav-link" data-scroll-target="#loss-functions-quantifying-prediction-error"><span class="header-section-number">29.3</span> Loss Functions: Quantifying Prediction Error</a>
  <ul class="collapse">
  <li><a href="#common-loss-functions-for-regression" id="toc-common-loss-functions-for-regression" class="nav-link" data-scroll-target="#common-loss-functions-for-regression">Common Loss Functions for Regression</a></li>
  <li><a href="#common-loss-functions-for-classification" id="toc-common-loss-functions-for-classification" class="nav-link" data-scroll-target="#common-loss-functions-for-classification">Common Loss Functions for Classification</a></li>
  <li><a href="#why-loss-functions-matter" id="toc-why-loss-functions-matter" class="nav-link" data-scroll-target="#why-loss-functions-matter">Why Loss Functions Matter</a></li>
  </ul></li>
  <li><a href="#cross-validation" id="toc-cross-validation" class="nav-link" data-scroll-target="#cross-validation"><span class="header-section-number">29.4</span> Cross-Validation</a>
  <ul class="collapse">
  <li><a href="#the-problem-training-error-vs.-test-error" id="toc-the-problem-training-error-vs.-test-error" class="nav-link" data-scroll-target="#the-problem-training-error-vs.-test-error">The Problem: Training Error vs.&nbsp;Test Error</a></li>
  <li><a href="#k-fold-cross-validation" id="toc-k-fold-cross-validation" class="nav-link" data-scroll-target="#k-fold-cross-validation">K-Fold Cross-Validation</a></li>
  <li><a href="#bootstrap-for-error-estimation" id="toc-bootstrap-for-error-estimation" class="nav-link" data-scroll-target="#bootstrap-for-error-estimation">Bootstrap for Error Estimation</a></li>
  </ul></li>
  <li><a href="#bias-variance-tradeoff" id="toc-bias-variance-tradeoff" class="nav-link" data-scroll-target="#bias-variance-tradeoff"><span class="header-section-number">29.5</span> Bias-Variance Tradeoff</a></li>
  <li><a href="#regularization-controlling-model-complexity" id="toc-regularization-controlling-model-complexity" class="nav-link" data-scroll-target="#regularization-controlling-model-complexity"><span class="header-section-number">29.6</span> Regularization: Controlling Model Complexity</a>
  <ul class="collapse">
  <li><a href="#the-regularization-idea" id="toc-the-regularization-idea" class="nav-link" data-scroll-target="#the-regularization-idea">The Regularization Idea</a></li>
  <li><a href="#ridge-regression-l2-penalty" id="toc-ridge-regression-l2-penalty" class="nav-link" data-scroll-target="#ridge-regression-l2-penalty">Ridge Regression (L2 Penalty)</a></li>
  <li><a href="#lasso-regression-l1-penalty" id="toc-lasso-regression-l1-penalty" class="nav-link" data-scroll-target="#lasso-regression-l1-penalty">Lasso Regression (L1 Penalty)</a></li>
  <li><a href="#elastic-net-combining-ridge-and-lasso" id="toc-elastic-net-combining-ridge-and-lasso" class="nav-link" data-scroll-target="#elastic-net-combining-ridge-and-lasso">Elastic Net: Combining Ridge and Lasso</a></li>
  <li><a href="#choosing-lambda-with-cross-validation" id="toc-choosing-lambda-with-cross-validation" class="nav-link" data-scroll-target="#choosing-lambda-with-cross-validation">Choosing Lambda with Cross-Validation</a></li>
  <li><a href="#comparing-regularization-methods" id="toc-comparing-regularization-methods" class="nav-link" data-scroll-target="#comparing-regularization-methods">Comparing Regularization Methods</a></li>
  </ul></li>
  <li><a href="#smoothing-from-simple-averages-to-flexible-curves" id="toc-smoothing-from-simple-averages-to-flexible-curves" class="nav-link" data-scroll-target="#smoothing-from-simple-averages-to-flexible-curves"><span class="header-section-number">29.7</span> Smoothing: From Simple Averages to Flexible Curves</a>
  <ul class="collapse">
  <li><a href="#bin-smoothing" id="toc-bin-smoothing" class="nav-link" data-scroll-target="#bin-smoothing">Bin Smoothing</a></li>
  <li><a href="#kernel-smoothing" id="toc-kernel-smoothing" class="nav-link" data-scroll-target="#kernel-smoothing">Kernel Smoothing</a></li>
  </ul></li>
  <li><a href="#splines-flexible-curve-fitting" id="toc-splines-flexible-curve-fitting" class="nav-link" data-scroll-target="#splines-flexible-curve-fitting"><span class="header-section-number">29.8</span> Splines: Flexible Curve Fitting</a>
  <ul class="collapse">
  <li><a href="#why-splines" id="toc-why-splines" class="nav-link" data-scroll-target="#why-splines">Why Splines?</a></li>
  <li><a href="#regression-splines" id="toc-regression-splines" class="nav-link" data-scroll-target="#regression-splines">Regression Splines</a></li>
  <li><a href="#natural-splines" id="toc-natural-splines" class="nav-link" data-scroll-target="#natural-splines">Natural Splines</a></li>
  <li><a href="#smoothing-splines" id="toc-smoothing-splines" class="nav-link" data-scroll-target="#smoothing-splines">Smoothing Splines</a></li>
  </ul></li>
  <li><a href="#loess-flexible-non-parametric-smoothing" id="toc-loess-flexible-non-parametric-smoothing" class="nav-link" data-scroll-target="#loess-flexible-non-parametric-smoothing"><span class="header-section-number">29.9</span> LOESS: Flexible Non-Parametric Smoothing</a></li>
  <li><a href="#classification" id="toc-classification" class="nav-link" data-scroll-target="#classification"><span class="header-section-number">29.10</span> Classification</a></li>
  <li><a href="#k-nearest-neighbors" id="toc-k-nearest-neighbors" class="nav-link" data-scroll-target="#k-nearest-neighbors"><span class="header-section-number">29.11</span> K-Nearest Neighbors</a>
  <ul class="collapse">
  <li><a href="#selecting-k-with-cross-validation" id="toc-selecting-k-with-cross-validation" class="nav-link" data-scroll-target="#selecting-k-with-cross-validation">Selecting k with Cross-Validation</a></li>
  </ul></li>
  <li><a href="#confusion-matrices" id="toc-confusion-matrices" class="nav-link" data-scroll-target="#confusion-matrices"><span class="header-section-number">29.12</span> Confusion Matrices</a>
  <ul class="collapse">
  <li><a href="#the-problem-with-accuracy" id="toc-the-problem-with-accuracy" class="nav-link" data-scroll-target="#the-problem-with-accuracy">The Problem with Accuracy</a></li>
  <li><a href="#f1-score-and-balanced-accuracy" id="toc-f1-score-and-balanced-accuracy" class="nav-link" data-scroll-target="#f1-score-and-balanced-accuracy">F1 Score and Balanced Accuracy</a></li>
  </ul></li>
  <li><a href="#roc-curves-and-auc" id="toc-roc-curves-and-auc" class="nav-link" data-scroll-target="#roc-curves-and-auc"><span class="header-section-number">29.13</span> ROC Curves and AUC</a>
  <ul class="collapse">
  <li><a href="#area-under-the-curve-auc" id="toc-area-under-the-curve-auc" class="nav-link" data-scroll-target="#area-under-the-curve-auc">Area Under the Curve (AUC)</a></li>
  <li><a href="#comparing-classifiers-with-roc" id="toc-comparing-classifiers-with-roc" class="nav-link" data-scroll-target="#comparing-classifiers-with-roc">Comparing Classifiers with ROC</a></li>
  <li><a href="#precision-recall-curves" id="toc-precision-recall-curves" class="nav-link" data-scroll-target="#precision-recall-curves">Precision-Recall Curves</a></li>
  </ul></li>
  <li><a href="#the-curse-of-dimensionality" id="toc-the-curse-of-dimensionality" class="nav-link" data-scroll-target="#the-curse-of-dimensionality"><span class="header-section-number">29.14</span> The Curse of Dimensionality</a></li>
  <li><a href="#decision-trees-cart" id="toc-decision-trees-cart" class="nav-link" data-scroll-target="#decision-trees-cart"><span class="header-section-number">29.15</span> Decision Trees (CART)</a>
  <ul class="collapse">
  <li><a href="#motivating-example-olive-oil-classification" id="toc-motivating-example-olive-oil-classification" class="nav-link" data-scroll-target="#motivating-example-olive-oil-classification">Motivating Example: Olive Oil Classification</a></li>
  <li><a href="#how-trees-work" id="toc-how-trees-work" class="nav-link" data-scroll-target="#how-trees-work">How Trees Work</a></li>
  <li><a href="#interpreting-tree-output" id="toc-interpreting-tree-output" class="nav-link" data-scroll-target="#interpreting-tree-output">Interpreting Tree Output</a></li>
  <li><a href="#regression-trees" id="toc-regression-trees" class="nav-link" data-scroll-target="#regression-trees">Regression Trees</a></li>
  <li><a href="#the-decision-boundary" id="toc-the-decision-boundary" class="nav-link" data-scroll-target="#the-decision-boundary">The Decision Boundary</a></li>
  <li><a href="#controlling-tree-complexity" id="toc-controlling-tree-complexity" class="nav-link" data-scroll-target="#controlling-tree-complexity">Controlling Tree Complexity</a></li>
  <li><a href="#pruning-with-cross-validation" id="toc-pruning-with-cross-validation" class="nav-link" data-scroll-target="#pruning-with-cross-validation">Pruning with Cross-Validation</a></li>
  </ul></li>
  <li><a href="#random-forests" id="toc-random-forests" class="nav-link" data-scroll-target="#random-forests"><span class="header-section-number">29.16</span> Random Forests</a>
  <ul class="collapse">
  <li><a href="#the-random-forest-algorithm" id="toc-the-random-forest-algorithm" class="nav-link" data-scroll-target="#the-random-forest-algorithm">The Random Forest Algorithm</a></li>
  <li><a href="#why-averaging-produces-smooth-estimates" id="toc-why-averaging-produces-smooth-estimates" class="nav-link" data-scroll-target="#why-averaging-produces-smooth-estimates">Why Averaging Produces Smooth Estimates</a></li>
  <li><a href="#random-forests-in-r" id="toc-random-forests-in-r" class="nav-link" data-scroll-target="#random-forests-in-r">Random Forests in R</a></li>
  <li><a href="#out-of-bag-oob-error" id="toc-out-of-bag-oob-error" class="nav-link" data-scroll-target="#out-of-bag-oob-error">Out-of-Bag (OOB) Error</a></li>
  <li><a href="#variable-importance" id="toc-variable-importance" class="nav-link" data-scroll-target="#variable-importance">Variable Importance</a></li>
  <li><a href="#tuning-random-forests" id="toc-tuning-random-forests" class="nav-link" data-scroll-target="#tuning-random-forests">Tuning Random Forests</a></li>
  <li><a href="#random-forest-for-regression" id="toc-random-forest-for-regression" class="nav-link" data-scroll-target="#random-forest-for-regression">Random Forest for Regression</a></li>
  </ul></li>
  <li><a href="#support-vector-machines-svm" id="toc-support-vector-machines-svm" class="nav-link" data-scroll-target="#support-vector-machines-svm"><span class="header-section-number">29.17</span> Support Vector Machines (SVM)</a>
  <ul class="collapse">
  <li><a href="#the-maximum-margin-classifier" id="toc-the-maximum-margin-classifier" class="nav-link" data-scroll-target="#the-maximum-margin-classifier">The Maximum Margin Classifier</a></li>
  <li><a href="#soft-margin-and-the-cost-parameter" id="toc-soft-margin-and-the-cost-parameter" class="nav-link" data-scroll-target="#soft-margin-and-the-cost-parameter">Soft Margin and the Cost Parameter</a></li>
  <li><a href="#the-kernel-trick" id="toc-the-kernel-trick" class="nav-link" data-scroll-target="#the-kernel-trick">The Kernel Trick</a></li>
  <li><a href="#tuning-svm-with-cross-validation" id="toc-tuning-svm-with-cross-validation" class="nav-link" data-scroll-target="#tuning-svm-with-cross-validation">Tuning SVM with Cross-Validation</a></li>
  <li><a href="#multiclass-svm" id="toc-multiclass-svm" class="nav-link" data-scroll-target="#multiclass-svm">Multiclass SVM</a></li>
  <li><a href="#support-vector-regression-svr" id="toc-support-vector-regression-svr" class="nav-link" data-scroll-target="#support-vector-regression-svr">Support Vector Regression (SVR)</a></li>
  </ul></li>
  <li><a href="#comparing-classification-methods" id="toc-comparing-classification-methods" class="nav-link" data-scroll-target="#comparing-classification-methods"><span class="header-section-number">29.18</span> Comparing Classification Methods</a></li>
  <li><a href="#practical-workflow" id="toc-practical-workflow" class="nav-link" data-scroll-target="#practical-workflow"><span class="header-section-number">29.19</span> Practical Workflow</a></li>
  <li><a href="#when-to-use-statistical-learning" id="toc-when-to-use-statistical-learning" class="nav-link" data-scroll-target="#when-to-use-statistical-learning"><span class="header-section-number">29.20</span> When to Use Statistical Learning</a></li>
  <li><a href="#connection-to-dimensionality-reduction" id="toc-connection-to-dimensionality-reduction" class="nav-link" data-scroll-target="#connection-to-dimensionality-reduction"><span class="header-section-number">29.21</span> Connection to Dimensionality Reduction</a></li>
  <li><a href="#exercises" id="toc-exercises" class="nav-link" data-scroll-target="#exercises"><span class="header-section-number">29.22</span> Exercises</a></li>
  <li><a href="#summary" id="toc-summary" class="nav-link" data-scroll-target="#summary"><span class="header-section-number">29.23</span> Summary</a></li>
  <li><a href="#additional-resources" id="toc-additional-resources" class="nav-link" data-scroll-target="#additional-resources"><span class="header-section-number">29.24</span> Additional Resources</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../chapters/22-statistical-learning.html">Statistical Learning</a></li><li class="breadcrumb-item"><a href="../chapters/22-statistical-learning.html"><span class="chapter-number">29</span>&nbsp; <span class="chapter-title">Core Concepts in Statistical Learning</span></a></li></ol></nav>
<div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title"><span id="sec-statistical-learning" class="quarto-section-identifier"><span class="chapter-number">29</span>&nbsp; <span class="chapter-title">Core Concepts in Statistical Learning</span></span></h1><button type="button" class="btn code-tools-button dropdown-toggle" id="quarto-code-tools-menu" data-bs-toggle="dropdown" aria-expanded="false"><i class="bi"></i> Code</button><ul class="dropdown-menu dropdown-menu-end" aria-labelelledby="quarto-code-tools-menu"><li><a id="quarto-show-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Show All Code</a></li><li><a id="quarto-hide-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Hide All Code</a></li><li><hr class="dropdown-divider"></li><li><a id="quarto-view-source" class="dropdown-item" href="javascript:void(0)" role="button">View Source</a></li></ul></div></div>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<section id="from-inference-to-prediction" class="level2" data-number="29.1">
<h2 data-number="29.1" class="anchored" data-anchor-id="from-inference-to-prediction"><span class="header-section-number">29.1</span> From Inference to Prediction</h2>
<p>Traditional statistics emphasizes inference—understanding relationships, testing hypotheses, and quantifying uncertainty. Statistical learning (or machine learning) shifts focus toward prediction—building models that accurately predict outcomes for new data <span class="citation" data-cites="hastie2009elements">(<a href="../references.html#ref-hastie2009elements" role="doc-biblioref">Hastie, Tibshirani, and Friedman 2009</a>)</span>.</p>
<p>Both approaches use similar mathematical tools, but the goals differ. In inference, we want to understand the true relationship between variables. In prediction, we want accurate predictions, even if the model does not perfectly capture the underlying mechanism.</p>
</section>
<section id="the-overfitting-problem" class="level2" data-number="29.2">
<h2 data-number="29.2" class="anchored" data-anchor-id="the-overfitting-problem"><span class="header-section-number">29.2</span> The Overfitting Problem</h2>
<p>Models are built to fit training data as closely as possible. A linear regression minimizes squared errors; a logistic regression maximizes likelihood. But models that fit training data too well often predict poorly on new data.</p>
<p><strong>Overfitting</strong> occurs when a model captures noise specific to the training data rather than the true underlying pattern. Complex models with many parameters are especially susceptible.</p>
<p>The solution is to evaluate models on data they have not seen—held-out test data or through cross-validation.</p>
</section>
<section id="loss-functions-quantifying-prediction-error" class="level2" data-number="29.3">
<h2 data-number="29.3" class="anchored" data-anchor-id="loss-functions-quantifying-prediction-error"><span class="header-section-number">29.3</span> Loss Functions: Quantifying Prediction Error</h2>
<p>A <strong>loss function</strong> (or <strong>cost function</strong>) measures how wrong a prediction is. It quantifies the penalty for predicting <span class="math inline">\(\hat{y}\)</span> when the true value is <span class="math inline">\(y\)</span>.</p>
<section id="common-loss-functions-for-regression" class="level3">
<h3 class="anchored" data-anchor-id="common-loss-functions-for-regression">Common Loss Functions for Regression</h3>
<p><strong>Squared Error Loss</strong> (L2): The most common loss for continuous outcomes: <span class="math display">\[L(y, \hat{y}) = (y - \hat{y})^2\]</span></p>
<p>Squaring penalizes large errors more heavily than small ones. Linear regression minimizes the sum of squared errors (SSE or RSS).</p>
<p><strong>Absolute Error Loss</strong> (L1): Less sensitive to outliers: <span class="math display">\[L(y, \hat{y}) = |y - \hat{y}|\]</span></p>
<p><strong>Mean Squared Error (MSE)</strong> and <strong>Root Mean Squared Error (RMSE)</strong> are averages across all predictions: <span class="math display">\[\text{MSE} = \frac{1}{n}\sum_{i=1}^n (y_i - \hat{y}_i)^2, \quad \text{RMSE} = \sqrt{\text{MSE}}\]</span></p>
</section>
<section id="common-loss-functions-for-classification" class="level3">
<h3 class="anchored" data-anchor-id="common-loss-functions-for-classification">Common Loss Functions for Classification</h3>
<p><strong>0-1 Loss</strong>: The simplest classification loss—1 if wrong, 0 if correct: <span class="math display">\[L(y, \hat{y}) = \mathbb{I}(y \neq \hat{y})\]</span></p>
<p>The average 0-1 loss is the <strong>error rate</strong>; one minus the error rate is <strong>accuracy</strong>.</p>
<p><strong>Log Loss</strong> (Cross-Entropy): Used when we have predicted probabilities <span class="math inline">\(\hat{p}\)</span>: <span class="math display">\[L(y, \hat{p}) = -[y \log(\hat{p}) + (1-y) \log(1-\hat{p})]\]</span></p>
<p>Log loss penalizes confident wrong predictions severely—predicting probability 0.99 for the wrong class incurs much larger loss than predicting 0.6.</p>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">2</span>))</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Regression loss functions</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>errors <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="sc">-</span><span class="dv">3</span>, <span class="dv">3</span>, <span class="at">length.out =</span> <span class="dv">100</span>)</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(errors, errors<span class="sc">^</span><span class="dv">2</span>, <span class="at">type =</span> <span class="st">"l"</span>, <span class="at">col =</span> <span class="st">"blue"</span>, <span class="at">lwd =</span> <span class="dv">2</span>,</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">"Prediction Error (y - ŷ)"</span>, <span class="at">ylab =</span> <span class="st">"Loss"</span>,</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>     <span class="at">main =</span> <span class="st">"Regression Loss Functions"</span>)</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(errors, <span class="fu">abs</span>(errors), <span class="at">col =</span> <span class="st">"red"</span>, <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="fu">legend</span>(<span class="st">"top"</span>, <span class="fu">c</span>(<span class="st">"Squared (L2)"</span>, <span class="st">"Absolute (L1)"</span>),</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>       <span class="at">col =</span> <span class="fu">c</span>(<span class="st">"blue"</span>, <span class="st">"red"</span>), <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Classification log loss</span></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>p <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="fl">0.01</span>, <span class="fl">0.99</span>, <span class="at">length.out =</span> <span class="dv">100</span>)</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(p, <span class="sc">-</span><span class="fu">log</span>(p), <span class="at">type =</span> <span class="st">"l"</span>, <span class="at">col =</span> <span class="st">"blue"</span>, <span class="at">lwd =</span> <span class="dv">2</span>,</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">"Predicted Probability for True Class"</span>, <span class="at">ylab =</span> <span class="st">"Log Loss"</span>,</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>     <span class="at">main =</span> <span class="st">"Classification Log Loss"</span>)</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">v =</span> <span class="fl">0.5</span>, <span class="at">lty =</span> <span class="dv">2</span>, <span class="at">col =</span> <span class="st">"gray"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div id="fig-loss-functions" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-loss-functions-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="22-statistical-learning_files/figure-html/fig-loss-functions-1.png" class="img-fluid figure-img" width="768">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-loss-functions-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;29.1: Comparison of squared loss (penalizes large errors heavily) versus absolute loss (more robust to outliers)
</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="why-loss-functions-matter" class="level3">
<h3 class="anchored" data-anchor-id="why-loss-functions-matter">Why Loss Functions Matter</h3>
<p>Different loss functions lead to different optimal predictions:</p>
<ul>
<li><strong>Squared loss</strong> → optimal prediction is the <strong>mean</strong></li>
<li><strong>Absolute loss</strong> → optimal prediction is the <strong>median</strong></li>
<li><strong>0-1 loss</strong> → optimal prediction is the <strong>mode</strong> (most frequent class)</li>
</ul>
<p>The choice of loss function should reflect how errors affect your application. Medical diagnosis may weight false negatives (missed disease) more heavily than false positives.</p>
</section>
</section>
<section id="cross-validation" class="level2" data-number="29.4">
<h2 data-number="29.4" class="anchored" data-anchor-id="cross-validation"><span class="header-section-number">29.4</span> Cross-Validation</h2>
<section id="the-problem-training-error-vs.-test-error" class="level3">
<h3 class="anchored" data-anchor-id="the-problem-training-error-vs.-test-error">The Problem: Training Error vs.&nbsp;Test Error</h3>
<p>A fundamental insight of statistical learning is that <strong>training error</strong> (how well we fit the data used to build the model) is an overly optimistic estimate of <strong>test error</strong> (how well we predict new data).</p>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Demonstrate training vs test error</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">42</span>)</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">100</span></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">sort</span>(<span class="fu">runif</span>(n, <span class="dv">0</span>, <span class="dv">10</span>))</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>y_true <span class="ot">&lt;-</span> <span class="fu">sin</span>(x) <span class="sc">+</span> <span class="fl">0.5</span> <span class="sc">*</span> <span class="fu">cos</span>(<span class="fl">0.5</span> <span class="sc">*</span> x)</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> y_true <span class="sc">+</span> <span class="fu">rnorm</span>(n, <span class="at">sd =</span> <span class="fl">0.3</span>)</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Split into training and test</span></span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>train_idx <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="dv">1</span><span class="sc">:</span>n, <span class="dv">70</span>)</span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>train_data <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">x =</span> x[train_idx], <span class="at">y =</span> y[train_idx])</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>test_data <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">x =</span> x[<span class="sc">-</span>train_idx], <span class="at">y =</span> y[<span class="sc">-</span>train_idx])</span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit polynomials of increasing degree</span></span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(splines)</span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a>degrees <span class="ot">&lt;-</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">15</span></span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a>train_error <span class="ot">&lt;-</span> test_error <span class="ot">&lt;-</span> <span class="fu">numeric</span>(<span class="fu">length</span>(degrees))</span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="fu">seq_along</span>(degrees)) {</span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a>  d <span class="ot">&lt;-</span> degrees[i]</span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a>  fit <span class="ot">&lt;-</span> <span class="fu">lm</span>(y <span class="sc">~</span> <span class="fu">poly</span>(x, d), <span class="at">data =</span> train_data)</span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a>  train_error[i] <span class="ot">&lt;-</span> <span class="fu">mean</span>((train_data<span class="sc">$</span>y <span class="sc">-</span> <span class="fu">predict</span>(fit, train_data))<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a>  test_error[i] <span class="ot">&lt;-</span> <span class="fu">mean</span>((test_data<span class="sc">$</span>y <span class="sc">-</span> <span class="fu">predict</span>(fit, test_data))<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb2-24"><a href="#cb2-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-25"><a href="#cb2-25" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot</span></span>
<span id="cb2-26"><a href="#cb2-26" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(degrees, train_error, <span class="at">type =</span> <span class="st">"b"</span>, <span class="at">pch =</span> <span class="dv">19</span>, <span class="at">col =</span> <span class="st">"blue"</span>,</span>
<span id="cb2-27"><a href="#cb2-27" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">"Model Complexity (Polynomial Degree)"</span>,</span>
<span id="cb2-28"><a href="#cb2-28" aria-hidden="true" tabindex="-1"></a>     <span class="at">ylab =</span> <span class="st">"Mean Squared Error"</span>,</span>
<span id="cb2-29"><a href="#cb2-29" aria-hidden="true" tabindex="-1"></a>     <span class="at">main =</span> <span class="st">"Training vs Test Error"</span>, <span class="at">ylim =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="fu">max</span>(test_error)))</span>
<span id="cb2-30"><a href="#cb2-30" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(degrees, test_error, <span class="at">type =</span> <span class="st">"b"</span>, <span class="at">pch =</span> <span class="dv">19</span>, <span class="at">col =</span> <span class="st">"red"</span>)</span>
<span id="cb2-31"><a href="#cb2-31" aria-hidden="true" tabindex="-1"></a><span class="fu">legend</span>(<span class="st">"topright"</span>, <span class="fu">c</span>(<span class="st">"Training Error"</span>, <span class="st">"Test Error"</span>),</span>
<span id="cb2-32"><a href="#cb2-32" aria-hidden="true" tabindex="-1"></a>       <span class="at">col =</span> <span class="fu">c</span>(<span class="st">"blue"</span>, <span class="st">"red"</span>), <span class="at">pch =</span> <span class="dv">19</span>, <span class="at">lty =</span> <span class="dv">1</span>)</span>
<span id="cb2-33"><a href="#cb2-33" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">v =</span> degrees[<span class="fu">which.min</span>(test_error)], <span class="at">lty =</span> <span class="dv">2</span>, <span class="at">col =</span> <span class="st">"gray"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div id="fig-train-test-error" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-train-test-error-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="22-statistical-learning_files/figure-html/fig-train-test-error-1.png" class="img-fluid figure-img" width="768">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-train-test-error-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;29.2: Training error always decreases with model complexity, but test error eventually increases due to overfitting. The optimal model minimizes test error.
</figcaption>
</figure>
</div>
</div>
</div>
<p>Notice that training error keeps decreasing as complexity increases, eventually reaching near zero. But test error follows a U-shape—it decreases initially as the model captures true patterns, then increases as the model starts fitting noise.</p>
</section>
<section id="k-fold-cross-validation" class="level3">
<h3 class="anchored" data-anchor-id="k-fold-cross-validation">K-Fold Cross-Validation</h3>
<p>Cross-validation estimates how well a model will generalize to new data without requiring a separate test set.</p>
<p><strong>K-fold cross-validation</strong>: 1. Split data into k roughly equal parts (folds) 2. For each fold: train on k-1 folds, test on the held-out fold 3. Average performance across all folds</p>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Simple CV example with linear regression</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(boot)</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate data</span></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">42</span>)</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(<span class="dv">100</span>)</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> <span class="dv">2</span> <span class="sc">+</span> <span class="dv">3</span><span class="sc">*</span>x <span class="sc">+</span> <span class="fu">rnorm</span>(<span class="dv">100</span>)</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>data <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(x, y)</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit model and perform CV</span></span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>model <span class="ot">&lt;-</span> <span class="fu">glm</span>(y <span class="sc">~</span> x, <span class="at">data =</span> data)</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a><span class="co"># 10-fold cross-validation</span></span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>cv_result <span class="ot">&lt;-</span> <span class="fu">cv.glm</span>(data, model, <span class="at">K =</span> <span class="dv">10</span>)</span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"CV estimate of prediction error:"</span>, <span class="fu">round</span>(cv_result<span class="sc">$</span>delta[<span class="dv">1</span>], <span class="dv">3</span>), <span class="st">"</span><span class="sc">\n</span><span class="st">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>CV estimate of prediction error: 0.846 </code></pre>
</div>
</div>
<p><strong>Leave-one-out cross-validation (LOOCV)</strong> is k-fold with k = n: each observation is held out once. More computationally expensive but lower variance.</p>
</section>
<section id="bootstrap-for-error-estimation" class="level3">
<h3 class="anchored" data-anchor-id="bootstrap-for-error-estimation">Bootstrap for Error Estimation</h3>
<p>The <strong>bootstrap</strong> can also estimate prediction error. The approach:</p>
<ol type="1">
<li>Draw a bootstrap sample (n observations with replacement)</li>
<li>Fit the model on the bootstrap sample</li>
<li>Evaluate on observations NOT selected (the “out-of-bag” observations)</li>
<li>Repeat and average</li>
</ol>
<p>This is similar to cross-validation but uses the natural ~37% of observations left out of each bootstrap sample.</p>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Bootstrap estimate of prediction error</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>n_boot <span class="ot">&lt;-</span> <span class="dv">100</span></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>boot_errors <span class="ot">&lt;-</span> <span class="fu">numeric</span>(n_boot)</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (b <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>n_boot) {</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Bootstrap sample</span></span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>  boot_idx <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="dv">1</span><span class="sc">:</span><span class="fu">nrow</span>(data), <span class="at">replace =</span> <span class="cn">TRUE</span>)</span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>  oob_idx <span class="ot">&lt;-</span> <span class="fu">setdiff</span>(<span class="dv">1</span><span class="sc">:</span><span class="fu">nrow</span>(data), <span class="fu">unique</span>(boot_idx))</span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span> (<span class="fu">length</span>(oob_idx) <span class="sc">&gt;</span> <span class="dv">0</span>) {</span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a>    fit <span class="ot">&lt;-</span> <span class="fu">lm</span>(y <span class="sc">~</span> x, <span class="at">data =</span> data[boot_idx, ])</span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a>    boot_errors[b] <span class="ot">&lt;-</span> <span class="fu">mean</span>((data<span class="sc">$</span>y[oob_idx] <span class="sc">-</span> <span class="fu">predict</span>(fit, data[oob_idx, ]))<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"Bootstrap estimate of prediction error:"</span>, <span class="fu">round</span>(<span class="fu">mean</span>(boot_errors), <span class="dv">3</span>), <span class="st">"</span><span class="sc">\n</span><span class="st">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Bootstrap estimate of prediction error: 0.867 </code></pre>
</div>
</div>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Choosing a CV Strategy
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><strong>k = 5 or k = 10</strong>: Standard choices that balance bias and variance</li>
<li><strong>LOOCV (k = n)</strong>: Low bias but high variance; expensive for large n</li>
<li><strong>Bootstrap</strong>: Useful when you also want confidence intervals</li>
<li><strong>Repeated CV</strong>: Run k-fold multiple times with different splits for more stable estimates</li>
</ul>
</div>
</div>
</section>
</section>
<section id="bias-variance-tradeoff" class="level2" data-number="29.5">
<h2 data-number="29.5" class="anchored" data-anchor-id="bias-variance-tradeoff"><span class="header-section-number">29.5</span> Bias-Variance Tradeoff</h2>
<p>Prediction error has two components:</p>
<p><strong>Bias</strong>: Error from approximating a complex reality with a simpler model. Simple models have high bias—they may miss important patterns.</p>
<p><strong>Variance</strong>: Error from sensitivity to training data. Complex models have high variance—they change substantially with different training samples.</p>
<p>The best predictions come from models that balance bias and variance. As model complexity increases, bias decreases but variance increases. The optimal complexity minimizes total prediction error.</p>
</section>
<section id="regularization-controlling-model-complexity" class="level2" data-number="29.6">
<h2 data-number="29.6" class="anchored" data-anchor-id="regularization-controlling-model-complexity"><span class="header-section-number">29.6</span> Regularization: Controlling Model Complexity</h2>
<p><strong>Regularization</strong> addresses overfitting by adding a penalty term that discourages complex models. This is particularly important when you have many predictors relative to observations, or when predictors are correlated.</p>
<section id="the-regularization-idea" class="level3">
<h3 class="anchored" data-anchor-id="the-regularization-idea">The Regularization Idea</h3>
<p>Standard linear regression minimizes the sum of squared residuals (RSS):</p>
<p><span class="math display">\[\text{RSS} = \sum_{i=1}^n (y_i - \beta_0 - \sum_{j=1}^p \beta_j x_{ij})^2\]</span></p>
<p>Regularized regression adds a penalty term <span class="math inline">\(\lambda P(\beta)\)</span> that shrinks coefficients toward zero:</p>
<p><span class="math display">\[\text{Minimize: } \text{RSS} + \lambda P(\beta)\]</span></p>
<p>The <strong>regularization parameter</strong> <span class="math inline">\(\lambda\)</span> controls the strength of the penalty: - <span class="math inline">\(\lambda = 0\)</span>: No penalty, equivalent to ordinary least squares - <span class="math inline">\(\lambda \to \infty\)</span>: Very strong penalty, coefficients shrink toward zero</p>
</section>
<section id="ridge-regression-l2-penalty" class="level3">
<h3 class="anchored" data-anchor-id="ridge-regression-l2-penalty">Ridge Regression (L2 Penalty)</h3>
<p><strong>Ridge regression</strong> <span class="citation" data-cites="hoerl1970ridge">(<a href="../references.html#ref-hoerl1970ridge" role="doc-biblioref">Hoerl and Kennard 1970</a>)</span> uses the sum of squared coefficients as the penalty:</p>
<p><span class="math display">\[P(\beta) = \sum_{j=1}^p \beta_j^2\]</span></p>
<p>This shrinks all coefficients toward zero but never exactly to zero. Ridge is particularly effective when predictors are correlated (multicollinearity).</p>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(glmnet)</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate sample data with correlated predictors</span></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">42</span>)</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">100</span></span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>p <span class="ot">&lt;-</span> <span class="dv">10</span></span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">rnorm</span>(n <span class="sc">*</span> p), n, p)</span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Create correlated predictors</span></span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>X[, <span class="dv">2</span>] <span class="ot">&lt;-</span> X[, <span class="dv">1</span>] <span class="sc">+</span> <span class="fu">rnorm</span>(n, <span class="at">sd =</span> <span class="fl">0.5</span>)</span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>X[, <span class="dv">3</span>] <span class="ot">&lt;-</span> X[, <span class="dv">1</span>] <span class="sc">+</span> <span class="fu">rnorm</span>(n, <span class="at">sd =</span> <span class="fl">0.5</span>)</span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a>true_beta <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">3</span>, <span class="sc">-</span><span class="dv">2</span>, <span class="fl">1.5</span>, <span class="fu">rep</span>(<span class="dv">0</span>, p <span class="sc">-</span> <span class="dv">3</span>))</span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> X <span class="sc">%*%</span> true_beta <span class="sc">+</span> <span class="fu">rnorm</span>(n)</span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit ridge regression across lambda values</span></span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a>ridge_fit <span class="ot">&lt;-</span> <span class="fu">glmnet</span>(X, y, <span class="at">alpha =</span> <span class="dv">0</span>)  <span class="co"># alpha = 0 for ridge</span></span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-17"><a href="#cb7-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot coefficient paths</span></span>
<span id="cb7-18"><a href="#cb7-18" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(ridge_fit, <span class="at">xvar =</span> <span class="st">"lambda"</span>, <span class="at">main =</span> <span class="st">"Ridge Regression Coefficients"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div id="fig-ridge-path" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-ridge-path-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="22-statistical-learning_files/figure-html/fig-ridge-path-1.png" class="img-fluid figure-img" width="768">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-ridge-path-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;29.3: Ridge regression coefficient paths: as lambda increases, coefficients shrink toward zero but never reach exactly zero
</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="lasso-regression-l1-penalty" class="level3">
<h3 class="anchored" data-anchor-id="lasso-regression-l1-penalty">Lasso Regression (L1 Penalty)</h3>
<p><strong>Lasso</strong> (Least Absolute Shrinkage and Selection Operator) <span class="citation" data-cites="tibshirani1996regression">(<a href="../references.html#ref-tibshirani1996regression" role="doc-biblioref">Tibshirani 1996</a>)</span> uses the sum of absolute values as the penalty:</p>
<p><span class="math display">\[P(\beta) = \sum_{j=1}^p |\beta_j|\]</span></p>
<p>Unlike ridge, lasso can shrink coefficients exactly to zero, effectively performing <strong>variable selection</strong>. This produces sparse models that are easier to interpret.</p>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit lasso regression</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>lasso_fit <span class="ot">&lt;-</span> <span class="fu">glmnet</span>(X, y, <span class="at">alpha =</span> <span class="dv">1</span>)  <span class="co"># alpha = 1 for lasso</span></span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(lasso_fit, <span class="at">xvar =</span> <span class="st">"lambda"</span>, <span class="at">main =</span> <span class="st">"Lasso Regression Coefficients"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div id="fig-lasso-path" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-lasso-path-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="22-statistical-learning_files/figure-html/fig-lasso-path-1.png" class="img-fluid figure-img" width="768">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-lasso-path-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;29.4: Lasso regression coefficient paths: as lambda increases, coefficients shrink and some become exactly zero (variable selection)
</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="elastic-net-combining-ridge-and-lasso" class="level3">
<h3 class="anchored" data-anchor-id="elastic-net-combining-ridge-and-lasso">Elastic Net: Combining Ridge and Lasso</h3>
<p><strong>Elastic net</strong> combines both penalties:</p>
<p><span class="math display">\[P(\beta) = \alpha \sum_{j=1}^p |\beta_j| + (1-\alpha) \sum_{j=1}^p \beta_j^2\]</span></p>
<p>The mixing parameter <span class="math inline">\(\alpha\)</span> controls the balance: - <span class="math inline">\(\alpha = 0\)</span>: Pure ridge - <span class="math inline">\(\alpha = 1\)</span>: Pure lasso - <span class="math inline">\(0 &lt; \alpha &lt; 1\)</span>: Combination</p>
<p>Elastic net is often preferred when predictors are correlated—it tends to select groups of correlated variables together.</p>
</section>
<section id="choosing-lambda-with-cross-validation" class="level3">
<h3 class="anchored" data-anchor-id="choosing-lambda-with-cross-validation">Choosing Lambda with Cross-Validation</h3>
<p>The regularization parameter <span class="math inline">\(\lambda\)</span> is typically chosen by cross-validation:</p>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Cross-validation for lasso</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>cv_lasso <span class="ot">&lt;-</span> <span class="fu">cv.glmnet</span>(X, y, <span class="at">alpha =</span> <span class="dv">1</span>)</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot cross-validation results</span></span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(cv_lasso)</span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Optimal lambda values</span></span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"Lambda with minimum CV error:"</span>, <span class="fu">round</span>(cv_lasso<span class="sc">$</span>lambda.min, <span class="dv">4</span>), <span class="st">"</span><span class="sc">\n</span><span class="st">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Lambda with minimum CV error: 0.0378 </code></pre>
</div>
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"Lambda within 1 SE of minimum:"</span>, <span class="fu">round</span>(cv_lasso<span class="sc">$</span>lambda<span class="fl">.1</span>se, <span class="dv">4</span>), <span class="st">"</span><span class="sc">\n</span><span class="st">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Lambda within 1 SE of minimum: 0.0957 </code></pre>
</div>
<div class="cell-output-display">
<div id="fig-cv-lambda" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-cv-lambda-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="22-statistical-learning_files/figure-html/fig-cv-lambda-1.png" class="img-fluid figure-img" width="768">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-cv-lambda-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;29.5: Cross-validation to select optimal lambda: the left dashed line marks the minimum error, the right marks the most regularized model within one standard error
</figcaption>
</figure>
</div>
</div>
</div>
<p>The <code>lambda.1se</code> (one standard error rule) often provides a more parsimonious model with nearly as good performance as the minimum.</p>
</section>
<section id="comparing-regularization-methods" class="level3">
<h3 class="anchored" data-anchor-id="comparing-regularization-methods">Comparing Regularization Methods</h3>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit models with optimal lambda</span></span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>ridge_cv <span class="ot">&lt;-</span> <span class="fu">cv.glmnet</span>(X, y, <span class="at">alpha =</span> <span class="dv">0</span>)</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>lasso_cv <span class="ot">&lt;-</span> <span class="fu">cv.glmnet</span>(X, y, <span class="at">alpha =</span> <span class="dv">1</span>)</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Extract coefficients</span></span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a>coef_ols <span class="ot">&lt;-</span> <span class="fu">coef</span>(<span class="fu">lm</span>(y <span class="sc">~</span> X))</span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a>coef_ridge <span class="ot">&lt;-</span> <span class="fu">coef</span>(ridge_cv, <span class="at">s =</span> <span class="st">"lambda.1se"</span>)</span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a>coef_lasso <span class="ot">&lt;-</span> <span class="fu">coef</span>(lasso_cv, <span class="at">s =</span> <span class="st">"lambda.1se"</span>)</span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Compare (excluding intercept)</span></span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a>comparison <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(</span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a>  <span class="at">True =</span> <span class="fu">c</span>(<span class="cn">NA</span>, true_beta),</span>
<span id="cb13-13"><a href="#cb13-13" aria-hidden="true" tabindex="-1"></a>  <span class="at">OLS =</span> <span class="fu">as.vector</span>(coef_ols),</span>
<span id="cb13-14"><a href="#cb13-14" aria-hidden="true" tabindex="-1"></a>  <span class="at">Ridge =</span> <span class="fu">as.vector</span>(coef_ridge),</span>
<span id="cb13-15"><a href="#cb13-15" aria-hidden="true" tabindex="-1"></a>  <span class="at">Lasso =</span> <span class="fu">as.vector</span>(coef_lasso)</span>
<span id="cb13-16"><a href="#cb13-16" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb13-17"><a href="#cb13-17" aria-hidden="true" tabindex="-1"></a><span class="fu">rownames</span>(comparison) <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">"Intercept"</span>, <span class="fu">paste0</span>(<span class="st">"X"</span>, <span class="dv">1</span><span class="sc">:</span>p))</span>
<span id="cb13-18"><a href="#cb13-18" aria-hidden="true" tabindex="-1"></a><span class="fu">round</span>(comparison, <span class="dv">3</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>          True    OLS  Ridge  Lasso
Intercept   NA  0.069  0.175  0.121
X1         3.0  3.214  1.587  2.248
X2        -2.0 -2.217 -0.764 -1.423
X3         1.5  1.559  1.499  1.618
X4         0.0 -0.126 -0.157 -0.053
X5         0.0 -0.061 -0.009  0.000
X6         0.0  0.103 -0.009  0.000
X7         0.0 -0.023 -0.008  0.000
X8         0.0  0.036  0.009  0.000
X9         0.0  0.051  0.081  0.000
X10        0.0 -0.113 -0.127 -0.046</code></pre>
</div>
</div>
<p>Notice that lasso correctly identifies the zero coefficients (variables 4-10), while ridge shrinks them but doesn’t eliminate them.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
When to Use Each Method
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><strong>Ridge</strong>: When you believe all predictors are relevant and want to handle multicollinearity</li>
<li><strong>Lasso</strong>: When you want automatic variable selection and a sparse model</li>
<li><strong>Elastic Net</strong>: When predictors are correlated and you want both selection and grouping</li>
</ul>
<p><strong>Important</strong>: Always standardize predictors before applying regularization, as the penalty treats all coefficients equally. The <code>glmnet</code> function does this automatically by default.</p>
</div>
</div>
</section>
</section>
<section id="smoothing-from-simple-averages-to-flexible-curves" class="level2" data-number="29.7">
<h2 data-number="29.7" class="anchored" data-anchor-id="smoothing-from-simple-averages-to-flexible-curves"><span class="header-section-number">29.7</span> Smoothing: From Simple Averages to Flexible Curves</h2>
<p>When the relationship between a predictor and outcome is non-linear, we need methods more flexible than linear regression. <strong>Smoothing</strong> methods estimate curves by averaging nearby observations, allowing the data to reveal its own pattern.</p>
<section id="bin-smoothing" class="level3">
<h3 class="anchored" data-anchor-id="bin-smoothing">Bin Smoothing</h3>
<p>The simplest smoothing approach is <strong>bin smoothing</strong> (also called <strong>binning</strong>): divide the predictor into intervals (bins) and estimate the outcome as the average within each bin.</p>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate non-linear data</span></span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">42</span>)</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">200</span></span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">runif</span>(n, <span class="dv">0</span>, <span class="dv">10</span>)</span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> <span class="fu">sin</span>(x) <span class="sc">+</span> <span class="fu">rnorm</span>(n, <span class="at">sd =</span> <span class="fl">0.3</span>)</span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Bin smoothing with different bin widths</span></span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">2</span>))</span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-10"><a href="#cb15-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Narrow bins</span></span>
<span id="cb15-11"><a href="#cb15-11" aria-hidden="true" tabindex="-1"></a>n_bins <span class="ot">&lt;-</span> <span class="dv">20</span></span>
<span id="cb15-12"><a href="#cb15-12" aria-hidden="true" tabindex="-1"></a>breaks <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="fu">min</span>(x), <span class="fu">max</span>(x), <span class="at">length.out =</span> n_bins <span class="sc">+</span> <span class="dv">1</span>)</span>
<span id="cb15-13"><a href="#cb15-13" aria-hidden="true" tabindex="-1"></a>bin_means <span class="ot">&lt;-</span> <span class="fu">sapply</span>(<span class="dv">1</span><span class="sc">:</span>n_bins, <span class="cf">function</span>(i) {</span>
<span id="cb15-14"><a href="#cb15-14" aria-hidden="true" tabindex="-1"></a>  in_bin <span class="ot">&lt;-</span> x <span class="sc">&gt;=</span> breaks[i] <span class="sc">&amp;</span> x <span class="sc">&lt;</span> breaks[i <span class="sc">+</span> <span class="dv">1</span>]</span>
<span id="cb15-15"><a href="#cb15-15" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span> (<span class="fu">sum</span>(in_bin) <span class="sc">&gt;</span> <span class="dv">0</span>) <span class="fu">mean</span>(y[in_bin]) <span class="cf">else</span> <span class="cn">NA</span></span>
<span id="cb15-16"><a href="#cb15-16" aria-hidden="true" tabindex="-1"></a>})</span>
<span id="cb15-17"><a href="#cb15-17" aria-hidden="true" tabindex="-1"></a>bin_centers <span class="ot">&lt;-</span> (breaks[<span class="sc">-</span><span class="dv">1</span>] <span class="sc">+</span> breaks[<span class="sc">-</span>(n_bins <span class="sc">+</span> <span class="dv">1</span>)]) <span class="sc">/</span> <span class="dv">2</span></span>
<span id="cb15-18"><a href="#cb15-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-19"><a href="#cb15-19" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(x, y, <span class="at">pch =</span> <span class="dv">16</span>, <span class="at">col =</span> <span class="st">"gray60"</span>, <span class="at">main =</span> <span class="st">"Narrow bins (20)"</span>)</span>
<span id="cb15-20"><a href="#cb15-20" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span>(bin_centers, bin_means, <span class="at">col =</span> <span class="st">"blue"</span>, <span class="at">pch =</span> <span class="dv">19</span>, <span class="at">cex =</span> <span class="fl">1.5</span>)</span>
<span id="cb15-21"><a href="#cb15-21" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(bin_centers, bin_means, <span class="at">col =</span> <span class="st">"blue"</span>, <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb15-22"><a href="#cb15-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-23"><a href="#cb15-23" aria-hidden="true" tabindex="-1"></a><span class="co"># Wide bins</span></span>
<span id="cb15-24"><a href="#cb15-24" aria-hidden="true" tabindex="-1"></a>n_bins <span class="ot">&lt;-</span> <span class="dv">5</span></span>
<span id="cb15-25"><a href="#cb15-25" aria-hidden="true" tabindex="-1"></a>breaks <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="fu">min</span>(x), <span class="fu">max</span>(x), <span class="at">length.out =</span> n_bins <span class="sc">+</span> <span class="dv">1</span>)</span>
<span id="cb15-26"><a href="#cb15-26" aria-hidden="true" tabindex="-1"></a>bin_means <span class="ot">&lt;-</span> <span class="fu">sapply</span>(<span class="dv">1</span><span class="sc">:</span>n_bins, <span class="cf">function</span>(i) {</span>
<span id="cb15-27"><a href="#cb15-27" aria-hidden="true" tabindex="-1"></a>  in_bin <span class="ot">&lt;-</span> x <span class="sc">&gt;=</span> breaks[i] <span class="sc">&amp;</span> x <span class="sc">&lt;</span> breaks[i <span class="sc">+</span> <span class="dv">1</span>]</span>
<span id="cb15-28"><a href="#cb15-28" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span> (<span class="fu">sum</span>(in_bin) <span class="sc">&gt;</span> <span class="dv">0</span>) <span class="fu">mean</span>(y[in_bin]) <span class="cf">else</span> <span class="cn">NA</span></span>
<span id="cb15-29"><a href="#cb15-29" aria-hidden="true" tabindex="-1"></a>})</span>
<span id="cb15-30"><a href="#cb15-30" aria-hidden="true" tabindex="-1"></a>bin_centers <span class="ot">&lt;-</span> (breaks[<span class="sc">-</span><span class="dv">1</span>] <span class="sc">+</span> breaks[<span class="sc">-</span>(n_bins <span class="sc">+</span> <span class="dv">1</span>)]) <span class="sc">/</span> <span class="dv">2</span></span>
<span id="cb15-31"><a href="#cb15-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-32"><a href="#cb15-32" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(x, y, <span class="at">pch =</span> <span class="dv">16</span>, <span class="at">col =</span> <span class="st">"gray60"</span>, <span class="at">main =</span> <span class="st">"Wide bins (5)"</span>)</span>
<span id="cb15-33"><a href="#cb15-33" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span>(bin_centers, bin_means, <span class="at">col =</span> <span class="st">"red"</span>, <span class="at">pch =</span> <span class="dv">19</span>, <span class="at">cex =</span> <span class="fl">1.5</span>)</span>
<span id="cb15-34"><a href="#cb15-34" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(bin_centers, bin_means, <span class="at">col =</span> <span class="st">"red"</span>, <span class="at">lwd =</span> <span class="dv">2</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div id="fig-bin-smoothing" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-bin-smoothing-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="22-statistical-learning_files/figure-html/fig-bin-smoothing-1.png" class="img-fluid figure-img" width="768">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-bin-smoothing-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;29.6: Bin smoothing divides data into intervals and estimates each segment as the mean of points in that bin
</figcaption>
</figure>
</div>
</div>
</div>
<p>Bin smoothing illustrates the <strong>bias-variance tradeoff</strong> in smoothing:</p>
<ul>
<li><strong>Narrow bins</strong>: Capture local variation (low bias) but are noisy (high variance)</li>
<li><strong>Wide bins</strong>: Smooth over noise (low variance) but may miss true curvature (high bias)</li>
</ul>
<p>The main limitation of bin smoothing is the <strong>discontinuity</strong> at bin boundaries—the estimate jumps from one bin to the next.</p>
</section>
<section id="kernel-smoothing" class="level3">
<h3 class="anchored" data-anchor-id="kernel-smoothing">Kernel Smoothing</h3>
<p><strong>Kernel smoothing</strong> improves on binning by using weighted averages, where closer points receive more weight. This creates smooth, continuous estimates.</p>
<p>The estimate at any point <span class="math inline">\(x_0\)</span> is:</p>
<p><span class="math display">\[\hat{f}(x_0) = \frac{\sum_{i=1}^n K\left(\frac{x_i - x_0}{h}\right) y_i}{\sum_{i=1}^n K\left(\frac{x_i - x_0}{h}\right)}\]</span></p>
<p>where <span class="math inline">\(K\)</span> is a <strong>kernel function</strong> (typically Gaussian or Epanechnikov) and <span class="math inline">\(h\)</span> is the <strong>bandwidth</strong> controlling smoothness.</p>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Kernel smoothing function</span></span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>kernel_smooth <span class="ot">&lt;-</span> <span class="cf">function</span>(x0, x, y, bandwidth) {</span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a>  weights <span class="ot">&lt;-</span> <span class="fu">dnorm</span>(x, <span class="at">mean =</span> x0, <span class="at">sd =</span> bandwidth)</span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">sum</span>(weights <span class="sc">*</span> y) <span class="sc">/</span> <span class="fu">sum</span>(weights)</span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Apply to grid of points</span></span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true" tabindex="-1"></a>x_grid <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="dv">0</span>, <span class="dv">10</span>, <span class="at">length.out =</span> <span class="dv">200</span>)</span>
<span id="cb16-9"><a href="#cb16-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-10"><a href="#cb16-10" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">3</span>))</span>
<span id="cb16-11"><a href="#cb16-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-12"><a href="#cb16-12" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (bw <span class="cf">in</span> <span class="fu">c</span>(<span class="fl">0.2</span>, <span class="fl">0.5</span>, <span class="fl">1.0</span>)) {</span>
<span id="cb16-13"><a href="#cb16-13" aria-hidden="true" tabindex="-1"></a>  y_smooth <span class="ot">&lt;-</span> <span class="fu">sapply</span>(x_grid, <span class="cf">function</span>(x0) <span class="fu">kernel_smooth</span>(x0, x, y, bw))</span>
<span id="cb16-14"><a href="#cb16-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-15"><a href="#cb16-15" aria-hidden="true" tabindex="-1"></a>  <span class="fu">plot</span>(x, y, <span class="at">pch =</span> <span class="dv">16</span>, <span class="at">col =</span> <span class="st">"gray60"</span>, <span class="at">main =</span> <span class="fu">paste</span>(<span class="st">"Bandwidth ="</span>, bw))</span>
<span id="cb16-16"><a href="#cb16-16" aria-hidden="true" tabindex="-1"></a>  <span class="fu">lines</span>(x_grid, y_smooth, <span class="at">col =</span> <span class="st">"blue"</span>, <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb16-17"><a href="#cb16-17" aria-hidden="true" tabindex="-1"></a>  <span class="fu">lines</span>(x_grid, <span class="fu">sin</span>(x_grid), <span class="at">col =</span> <span class="st">"red"</span>, <span class="at">lwd =</span> <span class="dv">2</span>, <span class="at">lty =</span> <span class="dv">2</span>)</span>
<span id="cb16-18"><a href="#cb16-18" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div id="fig-kernel-smoothing" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-kernel-smoothing-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="22-statistical-learning_files/figure-html/fig-kernel-smoothing-1.png" class="img-fluid figure-img" width="864">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-kernel-smoothing-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;29.7: Kernel smoothing uses weighted averages with Gaussian weights, creating smooth estimates
</figcaption>
</figure>
</div>
</div>
</div>
<p>The <strong>bandwidth</strong> parameter plays the same role as the number of bins:</p>
<ul>
<li><strong>Small bandwidth</strong>: More local, follows the data closely (risk of overfitting)</li>
<li><strong>Large bandwidth</strong>: More global, smoother curve (risk of over-smoothing)</li>
</ul>
<p>Kernel smoothing eliminates the discontinuity problem of bin smoothing while retaining its intuitive local-averaging interpretation.</p>
</section>
</section>
<section id="splines-flexible-curve-fitting" class="level2" data-number="29.8">
<h2 data-number="29.8" class="anchored" data-anchor-id="splines-flexible-curve-fitting"><span class="header-section-number">29.8</span> Splines: Flexible Curve Fitting</h2>
<p>While LOESS provides local smoothing, <strong>splines</strong> offer a more structured approach to fitting flexible curves. A spline is a piecewise polynomial function that joins smoothly at points called <strong>knots</strong>.</p>
<section id="why-splines" class="level3">
<h3 class="anchored" data-anchor-id="why-splines">Why Splines?</h3>
<p>Linear regression assumes a straight-line relationship, which is often too restrictive. We could fit polynomial regression (e.g., <span class="math inline">\(y = \beta_0 + \beta_1 x + \beta_2 x^2 + \beta_3 x^3\)</span>), but polynomials can behave erratically, especially at the edges of the data.</p>
<p>Splines provide flexibility while maintaining smooth, well-behaved curves.</p>
</section>
<section id="regression-splines" class="level3">
<h3 class="anchored" data-anchor-id="regression-splines">Regression Splines</h3>
<p><strong>Regression splines</strong> fit piecewise polynomials at fixed knot locations. The <code>splines</code> package provides basis functions for incorporating splines into linear models:</p>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(splines)</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate non-linear data</span></span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">42</span>)</span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="dv">0</span>, <span class="dv">10</span>, <span class="at">length.out =</span> <span class="dv">100</span>)</span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> <span class="fu">sin</span>(x) <span class="sc">+</span> <span class="fl">0.5</span> <span class="sc">*</span> <span class="fu">cos</span>(<span class="dv">2</span><span class="sc">*</span>x) <span class="sc">+</span> <span class="fu">rnorm</span>(<span class="dv">100</span>, <span class="at">sd =</span> <span class="fl">0.3</span>)</span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a>data <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(x, y)</span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-9"><a href="#cb17-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit different models</span></span>
<span id="cb17-10"><a href="#cb17-10" aria-hidden="true" tabindex="-1"></a>fit_linear <span class="ot">&lt;-</span> <span class="fu">lm</span>(y <span class="sc">~</span> x, <span class="at">data =</span> data)</span>
<span id="cb17-11"><a href="#cb17-11" aria-hidden="true" tabindex="-1"></a>fit_poly <span class="ot">&lt;-</span> <span class="fu">lm</span>(y <span class="sc">~</span> <span class="fu">poly</span>(x, <span class="dv">5</span>), <span class="at">data =</span> data)</span>
<span id="cb17-12"><a href="#cb17-12" aria-hidden="true" tabindex="-1"></a>fit_spline <span class="ot">&lt;-</span> <span class="fu">lm</span>(y <span class="sc">~</span> <span class="fu">bs</span>(x, <span class="at">df =</span> <span class="dv">6</span>), <span class="at">data =</span> data)  <span class="co"># B-spline with 6 df</span></span>
<span id="cb17-13"><a href="#cb17-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-14"><a href="#cb17-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Predictions</span></span>
<span id="cb17-15"><a href="#cb17-15" aria-hidden="true" tabindex="-1"></a>data<span class="sc">$</span>pred_linear <span class="ot">&lt;-</span> <span class="fu">predict</span>(fit_linear)</span>
<span id="cb17-16"><a href="#cb17-16" aria-hidden="true" tabindex="-1"></a>data<span class="sc">$</span>pred_poly <span class="ot">&lt;-</span> <span class="fu">predict</span>(fit_poly)</span>
<span id="cb17-17"><a href="#cb17-17" aria-hidden="true" tabindex="-1"></a>data<span class="sc">$</span>pred_spline <span class="ot">&lt;-</span> <span class="fu">predict</span>(fit_spline)</span>
<span id="cb17-18"><a href="#cb17-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-19"><a href="#cb17-19" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">3</span>))</span>
<span id="cb17-20"><a href="#cb17-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-21"><a href="#cb17-21" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(x, y, <span class="at">pch =</span> <span class="dv">16</span>, <span class="at">col =</span> <span class="st">"gray60"</span>, <span class="at">main =</span> <span class="st">"Linear"</span>)</span>
<span id="cb17-22"><a href="#cb17-22" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(x, data<span class="sc">$</span>pred_linear, <span class="at">col =</span> <span class="st">"red"</span>, <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb17-23"><a href="#cb17-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-24"><a href="#cb17-24" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(x, y, <span class="at">pch =</span> <span class="dv">16</span>, <span class="at">col =</span> <span class="st">"gray60"</span>, <span class="at">main =</span> <span class="st">"Polynomial (degree 5)"</span>)</span>
<span id="cb17-25"><a href="#cb17-25" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(x, data<span class="sc">$</span>pred_poly, <span class="at">col =</span> <span class="st">"blue"</span>, <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb17-26"><a href="#cb17-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-27"><a href="#cb17-27" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(x, y, <span class="at">pch =</span> <span class="dv">16</span>, <span class="at">col =</span> <span class="st">"gray60"</span>, <span class="at">main =</span> <span class="st">"Spline (6 df)"</span>)</span>
<span id="cb17-28"><a href="#cb17-28" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(x, data<span class="sc">$</span>pred_spline, <span class="at">col =</span> <span class="st">"darkgreen"</span>, <span class="at">lwd =</span> <span class="dv">2</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div id="fig-spline-comparison" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-spline-comparison-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="22-statistical-learning_files/figure-html/fig-spline-comparison-1.png" class="img-fluid figure-img" width="864">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-spline-comparison-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;29.8: Comparison of linear, polynomial, and spline fits for non-linear data
</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="natural-splines" class="level3">
<h3 class="anchored" data-anchor-id="natural-splines">Natural Splines</h3>
<p><strong>Natural splines</strong> add the constraint that the function is linear beyond the boundary knots. This prevents the wild behavior that polynomials often exhibit at the edges:</p>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Compare B-spline and natural spline</span></span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>fit_bs <span class="ot">&lt;-</span> <span class="fu">lm</span>(y <span class="sc">~</span> <span class="fu">bs</span>(x, <span class="at">df =</span> <span class="dv">6</span>), <span class="at">data =</span> data)</span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a>fit_ns <span class="ot">&lt;-</span> <span class="fu">lm</span>(y <span class="sc">~</span> <span class="fu">ns</span>(x, <span class="at">df =</span> <span class="dv">6</span>), <span class="at">data =</span> data)</span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Extend prediction range to see edge behavior</span></span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a>x_ext <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="sc">-</span><span class="dv">2</span>, <span class="dv">12</span>, <span class="at">length.out =</span> <span class="dv">200</span>)</span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true" tabindex="-1"></a>pred_bs <span class="ot">&lt;-</span> <span class="fu">predict</span>(fit_bs, <span class="at">newdata =</span> <span class="fu">data.frame</span>(<span class="at">x =</span> x_ext))</span>
<span id="cb18-8"><a href="#cb18-8" aria-hidden="true" tabindex="-1"></a>pred_ns <span class="ot">&lt;-</span> <span class="fu">predict</span>(fit_ns, <span class="at">newdata =</span> <span class="fu">data.frame</span>(<span class="at">x =</span> x_ext))</span>
<span id="cb18-9"><a href="#cb18-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-10"><a href="#cb18-10" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(x, y, <span class="at">pch =</span> <span class="dv">16</span>, <span class="at">col =</span> <span class="st">"gray60"</span>, <span class="at">xlim =</span> <span class="fu">c</span>(<span class="sc">-</span><span class="dv">2</span>, <span class="dv">12</span>), <span class="at">ylim =</span> <span class="fu">c</span>(<span class="sc">-</span><span class="dv">3</span>, <span class="dv">3</span>),</span>
<span id="cb18-11"><a href="#cb18-11" aria-hidden="true" tabindex="-1"></a>     <span class="at">main =</span> <span class="st">"B-spline vs Natural Spline at Boundaries"</span>)</span>
<span id="cb18-12"><a href="#cb18-12" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(x_ext, pred_bs, <span class="at">col =</span> <span class="st">"blue"</span>, <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb18-13"><a href="#cb18-13" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(x_ext, pred_ns, <span class="at">col =</span> <span class="st">"darkgreen"</span>, <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb18-14"><a href="#cb18-14" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">v =</span> <span class="fu">range</span>(x), <span class="at">lty =</span> <span class="dv">2</span>, <span class="at">col =</span> <span class="st">"gray"</span>)</span>
<span id="cb18-15"><a href="#cb18-15" aria-hidden="true" tabindex="-1"></a><span class="fu">legend</span>(<span class="st">"topright"</span>, <span class="fu">c</span>(<span class="st">"B-spline"</span>, <span class="st">"Natural spline"</span>, <span class="st">"Data range"</span>),</span>
<span id="cb18-16"><a href="#cb18-16" aria-hidden="true" tabindex="-1"></a>       <span class="at">col =</span> <span class="fu">c</span>(<span class="st">"blue"</span>, <span class="st">"darkgreen"</span>, <span class="st">"gray"</span>), <span class="at">lty =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">2</span>), <span class="at">lwd =</span> <span class="fu">c</span>(<span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">1</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div id="fig-natural-spline" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-natural-spline-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="22-statistical-learning_files/figure-html/fig-natural-spline-1.png" class="img-fluid figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-natural-spline-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;29.9: Natural splines constrain the fit to be linear beyond the data boundaries, reducing edge effects
</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="smoothing-splines" class="level3">
<h3 class="anchored" data-anchor-id="smoothing-splines">Smoothing Splines</h3>
<p><strong>Smoothing splines</strong> take a different approach: instead of pre-specifying knots, they place a knot at every data point and control smoothness through a penalty on the second derivative:</p>
<p><span class="math display">\[\text{Minimize: } \sum_{i=1}^n (y_i - f(x_i))^2 + \lambda \int f''(x)^2 dx\]</span></p>
<p>The smoothing parameter <span class="math inline">\(\lambda\)</span> is typically chosen by cross-validation:</p>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit smoothing spline with cross-validation</span></span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>smooth_fit <span class="ot">&lt;-</span> <span class="fu">smooth.spline</span>(x, y, <span class="at">cv =</span> <span class="cn">TRUE</span>)</span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(x, y, <span class="at">pch =</span> <span class="dv">16</span>, <span class="at">col =</span> <span class="st">"gray60"</span>, <span class="at">main =</span> <span class="st">"Smoothing Spline"</span>)</span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(smooth_fit, <span class="at">col =</span> <span class="st">"purple"</span>, <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"Optimal degrees of freedom:"</span>, <span class="fu">round</span>(smooth_fit<span class="sc">$</span>df, <span class="dv">2</span>), <span class="st">"</span><span class="sc">\n</span><span class="st">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Optimal degrees of freedom: 14.81 </code></pre>
</div>
<div class="cell-output-display">
<div id="fig-smoothing-spline" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-smoothing-spline-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="22-statistical-learning_files/figure-html/fig-smoothing-spline-1.png" class="img-fluid figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-smoothing-spline-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;29.10: Smoothing spline with automatic cross-validation selection of the smoothing parameter
</figcaption>
</figure>
</div>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Choosing the Right Approach
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><strong>Regression splines (bs, ns)</strong>: When you want to include splines in a regression model with other predictors</li>
<li><strong>Natural splines</strong>: When extrapolation behavior matters</li>
<li><strong>Smoothing splines</strong>: For exploratory smoothing with automatic tuning</li>
<li><strong>LOESS</strong>: For local, non-parametric smoothing (especially useful for visualization)</li>
</ul>
</div>
</div>
</section>
</section>
<section id="loess-flexible-non-parametric-smoothing" class="level2" data-number="29.9">
<h2 data-number="29.9" class="anchored" data-anchor-id="loess-flexible-non-parametric-smoothing"><span class="header-section-number">29.9</span> LOESS: Flexible Non-Parametric Smoothing</h2>
<p><strong>LOESS</strong> (Locally Estimated Scatterplot Smoothing) <span class="citation" data-cites="cleveland1979robust">(<a href="../references.html#ref-cleveland1979robust" role="doc-biblioref">Cleveland 1979</a>)</span> fits local regressions to subsets of data, weighted by distance from each point.</p>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Compare linear regression and LOESS</span></span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="dv">0</span>, <span class="dv">4</span><span class="sc">*</span>pi, <span class="at">length.out =</span> <span class="dv">100</span>)</span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> <span class="fu">sin</span>(x) <span class="sc">+</span> <span class="fu">rnorm</span>(<span class="dv">100</span>, <span class="at">sd =</span> <span class="fl">0.3</span>)</span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(x, y, <span class="at">pch =</span> <span class="dv">16</span>, <span class="at">col =</span> <span class="st">"gray60"</span>, <span class="at">main =</span> <span class="st">"Linear vs LOESS"</span>)</span>
<span id="cb21-7"><a href="#cb21-7" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="fu">lm</span>(y <span class="sc">~</span> x), <span class="at">col =</span> <span class="st">"red"</span>, <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb21-8"><a href="#cb21-8" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(x, <span class="fu">predict</span>(<span class="fu">loess</span>(y <span class="sc">~</span> x, <span class="at">span =</span> <span class="fl">0.3</span>)), <span class="at">col =</span> <span class="st">"blue"</span>, <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb21-9"><a href="#cb21-9" aria-hidden="true" tabindex="-1"></a><span class="fu">legend</span>(<span class="st">"topright"</span>, <span class="fu">c</span>(<span class="st">"Linear"</span>, <span class="st">"LOESS"</span>), <span class="at">col =</span> <span class="fu">c</span>(<span class="st">"red"</span>, <span class="st">"blue"</span>), <span class="at">lwd =</span> <span class="dv">2</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div id="fig-loess-comparison" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-loess-comparison-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="22-statistical-learning_files/figure-html/fig-loess-comparison-1.png" class="img-fluid figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-loess-comparison-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;29.11: Comparison of linear regression and LOESS smoothing for non-linear data
</figcaption>
</figure>
</div>
</div>
</div>
<p>The <strong>span</strong> parameter controls smoothness: smaller values fit more locally (more flexible), larger values average more broadly (smoother).</p>
</section>
<section id="classification" class="level2" data-number="29.10">
<h2 data-number="29.10" class="anchored" data-anchor-id="classification"><span class="header-section-number">29.10</span> Classification</h2>
<p>When the response is categorical, we have a classification problem rather than regression. The goal is to predict which category an observation belongs to.</p>
<p><strong>Logistic regression</strong> produces probabilities that can be converted to class predictions.</p>
<p><strong>Decision trees</strong> recursively partition the feature space based on simple rules.</p>
<p><strong>Random forests</strong> combine many decision trees for more robust predictions.</p>
</section>
<section id="k-nearest-neighbors" class="level2" data-number="29.11">
<h2 data-number="29.11" class="anchored" data-anchor-id="k-nearest-neighbors"><span class="header-section-number">29.11</span> K-Nearest Neighbors</h2>
<p><strong>K-nearest neighbors (kNN)</strong> is one of the simplest and most intuitive classification algorithms. To classify a new observation, kNN finds the k closest observations in the training data and assigns the most common class among those neighbors.</p>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Simulate two-class data</span></span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">42</span>)</span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">100</span></span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a>class1 <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(</span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a>  <span class="at">x1 =</span> <span class="fu">rnorm</span>(n<span class="sc">/</span><span class="dv">2</span>, <span class="at">mean =</span> <span class="dv">2</span>, <span class="at">sd =</span> <span class="dv">1</span>),</span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true" tabindex="-1"></a>  <span class="at">x2 =</span> <span class="fu">rnorm</span>(n<span class="sc">/</span><span class="dv">2</span>, <span class="at">mean =</span> <span class="dv">2</span>, <span class="at">sd =</span> <span class="dv">1</span>),</span>
<span id="cb22-7"><a href="#cb22-7" aria-hidden="true" tabindex="-1"></a>  <span class="at">class =</span> <span class="st">"A"</span></span>
<span id="cb22-8"><a href="#cb22-8" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb22-9"><a href="#cb22-9" aria-hidden="true" tabindex="-1"></a>class2 <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(</span>
<span id="cb22-10"><a href="#cb22-10" aria-hidden="true" tabindex="-1"></a>  <span class="at">x1 =</span> <span class="fu">rnorm</span>(n<span class="sc">/</span><span class="dv">2</span>, <span class="at">mean =</span> <span class="dv">4</span>, <span class="at">sd =</span> <span class="dv">1</span>),</span>
<span id="cb22-11"><a href="#cb22-11" aria-hidden="true" tabindex="-1"></a>  <span class="at">x2 =</span> <span class="fu">rnorm</span>(n<span class="sc">/</span><span class="dv">2</span>, <span class="at">mean =</span> <span class="dv">4</span>, <span class="at">sd =</span> <span class="dv">1</span>),</span>
<span id="cb22-12"><a href="#cb22-12" aria-hidden="true" tabindex="-1"></a>  <span class="at">class =</span> <span class="st">"B"</span></span>
<span id="cb22-13"><a href="#cb22-13" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb22-14"><a href="#cb22-14" aria-hidden="true" tabindex="-1"></a>train_data <span class="ot">&lt;-</span> <span class="fu">rbind</span>(class1, class2)</span>
<span id="cb22-15"><a href="#cb22-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-16"><a href="#cb22-16" aria-hidden="true" tabindex="-1"></a><span class="co"># New point to classify</span></span>
<span id="cb22-17"><a href="#cb22-17" aria-hidden="true" tabindex="-1"></a>new_point <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">x1 =</span> <span class="fl">3.2</span>, <span class="at">x2 =</span> <span class="fl">3.5</span>)</span>
<span id="cb22-18"><a href="#cb22-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-19"><a href="#cb22-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot</span></span>
<span id="cb22-20"><a href="#cb22-20" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(train_data<span class="sc">$</span>x1, train_data<span class="sc">$</span>x2,</span>
<span id="cb22-21"><a href="#cb22-21" aria-hidden="true" tabindex="-1"></a>     <span class="at">col =</span> <span class="fu">ifelse</span>(train_data<span class="sc">$</span>class <span class="sc">==</span> <span class="st">"A"</span>, <span class="st">"blue"</span>, <span class="st">"red"</span>),</span>
<span id="cb22-22"><a href="#cb22-22" aria-hidden="true" tabindex="-1"></a>     <span class="at">pch =</span> <span class="dv">16</span>, <span class="at">xlab =</span> <span class="st">"Feature 1"</span>, <span class="at">ylab =</span> <span class="st">"Feature 2"</span>,</span>
<span id="cb22-23"><a href="#cb22-23" aria-hidden="true" tabindex="-1"></a>     <span class="at">main =</span> <span class="st">"K-Nearest Neighbors (k=5)"</span>)</span>
<span id="cb22-24"><a href="#cb22-24" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span>(new_point<span class="sc">$</span>x1, new_point<span class="sc">$</span>x2, <span class="at">pch =</span> <span class="dv">8</span>, <span class="at">cex =</span> <span class="dv">2</span>, <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb22-25"><a href="#cb22-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-26"><a href="#cb22-26" aria-hidden="true" tabindex="-1"></a><span class="co"># Find 5 nearest neighbors</span></span>
<span id="cb22-27"><a href="#cb22-27" aria-hidden="true" tabindex="-1"></a>distances <span class="ot">&lt;-</span> <span class="fu">sqrt</span>((train_data<span class="sc">$</span>x1 <span class="sc">-</span> new_point<span class="sc">$</span>x1)<span class="sc">^</span><span class="dv">2</span> <span class="sc">+</span></span>
<span id="cb22-28"><a href="#cb22-28" aria-hidden="true" tabindex="-1"></a>                  (train_data<span class="sc">$</span>x2 <span class="sc">-</span> new_point<span class="sc">$</span>x2)<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb22-29"><a href="#cb22-29" aria-hidden="true" tabindex="-1"></a>nearest <span class="ot">&lt;-</span> <span class="fu">order</span>(distances)[<span class="dv">1</span><span class="sc">:</span><span class="dv">5</span>]</span>
<span id="cb22-30"><a href="#cb22-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-31"><a href="#cb22-31" aria-hidden="true" tabindex="-1"></a><span class="co"># Draw circles around nearest neighbors</span></span>
<span id="cb22-32"><a href="#cb22-32" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span>(train_data<span class="sc">$</span>x1[nearest], train_data<span class="sc">$</span>x2[nearest],</span>
<span id="cb22-33"><a href="#cb22-33" aria-hidden="true" tabindex="-1"></a>       <span class="at">cex =</span> <span class="dv">2</span>, <span class="at">col =</span> <span class="fu">ifelse</span>(train_data<span class="sc">$</span>class[nearest] <span class="sc">==</span> <span class="st">"A"</span>, <span class="st">"blue"</span>, <span class="st">"red"</span>))</span>
<span id="cb22-34"><a href="#cb22-34" aria-hidden="true" tabindex="-1"></a><span class="fu">legend</span>(<span class="st">"topleft"</span>, <span class="fu">c</span>(<span class="st">"Class A"</span>, <span class="st">"Class B"</span>, <span class="st">"New point"</span>),</span>
<span id="cb22-35"><a href="#cb22-35" aria-hidden="true" tabindex="-1"></a>       <span class="at">col =</span> <span class="fu">c</span>(<span class="st">"blue"</span>, <span class="st">"red"</span>, <span class="st">"black"</span>), <span class="at">pch =</span> <span class="fu">c</span>(<span class="dv">16</span>, <span class="dv">16</span>, <span class="dv">8</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div id="fig-knn-concept" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-knn-concept-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="22-statistical-learning_files/figure-html/fig-knn-concept-1.png" class="img-fluid figure-img" width="768">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-knn-concept-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;29.12: K-nearest neighbors classification: the new point (star) is classified based on its nearest neighbors
</figcaption>
</figure>
</div>
</div>
</div>
<p>The choice of <strong>k</strong> is critical and illustrates the bias-variance tradeoff:</p>
<ul>
<li><strong>Small k</strong> (e.g., k=1): Very flexible, low bias but high variance. The decision boundary is jagged and sensitive to individual training points—prone to overfitting.</li>
<li><strong>Large k</strong>: Smoother decision boundary, higher bias but lower variance. May miss local patterns—prone to underfitting.</li>
</ul>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(class)</span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a grid for visualization</span></span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true" tabindex="-1"></a>x1_grid <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="dv">0</span>, <span class="dv">6</span>, <span class="at">length.out =</span> <span class="dv">100</span>)</span>
<span id="cb23-5"><a href="#cb23-5" aria-hidden="true" tabindex="-1"></a>x2_grid <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="dv">0</span>, <span class="dv">6</span>, <span class="at">length.out =</span> <span class="dv">100</span>)</span>
<span id="cb23-6"><a href="#cb23-6" aria-hidden="true" tabindex="-1"></a>grid <span class="ot">&lt;-</span> <span class="fu">expand.grid</span>(<span class="at">x1 =</span> x1_grid, <span class="at">x2 =</span> x2_grid)</span>
<span id="cb23-7"><a href="#cb23-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-8"><a href="#cb23-8" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">3</span>))</span>
<span id="cb23-9"><a href="#cb23-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-10"><a href="#cb23-10" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (k_val <span class="cf">in</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">15</span>, <span class="dv">50</span>)) {</span>
<span id="cb23-11"><a href="#cb23-11" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Predict on grid</span></span>
<span id="cb23-12"><a href="#cb23-12" aria-hidden="true" tabindex="-1"></a>  pred <span class="ot">&lt;-</span> <span class="fu">knn</span>(<span class="at">train =</span> train_data[, <span class="dv">1</span><span class="sc">:</span><span class="dv">2</span>],</span>
<span id="cb23-13"><a href="#cb23-13" aria-hidden="true" tabindex="-1"></a>              <span class="at">test =</span> grid,</span>
<span id="cb23-14"><a href="#cb23-14" aria-hidden="true" tabindex="-1"></a>              <span class="at">cl =</span> train_data<span class="sc">$</span>class,</span>
<span id="cb23-15"><a href="#cb23-15" aria-hidden="true" tabindex="-1"></a>              <span class="at">k =</span> k_val)</span>
<span id="cb23-16"><a href="#cb23-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-17"><a href="#cb23-17" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Plot decision regions</span></span>
<span id="cb23-18"><a href="#cb23-18" aria-hidden="true" tabindex="-1"></a>  <span class="fu">plot</span>(grid<span class="sc">$</span>x1, grid<span class="sc">$</span>x2, <span class="at">col =</span> <span class="fu">ifelse</span>(pred <span class="sc">==</span> <span class="st">"A"</span>,</span>
<span id="cb23-19"><a href="#cb23-19" aria-hidden="true" tabindex="-1"></a>       <span class="fu">rgb</span>(<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">1</span>, <span class="fl">0.1</span>), <span class="fu">rgb</span>(<span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="fl">0.1</span>)),</span>
<span id="cb23-20"><a href="#cb23-20" aria-hidden="true" tabindex="-1"></a>       <span class="at">pch =</span> <span class="dv">15</span>, <span class="at">cex =</span> <span class="fl">0.5</span>, <span class="at">xlab =</span> <span class="st">"Feature 1"</span>, <span class="at">ylab =</span> <span class="st">"Feature 2"</span>,</span>
<span id="cb23-21"><a href="#cb23-21" aria-hidden="true" tabindex="-1"></a>       <span class="at">main =</span> <span class="fu">paste</span>(<span class="st">"k ="</span>, k_val))</span>
<span id="cb23-22"><a href="#cb23-22" aria-hidden="true" tabindex="-1"></a>  <span class="fu">points</span>(train_data<span class="sc">$</span>x1, train_data<span class="sc">$</span>x2,</span>
<span id="cb23-23"><a href="#cb23-23" aria-hidden="true" tabindex="-1"></a>         <span class="at">col =</span> <span class="fu">ifelse</span>(train_data<span class="sc">$</span>class <span class="sc">==</span> <span class="st">"A"</span>, <span class="st">"blue"</span>, <span class="st">"red"</span>), <span class="at">pch =</span> <span class="dv">16</span>)</span>
<span id="cb23-24"><a href="#cb23-24" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div id="fig-knn-k-comparison" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-knn-k-comparison-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="22-statistical-learning_files/figure-html/fig-knn-k-comparison-1.png" class="img-fluid figure-img" width="864">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-knn-k-comparison-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;29.13: Effect of k on kNN classification: small k creates complex boundaries (potential overfitting), large k creates smooth boundaries (potential underfitting)
</figcaption>
</figure>
</div>
</div>
</div>
<section id="selecting-k-with-cross-validation" class="level3">
<h3 class="anchored" data-anchor-id="selecting-k-with-cross-validation">Selecting k with Cross-Validation</h3>
<p>We choose k by evaluating classification accuracy across different values using cross-validation:</p>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Evaluate different k values</span></span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a>k_values <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="dv">1</span>, <span class="dv">50</span>, <span class="at">by =</span> <span class="dv">2</span>)</span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Simple holdout validation</span></span>
<span id="cb24-6"><a href="#cb24-6" aria-hidden="true" tabindex="-1"></a>test_idx <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="dv">1</span><span class="sc">:</span><span class="fu">nrow</span>(train_data), <span class="dv">30</span>)</span>
<span id="cb24-7"><a href="#cb24-7" aria-hidden="true" tabindex="-1"></a>train_subset <span class="ot">&lt;-</span> train_data[<span class="sc">-</span>test_idx, ]</span>
<span id="cb24-8"><a href="#cb24-8" aria-hidden="true" tabindex="-1"></a>test_subset <span class="ot">&lt;-</span> train_data[test_idx, ]</span>
<span id="cb24-9"><a href="#cb24-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-10"><a href="#cb24-10" aria-hidden="true" tabindex="-1"></a>accuracy <span class="ot">&lt;-</span> <span class="fu">sapply</span>(k_values, <span class="cf">function</span>(k) {</span>
<span id="cb24-11"><a href="#cb24-11" aria-hidden="true" tabindex="-1"></a>  pred <span class="ot">&lt;-</span> <span class="fu">knn</span>(<span class="at">train =</span> train_subset[, <span class="dv">1</span><span class="sc">:</span><span class="dv">2</span>],</span>
<span id="cb24-12"><a href="#cb24-12" aria-hidden="true" tabindex="-1"></a>              <span class="at">test =</span> test_subset[, <span class="dv">1</span><span class="sc">:</span><span class="dv">2</span>],</span>
<span id="cb24-13"><a href="#cb24-13" aria-hidden="true" tabindex="-1"></a>              <span class="at">cl =</span> train_subset<span class="sc">$</span>class,</span>
<span id="cb24-14"><a href="#cb24-14" aria-hidden="true" tabindex="-1"></a>              <span class="at">k =</span> k)</span>
<span id="cb24-15"><a href="#cb24-15" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mean</span>(pred <span class="sc">==</span> test_subset<span class="sc">$</span>class)</span>
<span id="cb24-16"><a href="#cb24-16" aria-hidden="true" tabindex="-1"></a>})</span>
<span id="cb24-17"><a href="#cb24-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-18"><a href="#cb24-18" aria-hidden="true" tabindex="-1"></a>train_accuracy <span class="ot">&lt;-</span> <span class="fu">sapply</span>(k_values, <span class="cf">function</span>(k) {</span>
<span id="cb24-19"><a href="#cb24-19" aria-hidden="true" tabindex="-1"></a>  pred <span class="ot">&lt;-</span> <span class="fu">knn</span>(<span class="at">train =</span> train_subset[, <span class="dv">1</span><span class="sc">:</span><span class="dv">2</span>],</span>
<span id="cb24-20"><a href="#cb24-20" aria-hidden="true" tabindex="-1"></a>              <span class="at">test =</span> train_subset[, <span class="dv">1</span><span class="sc">:</span><span class="dv">2</span>],</span>
<span id="cb24-21"><a href="#cb24-21" aria-hidden="true" tabindex="-1"></a>              <span class="at">cl =</span> train_subset<span class="sc">$</span>class,</span>
<span id="cb24-22"><a href="#cb24-22" aria-hidden="true" tabindex="-1"></a>              <span class="at">k =</span> k)</span>
<span id="cb24-23"><a href="#cb24-23" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mean</span>(pred <span class="sc">==</span> train_subset<span class="sc">$</span>class)</span>
<span id="cb24-24"><a href="#cb24-24" aria-hidden="true" tabindex="-1"></a>})</span>
<span id="cb24-25"><a href="#cb24-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-26"><a href="#cb24-26" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(k_values, train_accuracy, <span class="at">type =</span> <span class="st">"l"</span>, <span class="at">col =</span> <span class="st">"blue"</span>, <span class="at">lwd =</span> <span class="dv">2</span>,</span>
<span id="cb24-27"><a href="#cb24-27" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">"k (number of neighbors)"</span>, <span class="at">ylab =</span> <span class="st">"Accuracy"</span>,</span>
<span id="cb24-28"><a href="#cb24-28" aria-hidden="true" tabindex="-1"></a>     <span class="at">main =</span> <span class="st">"Training vs Test Accuracy"</span>, <span class="at">ylim =</span> <span class="fu">c</span>(<span class="fl">0.5</span>, <span class="dv">1</span>))</span>
<span id="cb24-29"><a href="#cb24-29" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(k_values, accuracy, <span class="at">col =</span> <span class="st">"red"</span>, <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb24-30"><a href="#cb24-30" aria-hidden="true" tabindex="-1"></a><span class="fu">legend</span>(<span class="st">"bottomright"</span>, <span class="fu">c</span>(<span class="st">"Training"</span>, <span class="st">"Test"</span>), <span class="at">col =</span> <span class="fu">c</span>(<span class="st">"blue"</span>, <span class="st">"red"</span>), <span class="at">lwd =</span> <span class="dv">2</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div id="fig-knn-cv" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-knn-cv-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="22-statistical-learning_files/figure-html/fig-knn-cv-1.png" class="img-fluid figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-knn-cv-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;29.14: Cross-validation accuracy for different values of k: accuracy on training data decreases with k, but test accuracy peaks at intermediate values
</figcaption>
</figure>
</div>
</div>
</div>
<p>Notice that training accuracy is perfect (1.0) when k=1—each point is its own nearest neighbor. But test accuracy tells the true story of generalization performance.</p>
</section>
</section>
<section id="confusion-matrices" class="level2" data-number="29.12">
<h2 data-number="29.12" class="anchored" data-anchor-id="confusion-matrices"><span class="header-section-number">29.12</span> Confusion Matrices</h2>
<p>Classification performance is evaluated with a <strong>confusion matrix</strong>:</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th style="text-align: left;"></th>
<th style="text-align: center;">Predicted Positive</th>
<th style="text-align: center;">Predicted Negative</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Actual Positive</td>
<td style="text-align: center;">True Positive (TP)</td>
<td style="text-align: center;">False Negative (FN)</td>
</tr>
<tr class="even">
<td style="text-align: left;">Actual Negative</td>
<td style="text-align: center;">False Positive (FP)</td>
<td style="text-align: center;">True Negative (TN)</td>
</tr>
</tbody>
</table>
<p>Key metrics: - <strong>Accuracy</strong>: (TP + TN) / Total - <strong>Sensitivity</strong> (Recall): TP / (TP + FN) — how many positives were caught - <strong>Specificity</strong>: TN / (TN + FP) — how many negatives were correctly identified - <strong>Precision</strong>: TP / (TP + FP) — among positive predictions, how many were correct</p>
<section id="the-problem-with-accuracy" class="level3">
<h3 class="anchored" data-anchor-id="the-problem-with-accuracy">The Problem with Accuracy</h3>
<p>Accuracy can be misleading with <strong>imbalanced classes</strong>. If 95% of emails are legitimate, a classifier that labels everything as “not spam” achieves 95% accuracy while being completely useless for its intended purpose.</p>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Imbalanced class example</span></span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">42</span>)</span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a><span class="co"># 95% negative, 5% positive (e.g., rare disease screening)</span></span>
<span id="cb25-4"><a href="#cb25-4" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">1000</span></span>
<span id="cb25-5"><a href="#cb25-5" aria-hidden="true" tabindex="-1"></a>actual <span class="ot">&lt;-</span> <span class="fu">factor</span>(<span class="fu">c</span>(<span class="fu">rep</span>(<span class="st">"Negative"</span>, <span class="dv">950</span>), <span class="fu">rep</span>(<span class="st">"Positive"</span>, <span class="dv">50</span>)))</span>
<span id="cb25-6"><a href="#cb25-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-7"><a href="#cb25-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Naive classifier: always predict negative</span></span>
<span id="cb25-8"><a href="#cb25-8" aria-hidden="true" tabindex="-1"></a>naive_pred <span class="ot">&lt;-</span> <span class="fu">factor</span>(<span class="fu">rep</span>(<span class="st">"Negative"</span>, n), <span class="at">levels =</span> <span class="fu">c</span>(<span class="st">"Negative"</span>, <span class="st">"Positive"</span>))</span>
<span id="cb25-9"><a href="#cb25-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-10"><a href="#cb25-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate metrics</span></span>
<span id="cb25-11"><a href="#cb25-11" aria-hidden="true" tabindex="-1"></a>TP <span class="ot">&lt;-</span> <span class="fu">sum</span>(naive_pred <span class="sc">==</span> <span class="st">"Positive"</span> <span class="sc">&amp;</span> actual <span class="sc">==</span> <span class="st">"Positive"</span>)</span>
<span id="cb25-12"><a href="#cb25-12" aria-hidden="true" tabindex="-1"></a>TN <span class="ot">&lt;-</span> <span class="fu">sum</span>(naive_pred <span class="sc">==</span> <span class="st">"Negative"</span> <span class="sc">&amp;</span> actual <span class="sc">==</span> <span class="st">"Negative"</span>)</span>
<span id="cb25-13"><a href="#cb25-13" aria-hidden="true" tabindex="-1"></a>FP <span class="ot">&lt;-</span> <span class="fu">sum</span>(naive_pred <span class="sc">==</span> <span class="st">"Positive"</span> <span class="sc">&amp;</span> actual <span class="sc">==</span> <span class="st">"Negative"</span>)</span>
<span id="cb25-14"><a href="#cb25-14" aria-hidden="true" tabindex="-1"></a>FN <span class="ot">&lt;-</span> <span class="fu">sum</span>(naive_pred <span class="sc">==</span> <span class="st">"Negative"</span> <span class="sc">&amp;</span> actual <span class="sc">==</span> <span class="st">"Positive"</span>)</span>
<span id="cb25-15"><a href="#cb25-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-16"><a href="#cb25-16" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"Accuracy:"</span>, (TP <span class="sc">+</span> TN) <span class="sc">/</span> n, <span class="st">"</span><span class="sc">\n</span><span class="st">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Accuracy: 0.95 </code></pre>
</div>
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"Sensitivity (Recall):"</span>, TP <span class="sc">/</span> (TP <span class="sc">+</span> FN), <span class="st">"</span><span class="sc">\n</span><span class="st">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Sensitivity (Recall): 0 </code></pre>
</div>
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"The classifier catches 0% of positive cases!</span><span class="sc">\n</span><span class="st">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>The classifier catches 0% of positive cases!</code></pre>
</div>
</div>
</section>
<section id="f1-score-and-balanced-accuracy" class="level3">
<h3 class="anchored" data-anchor-id="f1-score-and-balanced-accuracy">F1 Score and Balanced Accuracy</h3>
<p>For imbalanced data, better metrics include:</p>
<p><strong>F1 Score</strong>: The harmonic mean of precision and recall, balancing both concerns:</p>
<p><span class="math display">\[F_1 = 2 \cdot \frac{\text{Precision} \cdot \text{Recall}}{\text{Precision} + \text{Recall}} = \frac{2 \cdot TP}{2 \cdot TP + FP + FN}\]</span></p>
<p><strong>Balanced Accuracy</strong>: The average of sensitivity and specificity:</p>
<p><span class="math display">\[\text{Balanced Accuracy} = \frac{\text{Sensitivity} + \text{Specificity}}{2}\]</span></p>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Better classifier for the imbalanced data</span></span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb31-3"><a href="#cb31-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Suppose we have a model that catches 80% of positives but has some false positives</span></span>
<span id="cb31-4"><a href="#cb31-4" aria-hidden="true" tabindex="-1"></a>better_pred <span class="ot">&lt;-</span> actual  <span class="co"># Start with actual</span></span>
<span id="cb31-5"><a href="#cb31-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Correctly identify 80% of positives</span></span>
<span id="cb31-6"><a href="#cb31-6" aria-hidden="true" tabindex="-1"></a>pos_idx <span class="ot">&lt;-</span> <span class="fu">which</span>(actual <span class="sc">==</span> <span class="st">"Positive"</span>)</span>
<span id="cb31-7"><a href="#cb31-7" aria-hidden="true" tabindex="-1"></a>neg_idx <span class="ot">&lt;-</span> <span class="fu">which</span>(actual <span class="sc">==</span> <span class="st">"Negative"</span>)</span>
<span id="cb31-8"><a href="#cb31-8" aria-hidden="true" tabindex="-1"></a>better_pred[<span class="fu">sample</span>(pos_idx, <span class="dv">10</span>)] <span class="ot">&lt;-</span> <span class="st">"Negative"</span>  <span class="co"># Miss 10 of 50 positives (20%)</span></span>
<span id="cb31-9"><a href="#cb31-9" aria-hidden="true" tabindex="-1"></a>better_pred[<span class="fu">sample</span>(neg_idx, <span class="dv">50</span>)] <span class="ot">&lt;-</span> <span class="st">"Positive"</span>  <span class="co"># 50 false positives</span></span>
<span id="cb31-10"><a href="#cb31-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-11"><a href="#cb31-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Confusion matrix</span></span>
<span id="cb31-12"><a href="#cb31-12" aria-hidden="true" tabindex="-1"></a>TP <span class="ot">&lt;-</span> <span class="fu">sum</span>(better_pred <span class="sc">==</span> <span class="st">"Positive"</span> <span class="sc">&amp;</span> actual <span class="sc">==</span> <span class="st">"Positive"</span>)</span>
<span id="cb31-13"><a href="#cb31-13" aria-hidden="true" tabindex="-1"></a>TN <span class="ot">&lt;-</span> <span class="fu">sum</span>(better_pred <span class="sc">==</span> <span class="st">"Negative"</span> <span class="sc">&amp;</span> actual <span class="sc">==</span> <span class="st">"Negative"</span>)</span>
<span id="cb31-14"><a href="#cb31-14" aria-hidden="true" tabindex="-1"></a>FP <span class="ot">&lt;-</span> <span class="fu">sum</span>(better_pred <span class="sc">==</span> <span class="st">"Positive"</span> <span class="sc">&amp;</span> actual <span class="sc">==</span> <span class="st">"Negative"</span>)</span>
<span id="cb31-15"><a href="#cb31-15" aria-hidden="true" tabindex="-1"></a>FN <span class="ot">&lt;-</span> <span class="fu">sum</span>(better_pred <span class="sc">==</span> <span class="st">"Negative"</span> <span class="sc">&amp;</span> actual <span class="sc">==</span> <span class="st">"Positive"</span>)</span>
<span id="cb31-16"><a href="#cb31-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-17"><a href="#cb31-17" aria-hidden="true" tabindex="-1"></a>precision <span class="ot">&lt;-</span> TP <span class="sc">/</span> (TP <span class="sc">+</span> FP)</span>
<span id="cb31-18"><a href="#cb31-18" aria-hidden="true" tabindex="-1"></a>recall <span class="ot">&lt;-</span> TP <span class="sc">/</span> (TP <span class="sc">+</span> FN)  <span class="co"># Sensitivity</span></span>
<span id="cb31-19"><a href="#cb31-19" aria-hidden="true" tabindex="-1"></a>specificity <span class="ot">&lt;-</span> TN <span class="sc">/</span> (TN <span class="sc">+</span> FP)</span>
<span id="cb31-20"><a href="#cb31-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-21"><a href="#cb31-21" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate metrics</span></span>
<span id="cb31-22"><a href="#cb31-22" aria-hidden="true" tabindex="-1"></a>accuracy <span class="ot">&lt;-</span> (TP <span class="sc">+</span> TN) <span class="sc">/</span> n</span>
<span id="cb31-23"><a href="#cb31-23" aria-hidden="true" tabindex="-1"></a>f1 <span class="ot">&lt;-</span> <span class="dv">2</span> <span class="sc">*</span> precision <span class="sc">*</span> recall <span class="sc">/</span> (precision <span class="sc">+</span> recall)</span>
<span id="cb31-24"><a href="#cb31-24" aria-hidden="true" tabindex="-1"></a>balanced_acc <span class="ot">&lt;-</span> (recall <span class="sc">+</span> specificity) <span class="sc">/</span> <span class="dv">2</span></span>
<span id="cb31-25"><a href="#cb31-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-26"><a href="#cb31-26" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"Accuracy:"</span>, <span class="fu">round</span>(accuracy, <span class="dv">3</span>), <span class="st">"</span><span class="sc">\n</span><span class="st">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Accuracy: 0.94 </code></pre>
</div>
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb33"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"Precision:"</span>, <span class="fu">round</span>(precision, <span class="dv">3</span>), <span class="st">"</span><span class="sc">\n</span><span class="st">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Precision: 0.444 </code></pre>
</div>
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb35"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"Recall (Sensitivity):"</span>, <span class="fu">round</span>(recall, <span class="dv">3</span>), <span class="st">"</span><span class="sc">\n</span><span class="st">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Recall (Sensitivity): 0.8 </code></pre>
</div>
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb37"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"F1 Score:"</span>, <span class="fu">round</span>(f1, <span class="dv">3</span>), <span class="st">"</span><span class="sc">\n</span><span class="st">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>F1 Score: 0.571 </code></pre>
</div>
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb39"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"Balanced Accuracy:"</span>, <span class="fu">round</span>(balanced_acc, <span class="dv">3</span>), <span class="st">"</span><span class="sc">\n</span><span class="st">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Balanced Accuracy: 0.874 </code></pre>
</div>
</div>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Which Metric to Use?
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><strong>Accuracy</strong>: Only when classes are balanced</li>
<li><strong>F1 Score</strong>: When you care about both precision and recall equally</li>
<li><strong>Sensitivity/Recall</strong>: When missing positives is costly (disease screening)</li>
<li><strong>Precision</strong>: When false positives are costly (spam filtering)</li>
<li><strong>Balanced Accuracy</strong>: Quick summary for imbalanced data</li>
</ul>
</div>
</div>
</section>
</section>
<section id="roc-curves-and-auc" class="level2" data-number="29.13">
<h2 data-number="29.13" class="anchored" data-anchor-id="roc-curves-and-auc"><span class="header-section-number">29.13</span> ROC Curves and AUC</h2>
<p>Many classifiers output probabilities rather than hard class labels. By varying the <strong>threshold</strong> for classifying as positive, we trade off sensitivity against specificity.</p>
<p>The <strong>Receiver Operating Characteristic (ROC) curve</strong> plots sensitivity (true positive rate) against 1 - specificity (false positive rate) at all possible thresholds.</p>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb41"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Simulate a classifier with probabilities</span></span>
<span id="cb41-2"><a href="#cb41-2" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">42</span>)</span>
<span id="cb41-3"><a href="#cb41-3" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">500</span></span>
<span id="cb41-4"><a href="#cb41-4" aria-hidden="true" tabindex="-1"></a>actual <span class="ot">&lt;-</span> <span class="fu">factor</span>(<span class="fu">c</span>(<span class="fu">rep</span>(<span class="dv">1</span>, <span class="dv">100</span>), <span class="fu">rep</span>(<span class="dv">0</span>, <span class="dv">400</span>)))  <span class="co"># 20% positive</span></span>
<span id="cb41-5"><a href="#cb41-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-6"><a href="#cb41-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate predicted probabilities (imperfect classifier)</span></span>
<span id="cb41-7"><a href="#cb41-7" aria-hidden="true" tabindex="-1"></a>probs <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fu">rbeta</span>(<span class="dv">100</span>, <span class="dv">3</span>, <span class="dv">2</span>),   <span class="co"># Positives: higher probs</span></span>
<span id="cb41-8"><a href="#cb41-8" aria-hidden="true" tabindex="-1"></a>           <span class="fu">rbeta</span>(<span class="dv">400</span>, <span class="dv">2</span>, <span class="dv">3</span>))   <span class="co"># Negatives: lower probs</span></span>
<span id="cb41-9"><a href="#cb41-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-10"><a href="#cb41-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate ROC curve manually</span></span>
<span id="cb41-11"><a href="#cb41-11" aria-hidden="true" tabindex="-1"></a>thresholds <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="at">by =</span> <span class="fl">0.01</span>)</span>
<span id="cb41-12"><a href="#cb41-12" aria-hidden="true" tabindex="-1"></a>roc_data <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(</span>
<span id="cb41-13"><a href="#cb41-13" aria-hidden="true" tabindex="-1"></a>  <span class="at">threshold =</span> thresholds,</span>
<span id="cb41-14"><a href="#cb41-14" aria-hidden="true" tabindex="-1"></a>  <span class="at">TPR =</span> <span class="fu">sapply</span>(thresholds, <span class="cf">function</span>(t) {</span>
<span id="cb41-15"><a href="#cb41-15" aria-hidden="true" tabindex="-1"></a>    pred <span class="ot">&lt;-</span> <span class="fu">ifelse</span>(probs <span class="sc">&gt;=</span> t, <span class="dv">1</span>, <span class="dv">0</span>)</span>
<span id="cb41-16"><a href="#cb41-16" aria-hidden="true" tabindex="-1"></a>    <span class="fu">sum</span>(pred <span class="sc">==</span> <span class="dv">1</span> <span class="sc">&amp;</span> actual <span class="sc">==</span> <span class="dv">1</span>) <span class="sc">/</span> <span class="fu">sum</span>(actual <span class="sc">==</span> <span class="dv">1</span>)</span>
<span id="cb41-17"><a href="#cb41-17" aria-hidden="true" tabindex="-1"></a>  }),</span>
<span id="cb41-18"><a href="#cb41-18" aria-hidden="true" tabindex="-1"></a>  <span class="at">FPR =</span> <span class="fu">sapply</span>(thresholds, <span class="cf">function</span>(t) {</span>
<span id="cb41-19"><a href="#cb41-19" aria-hidden="true" tabindex="-1"></a>    pred <span class="ot">&lt;-</span> <span class="fu">ifelse</span>(probs <span class="sc">&gt;=</span> t, <span class="dv">1</span>, <span class="dv">0</span>)</span>
<span id="cb41-20"><a href="#cb41-20" aria-hidden="true" tabindex="-1"></a>    <span class="fu">sum</span>(pred <span class="sc">==</span> <span class="dv">1</span> <span class="sc">&amp;</span> actual <span class="sc">==</span> <span class="dv">0</span>) <span class="sc">/</span> <span class="fu">sum</span>(actual <span class="sc">==</span> <span class="dv">0</span>)</span>
<span id="cb41-21"><a href="#cb41-21" aria-hidden="true" tabindex="-1"></a>  })</span>
<span id="cb41-22"><a href="#cb41-22" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb41-23"><a href="#cb41-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-24"><a href="#cb41-24" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot ROC curve</span></span>
<span id="cb41-25"><a href="#cb41-25" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(roc_data<span class="sc">$</span>FPR, roc_data<span class="sc">$</span>TPR, <span class="at">type =</span> <span class="st">"l"</span>, <span class="at">lwd =</span> <span class="dv">2</span>, <span class="at">col =</span> <span class="st">"blue"</span>,</span>
<span id="cb41-26"><a href="#cb41-26" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">"False Positive Rate (1 - Specificity)"</span>,</span>
<span id="cb41-27"><a href="#cb41-27" aria-hidden="true" tabindex="-1"></a>     <span class="at">ylab =</span> <span class="st">"True Positive Rate (Sensitivity)"</span>,</span>
<span id="cb41-28"><a href="#cb41-28" aria-hidden="true" tabindex="-1"></a>     <span class="at">main =</span> <span class="st">"ROC Curve"</span>)</span>
<span id="cb41-29"><a href="#cb41-29" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="at">lty =</span> <span class="dv">2</span>, <span class="at">col =</span> <span class="st">"gray"</span>)  <span class="co"># Random classifier line</span></span>
<span id="cb41-30"><a href="#cb41-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-31"><a href="#cb41-31" aria-hidden="true" tabindex="-1"></a><span class="co"># Add points for specific thresholds</span></span>
<span id="cb41-32"><a href="#cb41-32" aria-hidden="true" tabindex="-1"></a>highlight <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fl">0.3</span>, <span class="fl">0.5</span>, <span class="fl">0.7</span>)</span>
<span id="cb41-33"><a href="#cb41-33" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (t <span class="cf">in</span> highlight) {</span>
<span id="cb41-34"><a href="#cb41-34" aria-hidden="true" tabindex="-1"></a>  idx <span class="ot">&lt;-</span> <span class="fu">which.min</span>(<span class="fu">abs</span>(roc_data<span class="sc">$</span>threshold <span class="sc">-</span> t))</span>
<span id="cb41-35"><a href="#cb41-35" aria-hidden="true" tabindex="-1"></a>  <span class="fu">points</span>(roc_data<span class="sc">$</span>FPR[idx], roc_data<span class="sc">$</span>TPR[idx], <span class="at">pch =</span> <span class="dv">19</span>, <span class="at">cex =</span> <span class="fl">1.5</span>)</span>
<span id="cb41-36"><a href="#cb41-36" aria-hidden="true" tabindex="-1"></a>  <span class="fu">text</span>(roc_data<span class="sc">$</span>FPR[idx] <span class="sc">+</span> <span class="fl">0.05</span>, roc_data<span class="sc">$</span>TPR[idx],</span>
<span id="cb41-37"><a href="#cb41-37" aria-hidden="true" tabindex="-1"></a>       <span class="fu">paste</span>(<span class="st">"t ="</span>, t), <span class="at">cex =</span> <span class="fl">0.8</span>)</span>
<span id="cb41-38"><a href="#cb41-38" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb41-39"><a href="#cb41-39" aria-hidden="true" tabindex="-1"></a><span class="fu">legend</span>(<span class="st">"bottomright"</span>, <span class="fu">c</span>(<span class="st">"ROC Curve"</span>, <span class="st">"Random Classifier"</span>),</span>
<span id="cb41-40"><a href="#cb41-40" aria-hidden="true" tabindex="-1"></a>       <span class="at">col =</span> <span class="fu">c</span>(<span class="st">"blue"</span>, <span class="st">"gray"</span>), <span class="at">lty =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">2</span>), <span class="at">lwd =</span> <span class="fu">c</span>(<span class="dv">2</span>, <span class="dv">1</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div id="fig-roc-curve" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-roc-curve-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="22-statistical-learning_files/figure-html/fig-roc-curve-1.png" class="img-fluid figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-roc-curve-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;29.15: ROC curve showing the tradeoff between sensitivity and false positive rate; the dashed diagonal represents random guessing
</figcaption>
</figure>
</div>
</div>
</div>
<section id="area-under-the-curve-auc" class="level3">
<h3 class="anchored" data-anchor-id="area-under-the-curve-auc">Area Under the Curve (AUC)</h3>
<p>The <strong>AUC</strong> (Area Under the ROC Curve) summarizes classifier performance in a single number:</p>
<ul>
<li><strong>AUC = 0.5</strong>: No better than random guessing</li>
<li><strong>AUC = 1.0</strong>: Perfect classification</li>
<li><strong>AUC &gt; 0.8</strong>: Generally considered good</li>
</ul>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb42"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate AUC using trapezoidal rule</span></span>
<span id="cb42-2"><a href="#cb42-2" aria-hidden="true" tabindex="-1"></a>auc <span class="ot">&lt;-</span> <span class="fu">sum</span>(<span class="fu">diff</span>(roc_data<span class="sc">$</span>FPR[<span class="fu">order</span>(roc_data<span class="sc">$</span>FPR)]) <span class="sc">*</span></span>
<span id="cb42-3"><a href="#cb42-3" aria-hidden="true" tabindex="-1"></a>           (<span class="fu">head</span>(roc_data<span class="sc">$</span>TPR[<span class="fu">order</span>(roc_data<span class="sc">$</span>FPR)], <span class="sc">-</span><span class="dv">1</span>) <span class="sc">+</span></span>
<span id="cb42-4"><a href="#cb42-4" aria-hidden="true" tabindex="-1"></a>            <span class="fu">tail</span>(roc_data<span class="sc">$</span>TPR[<span class="fu">order</span>(roc_data<span class="sc">$</span>FPR)], <span class="sc">-</span><span class="dv">1</span>)) <span class="sc">/</span> <span class="dv">2</span>)</span>
<span id="cb42-5"><a href="#cb42-5" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"AUC:"</span>, <span class="fu">round</span>(<span class="fu">abs</span>(auc), <span class="dv">3</span>), <span class="st">"</span><span class="sc">\n</span><span class="st">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>AUC: 0.765 </code></pre>
</div>
</div>
</section>
<section id="comparing-classifiers-with-roc" class="level3">
<h3 class="anchored" data-anchor-id="comparing-classifiers-with-roc">Comparing Classifiers with ROC</h3>
<p>ROC curves allow direct comparison of classifiers:</p>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb44"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb44-1"><a href="#cb44-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Simulate three classifiers of varying quality</span></span>
<span id="cb44-2"><a href="#cb44-2" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">42</span>)</span>
<span id="cb44-3"><a href="#cb44-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-4"><a href="#cb44-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Good classifier</span></span>
<span id="cb44-5"><a href="#cb44-5" aria-hidden="true" tabindex="-1"></a>probs_good <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fu">rbeta</span>(<span class="dv">100</span>, <span class="dv">4</span>, <span class="fl">1.5</span>), <span class="fu">rbeta</span>(<span class="dv">400</span>, <span class="fl">1.5</span>, <span class="dv">4</span>))</span>
<span id="cb44-6"><a href="#cb44-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-7"><a href="#cb44-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Medium classifier (our original)</span></span>
<span id="cb44-8"><a href="#cb44-8" aria-hidden="true" tabindex="-1"></a>probs_medium <span class="ot">&lt;-</span> probs</span>
<span id="cb44-9"><a href="#cb44-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-10"><a href="#cb44-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Poor classifier</span></span>
<span id="cb44-11"><a href="#cb44-11" aria-hidden="true" tabindex="-1"></a>probs_poor <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fu">rbeta</span>(<span class="dv">100</span>, <span class="dv">2</span>, <span class="dv">2</span>), <span class="fu">rbeta</span>(<span class="dv">400</span>, <span class="dv">2</span>, <span class="dv">2</span>))</span>
<span id="cb44-12"><a href="#cb44-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-13"><a href="#cb44-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Function to calculate ROC data</span></span>
<span id="cb44-14"><a href="#cb44-14" aria-hidden="true" tabindex="-1"></a>calc_roc <span class="ot">&lt;-</span> <span class="cf">function</span>(probs, actual) {</span>
<span id="cb44-15"><a href="#cb44-15" aria-hidden="true" tabindex="-1"></a>  thresholds <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="at">by =</span> <span class="fl">0.01</span>)</span>
<span id="cb44-16"><a href="#cb44-16" aria-hidden="true" tabindex="-1"></a>  <span class="fu">data.frame</span>(</span>
<span id="cb44-17"><a href="#cb44-17" aria-hidden="true" tabindex="-1"></a>    <span class="at">TPR =</span> <span class="fu">sapply</span>(thresholds, <span class="cf">function</span>(t) {</span>
<span id="cb44-18"><a href="#cb44-18" aria-hidden="true" tabindex="-1"></a>      pred <span class="ot">&lt;-</span> <span class="fu">ifelse</span>(probs <span class="sc">&gt;=</span> t, <span class="dv">1</span>, <span class="dv">0</span>)</span>
<span id="cb44-19"><a href="#cb44-19" aria-hidden="true" tabindex="-1"></a>      <span class="fu">sum</span>(pred <span class="sc">==</span> <span class="dv">1</span> <span class="sc">&amp;</span> actual <span class="sc">==</span> <span class="dv">1</span>) <span class="sc">/</span> <span class="fu">sum</span>(actual <span class="sc">==</span> <span class="dv">1</span>)</span>
<span id="cb44-20"><a href="#cb44-20" aria-hidden="true" tabindex="-1"></a>    }),</span>
<span id="cb44-21"><a href="#cb44-21" aria-hidden="true" tabindex="-1"></a>    <span class="at">FPR =</span> <span class="fu">sapply</span>(thresholds, <span class="cf">function</span>(t) {</span>
<span id="cb44-22"><a href="#cb44-22" aria-hidden="true" tabindex="-1"></a>      pred <span class="ot">&lt;-</span> <span class="fu">ifelse</span>(probs <span class="sc">&gt;=</span> t, <span class="dv">1</span>, <span class="dv">0</span>)</span>
<span id="cb44-23"><a href="#cb44-23" aria-hidden="true" tabindex="-1"></a>      <span class="fu">sum</span>(pred <span class="sc">==</span> <span class="dv">1</span> <span class="sc">&amp;</span> actual <span class="sc">==</span> <span class="dv">0</span>) <span class="sc">/</span> <span class="fu">sum</span>(actual <span class="sc">==</span> <span class="dv">0</span>)</span>
<span id="cb44-24"><a href="#cb44-24" aria-hidden="true" tabindex="-1"></a>    })</span>
<span id="cb44-25"><a href="#cb44-25" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb44-26"><a href="#cb44-26" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb44-27"><a href="#cb44-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-28"><a href="#cb44-28" aria-hidden="true" tabindex="-1"></a>roc_good <span class="ot">&lt;-</span> <span class="fu">calc_roc</span>(probs_good, actual)</span>
<span id="cb44-29"><a href="#cb44-29" aria-hidden="true" tabindex="-1"></a>roc_medium <span class="ot">&lt;-</span> <span class="fu">calc_roc</span>(probs_medium, actual)</span>
<span id="cb44-30"><a href="#cb44-30" aria-hidden="true" tabindex="-1"></a>roc_poor <span class="ot">&lt;-</span> <span class="fu">calc_roc</span>(probs_poor, actual)</span>
<span id="cb44-31"><a href="#cb44-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-32"><a href="#cb44-32" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot comparison</span></span>
<span id="cb44-33"><a href="#cb44-33" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(roc_good<span class="sc">$</span>FPR, roc_good<span class="sc">$</span>TPR, <span class="at">type =</span> <span class="st">"l"</span>, <span class="at">lwd =</span> <span class="dv">2</span>, <span class="at">col =</span> <span class="st">"darkgreen"</span>,</span>
<span id="cb44-34"><a href="#cb44-34" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">"False Positive Rate"</span>, <span class="at">ylab =</span> <span class="st">"True Positive Rate"</span>,</span>
<span id="cb44-35"><a href="#cb44-35" aria-hidden="true" tabindex="-1"></a>     <span class="at">main =</span> <span class="st">"ROC Curve Comparison"</span>)</span>
<span id="cb44-36"><a href="#cb44-36" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(roc_medium<span class="sc">$</span>FPR, roc_medium<span class="sc">$</span>TPR, <span class="at">lwd =</span> <span class="dv">2</span>, <span class="at">col =</span> <span class="st">"blue"</span>)</span>
<span id="cb44-37"><a href="#cb44-37" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(roc_poor<span class="sc">$</span>FPR, roc_poor<span class="sc">$</span>TPR, <span class="at">lwd =</span> <span class="dv">2</span>, <span class="at">col =</span> <span class="st">"red"</span>)</span>
<span id="cb44-38"><a href="#cb44-38" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="at">lty =</span> <span class="dv">2</span>, <span class="at">col =</span> <span class="st">"gray"</span>)</span>
<span id="cb44-39"><a href="#cb44-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-40"><a href="#cb44-40" aria-hidden="true" tabindex="-1"></a><span class="fu">legend</span>(<span class="st">"bottomright"</span>,</span>
<span id="cb44-41"><a href="#cb44-41" aria-hidden="true" tabindex="-1"></a>       <span class="fu">c</span>(<span class="st">"Good (AUC ≈ 0.90)"</span>, <span class="st">"Medium (AUC ≈ 0.75)"</span>, <span class="st">"Poor (AUC ≈ 0.50)"</span>),</span>
<span id="cb44-42"><a href="#cb44-42" aria-hidden="true" tabindex="-1"></a>       <span class="at">col =</span> <span class="fu">c</span>(<span class="st">"darkgreen"</span>, <span class="st">"blue"</span>, <span class="st">"red"</span>), <span class="at">lwd =</span> <span class="dv">2</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div id="fig-roc-comparison" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-roc-comparison-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="22-statistical-learning_files/figure-html/fig-roc-comparison-1.png" class="img-fluid figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-roc-comparison-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;29.16: Comparing classifiers using ROC curves: higher curves (larger AUC) indicate better performance
</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="precision-recall-curves" class="level3">
<h3 class="anchored" data-anchor-id="precision-recall-curves">Precision-Recall Curves</h3>
<p>For highly imbalanced data, <strong>precision-recall curves</strong> can be more informative than ROC curves because they focus on the minority (positive) class:</p>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb45"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb45-1"><a href="#cb45-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate precision-recall curve</span></span>
<span id="cb45-2"><a href="#cb45-2" aria-hidden="true" tabindex="-1"></a>pr_data <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(</span>
<span id="cb45-3"><a href="#cb45-3" aria-hidden="true" tabindex="-1"></a>  <span class="at">threshold =</span> thresholds,</span>
<span id="cb45-4"><a href="#cb45-4" aria-hidden="true" tabindex="-1"></a>  <span class="at">precision =</span> <span class="fu">sapply</span>(thresholds, <span class="cf">function</span>(t) {</span>
<span id="cb45-5"><a href="#cb45-5" aria-hidden="true" tabindex="-1"></a>    pred <span class="ot">&lt;-</span> <span class="fu">ifelse</span>(probs <span class="sc">&gt;=</span> t, <span class="dv">1</span>, <span class="dv">0</span>)</span>
<span id="cb45-6"><a href="#cb45-6" aria-hidden="true" tabindex="-1"></a>    tp <span class="ot">&lt;-</span> <span class="fu">sum</span>(pred <span class="sc">==</span> <span class="dv">1</span> <span class="sc">&amp;</span> actual <span class="sc">==</span> <span class="dv">1</span>)</span>
<span id="cb45-7"><a href="#cb45-7" aria-hidden="true" tabindex="-1"></a>    fp <span class="ot">&lt;-</span> <span class="fu">sum</span>(pred <span class="sc">==</span> <span class="dv">1</span> <span class="sc">&amp;</span> actual <span class="sc">==</span> <span class="dv">0</span>)</span>
<span id="cb45-8"><a href="#cb45-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> (tp <span class="sc">+</span> fp <span class="sc">==</span> <span class="dv">0</span>) <span class="fu">return</span>(<span class="cn">NA</span>)</span>
<span id="cb45-9"><a href="#cb45-9" aria-hidden="true" tabindex="-1"></a>    tp <span class="sc">/</span> (tp <span class="sc">+</span> fp)</span>
<span id="cb45-10"><a href="#cb45-10" aria-hidden="true" tabindex="-1"></a>  }),</span>
<span id="cb45-11"><a href="#cb45-11" aria-hidden="true" tabindex="-1"></a>  <span class="at">recall =</span> <span class="fu">sapply</span>(thresholds, <span class="cf">function</span>(t) {</span>
<span id="cb45-12"><a href="#cb45-12" aria-hidden="true" tabindex="-1"></a>    pred <span class="ot">&lt;-</span> <span class="fu">ifelse</span>(probs <span class="sc">&gt;=</span> t, <span class="dv">1</span>, <span class="dv">0</span>)</span>
<span id="cb45-13"><a href="#cb45-13" aria-hidden="true" tabindex="-1"></a>    <span class="fu">sum</span>(pred <span class="sc">==</span> <span class="dv">1</span> <span class="sc">&amp;</span> actual <span class="sc">==</span> <span class="dv">1</span>) <span class="sc">/</span> <span class="fu">sum</span>(actual <span class="sc">==</span> <span class="dv">1</span>)</span>
<span id="cb45-14"><a href="#cb45-14" aria-hidden="true" tabindex="-1"></a>  })</span>
<span id="cb45-15"><a href="#cb45-15" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb45-16"><a href="#cb45-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-17"><a href="#cb45-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Remove NA values</span></span>
<span id="cb45-18"><a href="#cb45-18" aria-hidden="true" tabindex="-1"></a>pr_data <span class="ot">&lt;-</span> pr_data[<span class="sc">!</span><span class="fu">is.na</span>(pr_data<span class="sc">$</span>precision), ]</span>
<span id="cb45-19"><a href="#cb45-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-20"><a href="#cb45-20" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(pr_data<span class="sc">$</span>recall, pr_data<span class="sc">$</span>precision, <span class="at">type =</span> <span class="st">"l"</span>, <span class="at">lwd =</span> <span class="dv">2</span>, <span class="at">col =</span> <span class="st">"purple"</span>,</span>
<span id="cb45-21"><a href="#cb45-21" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">"Recall (Sensitivity)"</span>, <span class="at">ylab =</span> <span class="st">"Precision"</span>,</span>
<span id="cb45-22"><a href="#cb45-22" aria-hidden="true" tabindex="-1"></a>     <span class="at">main =</span> <span class="st">"Precision-Recall Curve"</span>, <span class="at">ylim =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">1</span>))</span>
<span id="cb45-23"><a href="#cb45-23" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">h =</span> <span class="fu">mean</span>(actual <span class="sc">==</span> <span class="dv">1</span>), <span class="at">lty =</span> <span class="dv">2</span>, <span class="at">col =</span> <span class="st">"gray"</span>)  <span class="co"># Baseline</span></span>
<span id="cb45-24"><a href="#cb45-24" aria-hidden="true" tabindex="-1"></a><span class="fu">legend</span>(<span class="st">"topright"</span>, <span class="fu">c</span>(<span class="st">"PR Curve"</span>, <span class="st">"Baseline (random)"</span>),</span>
<span id="cb45-25"><a href="#cb45-25" aria-hidden="true" tabindex="-1"></a>       <span class="at">col =</span> <span class="fu">c</span>(<span class="st">"purple"</span>, <span class="st">"gray"</span>), <span class="at">lty =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">2</span>), <span class="at">lwd =</span> <span class="fu">c</span>(<span class="dv">2</span>, <span class="dv">1</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div id="fig-pr-curve" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-pr-curve-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="22-statistical-learning_files/figure-html/fig-pr-curve-1.png" class="img-fluid figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-pr-curve-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;29.17: Precision-recall curve for imbalanced classification; the horizontal dashed line shows baseline precision (proportion of positives)
</figcaption>
</figure>
</div>
</div>
</div>
</section>
</section>
<section id="the-curse-of-dimensionality" class="level2" data-number="29.14">
<h2 data-number="29.14" class="anchored" data-anchor-id="the-curse-of-dimensionality"><span class="header-section-number">29.14</span> The Curse of Dimensionality</h2>
<p>We described how methods such as LDA and QDA are not meant to be used with many predictors <span class="math inline">\(p\)</span> because the number of parameters that we need to estimate becomes too large. Kernel methods such as kNN or local regression do not have model parameters to estimate. However, they also face a challenge when multiple predictors are used due to what is referred to as the <strong>curse of dimensionality</strong>. The <em>dimension</em> here refers to the fact that when we have <span class="math inline">\(p\)</span> predictors, the distance between two observations is computed in <span class="math inline">\(p\)</span>-dimensional space.</p>
<p>A useful way of understanding the curse of dimensionality is by considering how large we have to make a span/neighborhood/window to include a given percentage of the data. Remember that with larger neighborhoods, our methods lose flexibility.</p>
<p>For example, suppose we have one continuous predictor with equally spaced points in the [0,1] interval and we want to create windows that include 1/10th of data. Then it’s easy to see that our windows have to be of size 0.1.</p>
<p>Now, for two predictors, if we decide to keep the neighborhood just as small (10% for each dimension), we include only 1 point. If we want to include 10% of the data, then we need to increase the size of each side of the square to <span class="math inline">\(\sqrt{.10} \approx .316\)</span>.</p>
<p>Using the same logic, if we want to include 10% of the data in a three-dimensional space, then the side of each cube is <span class="math inline">\(\sqrt[3]{.10} \approx 0.464\)</span>. In general, to include 10% of the data in a case with <span class="math inline">\(p\)</span> dimensions, we need an interval with each side of size <span class="math inline">\(\sqrt[p]{.10}\)</span> of the total. This proportion gets close to 1 quickly, and if the proportion is 1 it means we include all the data and are no longer smoothing.</p>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb46"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb46-1"><a href="#cb46-1" aria-hidden="true" tabindex="-1"></a>p <span class="ot">&lt;-</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">100</span></span>
<span id="cb46-2"><a href="#cb46-2" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(p, .<span class="dv">1</span><span class="sc">^</span>(<span class="dv">1</span><span class="sc">/</span>p), <span class="at">type =</span> <span class="st">"l"</span>, <span class="at">lwd =</span> <span class="dv">2</span>, <span class="at">col =</span> <span class="st">"steelblue"</span>,</span>
<span id="cb46-3"><a href="#cb46-3" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">"Number of Dimensions (p)"</span>,</span>
<span id="cb46-4"><a href="#cb46-4" aria-hidden="true" tabindex="-1"></a>     <span class="at">ylab =</span> <span class="st">"Side Length to Include 10% of Data"</span>,</span>
<span id="cb46-5"><a href="#cb46-5" aria-hidden="true" tabindex="-1"></a>     <span class="at">main =</span> <span class="st">"The Curse of Dimensionality"</span>,</span>
<span id="cb46-6"><a href="#cb46-6" aria-hidden="true" tabindex="-1"></a>     <span class="at">ylim =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">1</span>))</span>
<span id="cb46-7"><a href="#cb46-7" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">h =</span> <span class="dv">1</span>, <span class="at">lty =</span> <span class="dv">2</span>, <span class="at">col =</span> <span class="st">"gray50"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div id="fig-curse-dimensionality" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-curse-dimensionality-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="22-statistical-learning_files/figure-html/fig-curse-dimensionality-1.png" class="img-fluid figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-curse-dimensionality-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;29.18: As dimensions increase, the neighborhood size needed to include a fixed proportion of data grows rapidly. By 100 dimensions, any ‘local’ neighborhood must span nearly the entire data range.
</figcaption>
</figure>
</div>
</div>
</div>
<p>By the time we reach 100 predictors, the neighborhood is no longer very local, as each side covers almost the entire dataset.</p>
<p>This motivates the use of methods that adapt to higher dimensions while still producing interpretable models. Decision trees and random forests are examples of such methods.</p>
</section>
<section id="decision-trees-cart" class="level2" data-number="29.15">
<h2 data-number="29.15" class="anchored" data-anchor-id="decision-trees-cart"><span class="header-section-number">29.15</span> Decision Trees (CART)</h2>
<p><strong>Classification and Regression Trees (CART)</strong> make predictions by recursively partitioning the feature space into regions. At each node, the algorithm asks a yes/no question about a single feature, splitting observations into two groups. The process continues until a stopping criterion is met.</p>
<section id="motivating-example-olive-oil-classification" class="level3">
<h3 class="anchored" data-anchor-id="motivating-example-olive-oil-classification">Motivating Example: Olive Oil Classification</h3>
<p>To motivate decision trees, consider a dataset that includes the breakdown of olive oil composition into 8 fatty acids:</p>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb47"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb47-1"><a href="#cb47-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(dslabs)</span>
<span id="cb47-2"><a href="#cb47-2" aria-hidden="true" tabindex="-1"></a><span class="fu">data</span>(<span class="st">"olive"</span>)</span>
<span id="cb47-3"><a href="#cb47-3" aria-hidden="true" tabindex="-1"></a><span class="fu">names</span>(olive)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code> [1] "region"      "area"        "palmitic"    "palmitoleic" "stearic"    
 [6] "oleic"       "linoleic"    "linolenic"   "arachidic"   "eicosenoic" </code></pre>
</div>
</div>
<p>We will try to predict the region of origin using the fatty acid composition values as predictors.</p>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb49"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb49-1"><a href="#cb49-1" aria-hidden="true" tabindex="-1"></a><span class="fu">table</span>(olive<span class="sc">$</span>region)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>
Northern Italy       Sardinia Southern Italy 
           151             98            323 </code></pre>
</div>
</div>
<p>We remove the <code>area</code> column because we won’t use it as a predictor.</p>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb51"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb51-1"><a href="#cb51-1" aria-hidden="true" tabindex="-1"></a>olive <span class="ot">&lt;-</span> <span class="fu">select</span>(olive, <span class="sc">-</span>area)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>If we examine the distribution of each predictor stratified by region, we see that eicosenoic is only present in Southern Italy and that linoleic separates Northern Italy from Sardinia:</p>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb52"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb52-1"><a href="#cb52-1" aria-hidden="true" tabindex="-1"></a>olive <span class="sc">%&gt;%</span></span>
<span id="cb52-2"><a href="#cb52-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">pivot_longer</span>(<span class="sc">-</span>region, <span class="at">names_to =</span> <span class="st">"fatty_acid"</span>, <span class="at">values_to =</span> <span class="st">"percentage"</span>) <span class="sc">%&gt;%</span></span>
<span id="cb52-3"><a href="#cb52-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(region, percentage, <span class="at">fill =</span> region)) <span class="sc">+</span></span>
<span id="cb52-4"><a href="#cb52-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_boxplot</span>() <span class="sc">+</span></span>
<span id="cb52-5"><a href="#cb52-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">facet_wrap</span>(<span class="sc">~</span>fatty_acid, <span class="at">scales =</span> <span class="st">"free"</span>, <span class="at">ncol =</span> <span class="dv">4</span>) <span class="sc">+</span></span>
<span id="cb52-6"><a href="#cb52-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">axis.text.x =</span> <span class="fu">element_blank</span>(), <span class="at">legend.position =</span> <span class="st">"bottom"</span>) <span class="sc">+</span></span>
<span id="cb52-7"><a href="#cb52-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">"Fatty Acid Composition by Region"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div id="fig-olive-eda" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-olive-eda-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="22-statistical-learning_files/figure-html/fig-olive-eda-1.png" class="img-fluid figure-img" width="960">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-olive-eda-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;29.19: Distribution of fatty acid composition by region. Some predictors like eicosenoic and linoleic clearly separate regions.
</figcaption>
</figure>
</div>
</div>
</div>
<p>This implies that we should be able to build an algorithm that predicts perfectly. We can see this clearly by plotting the values for eicosenoic and linoleic:</p>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb53"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb53-1"><a href="#cb53-1" aria-hidden="true" tabindex="-1"></a>olive <span class="sc">%&gt;%</span></span>
<span id="cb53-2"><a href="#cb53-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(eicosenoic, linoleic, <span class="at">color =</span> region)) <span class="sc">+</span></span>
<span id="cb53-3"><a href="#cb53-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>() <span class="sc">+</span></span>
<span id="cb53-4"><a href="#cb53-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_vline</span>(<span class="at">xintercept =</span> <span class="fl">0.065</span>, <span class="at">lty =</span> <span class="dv">2</span>) <span class="sc">+</span></span>
<span id="cb53-5"><a href="#cb53-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_segment</span>(<span class="at">x =</span> <span class="sc">-</span><span class="fl">0.2</span>, <span class="at">y =</span> <span class="fl">10.54</span>, <span class="at">xend =</span> <span class="fl">0.065</span>, <span class="at">yend =</span> <span class="fl">10.54</span>,</span>
<span id="cb53-6"><a href="#cb53-6" aria-hidden="true" tabindex="-1"></a>               <span class="at">color =</span> <span class="st">"black"</span>, <span class="at">lty =</span> <span class="dv">2</span>) <span class="sc">+</span></span>
<span id="cb53-7"><a href="#cb53-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">"Perfect Classification with Simple Rules"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div id="fig-olive-two-predictors" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-olive-two-predictors-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="22-statistical-learning_files/figure-html/fig-olive-two-predictors-1.png" class="img-fluid figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-olive-two-predictors-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;29.20: With just two predictors, we can draw decision boundaries that perfectly separate the regions
</figcaption>
</figure>
</div>
</div>
</div>
<p>By eye, we can construct a prediction rule that partitions the predictor space so that each partition contains outcomes of only one category:</p>
<ul>
<li>If eicosenoic &gt; 0.065, predict Southern Italy</li>
<li>If not, then if linoleic &gt; 10.535, predict Sardinia</li>
<li>Otherwise, predict Northern Italy</li>
</ul>
<p>This is exactly what a decision tree does—it learns these rules from data. A tree is basically a flow chart of yes/no questions. The general idea is to use data to create these trees with predictions at the ends, referred to as <strong>nodes</strong>.</p>
</section>
<section id="how-trees-work" class="level3">
<h3 class="anchored" data-anchor-id="how-trees-work">How Trees Work</h3>
<p>The key idea: find the split that best separates the data at each step.</p>
<p>Regression and decision trees operate by predicting an outcome variable <span class="math inline">\(Y\)</span> by partitioning the predictors. We partition the predictor space into <span class="math inline">\(J\)</span> non-overlapping regions, <span class="math inline">\(R_1, R_2, \ldots, R_J\)</span>, and then for any predictor <span class="math inline">\(x\)</span> that falls within region <span class="math inline">\(R_j\)</span>, we estimate <span class="math inline">\(f(x)\)</span> with the average (for regression) or majority class (for classification) of the training observations in that region.</p>
<p>Trees create partitions recursively. We start with one partition, the entire predictor space. After the first step we have two partitions. After the second step we split one of these partitions into two and have three partitions, then four, and so on.</p>
<p>Once we select a partition to split, we find a predictor <span class="math inline">\(j\)</span> and value <span class="math inline">\(s\)</span> that define two new partitions:</p>
<p><span class="math display">\[
R_1(j,s) = \{\mathbf{x} \mid x_j &lt; s\} \mbox{  and  } R_2(j,s) = \{\mathbf{x} \mid x_j \geq s\}
\]</span></p>
<p>But how do we pick <span class="math inline">\(j\)</span> and <span class="math inline">\(s\)</span>? We find the pair that minimizes our loss function.</p>
<p>For <strong>regression trees</strong>, we minimize the <strong>residual sum of squares (RSS)</strong>:</p>
<p><span class="math display">\[
\sum_{i:\, x_i \in R_1(j,s)} (y_i - \hat{y}_{R_1})^2 +
\sum_{i:\, x_i \in R_2(j,s)} (y_i - \hat{y}_{R_2})^2
\]</span></p>
<p>where <span class="math inline">\(\hat{y}_{R_1}\)</span> and <span class="math inline">\(\hat{y}_{R_2}\)</span> are the average outcomes in each region.</p>
<p>For <strong>classification trees</strong>, we use the <strong>Gini impurity</strong>:</p>
<p><span class="math display">\[
\mbox{Gini}(j) = \sum_{k=1}^K \hat{p}_{j,k}(1-\hat{p}_{j,k})
\]</span></p>
<p>where <span class="math inline">\(\hat{p}_{j,k}\)</span> is the proportion of observations in partition <span class="math inline">\(j\)</span> that are of class <span class="math inline">\(k\)</span>. A pure node (all one class) has Gini = 0.</p>
<p><strong>Entropy</strong> is a very similar quantity:</p>
<p><span class="math display">\[
\mbox{Entropy}(j) = -\sum_{k=1}^K \hat{p}_{j,k}\log(\hat{p}_{j,k})
\]</span></p>
<p>(with <span class="math inline">\(0 \times \log(0)\)</span> defined as 0). Both Gini and entropy are 0 for perfectly pure nodes and increase as the class distribution becomes more mixed.</p>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb54"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb54-1"><a href="#cb54-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(rpart)</span>
<span id="cb54-2"><a href="#cb54-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(rpart.plot)</span>
<span id="cb54-3"><a href="#cb54-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-4"><a href="#cb54-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Build a classification tree</span></span>
<span id="cb54-5"><a href="#cb54-5" aria-hidden="true" tabindex="-1"></a><span class="fu">data</span>(iris)</span>
<span id="cb54-6"><a href="#cb54-6" aria-hidden="true" tabindex="-1"></a>tree_class <span class="ot">&lt;-</span> <span class="fu">rpart</span>(Species <span class="sc">~</span> ., <span class="at">data =</span> iris, <span class="at">method =</span> <span class="st">"class"</span>)</span>
<span id="cb54-7"><a href="#cb54-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-8"><a href="#cb54-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualize with rpart.plot</span></span>
<span id="cb54-9"><a href="#cb54-9" aria-hidden="true" tabindex="-1"></a><span class="fu">rpart.plot</span>(tree_class, <span class="at">extra =</span> <span class="dv">104</span>, <span class="at">box.palette =</span> <span class="st">"RdYlGn"</span>,</span>
<span id="cb54-10"><a href="#cb54-10" aria-hidden="true" tabindex="-1"></a>           <span class="at">main =</span> <span class="st">"Classification Tree for Iris Species"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div id="fig-cart-classification" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-cart-classification-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="22-statistical-learning_files/figure-html/fig-cart-classification-1.png" class="img-fluid figure-img" width="768">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-cart-classification-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;29.21: A CART decision tree for classifying iris species. Each node shows the predicted class, proportion of observations, and the splitting rule.
</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="interpreting-tree-output" class="level3">
<h3 class="anchored" data-anchor-id="interpreting-tree-output">Interpreting Tree Output</h3>
<p>The tree visualization shows:</p>
<ul>
<li><strong>Node prediction</strong>: The predicted class (or value for regression)</li>
<li><strong>Split rule</strong>: The feature and threshold used to split</li>
<li><strong>Proportions</strong>: Distribution of classes at each node</li>
<li><strong>Sample size</strong>: Number of observations reaching each node</li>
</ul>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb55"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb55-1"><a href="#cb55-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Detailed tree summary</span></span>
<span id="cb55-2"><a href="#cb55-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(tree_class, <span class="at">cp =</span> <span class="fl">0.1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Call:
rpart(formula = Species ~ ., data = iris, method = "class")
  n= 150 

    CP nsplit rel error xerror       xstd
1 0.50      0      1.00   1.18 0.05017303
2 0.44      1      0.50   0.71 0.06115009
3 0.01      2      0.06   0.12 0.03322650

Variable importance
 Petal.Width Petal.Length Sepal.Length  Sepal.Width 
          34           31           21           14 

Node number 1: 150 observations,    complexity param=0.5
  predicted class=setosa      expected loss=0.6666667  P(node) =1
    class counts:    50    50    50
   probabilities: 0.333 0.333 0.333 
  left son=2 (50 obs) right son=3 (100 obs)
  Primary splits:
      Petal.Length &lt; 2.45 to the left,  improve=50.00000, (0 missing)
      Petal.Width  &lt; 0.8  to the left,  improve=50.00000, (0 missing)
      Sepal.Length &lt; 5.45 to the left,  improve=34.16405, (0 missing)
      Sepal.Width  &lt; 3.35 to the right, improve=19.03851, (0 missing)
  Surrogate splits:
      Petal.Width  &lt; 0.8  to the left,  agree=1.000, adj=1.00, (0 split)
      Sepal.Length &lt; 5.45 to the left,  agree=0.920, adj=0.76, (0 split)
      Sepal.Width  &lt; 3.35 to the right, agree=0.833, adj=0.50, (0 split)

Node number 2: 50 observations
  predicted class=setosa      expected loss=0  P(node) =0.3333333
    class counts:    50     0     0
   probabilities: 1.000 0.000 0.000 

Node number 3: 100 observations,    complexity param=0.44
  predicted class=versicolor  expected loss=0.5  P(node) =0.6666667
    class counts:     0    50    50
   probabilities: 0.000 0.500 0.500 
  left son=6 (54 obs) right son=7 (46 obs)
  Primary splits:
      Petal.Width  &lt; 1.75 to the left,  improve=38.969400, (0 missing)
      Petal.Length &lt; 4.75 to the left,  improve=37.353540, (0 missing)
      Sepal.Length &lt; 6.15 to the left,  improve=10.686870, (0 missing)
      Sepal.Width  &lt; 2.45 to the left,  improve= 3.555556, (0 missing)
  Surrogate splits:
      Petal.Length &lt; 4.75 to the left,  agree=0.91, adj=0.804, (0 split)
      Sepal.Length &lt; 6.15 to the left,  agree=0.73, adj=0.413, (0 split)
      Sepal.Width  &lt; 2.95 to the left,  agree=0.67, adj=0.283, (0 split)

Node number 6: 54 observations
  predicted class=versicolor  expected loss=0.09259259  P(node) =0.36
    class counts:     0    49     5
   probabilities: 0.000 0.907 0.093 

Node number 7: 46 observations
  predicted class=virginica   expected loss=0.02173913  P(node) =0.3066667
    class counts:     0     1    45
   probabilities: 0.000 0.022 0.978 </code></pre>
</div>
</div>
</section>
<section id="regression-trees" class="level3">
<h3 class="anchored" data-anchor-id="regression-trees">Regression Trees</h3>
<p>When the outcome is continuous, we call the method a <strong>regression tree</strong>. To illustrate, we will use poll data from the 2008 presidential election where we try to estimate the conditional expectation of poll margin <span class="math inline">\(Y\)</span> given day <span class="math inline">\(x\)</span>.</p>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb57"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb57-1"><a href="#cb57-1" aria-hidden="true" tabindex="-1"></a><span class="fu">data</span>(<span class="st">"polls_2008"</span>)</span>
<span id="cb57-2"><a href="#cb57-2" aria-hidden="true" tabindex="-1"></a><span class="fu">qplot</span>(day, margin, <span class="at">data =</span> polls_2008) <span class="sc">+</span></span>
<span id="cb57-3"><a href="#cb57-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">"2008 Presidential Poll Data"</span>, <span class="at">x =</span> <span class="st">"Days before election"</span>, <span class="at">y =</span> <span class="st">"Poll margin"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div id="fig-polls-2008-data" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-polls-2008-data-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="22-statistical-learning_files/figure-html/fig-polls-2008-data-1.png" class="img-fluid figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-polls-2008-data-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;29.22: 2008 presidential poll data: margin (Obama - McCain) over time
</figcaption>
</figure>
</div>
</div>
</div>
<p>Let’s fit a regression tree using the <code>rpart</code> function:</p>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb58"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb58-1"><a href="#cb58-1" aria-hidden="true" tabindex="-1"></a>fit <span class="ot">&lt;-</span> <span class="fu">rpart</span>(margin <span class="sc">~</span> ., <span class="at">data =</span> polls_2008)</span>
<span id="cb58-2"><a href="#cb58-2" aria-hidden="true" tabindex="-1"></a>rafalib<span class="sc">::</span><span class="fu">mypar</span>()</span>
<span id="cb58-3"><a href="#cb58-3" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(fit, <span class="at">margin =</span> <span class="fl">0.1</span>)</span>
<span id="cb58-4"><a href="#cb58-4" aria-hidden="true" tabindex="-1"></a><span class="fu">text</span>(fit, <span class="at">cex =</span> <span class="fl">0.75</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div id="fig-polls-tree" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-polls-tree-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="22-statistical-learning_files/figure-html/fig-polls-tree-1.png" class="img-fluid figure-img" width="768">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-polls-tree-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;29.23: Regression tree for poll data showing where the algorithm decided to split
</figcaption>
</figure>
</div>
</div>
</div>
<p>The tree shows that the first split is made at day 39.5, then further splits occur at days 86.5, 49.5, 117.5, and so on. The final estimate <span class="math inline">\(\hat{f}(x)\)</span> is a step function:</p>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb59"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb59-1"><a href="#cb59-1" aria-hidden="true" tabindex="-1"></a>polls_2008 <span class="sc">%&gt;%</span></span>
<span id="cb59-2"><a href="#cb59-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">y_hat =</span> <span class="fu">predict</span>(fit)) <span class="sc">%&gt;%</span></span>
<span id="cb59-3"><a href="#cb59-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>() <span class="sc">+</span></span>
<span id="cb59-4"><a href="#cb59-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="fu">aes</span>(day, margin)) <span class="sc">+</span></span>
<span id="cb59-5"><a href="#cb59-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_step</span>(<span class="fu">aes</span>(day, y_hat), <span class="at">col =</span> <span class="st">"red"</span>, <span class="at">linewidth =</span> <span class="dv">1</span>) <span class="sc">+</span></span>
<span id="cb59-6"><a href="#cb59-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">"Regression Tree Fit"</span>, <span class="at">x =</span> <span class="st">"Day"</span>, <span class="at">y =</span> <span class="st">"Poll margin"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div id="fig-polls-tree-fit" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-polls-tree-fit-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="22-statistical-learning_files/figure-html/fig-polls-tree-fit-1.png" class="img-fluid figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-polls-tree-fit-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;29.24: Regression tree predictions for poll data create a step function
</figcaption>
</figure>
</div>
</div>
</div>
<p>Trees can also be applied to multiple predictors:</p>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb60"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb60-1"><a href="#cb60-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Build a regression tree</span></span>
<span id="cb60-2"><a href="#cb60-2" aria-hidden="true" tabindex="-1"></a>tree_reg <span class="ot">&lt;-</span> <span class="fu">rpart</span>(mpg <span class="sc">~</span> wt <span class="sc">+</span> hp <span class="sc">+</span> cyl, <span class="at">data =</span> mtcars, <span class="at">method =</span> <span class="st">"anova"</span>)</span>
<span id="cb60-3"><a href="#cb60-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-4"><a href="#cb60-4" aria-hidden="true" tabindex="-1"></a><span class="fu">rpart.plot</span>(tree_reg, <span class="at">extra =</span> <span class="dv">101</span>, <span class="at">box.palette =</span> <span class="st">"Blues"</span>,</span>
<span id="cb60-5"><a href="#cb60-5" aria-hidden="true" tabindex="-1"></a>           <span class="at">main =</span> <span class="st">"Regression Tree for MPG"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div id="fig-cart-regression" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-cart-regression-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="22-statistical-learning_files/figure-html/fig-cart-regression-1.png" class="img-fluid figure-img" width="768">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-cart-regression-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;29.25: A regression tree predicting car fuel efficiency (mpg) from weight and horsepower
</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="the-decision-boundary" class="level3">
<h3 class="anchored" data-anchor-id="the-decision-boundary">The Decision Boundary</h3>
<p>Trees partition the feature space into rectangular regions:</p>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb61"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb61-1"><a href="#cb61-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualize decision boundary for 2D case</span></span>
<span id="cb61-2"><a href="#cb61-2" aria-hidden="true" tabindex="-1"></a>tree_2d <span class="ot">&lt;-</span> <span class="fu">rpart</span>(Species <span class="sc">~</span> Petal.Length <span class="sc">+</span> Petal.Width, <span class="at">data =</span> iris, <span class="at">method =</span> <span class="st">"class"</span>)</span>
<span id="cb61-3"><a href="#cb61-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-4"><a href="#cb61-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Create grid for prediction</span></span>
<span id="cb61-5"><a href="#cb61-5" aria-hidden="true" tabindex="-1"></a>petal_length_seq <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="dv">0</span>, <span class="dv">7</span>, <span class="at">length.out =</span> <span class="dv">200</span>)</span>
<span id="cb61-6"><a href="#cb61-6" aria-hidden="true" tabindex="-1"></a>petal_width_seq <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="dv">0</span>, <span class="dv">3</span>, <span class="at">length.out =</span> <span class="dv">200</span>)</span>
<span id="cb61-7"><a href="#cb61-7" aria-hidden="true" tabindex="-1"></a>grid <span class="ot">&lt;-</span> <span class="fu">expand.grid</span>(<span class="at">Petal.Length =</span> petal_length_seq,</span>
<span id="cb61-8"><a href="#cb61-8" aria-hidden="true" tabindex="-1"></a>                    <span class="at">Petal.Width =</span> petal_width_seq)</span>
<span id="cb61-9"><a href="#cb61-9" aria-hidden="true" tabindex="-1"></a>grid<span class="sc">$</span>pred <span class="ot">&lt;-</span> <span class="fu">predict</span>(tree_2d, grid, <span class="at">type =</span> <span class="st">"class"</span>)</span>
<span id="cb61-10"><a href="#cb61-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-11"><a href="#cb61-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot</span></span>
<span id="cb61-12"><a href="#cb61-12" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(grid<span class="sc">$</span>Petal.Length, grid<span class="sc">$</span>Petal.Width,</span>
<span id="cb61-13"><a href="#cb61-13" aria-hidden="true" tabindex="-1"></a>     <span class="at">col =</span> <span class="fu">c</span>(<span class="fu">rgb</span>(<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="fl">0.1</span>), <span class="fu">rgb</span>(<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="fl">0.1</span>), <span class="fu">rgb</span>(<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="fl">0.1</span>))[grid<span class="sc">$</span>pred],</span>
<span id="cb61-14"><a href="#cb61-14" aria-hidden="true" tabindex="-1"></a>     <span class="at">pch =</span> <span class="dv">15</span>, <span class="at">cex =</span> <span class="fl">0.3</span>,</span>
<span id="cb61-15"><a href="#cb61-15" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">"Petal Length"</span>, <span class="at">ylab =</span> <span class="st">"Petal Width"</span>,</span>
<span id="cb61-16"><a href="#cb61-16" aria-hidden="true" tabindex="-1"></a>     <span class="at">main =</span> <span class="st">"Decision Tree Boundaries"</span>)</span>
<span id="cb61-17"><a href="#cb61-17" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span>(iris<span class="sc">$</span>Petal.Length, iris<span class="sc">$</span>Petal.Width,</span>
<span id="cb61-18"><a href="#cb61-18" aria-hidden="true" tabindex="-1"></a>       <span class="at">col =</span> <span class="fu">c</span>(<span class="st">"red"</span>, <span class="st">"green"</span>, <span class="st">"blue"</span>)[iris<span class="sc">$</span>Species],</span>
<span id="cb61-19"><a href="#cb61-19" aria-hidden="true" tabindex="-1"></a>       <span class="at">pch =</span> <span class="dv">19</span>, <span class="at">cex =</span> <span class="fl">0.8</span>)</span>
<span id="cb61-20"><a href="#cb61-20" aria-hidden="true" tabindex="-1"></a><span class="fu">legend</span>(<span class="st">"topleft"</span>, <span class="fu">levels</span>(iris<span class="sc">$</span>Species),</span>
<span id="cb61-21"><a href="#cb61-21" aria-hidden="true" tabindex="-1"></a>       <span class="at">col =</span> <span class="fu">c</span>(<span class="st">"red"</span>, <span class="st">"green"</span>, <span class="st">"blue"</span>), <span class="at">pch =</span> <span class="dv">19</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div id="fig-tree-boundary" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-tree-boundary-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="22-statistical-learning_files/figure-html/fig-tree-boundary-1.png" class="img-fluid figure-img" width="768">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-tree-boundary-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;29.26: Decision tree partition of the feature space. Each rectangular region is assigned to a class based on the majority vote of training points in that region.
</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="controlling-tree-complexity" class="level3">
<h3 class="anchored" data-anchor-id="controlling-tree-complexity">Controlling Tree Complexity</h3>
<p>Trees easily overfit—they can keep splitting until each leaf contains a single observation. Several parameters control complexity:</p>
<ul>
<li><strong>cp (complexity parameter)</strong>: Every time we split and define two new partitions, our training set RSS decreases. The RSS must improve by a factor of cp for the new partition to be added. Large values of cp force the algorithm to stop earlier, resulting in fewer nodes.</li>
<li><strong>minsplit</strong>: Minimum observations required in a partition before attempting to split further. The default in <code>rpart</code> is 20.</li>
<li><strong>minbucket</strong>: Minimum number of observations in each terminal node (leaf). Defaults to <code>round(minsplit/3)</code>.</li>
<li><strong>maxdepth</strong>: Maximum depth of the tree</li>
</ul>
<p>If we set <code>cp = 0</code> and <code>minsplit = 2</code>, our prediction becomes as flexible as possible and simply memorizes the training data—a clear case of overfitting:</p>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb62"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb62-1"><a href="#cb62-1" aria-hidden="true" tabindex="-1"></a>fit_overfit <span class="ot">&lt;-</span> <span class="fu">rpart</span>(margin <span class="sc">~</span> ., <span class="at">data =</span> polls_2008,</span>
<span id="cb62-2"><a href="#cb62-2" aria-hidden="true" tabindex="-1"></a>                      <span class="at">control =</span> <span class="fu">rpart.control</span>(<span class="at">cp =</span> <span class="dv">0</span>, <span class="at">minsplit =</span> <span class="dv">2</span>))</span>
<span id="cb62-3"><a href="#cb62-3" aria-hidden="true" tabindex="-1"></a>polls_2008 <span class="sc">%&gt;%</span></span>
<span id="cb62-4"><a href="#cb62-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">y_hat =</span> <span class="fu">predict</span>(fit_overfit)) <span class="sc">%&gt;%</span></span>
<span id="cb62-5"><a href="#cb62-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>() <span class="sc">+</span></span>
<span id="cb62-6"><a href="#cb62-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="fu">aes</span>(day, margin)) <span class="sc">+</span></span>
<span id="cb62-7"><a href="#cb62-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_step</span>(<span class="fu">aes</span>(day, y_hat), <span class="at">col =</span> <span class="st">"red"</span>) <span class="sc">+</span></span>
<span id="cb62-8"><a href="#cb62-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">"Overfitting with cp = 0"</span>, <span class="at">x =</span> <span class="st">"Day"</span>, <span class="at">y =</span> <span class="st">"Poll margin"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div id="fig-polls-overfit" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-polls-overfit-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="22-statistical-learning_files/figure-html/fig-polls-overfit-1.png" class="img-fluid figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-polls-overfit-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;29.27: With cp=0 and minsplit=2, the tree overfits by memorizing every point in the training data
</figcaption>
</figure>
</div>
</div>
</div>
<p>The larger these values are, the more data is averaged to compute a predictor, reducing variability but restricting flexibility. We use cross-validation to select the optimal balance.</p>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb63"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb63-1"><a href="#cb63-1" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">3</span>))</span>
<span id="cb63-2"><a href="#cb63-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb63-3"><a href="#cb63-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Simple tree (high cp)</span></span>
<span id="cb63-4"><a href="#cb63-4" aria-hidden="true" tabindex="-1"></a>tree_simple <span class="ot">&lt;-</span> <span class="fu">rpart</span>(Species <span class="sc">~</span> ., <span class="at">data =</span> iris, <span class="at">cp =</span> <span class="fl">0.1</span>)</span>
<span id="cb63-5"><a href="#cb63-5" aria-hidden="true" tabindex="-1"></a><span class="fu">rpart.plot</span>(tree_simple, <span class="at">main =</span> <span class="st">"cp = 0.1 (simple)"</span>)</span>
<span id="cb63-6"><a href="#cb63-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb63-7"><a href="#cb63-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Medium tree</span></span>
<span id="cb63-8"><a href="#cb63-8" aria-hidden="true" tabindex="-1"></a>tree_medium <span class="ot">&lt;-</span> <span class="fu">rpart</span>(Species <span class="sc">~</span> ., <span class="at">data =</span> iris, <span class="at">cp =</span> <span class="fl">0.02</span>)</span>
<span id="cb63-9"><a href="#cb63-9" aria-hidden="true" tabindex="-1"></a><span class="fu">rpart.plot</span>(tree_medium, <span class="at">main =</span> <span class="st">"cp = 0.02 (medium)"</span>)</span>
<span id="cb63-10"><a href="#cb63-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb63-11"><a href="#cb63-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Complex tree (low cp)</span></span>
<span id="cb63-12"><a href="#cb63-12" aria-hidden="true" tabindex="-1"></a>tree_complex <span class="ot">&lt;-</span> <span class="fu">rpart</span>(Species <span class="sc">~</span> ., <span class="at">data =</span> iris, <span class="at">cp =</span> <span class="fl">0.001</span>)</span>
<span id="cb63-13"><a href="#cb63-13" aria-hidden="true" tabindex="-1"></a><span class="fu">rpart.plot</span>(tree_complex, <span class="at">main =</span> <span class="st">"cp = 0.001 (complex)"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div id="fig-tree-complexity" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-tree-complexity-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="22-statistical-learning_files/figure-html/fig-tree-complexity-1.png" class="img-fluid figure-img" width="864">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-tree-complexity-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;29.28: Effect of the complexity parameter on tree structure: smaller cp allows more splits and greater complexity
</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="pruning-with-cross-validation" class="level3">
<h3 class="anchored" data-anchor-id="pruning-with-cross-validation">Pruning with Cross-Validation</h3>
<p>The optimal complexity is typically chosen by cross-validation:</p>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb64"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb64-1"><a href="#cb64-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit full tree</span></span>
<span id="cb64-2"><a href="#cb64-2" aria-hidden="true" tabindex="-1"></a>full_tree <span class="ot">&lt;-</span> <span class="fu">rpart</span>(Species <span class="sc">~</span> ., <span class="at">data =</span> iris, <span class="at">cp =</span> <span class="fl">0.001</span>)</span>
<span id="cb64-3"><a href="#cb64-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-4"><a href="#cb64-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot CV error vs complexity</span></span>
<span id="cb64-5"><a href="#cb64-5" aria-hidden="true" tabindex="-1"></a><span class="fu">plotcp</span>(full_tree)</span>
<span id="cb64-6"><a href="#cb64-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-7"><a href="#cb64-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Print CP table</span></span>
<span id="cb64-8"><a href="#cb64-8" aria-hidden="true" tabindex="-1"></a><span class="fu">printcp</span>(full_tree)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>
Classification tree:
rpart(formula = Species ~ ., data = iris, cp = 0.001)

Variables actually used in tree construction:
[1] Petal.Length Petal.Width 

Root node error: 100/150 = 0.66667

n= 150 

     CP nsplit rel error xerror     xstd
1 0.500      0      1.00   1.17 0.050735
2 0.440      1      0.50   0.60 0.060000
3 0.001      2      0.06   0.08 0.027520</code></pre>
</div>
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb66"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb66-1"><a href="#cb66-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Prune to optimal cp</span></span>
<span id="cb66-2"><a href="#cb66-2" aria-hidden="true" tabindex="-1"></a>best_cp <span class="ot">&lt;-</span> full_tree<span class="sc">$</span>cptable[<span class="fu">which.min</span>(full_tree<span class="sc">$</span>cptable[, <span class="st">"xerror"</span>]), <span class="st">"CP"</span>]</span>
<span id="cb66-3"><a href="#cb66-3" aria-hidden="true" tabindex="-1"></a>pruned_tree <span class="ot">&lt;-</span> <span class="fu">prune</span>(full_tree, <span class="at">cp =</span> best_cp)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div id="fig-tree-cv" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-tree-cv-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="22-statistical-learning_files/figure-html/fig-tree-cv-1.png" class="img-fluid figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-tree-cv-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;29.29: Cross-validation error as a function of tree complexity. The dashed line shows one standard error above the minimum, often used to select a simpler tree.
</figcaption>
</figure>
</div>
</div>
</div>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Advantages and Disadvantages of Trees
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Advantages:</strong> - Highly interpretable—easy to explain to non-statisticians - Handle both numeric and categorical predictors - Capture non-linear relationships and interactions automatically - Robust to outliers and don’t require feature scaling</p>
<p><strong>Disadvantages:</strong> - High variance—small changes in data can produce very different trees - Prone to overfitting without careful tuning - Axis-aligned splits can’t capture diagonal relationships efficiently - Generally lower predictive accuracy than ensemble methods</p>
</div>
</div>
</section>
</section>
<section id="random-forests" class="level2" data-number="29.16">
<h2 data-number="29.16" class="anchored" data-anchor-id="random-forests"><span class="header-section-number">29.16</span> Random Forests</h2>
<p><strong>Random forests</strong> <span class="citation" data-cites="breiman2001random">(<a href="../references.html#ref-breiman2001random" role="doc-biblioref">Breiman 2001</a>)</span> are a <strong>very popular</strong> machine learning approach that addresses the shortcomings of decision trees using a clever idea. The goal is to improve prediction performance and reduce instability by <em>averaging</em> multiple decision trees (a forest of trees constructed with randomness). It has two features that help accomplish this.</p>
<p>The first step is <strong>bootstrap aggregation</strong> or <strong>bagging</strong>. The general idea is to generate many predictors, each using regression or classification trees, and then form a final prediction based on the average prediction of all these trees. To assure that the individual trees are not the same, we use the bootstrap to induce randomness. These two features combined explain the name: the bootstrap makes the individual trees <strong>randomly</strong> different, and the combination of trees is the <strong>forest</strong>.</p>
<section id="the-random-forest-algorithm" class="level3">
<h3 class="anchored" data-anchor-id="the-random-forest-algorithm">The Random Forest Algorithm</h3>
<p>The specific steps are:</p>
<ol type="1">
<li>Build <span class="math inline">\(B\)</span> decision trees using the training set. We refer to the fitted models as <span class="math inline">\(T_1, T_2, \dots, T_B\)</span>.</li>
<li>For every observation in the test set, form a prediction <span class="math inline">\(\hat{y}_j\)</span> using tree <span class="math inline">\(T_j\)</span>.</li>
<li>For continuous outcomes, form a final prediction with the average <span class="math inline">\(\hat{y} = \frac{1}{B} \sum_{j=1}^B \hat{y}_j\)</span>. For categorical data classification, predict <span class="math inline">\(\hat{y}\)</span> with majority vote (most frequent class among <span class="math inline">\(\hat{y}_1, \dots, \hat{y}_B\)</span>).</li>
</ol>
<p>To create <span class="math inline">\(T_j, \, j=1,\ldots,B\)</span> from the training set:</p>
<ol type="1">
<li>Create a bootstrap training set by sampling <span class="math inline">\(N\)</span> observations from the training set <strong>with replacement</strong>. This is the first way to induce randomness.</li>
<li>At each split, consider only a random subset of <span class="math inline">\(m\)</span> features (typically <span class="math inline">\(m = \sqrt{p}\)</span> for classification, <span class="math inline">\(m = p/3\)</span> for regression). This reduces correlation between trees in the forest, thereby improving prediction accuracy.</li>
</ol>
<p>The randomness serves two purposes: - <strong>Bagging</strong> reduces variance by averaging many noisy but unbiased trees - <strong>Random feature selection</strong> decorrelates the trees, making the average more effective</p>
</section>
<section id="why-averaging-produces-smooth-estimates" class="level3">
<h3 class="anchored" data-anchor-id="why-averaging-produces-smooth-estimates">Why Averaging Produces Smooth Estimates</h3>
<p>A key insight is that the average of many step functions can be smooth. Let’s illustrate with the polls data:</p>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb67"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb67-1"><a href="#cb67-1" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">2</span>))</span>
<span id="cb67-2"><a href="#cb67-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-3"><a href="#cb67-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Single tree (from before)</span></span>
<span id="cb67-4"><a href="#cb67-4" aria-hidden="true" tabindex="-1"></a>fit_tree <span class="ot">&lt;-</span> <span class="fu">rpart</span>(margin <span class="sc">~</span> ., <span class="at">data =</span> polls_2008)</span>
<span id="cb67-5"><a href="#cb67-5" aria-hidden="true" tabindex="-1"></a>polls_2008 <span class="sc">%&gt;%</span></span>
<span id="cb67-6"><a href="#cb67-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">y_hat =</span> <span class="fu">predict</span>(fit_tree)) <span class="sc">%&gt;%</span></span>
<span id="cb67-7"><a href="#cb67-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">with</span>(<span class="fu">plot</span>(day, margin, <span class="at">pch =</span> <span class="dv">16</span>, <span class="at">cex =</span> <span class="fl">0.5</span>,</span>
<span id="cb67-8"><a href="#cb67-8" aria-hidden="true" tabindex="-1"></a>            <span class="at">main =</span> <span class="st">"Single Regression Tree"</span>))</span>
<span id="cb67-9"><a href="#cb67-9" aria-hidden="true" tabindex="-1"></a>polls_2008 <span class="sc">%&gt;%</span></span>
<span id="cb67-10"><a href="#cb67-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">y_hat =</span> <span class="fu">predict</span>(fit_tree)) <span class="sc">%&gt;%</span></span>
<span id="cb67-11"><a href="#cb67-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">with</span>(<span class="fu">lines</span>(day, y_hat, <span class="at">col =</span> <span class="st">"red"</span>, <span class="at">type =</span> <span class="st">"s"</span>, <span class="at">lwd =</span> <span class="dv">2</span>))</span>
<span id="cb67-12"><a href="#cb67-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-13"><a href="#cb67-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Random forest</span></span>
<span id="cb67-14"><a href="#cb67-14" aria-hidden="true" tabindex="-1"></a>fit_rf <span class="ot">&lt;-</span> <span class="fu">randomForest</span>(margin <span class="sc">~</span> ., <span class="at">data =</span> polls_2008)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-error">
<pre><code>Error in `randomForest()`:
! could not find function "randomForest"</code></pre>
</div>
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb69"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb69-1"><a href="#cb69-1" aria-hidden="true" tabindex="-1"></a>polls_2008 <span class="sc">%&gt;%</span></span>
<span id="cb69-2"><a href="#cb69-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">y_hat =</span> <span class="fu">predict</span>(fit_rf, <span class="at">newdata =</span> polls_2008)) <span class="sc">%&gt;%</span></span>
<span id="cb69-3"><a href="#cb69-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">with</span>(<span class="fu">plot</span>(day, margin, <span class="at">pch =</span> <span class="dv">16</span>, <span class="at">cex =</span> <span class="fl">0.5</span>,</span>
<span id="cb69-4"><a href="#cb69-4" aria-hidden="true" tabindex="-1"></a>            <span class="at">main =</span> <span class="st">"Random Forest"</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-error">
<pre><code>Error in `mutate()`:
ℹ In argument: `y_hat = predict(fit_rf, newdata = polls_2008)`.
Caused by error:
! object 'fit_rf' not found</code></pre>
</div>
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb71"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb71-1"><a href="#cb71-1" aria-hidden="true" tabindex="-1"></a>polls_2008 <span class="sc">%&gt;%</span></span>
<span id="cb71-2"><a href="#cb71-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">y_hat =</span> <span class="fu">predict</span>(fit_rf, <span class="at">newdata =</span> polls_2008)) <span class="sc">%&gt;%</span></span>
<span id="cb71-3"><a href="#cb71-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">with</span>(<span class="fu">lines</span>(day, y_hat, <span class="at">col =</span> <span class="st">"blue"</span>, <span class="at">lwd =</span> <span class="dv">2</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-error">
<pre><code>Error in `mutate()`:
ℹ In argument: `y_hat = predict(fit_rf, newdata = polls_2008)`.
Caused by error:
! object 'fit_rf' not found</code></pre>
</div>
<div class="cell-output-display">
<div id="fig-rf-polls-smooth" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-rf-polls-smooth-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="22-statistical-learning_files/figure-html/fig-rf-polls-smooth-1.png" class="img-fluid figure-img" width="864">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-rf-polls-smooth-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;29.30: Random forest predictions are much smoother than single trees because averaging many step functions produces a smooth curve
</figcaption>
</figure>
</div>
</div>
</div>
<p>Notice that the random forest estimate is much smoother than what we achieved with the single regression tree. This is possible because the average of many step functions can be smooth—each bootstrap sample produces a slightly different tree, and their average traces out a smooth curve.</p>
</section>
<section id="random-forests-in-r" class="level3">
<h3 class="anchored" data-anchor-id="random-forests-in-r">Random Forests in R</h3>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb73"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb73-1"><a href="#cb73-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(randomForest)</span>
<span id="cb73-2"><a href="#cb73-2" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">42</span>)</span>
<span id="cb73-3"><a href="#cb73-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-4"><a href="#cb73-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit random forest</span></span>
<span id="cb73-5"><a href="#cb73-5" aria-hidden="true" tabindex="-1"></a>rf_model <span class="ot">&lt;-</span> <span class="fu">randomForest</span>(Species <span class="sc">~</span> ., <span class="at">data =</span> iris,</span>
<span id="cb73-6"><a href="#cb73-6" aria-hidden="true" tabindex="-1"></a>                          <span class="at">ntree =</span> <span class="dv">500</span>,       <span class="co"># Number of trees</span></span>
<span id="cb73-7"><a href="#cb73-7" aria-hidden="true" tabindex="-1"></a>                          <span class="at">mtry =</span> <span class="dv">2</span>,          <span class="co"># Features tried at each split</span></span>
<span id="cb73-8"><a href="#cb73-8" aria-hidden="true" tabindex="-1"></a>                          <span class="at">importance =</span> <span class="cn">TRUE</span>)  <span class="co"># Calculate variable importance</span></span>
<span id="cb73-9"><a href="#cb73-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-10"><a href="#cb73-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Model summary</span></span>
<span id="cb73-11"><a href="#cb73-11" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(rf_model)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
 randomForest(formula = Species ~ ., data = iris, ntree = 500,      mtry = 2, importance = TRUE) 
               Type of random forest: classification
                     Number of trees: 500
No. of variables tried at each split: 2

        OOB estimate of  error rate: 4%
Confusion matrix:
           setosa versicolor virginica class.error
setosa         50          0         0        0.00
versicolor      0         47         3        0.06
virginica       0          3        47        0.06</code></pre>
</div>
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb75"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb75-1"><a href="#cb75-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot error vs number of trees</span></span>
<span id="cb75-2"><a href="#cb75-2" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(rf_model, <span class="at">main =</span> <span class="st">"Random Forest: Error vs. Number of Trees"</span>)</span>
<span id="cb75-3"><a href="#cb75-3" aria-hidden="true" tabindex="-1"></a><span class="fu">legend</span>(<span class="st">"topright"</span>, <span class="fu">colnames</span>(rf_model<span class="sc">$</span>err.rate), <span class="at">col =</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">4</span>, <span class="at">lty =</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">4</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div id="fig-rf-model" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-rf-model-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="22-statistical-learning_files/figure-html/fig-rf-model-1.png" class="img-fluid figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-rf-model-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;29.31: Random forest OOB error rate decreasing as more trees are added
</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="out-of-bag-oob-error" class="level3">
<h3 class="anchored" data-anchor-id="out-of-bag-oob-error">Out-of-Bag (OOB) Error</h3>
<p>Each bootstrap sample uses about 63% of observations. The remaining 37% (out-of-bag samples) provide a built-in test set:</p>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb76"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb76-1"><a href="#cb76-1" aria-hidden="true" tabindex="-1"></a><span class="co"># OOB confusion matrix</span></span>
<span id="cb76-2"><a href="#cb76-2" aria-hidden="true" tabindex="-1"></a>rf_model<span class="sc">$</span>confusion</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>           setosa versicolor virginica class.error
setosa         50          0         0        0.00
versicolor      0         47         3        0.06
virginica       0          3        47        0.06</code></pre>
</div>
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb78"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb78-1"><a href="#cb78-1" aria-hidden="true" tabindex="-1"></a><span class="co"># OOB error rate</span></span>
<span id="cb78-2"><a href="#cb78-2" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"OOB Error Rate:"</span>, <span class="fu">round</span>(<span class="dv">1</span> <span class="sc">-</span> <span class="fu">sum</span>(<span class="fu">diag</span>(rf_model<span class="sc">$</span>confusion[,<span class="dv">1</span><span class="sc">:</span><span class="dv">3</span>])) <span class="sc">/</span></span>
<span id="cb78-3"><a href="#cb78-3" aria-hidden="true" tabindex="-1"></a>                              <span class="fu">sum</span>(rf_model<span class="sc">$</span>confusion[,<span class="dv">1</span><span class="sc">:</span><span class="dv">3</span>]), <span class="dv">3</span>), <span class="st">"</span><span class="sc">\n</span><span class="st">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>OOB Error Rate: 0.04 </code></pre>
</div>
</div>
<p>OOB error is nearly as accurate as cross-validation but comes “for free” during training.</p>
</section>
<section id="variable-importance" class="level3">
<h3 class="anchored" data-anchor-id="variable-importance">Variable Importance</h3>
<p>Random forests provide measures of how important each predictor is:</p>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb80"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb80-1"><a href="#cb80-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Variable importance plot</span></span>
<span id="cb80-2"><a href="#cb80-2" aria-hidden="true" tabindex="-1"></a><span class="fu">varImpPlot</span>(rf_model, <span class="at">main =</span> <span class="st">"Variable Importance"</span>)</span>
<span id="cb80-3"><a href="#cb80-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-4"><a href="#cb80-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Numeric importance values</span></span>
<span id="cb80-5"><a href="#cb80-5" aria-hidden="true" tabindex="-1"></a><span class="fu">importance</span>(rf_model)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>                setosa versicolor virginica MeanDecreaseAccuracy
Sepal.Length  6.202069  8.1714623  7.112845            10.837857
Sepal.Width   4.389926 -0.0394507  4.433121             4.505871
Petal.Length 22.142310 32.6681049 28.411695            33.420235
Petal.Width  22.452771 32.9325603 30.673079            33.808242
             MeanDecreaseGini
Sepal.Length         9.273382
Sepal.Width          2.178884
Petal.Length        43.873860
Petal.Width         43.879867</code></pre>
</div>
<div class="cell-output-display">
<div id="fig-rf-importance" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-rf-importance-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="22-statistical-learning_files/figure-html/fig-rf-importance-1.png" class="img-fluid figure-img" width="768">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-rf-importance-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;29.32: Variable importance from random forest: Mean Decrease Accuracy measures how much removing a variable hurts prediction; Mean Decrease Gini measures the total reduction in node impurity
</figcaption>
</figure>
</div>
</div>
</div>
<p><strong>Mean Decrease Accuracy</strong>: For each tree, predictions are made on OOB samples. Then the values of variable <span class="math inline">\(j\)</span> are randomly permuted, and predictions are made again. The decrease in accuracy from permutation measures importance.</p>
<p><strong>Mean Decrease Gini</strong>: Total decrease in Gini impurity from splits on variable <span class="math inline">\(j\)</span>, averaged over all trees.</p>
</section>
<section id="tuning-random-forests" class="level3">
<h3 class="anchored" data-anchor-id="tuning-random-forests">Tuning Random Forests</h3>
<p>Key parameters to tune:</p>
<ul>
<li><strong>ntree</strong>: Number of trees (more is generally better, but with diminishing returns)</li>
<li><strong>mtry</strong>: Number of features considered at each split</li>
<li><strong>nodesize</strong>: Minimum size of terminal nodes</li>
</ul>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb82"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb82-1"><a href="#cb82-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Tune mtry</span></span>
<span id="cb82-2"><a href="#cb82-2" aria-hidden="true" tabindex="-1"></a>oob_error <span class="ot">&lt;-</span> <span class="fu">sapply</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">4</span>, <span class="cf">function</span>(m) {</span>
<span id="cb82-3"><a href="#cb82-3" aria-hidden="true" tabindex="-1"></a>  rf <span class="ot">&lt;-</span> <span class="fu">randomForest</span>(Species <span class="sc">~</span> ., <span class="at">data =</span> iris, <span class="at">mtry =</span> m, <span class="at">ntree =</span> <span class="dv">200</span>)</span>
<span id="cb82-4"><a href="#cb82-4" aria-hidden="true" tabindex="-1"></a>  rf<span class="sc">$</span>err.rate[<span class="dv">200</span>, <span class="st">"OOB"</span>]</span>
<span id="cb82-5"><a href="#cb82-5" aria-hidden="true" tabindex="-1"></a>})</span>
<span id="cb82-6"><a href="#cb82-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb82-7"><a href="#cb82-7" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">4</span>, oob_error, <span class="at">type =</span> <span class="st">"b"</span>, <span class="at">pch =</span> <span class="dv">19</span>,</span>
<span id="cb82-8"><a href="#cb82-8" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">"mtry (features at each split)"</span>,</span>
<span id="cb82-9"><a href="#cb82-9" aria-hidden="true" tabindex="-1"></a>     <span class="at">ylab =</span> <span class="st">"OOB Error Rate"</span>,</span>
<span id="cb82-10"><a href="#cb82-10" aria-hidden="true" tabindex="-1"></a>     <span class="at">main =</span> <span class="st">"Tuning mtry Parameter"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div id="fig-rf-tuning" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-rf-tuning-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="22-statistical-learning_files/figure-html/fig-rf-tuning-1.png" class="img-fluid figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-rf-tuning-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;29.33: Random forest OOB error as a function of mtry (number of features considered at each split)
</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="random-forest-for-regression" class="level3">
<h3 class="anchored" data-anchor-id="random-forest-for-regression">Random Forest for Regression</h3>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb83"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb83-1"><a href="#cb83-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Regression random forest</span></span>
<span id="cb83-2"><a href="#cb83-2" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">42</span>)</span>
<span id="cb83-3"><a href="#cb83-3" aria-hidden="true" tabindex="-1"></a>rf_reg <span class="ot">&lt;-</span> <span class="fu">randomForest</span>(mpg <span class="sc">~</span> ., <span class="at">data =</span> mtcars, <span class="at">ntree =</span> <span class="dv">500</span>, <span class="at">importance =</span> <span class="cn">TRUE</span>)</span>
<span id="cb83-4"><a href="#cb83-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb83-5"><a href="#cb83-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Performance</span></span>
<span id="cb83-6"><a href="#cb83-6" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"Variance explained:"</span>, <span class="fu">round</span>(rf_reg<span class="sc">$</span>rsq[<span class="dv">500</span>] <span class="sc">*</span> <span class="dv">100</span>, <span class="dv">1</span>), <span class="st">"%</span><span class="sc">\n</span><span class="st">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Variance explained: 83.8 %</code></pre>
</div>
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb85"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb85-1"><a href="#cb85-1" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"MSE:"</span>, <span class="fu">round</span>(rf_reg<span class="sc">$</span>mse[<span class="dv">500</span>], <span class="dv">2</span>), <span class="st">"</span><span class="sc">\n</span><span class="st">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>MSE: 5.71 </code></pre>
</div>
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb87"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb87-1"><a href="#cb87-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Variable importance for regression</span></span>
<span id="cb87-2"><a href="#cb87-2" aria-hidden="true" tabindex="-1"></a><span class="fu">varImpPlot</span>(rf_reg, <span class="at">main =</span> <span class="st">"Variable Importance for MPG Prediction"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="22-statistical-learning_files/figure-html/unnamed-chunk-12-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
</section>
</section>
<section id="support-vector-machines-svm" class="level2" data-number="29.17">
<h2 data-number="29.17" class="anchored" data-anchor-id="support-vector-machines-svm"><span class="header-section-number">29.17</span> Support Vector Machines (SVM)</h2>
<p><strong>Support Vector Machines</strong> <span class="citation" data-cites="cortes1995support">(<a href="../references.html#ref-cortes1995support" role="doc-biblioref">Cortes and Vapnik 1995</a>)</span> find the hyperplane that best separates classes by maximizing the margin—the distance between the boundary and the nearest points from each class.</p>
<section id="the-maximum-margin-classifier" class="level3">
<h3 class="anchored" data-anchor-id="the-maximum-margin-classifier">The Maximum Margin Classifier</h3>
<p>For linearly separable data, infinitely many lines could separate the classes. SVM chooses the line with the largest margin:</p>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb88"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb88-1"><a href="#cb88-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(e1071)</span>
<span id="cb88-2"><a href="#cb88-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-3"><a href="#cb88-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Create simple 2D data</span></span>
<span id="cb88-4"><a href="#cb88-4" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">42</span>)</span>
<span id="cb88-5"><a href="#cb88-5" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">40</span></span>
<span id="cb88-6"><a href="#cb88-6" aria-hidden="true" tabindex="-1"></a>x1 <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fu">rnorm</span>(n<span class="sc">/</span><span class="dv">2</span>, <span class="at">mean =</span> <span class="dv">0</span>), <span class="fu">rnorm</span>(n<span class="sc">/</span><span class="dv">2</span>, <span class="at">mean =</span> <span class="dv">3</span>))</span>
<span id="cb88-7"><a href="#cb88-7" aria-hidden="true" tabindex="-1"></a>x2 <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fu">rnorm</span>(n<span class="sc">/</span><span class="dv">2</span>, <span class="at">mean =</span> <span class="dv">0</span>), <span class="fu">rnorm</span>(n<span class="sc">/</span><span class="dv">2</span>, <span class="at">mean =</span> <span class="dv">3</span>))</span>
<span id="cb88-8"><a href="#cb88-8" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> <span class="fu">factor</span>(<span class="fu">c</span>(<span class="fu">rep</span>(<span class="sc">-</span><span class="dv">1</span>, n<span class="sc">/</span><span class="dv">2</span>), <span class="fu">rep</span>(<span class="dv">1</span>, n<span class="sc">/</span><span class="dv">2</span>)))</span>
<span id="cb88-9"><a href="#cb88-9" aria-hidden="true" tabindex="-1"></a>svm_data <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(x1, x2, y)</span>
<span id="cb88-10"><a href="#cb88-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-11"><a href="#cb88-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit linear SVM</span></span>
<span id="cb88-12"><a href="#cb88-12" aria-hidden="true" tabindex="-1"></a>svm_linear <span class="ot">&lt;-</span> <span class="fu">svm</span>(y <span class="sc">~</span> x1 <span class="sc">+</span> x2, <span class="at">data =</span> svm_data, <span class="at">kernel =</span> <span class="st">"linear"</span>,</span>
<span id="cb88-13"><a href="#cb88-13" aria-hidden="true" tabindex="-1"></a>                   <span class="at">cost =</span> <span class="dv">10</span>, <span class="at">scale =</span> <span class="cn">FALSE</span>)</span>
<span id="cb88-14"><a href="#cb88-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-15"><a href="#cb88-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot</span></span>
<span id="cb88-16"><a href="#cb88-16" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(svm_linear, svm_data, x1 <span class="sc">~</span> x2,</span>
<span id="cb88-17"><a href="#cb88-17" aria-hidden="true" tabindex="-1"></a>     <span class="at">col =</span> <span class="fu">c</span>(<span class="st">"lightblue"</span>, <span class="st">"lightpink"</span>),</span>
<span id="cb88-18"><a href="#cb88-18" aria-hidden="true" tabindex="-1"></a>     <span class="at">symbolPalette =</span> <span class="fu">c</span>(<span class="st">"blue"</span>, <span class="st">"red"</span>),</span>
<span id="cb88-19"><a href="#cb88-19" aria-hidden="true" tabindex="-1"></a>     <span class="at">svSymbol =</span> <span class="st">"x"</span>, <span class="at">dataSymbol =</span> <span class="st">"o"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div id="fig-svm-concept" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-svm-concept-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="22-statistical-learning_files/figure-html/fig-svm-concept-1.png" class="img-fluid figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-svm-concept-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;29.34: Support Vector Machine concept: the decision boundary (solid line) maximizes the margin (distance to nearest points). Support vectors are the points on the margin boundaries.
</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="soft-margin-and-the-cost-parameter" class="level3">
<h3 class="anchored" data-anchor-id="soft-margin-and-the-cost-parameter">Soft Margin and the Cost Parameter</h3>
<p>Real data is rarely perfectly separable. <strong>Soft margin</strong> SVM allows some points to violate the margin, controlled by the cost parameter <span class="math inline">\(C\)</span>:</p>
<ul>
<li><strong>High C</strong>: Small margin, few violations (may overfit)</li>
<li><strong>Low C</strong>: Large margin, more violations (may underfit)</li>
</ul>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb89"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb89-1"><a href="#cb89-1" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">3</span>))</span>
<span id="cb89-2"><a href="#cb89-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb89-3"><a href="#cb89-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (cost_val <span class="cf">in</span> <span class="fu">c</span>(<span class="fl">0.1</span>, <span class="dv">1</span>, <span class="dv">100</span>)) {</span>
<span id="cb89-4"><a href="#cb89-4" aria-hidden="true" tabindex="-1"></a>  svm_fit <span class="ot">&lt;-</span> <span class="fu">svm</span>(y <span class="sc">~</span> x1 <span class="sc">+</span> x2, <span class="at">data =</span> svm_data, <span class="at">kernel =</span> <span class="st">"linear"</span>,</span>
<span id="cb89-5"><a href="#cb89-5" aria-hidden="true" tabindex="-1"></a>                  <span class="at">cost =</span> cost_val, <span class="at">scale =</span> <span class="cn">FALSE</span>)</span>
<span id="cb89-6"><a href="#cb89-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">plot</span>(svm_fit, svm_data, x1 <span class="sc">~</span> x2,</span>
<span id="cb89-7"><a href="#cb89-7" aria-hidden="true" tabindex="-1"></a>       <span class="at">col =</span> <span class="fu">c</span>(<span class="st">"lightblue"</span>, <span class="st">"lightpink"</span>),</span>
<span id="cb89-8"><a href="#cb89-8" aria-hidden="true" tabindex="-1"></a>       <span class="at">main =</span> <span class="fu">paste</span>(<span class="st">"Cost ="</span>, cost_val))</span>
<span id="cb89-9"><a href="#cb89-9" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-error">
<pre><code>Error in `plot.new()`:
! figure margins too large</code></pre>
</div>
</div>
</section>
<section id="the-kernel-trick" class="level3">
<h3 class="anchored" data-anchor-id="the-kernel-trick">The Kernel Trick</h3>
<p>For non-linear boundaries, SVM uses <strong>kernels</strong> to implicitly map data to higher dimensions where classes become linearly separable:</p>
<p><strong>Common kernels:</strong></p>
<ul>
<li><strong>Linear</strong>: <span class="math inline">\(K(x, x') = x \cdot x'\)</span> (no transformation)</li>
<li><strong>Polynomial</strong>: <span class="math inline">\(K(x, x') = (1 + x \cdot x')^d\)</span></li>
<li><strong>Radial Basis Function (RBF)</strong>: <span class="math inline">\(K(x, x') = \exp(-\gamma \|x - x'\|^2)\)</span></li>
</ul>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb91"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb91-1"><a href="#cb91-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create non-linear data</span></span>
<span id="cb91-2"><a href="#cb91-2" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb91-3"><a href="#cb91-3" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">200</span></span>
<span id="cb91-4"><a href="#cb91-4" aria-hidden="true" tabindex="-1"></a>r1 <span class="ot">&lt;-</span> <span class="fu">runif</span>(n<span class="sc">/</span><span class="dv">2</span>, <span class="dv">0</span>, <span class="dv">2</span>)</span>
<span id="cb91-5"><a href="#cb91-5" aria-hidden="true" tabindex="-1"></a>theta1 <span class="ot">&lt;-</span> <span class="fu">runif</span>(n<span class="sc">/</span><span class="dv">2</span>, <span class="dv">0</span>, <span class="dv">2</span><span class="sc">*</span>pi)</span>
<span id="cb91-6"><a href="#cb91-6" aria-hidden="true" tabindex="-1"></a>r2 <span class="ot">&lt;-</span> <span class="fu">runif</span>(n<span class="sc">/</span><span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">5</span>)</span>
<span id="cb91-7"><a href="#cb91-7" aria-hidden="true" tabindex="-1"></a>theta2 <span class="ot">&lt;-</span> <span class="fu">runif</span>(n<span class="sc">/</span><span class="dv">2</span>, <span class="dv">0</span>, <span class="dv">2</span><span class="sc">*</span>pi)</span>
<span id="cb91-8"><a href="#cb91-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-9"><a href="#cb91-9" aria-hidden="true" tabindex="-1"></a>x1 <span class="ot">&lt;-</span> <span class="fu">c</span>(r1 <span class="sc">*</span> <span class="fu">cos</span>(theta1), r2 <span class="sc">*</span> <span class="fu">cos</span>(theta2))</span>
<span id="cb91-10"><a href="#cb91-10" aria-hidden="true" tabindex="-1"></a>x2 <span class="ot">&lt;-</span> <span class="fu">c</span>(r1 <span class="sc">*</span> <span class="fu">sin</span>(theta1), r2 <span class="sc">*</span> <span class="fu">sin</span>(theta2))</span>
<span id="cb91-11"><a href="#cb91-11" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> <span class="fu">factor</span>(<span class="fu">c</span>(<span class="fu">rep</span>(<span class="st">"inner"</span>, n<span class="sc">/</span><span class="dv">2</span>), <span class="fu">rep</span>(<span class="st">"outer"</span>, n<span class="sc">/</span><span class="dv">2</span>)))</span>
<span id="cb91-12"><a href="#cb91-12" aria-hidden="true" tabindex="-1"></a>circle_data <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(x1, x2, y)</span>
<span id="cb91-13"><a href="#cb91-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-14"><a href="#cb91-14" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">3</span>))</span>
<span id="cb91-15"><a href="#cb91-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-16"><a href="#cb91-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Linear (fails)</span></span>
<span id="cb91-17"><a href="#cb91-17" aria-hidden="true" tabindex="-1"></a>svm_lin <span class="ot">&lt;-</span> <span class="fu">svm</span>(y <span class="sc">~</span> ., <span class="at">data =</span> circle_data, <span class="at">kernel =</span> <span class="st">"linear"</span>)</span>
<span id="cb91-18"><a href="#cb91-18" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(svm_lin, circle_data, <span class="at">col =</span> <span class="fu">c</span>(<span class="st">"lightblue"</span>, <span class="st">"lightpink"</span>),</span>
<span id="cb91-19"><a href="#cb91-19" aria-hidden="true" tabindex="-1"></a>     <span class="at">main =</span> <span class="st">"Linear Kernel"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-error">
<pre><code>Error in `plot.new()`:
! figure margins too large</code></pre>
</div>
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb93"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb93-1"><a href="#cb93-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Polynomial</span></span>
<span id="cb93-2"><a href="#cb93-2" aria-hidden="true" tabindex="-1"></a>svm_poly <span class="ot">&lt;-</span> <span class="fu">svm</span>(y <span class="sc">~</span> ., <span class="at">data =</span> circle_data, <span class="at">kernel =</span> <span class="st">"polynomial"</span>, <span class="at">degree =</span> <span class="dv">2</span>)</span>
<span id="cb93-3"><a href="#cb93-3" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(svm_poly, circle_data, <span class="at">col =</span> <span class="fu">c</span>(<span class="st">"lightblue"</span>, <span class="st">"lightpink"</span>),</span>
<span id="cb93-4"><a href="#cb93-4" aria-hidden="true" tabindex="-1"></a>     <span class="at">main =</span> <span class="st">"Polynomial Kernel (d=2)"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-error">
<pre><code>Error in `plot.new()`:
! figure margins too large</code></pre>
</div>
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb95"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb95-1"><a href="#cb95-1" aria-hidden="true" tabindex="-1"></a><span class="co"># RBF</span></span>
<span id="cb95-2"><a href="#cb95-2" aria-hidden="true" tabindex="-1"></a>svm_rbf <span class="ot">&lt;-</span> <span class="fu">svm</span>(y <span class="sc">~</span> ., <span class="at">data =</span> circle_data, <span class="at">kernel =</span> <span class="st">"radial"</span>, <span class="at">gamma =</span> <span class="fl">0.5</span>)</span>
<span id="cb95-3"><a href="#cb95-3" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(svm_rbf, circle_data, <span class="at">col =</span> <span class="fu">c</span>(<span class="st">"lightblue"</span>, <span class="st">"lightpink"</span>),</span>
<span id="cb95-4"><a href="#cb95-4" aria-hidden="true" tabindex="-1"></a>     <span class="at">main =</span> <span class="st">"RBF Kernel"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-error">
<pre><code>Error in `plot.new()`:
! figure margins too large</code></pre>
</div>
</div>
</section>
<section id="tuning-svm-with-cross-validation" class="level3">
<h3 class="anchored" data-anchor-id="tuning-svm-with-cross-validation">Tuning SVM with Cross-Validation</h3>
<p>The key parameters to tune are: - <strong>cost</strong>: Penalty for margin violations - <strong>gamma</strong>: For RBF kernel, controls the “reach” of each training example</p>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb97"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb97-1"><a href="#cb97-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Tune SVM using cross-validation</span></span>
<span id="cb97-2"><a href="#cb97-2" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">42</span>)</span>
<span id="cb97-3"><a href="#cb97-3" aria-hidden="true" tabindex="-1"></a>tune_result <span class="ot">&lt;-</span> <span class="fu">tune</span>(svm, Species <span class="sc">~</span> ., <span class="at">data =</span> iris,</span>
<span id="cb97-4"><a href="#cb97-4" aria-hidden="true" tabindex="-1"></a>                     <span class="at">kernel =</span> <span class="st">"radial"</span>,</span>
<span id="cb97-5"><a href="#cb97-5" aria-hidden="true" tabindex="-1"></a>                     <span class="at">ranges =</span> <span class="fu">list</span>(</span>
<span id="cb97-6"><a href="#cb97-6" aria-hidden="true" tabindex="-1"></a>                       <span class="at">cost =</span> <span class="fu">c</span>(<span class="fl">0.1</span>, <span class="dv">1</span>, <span class="dv">10</span>, <span class="dv">100</span>),</span>
<span id="cb97-7"><a href="#cb97-7" aria-hidden="true" tabindex="-1"></a>                       <span class="at">gamma =</span> <span class="fu">c</span>(<span class="fl">0.01</span>, <span class="fl">0.1</span>, <span class="fl">0.5</span>, <span class="dv">1</span>)</span>
<span id="cb97-8"><a href="#cb97-8" aria-hidden="true" tabindex="-1"></a>                     ))</span>
<span id="cb97-9"><a href="#cb97-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb97-10"><a href="#cb97-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Best parameters</span></span>
<span id="cb97-11"><a href="#cb97-11" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"Best parameters:</span><span class="sc">\n</span><span class="st">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Best parameters:</code></pre>
</div>
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb99"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb99-1"><a href="#cb99-1" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(tune_result<span class="sc">$</span>best.parameters)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>  cost gamma
4  100  0.01</code></pre>
</div>
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb101"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb101-1"><a href="#cb101-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Best model performance</span></span>
<span id="cb101-2"><a href="#cb101-2" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"</span><span class="sc">\n</span><span class="st">Best model error:"</span>, <span class="fu">round</span>(tune_result<span class="sc">$</span>best.performance, <span class="dv">3</span>), <span class="st">"</span><span class="sc">\n</span><span class="st">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>
Best model error: 0.027 </code></pre>
</div>
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb103"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb103-1"><a href="#cb103-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Use best model</span></span>
<span id="cb103-2"><a href="#cb103-2" aria-hidden="true" tabindex="-1"></a>best_svm <span class="ot">&lt;-</span> tune_result<span class="sc">$</span>best.model</span>
<span id="cb103-3"><a href="#cb103-3" aria-hidden="true" tabindex="-1"></a><span class="fu">table</span>(<span class="at">Predicted =</span> <span class="fu">predict</span>(best_svm), <span class="at">Actual =</span> iris<span class="sc">$</span>Species)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>            Actual
Predicted    setosa versicolor virginica
  setosa         50          0         0
  versicolor      0         45         0
  virginica       0          5        50</code></pre>
</div>
</div>
</section>
<section id="multiclass-svm" class="level3">
<h3 class="anchored" data-anchor-id="multiclass-svm">Multiclass SVM</h3>
<p>SVM is inherently binary, but extends to multiple classes via:</p>
<ul>
<li><strong>One-vs-One</strong>: Fit <span class="math inline">\(\binom{K}{2}\)</span> classifiers for all pairs of classes; classify by voting</li>
<li><strong>One-vs-All</strong>: Fit <span class="math inline">\(K\)</span> classifiers (each class vs.&nbsp;rest); classify to highest-scoring class</li>
</ul>
<p>R’s <code>svm()</code> uses one-vs-one by default.</p>
</section>
<section id="support-vector-regression-svr" class="level3">
<h3 class="anchored" data-anchor-id="support-vector-regression-svr">Support Vector Regression (SVR)</h3>
<p>SVMs can also be used for regression problems. <strong>Support Vector Regression</strong> works by fitting a tube of width <span class="math inline">\(\epsilon\)</span> around the data—points inside the tube contribute no loss, while points outside are penalized.</p>
<p>The key idea is that instead of minimizing squared errors (like in linear regression), SVR minimizes how much predictions deviate beyond a tolerance margin <span class="math inline">\(\epsilon\)</span>:</p>
<p><span class="math display">\[
L_\epsilon(y, \hat{y}) = \begin{cases} 0 &amp; \text{if } |y - \hat{y}| \leq \epsilon \\ |y - \hat{y}| - \epsilon &amp; \text{otherwise} \end{cases}
\]</span></p>
<p>This is called the <strong><span class="math inline">\(\epsilon\)</span>-insensitive loss function</strong>.</p>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb105"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb105-1"><a href="#cb105-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate non-linear data</span></span>
<span id="cb105-2"><a href="#cb105-2" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">42</span>)</span>
<span id="cb105-3"><a href="#cb105-3" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">100</span></span>
<span id="cb105-4"><a href="#cb105-4" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="dv">0</span>, <span class="dv">4</span><span class="sc">*</span>pi, <span class="at">length.out =</span> n)</span>
<span id="cb105-5"><a href="#cb105-5" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> <span class="fu">sin</span>(x) <span class="sc">+</span> <span class="fu">rnorm</span>(n, <span class="at">sd =</span> <span class="fl">0.3</span>)</span>
<span id="cb105-6"><a href="#cb105-6" aria-hidden="true" tabindex="-1"></a>svr_data <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">x =</span> x, <span class="at">y =</span> y)</span>
<span id="cb105-7"><a href="#cb105-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb105-8"><a href="#cb105-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit SVR with RBF kernel</span></span>
<span id="cb105-9"><a href="#cb105-9" aria-hidden="true" tabindex="-1"></a>svr_model <span class="ot">&lt;-</span> <span class="fu">svm</span>(y <span class="sc">~</span> x, <span class="at">data =</span> svr_data, <span class="at">kernel =</span> <span class="st">"radial"</span>,</span>
<span id="cb105-10"><a href="#cb105-10" aria-hidden="true" tabindex="-1"></a>                  <span class="at">epsilon =</span> <span class="fl">0.3</span>, <span class="at">cost =</span> <span class="dv">10</span>)</span>
<span id="cb105-11"><a href="#cb105-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb105-12"><a href="#cb105-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Predictions</span></span>
<span id="cb105-13"><a href="#cb105-13" aria-hidden="true" tabindex="-1"></a>svr_data<span class="sc">$</span>pred <span class="ot">&lt;-</span> <span class="fu">predict</span>(svr_model, svr_data)</span>
<span id="cb105-14"><a href="#cb105-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb105-15"><a href="#cb105-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot</span></span>
<span id="cb105-16"><a href="#cb105-16" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(x, y, <span class="at">pch =</span> <span class="dv">16</span>, <span class="at">col =</span> <span class="st">"gray60"</span>, <span class="at">main =</span> <span class="st">"Support Vector Regression"</span>)</span>
<span id="cb105-17"><a href="#cb105-17" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(x, svr_data<span class="sc">$</span>pred, <span class="at">col =</span> <span class="st">"blue"</span>, <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb105-18"><a href="#cb105-18" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(x, <span class="fu">sin</span>(x), <span class="at">col =</span> <span class="st">"red"</span>, <span class="at">lwd =</span> <span class="dv">2</span>, <span class="at">lty =</span> <span class="dv">2</span>)</span>
<span id="cb105-19"><a href="#cb105-19" aria-hidden="true" tabindex="-1"></a><span class="fu">legend</span>(<span class="st">"topright"</span>, <span class="fu">c</span>(<span class="st">"SVR fit"</span>, <span class="st">"True function"</span>),</span>
<span id="cb105-20"><a href="#cb105-20" aria-hidden="true" tabindex="-1"></a>       <span class="at">col =</span> <span class="fu">c</span>(<span class="st">"blue"</span>, <span class="st">"red"</span>), <span class="at">lty =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">2</span>), <span class="at">lwd =</span> <span class="dv">2</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div id="fig-svr" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-svr-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="22-statistical-learning_files/figure-html/fig-svr-1.png" class="img-fluid figure-img" width="768">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-svr-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;29.35: Support Vector Regression fits a tube around the data. Points within the tube (width epsilon) have zero loss.
</figcaption>
</figure>
</div>
</div>
</div>
<p>Key SVR parameters:</p>
<ul>
<li><strong>epsilon</strong>: Width of the insensitive tube. Larger values give smoother fits.</li>
<li><strong>cost (C)</strong>: Penalty for points outside the tube. Higher C fits the data more closely.</li>
<li><strong>kernel</strong>: As with classification, RBF kernels can capture non-linear patterns.</li>
</ul>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb106"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb106-1"><a href="#cb106-1" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">3</span>))</span>
<span id="cb106-2"><a href="#cb106-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb106-3"><a href="#cb106-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (eps <span class="cf">in</span> <span class="fu">c</span>(<span class="fl">0.1</span>, <span class="fl">0.3</span>, <span class="fl">0.5</span>)) {</span>
<span id="cb106-4"><a href="#cb106-4" aria-hidden="true" tabindex="-1"></a>  svr_fit <span class="ot">&lt;-</span> <span class="fu">svm</span>(y <span class="sc">~</span> x, <span class="at">data =</span> svr_data, <span class="at">kernel =</span> <span class="st">"radial"</span>,</span>
<span id="cb106-5"><a href="#cb106-5" aria-hidden="true" tabindex="-1"></a>                  <span class="at">epsilon =</span> eps, <span class="at">cost =</span> <span class="dv">10</span>)</span>
<span id="cb106-6"><a href="#cb106-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">plot</span>(x, y, <span class="at">pch =</span> <span class="dv">16</span>, <span class="at">col =</span> <span class="st">"gray60"</span>, <span class="at">main =</span> <span class="fu">paste</span>(<span class="st">"epsilon ="</span>, eps))</span>
<span id="cb106-7"><a href="#cb106-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">lines</span>(x, <span class="fu">predict</span>(svr_fit), <span class="at">col =</span> <span class="st">"blue"</span>, <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb106-8"><a href="#cb106-8" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div id="fig-svr-epsilon" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-svr-epsilon-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="22-statistical-learning_files/figure-html/fig-svr-epsilon-1.png" class="img-fluid figure-img" width="864">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-svr-epsilon-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;29.36: Effect of epsilon on SVR: larger epsilon creates wider tubes and smoother fits
</figcaption>
</figure>
</div>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
SVM vs.&nbsp;Other Methods
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Advantages of SVM:</strong> - Effective in high-dimensional spaces (even when dimensions &gt; samples) - Memory efficient (uses only support vectors) - Versatile through different kernels</p>
<p><strong>Disadvantages:</strong> - Doesn’t provide probability estimates directly (though they can be computed) - Sensitive to feature scaling—always standardize! - Can be slow on very large datasets - Kernel and parameter selection can be tricky</p>
</div>
</div>
</section>
</section>
<section id="comparing-classification-methods" class="level2" data-number="29.18">
<h2 data-number="29.18" class="anchored" data-anchor-id="comparing-classification-methods"><span class="header-section-number">29.18</span> Comparing Classification Methods</h2>
<p>Different methods have different strengths:</p>
<table class="caption-top table">
<colgroup>
<col style="width: 12%">
<col style="width: 27%">
<col style="width: 34%">
<col style="width: 10%">
<col style="width: 15%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Method</th>
<th style="text-align: left;">Interpretability</th>
<th style="text-align: left;">Handles Non-linearity</th>
<th style="text-align: left;">Speed</th>
<th style="text-align: left;">Best For</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">kNN</td>
<td style="text-align: left;">Low</td>
<td style="text-align: left;">Yes (inherently)</td>
<td style="text-align: left;">Slow for large data</td>
<td style="text-align: left;">Simple problems, few features</td>
</tr>
<tr class="even">
<td style="text-align: left;">Decision Tree</td>
<td style="text-align: left;">High</td>
<td style="text-align: left;">Yes</td>
<td style="text-align: left;">Fast</td>
<td style="text-align: left;">Interpretability needed</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Random Forest</td>
<td style="text-align: left;">Medium</td>
<td style="text-align: left;">Yes</td>
<td style="text-align: left;">Moderate</td>
<td style="text-align: left;">General purpose, variable importance</td>
</tr>
<tr class="even">
<td style="text-align: left;">SVM</td>
<td style="text-align: left;">Low</td>
<td style="text-align: left;">Yes (with kernels)</td>
<td style="text-align: left;">Moderate</td>
<td style="text-align: left;">High-dimensional data</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Logistic Regression</td>
<td style="text-align: left;">High</td>
<td style="text-align: left;">No (needs feature engineering)</td>
<td style="text-align: left;">Fast</td>
<td style="text-align: left;">Probability estimates, inference</td>
</tr>
</tbody>
</table>
</section>
<section id="practical-workflow" class="level2" data-number="29.19">
<h2 data-number="29.19" class="anchored" data-anchor-id="practical-workflow"><span class="header-section-number">29.19</span> Practical Workflow</h2>
<p>A typical statistical learning workflow:</p>
<ol type="1">
<li><strong>Split data</strong> into training and test sets</li>
<li><strong>Explore</strong> the training data</li>
<li><strong>Build candidate models</strong> with different algorithms or parameters</li>
<li><strong>Evaluate</strong> using cross-validation on training data</li>
<li><strong>Select</strong> the best model</li>
<li><strong>Final evaluation</strong> on held-out test data</li>
<li><strong>Report</strong> honest estimates of performance</li>
</ol>
<p>Never use test data for model building or selection—that defeats the purpose of holding it out.</p>
</section>
<section id="when-to-use-statistical-learning" class="level2" data-number="29.20">
<h2 data-number="29.20" class="anchored" data-anchor-id="when-to-use-statistical-learning"><span class="header-section-number">29.20</span> When to Use Statistical Learning</h2>
<p>Statistical learning excels when: - Prediction is the primary goal - Relationships are complex or non-linear - You have substantial data - Interpretability is less critical</p>
<p>Traditional statistical methods may be preferable when: - Understanding relationships matters more than prediction - Sample sizes are small - You need confidence intervals and hypothesis tests - Interpretability is essential</p>
</section>
<section id="connection-to-dimensionality-reduction" class="level2" data-number="29.21">
<h2 data-number="29.21" class="anchored" data-anchor-id="connection-to-dimensionality-reduction"><span class="header-section-number">29.21</span> Connection to Dimensionality Reduction</h2>
<p>High-dimensional data often benefit from dimensionality reduction before applying statistical learning methods. Techniques like PCA, clustering, and discriminant analysis are covered in detail in <a href="23-dimensionality-reduction.html" class="quarto-xref"><span>Chapter 33</span></a>.</p>
</section>
<section id="exercises" class="level2" data-number="29.22">
<h2 data-number="29.22" class="anchored" data-anchor-id="exercises"><span class="header-section-number">29.22</span> Exercises</h2>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Exercise SL.1: Decision Trees and Random Forests
</div>
</div>
<div class="callout-body-container callout-body">
<ol type="1">
<li>Create a simple dataset where the outcome grows 0.75 units on average for every increase in a predictor:</li>
</ol>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb107"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb107-1"><a href="#cb107-1" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">1000</span></span>
<span id="cb107-2"><a href="#cb107-2" aria-hidden="true" tabindex="-1"></a>sigma <span class="ot">&lt;-</span> <span class="fl">0.25</span></span>
<span id="cb107-3"><a href="#cb107-3" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(n, <span class="dv">0</span>, <span class="dv">1</span>)</span>
<span id="cb107-4"><a href="#cb107-4" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> <span class="fl">0.75</span> <span class="sc">*</span> x <span class="sc">+</span> <span class="fu">rnorm</span>(n, <span class="dv">0</span>, sigma)</span>
<span id="cb107-5"><a href="#cb107-5" aria-hidden="true" tabindex="-1"></a>dat <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">x =</span> x, <span class="at">y =</span> y)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>Use <code>rpart</code> to fit a regression tree and save the result to <code>fit</code>.</p>
<ol start="2" type="1">
<li><p>Plot the final tree so that you can see where the partitions occurred.</p></li>
<li><p>Make a scatterplot of <code>y</code> versus <code>x</code> along with the predicted values based on the fit.</p></li>
<li><p>Now model with a random forest instead of a regression tree using <code>randomForest</code> from the <strong>randomForest</strong> package, and remake the scatterplot with the prediction line.</p></li>
<li><p>Use the function <code>plot</code> to see if the random forest has converged or if we need more trees.</p></li>
<li><p>It seems that the default values for the random forest result in an estimate that is too flexible (not smooth). Re-run the random forest but this time with <code>nodesize</code> set at 50 and <code>maxnodes</code> set at 25. Remake the plot.</p></li>
<li><p>We see that this yields smoother results. Let’s use the <code>train</code> function to help us pick these values. From the <strong>caret</strong> manual we see that we can’t tune the <code>maxnodes</code> parameter or the <code>nodesize</code> argument with <code>randomForest</code>, so we will use the <strong>Rborist</strong> package and tune the <code>minNode</code> argument. Use the <code>train</code> function to try values <code>minNode &lt;- seq(5, 250, 25)</code>. See which value minimizes the estimated RMSE.</p></li>
<li><p>Make a scatterplot along with the prediction from the best fitted model.</p></li>
</ol>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Exercise SL.2: Classification Trees
</div>
</div>
<div class="callout-body-container callout-body">
<ol start="9" type="1">
<li>Use the <code>rpart</code> function to fit a classification tree to the <code>tissue_gene_expression</code> dataset. Use the <code>train</code> function to estimate the accuracy. Try out <code>cp</code> values of <code>seq(0, 0.05, 0.01)</code>. Plot the accuracy to report the results of the best model.</li>
</ol>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb108"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb108-1"><a href="#cb108-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(dslabs)</span>
<span id="cb108-2"><a href="#cb108-2" aria-hidden="true" tabindex="-1"></a><span class="fu">data</span>(<span class="st">"tissue_gene_expression"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<ol start="10" type="1">
<li><p>Study the confusion matrix for the best fitting classification tree. What do you observe happening for placenta?</p></li>
<li><p>Notice that placentas are called endometrium more often than placenta. Note also that the number of placentas is just six, and that, by default, <code>rpart</code> requires 20 observations before splitting a node. Thus it is not possible with these parameters to have a node in which placentas are the majority. Rerun the above analysis but this time permit <code>rpart</code> to split any node by using the argument <code>control = rpart.control(minsplit = 0)</code>. Does the accuracy increase? Look at the confusion matrix again.</p></li>
<li><p>Plot the tree from the best fitting model obtained in exercise 11.</p></li>
<li><p>We can see that with just six genes, we are able to predict the tissue type. Now let’s see if we can do even better with a random forest. Use the <code>train</code> function and the <code>rf</code> method to train a random forest. Try out values of <code>mtry</code> ranging from, at least, <code>seq(50, 200, 25)</code>. What <code>mtry</code> value maximizes accuracy? To permit small <code>nodesize</code> to grow as we did with the classification trees, use the following argument: <code>nodesize = 1</code>. This will take several seconds to run. If you want to test it out, try using smaller values with <code>ntree</code>. Set the seed to 1990.</p></li>
<li><p>Use the function <code>varImp</code> on the output of <code>train</code> and save it to an object called <code>imp</code>.</p></li>
<li><p>The <code>rpart</code> model we ran above produced a tree that used just six predictors. Extracting the predictor names is not straightforward, but can be done. If the output of the call to train was <code>fit_rpart</code>, we can extract the names like this:</p></li>
</ol>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb109"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb109-1"><a href="#cb109-1" aria-hidden="true" tabindex="-1"></a>ind <span class="ot">&lt;-</span> <span class="sc">!</span>(fit_rpart<span class="sc">$</span>finalModel<span class="sc">$</span>frame<span class="sc">$</span>var <span class="sc">==</span> <span class="st">"&lt;leaf&gt;"</span>)</span>
<span id="cb109-2"><a href="#cb109-2" aria-hidden="true" tabindex="-1"></a>tree_terms <span class="ot">&lt;-</span></span>
<span id="cb109-3"><a href="#cb109-3" aria-hidden="true" tabindex="-1"></a>  fit_rpart<span class="sc">$</span>finalModel<span class="sc">$</span>frame<span class="sc">$</span>var[ind] <span class="sc">%&gt;%</span></span>
<span id="cb109-4"><a href="#cb109-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">unique</span>() <span class="sc">%&gt;%</span></span>
<span id="cb109-5"><a href="#cb109-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">as.character</span>()</span>
<span id="cb109-6"><a href="#cb109-6" aria-hidden="true" tabindex="-1"></a>tree_terms</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>What is the variable importance in the random forest call for these predictors? Where do they rank?</p>
<ol start="16" type="1">
<li>Advanced: Extract the top 50 predictors based on importance, take a subset of <code>x</code> with just these predictors and apply the function <code>heatmap</code> to see how these genes behave across the tissues. We will introduce the <code>heatmap</code> function in <a href="33-clustering.html" class="quarto-xref"><span>Chapter 32</span></a>.</li>
</ol>
</div>
</div>
</section>
<section id="summary" class="level2" data-number="29.23">
<h2 data-number="29.23" class="anchored" data-anchor-id="summary"><span class="header-section-number">29.23</span> Summary</h2>
<p>Statistical learning provides powerful tools for prediction and pattern discovery:</p>
<ul>
<li><strong>Overfitting</strong> is the central challenge—models that fit training data too well predict poorly</li>
<li><strong>Loss functions</strong> quantify prediction error (squared loss for regression, log loss for classification)</li>
<li><strong>Cross-validation</strong> provides honest estimates of predictive performance
<ul>
<li>Training error is always optimistic; test error reveals true performance</li>
<li>K-fold CV and bootstrap estimate generalization error</li>
</ul></li>
<li>The <strong>bias-variance tradeoff</strong> governs model complexity choices</li>
<li><strong>Regularization</strong> (ridge, lasso, elastic net) controls overfitting by penalizing model complexity
<ul>
<li>Ridge shrinks coefficients but keeps all predictors</li>
<li>Lasso performs variable selection by shrinking some coefficients to zero</li>
<li>Cross-validation selects the optimal regularization strength</li>
</ul></li>
<li><strong>Smoothing methods</strong> estimate flexible curves from data
<ul>
<li>Bin smoothing divides data into intervals</li>
<li>Kernel smoothing uses weighted averages for continuous estimates</li>
<li>Splines fit piecewise polynomials with controlled smoothness</li>
<li>LOESS fits local regressions weighted by distance</li>
</ul></li>
<li><strong>K-nearest neighbors</strong> illustrates how hyperparameters control model complexity</li>
<li><strong>Decision trees (CART)</strong> recursively partition data using simple rules
<ul>
<li>Highly interpretable but prone to overfitting</li>
<li>Controlled via complexity parameters and pruning</li>
</ul></li>
<li><strong>Random forests</strong> combine many trees for robust predictions
<ul>
<li>Bagging and random feature selection reduce variance</li>
<li>Out-of-bag error provides built-in validation</li>
<li>Variable importance measures identify key predictors</li>
</ul></li>
<li><strong>Support vector machines</strong> find maximum-margin decision boundaries
<ul>
<li>Kernel trick enables non-linear classification</li>
<li>Support vector regression (SVR) extends to continuous outcomes</li>
<li>Effective in high-dimensional spaces</li>
</ul></li>
<li><strong>Confusion matrices</strong> summarize classification performance with metrics like accuracy, sensitivity, and precision</li>
<li><strong>F1 score and balanced accuracy</strong> are better metrics for imbalanced data</li>
<li><strong>ROC curves and AUC</strong> allow comparison of classifiers across all thresholds</li>
<li><strong>Precision-recall curves</strong> are preferred for highly imbalanced problems</li>
<li>The choice between traditional statistics and machine learning depends on goals</li>
</ul>
</section>
<section id="additional-resources" class="level2" data-number="29.24">
<h2 data-number="29.24" class="anchored" data-anchor-id="additional-resources"><span class="header-section-number">29.24</span> Additional Resources</h2>
<ul>
<li><span class="citation" data-cites="james2023islr">James et al. (<a href="../references.html#ref-james2023islr" role="doc-biblioref">2023</a>)</span> - The standard introduction to statistical learning</li>
<li><span class="citation" data-cites="thulin2025msr">Thulin (<a href="../references.html#ref-thulin2025msr" role="doc-biblioref">2025</a>)</span> - Modern perspectives on statistics with R</li>
<li><span class="citation" data-cites="crawley2007r">Crawley (<a href="../references.html#ref-crawley2007r" role="doc-biblioref">2007</a>)</span> - Practical statistical methods in R</li>
</ul>


<!-- -->

<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list" style="display: none">
<div id="ref-breiman2001random" class="csl-entry" role="listitem">
Breiman, Leo. 2001. <span>“Random Forests.”</span> <em>Machine Learning</em> 45 (1): 5–32.
</div>
<div id="ref-cleveland1979robust" class="csl-entry" role="listitem">
Cleveland, William S. 1979. <span>“Robust Locally Weighted Regression and Smoothing Scatterplots.”</span> <em>Journal of the American Statistical Association</em> 74 (368): 829–36.
</div>
<div id="ref-cortes1995support" class="csl-entry" role="listitem">
Cortes, Corinna, and Vladimir Vapnik. 1995. <span>“Support-Vector Networks.”</span> <em>Machine Learning</em> 20 (3): 273–97.
</div>
<div id="ref-crawley2007r" class="csl-entry" role="listitem">
Crawley, Michael J. 2007. <em>The r Book</em>. John Wiley &amp; Sons.
</div>
<div id="ref-hastie2009elements" class="csl-entry" role="listitem">
Hastie, Trevor, Robert Tibshirani, and Jerome Friedman. 2009. <em>The Elements of Statistical Learning: Data Mining, Inference, and Prediction</em>. 2nd ed. New York: Springer.
</div>
<div id="ref-hoerl1970ridge" class="csl-entry" role="listitem">
Hoerl, Arthur E., and Robert W. Kennard. 1970. <span>“Ridge Regression: Biased Estimation for Nonorthogonal Problems.”</span> <em>Technometrics</em> 12 (1): 55–67.
</div>
<div id="ref-james2023islr" class="csl-entry" role="listitem">
James, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2023. <em>An Introduction to Statistical Learning with Applications in r</em>. 2nd ed. Springer. <a href="https://www.statlearning.com">https://www.statlearning.com</a>.
</div>
<div id="ref-thulin2025msr" class="csl-entry" role="listitem">
Thulin, Måns. 2025. <em>Modern Statistics with r</em>. CRC Press. <a href="https://www.modernstatisticswithr.com">https://www.modernstatisticswithr.com</a>.
</div>
<div id="ref-tibshirani1996regression" class="csl-entry" role="listitem">
Tibshirani, Robert. 1996. <span>“Regression Shrinkage and Selection via the Lasso.”</span> <em>Journal of the Royal Statistical Society: Series B (Methodological)</em> 58 (1): 267–88.
</div>
</div>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
    const viewSource = window.document.getElementById('quarto-view-source') ||
                       window.document.getElementById('quarto-code-tools-source');
    if (viewSource) {
      const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
      viewSource.addEventListener("click", function(e) {
        if (sourceUrl) {
          // rstudio viewer pane
          if (/\bcapabilities=\b/.test(window.location)) {
            window.open(sourceUrl);
          } else {
            window.location.href = sourceUrl;
          }
        } else {
          const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
          modal.show();
        }
        return false;
      });
    }
    function toggleCodeHandler(show) {
      return function(e) {
        const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
        for (let i=0; i<detailsSrc.length; i++) {
          const details = detailsSrc[i].parentElement;
          if (show) {
            details.open = true;
          } else {
            details.removeAttribute("open");
          }
        }
        const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
        const fromCls = show ? "hidden" : "unhidden";
        const toCls = show ? "unhidden" : "hidden";
        for (let i=0; i<cellCodeDivs.length; i++) {
          const codeDiv = cellCodeDivs[i];
          if (codeDiv.classList.contains(fromCls)) {
            codeDiv.classList.remove(fromCls);
            codeDiv.classList.add(toCls);
          } 
        }
        return false;
      }
    }
    const hideAllCode = window.document.getElementById("quarto-hide-all-code");
    if (hideAllCode) {
      hideAllCode.addEventListener("click", toggleCodeHandler(false));
    }
    const showAllCode = window.document.getElementById("quarto-show-all-code");
    if (showAllCode) {
      showAllCode.addEventListener("click", toggleCodeHandler(true));
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../chapters/21-glm.html" class="pagination-link" aria-label="Generalized Linear Models">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">28</span>&nbsp; <span class="chapter-title">Generalized Linear Models</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../chapters/31-bayesian-statistics.html" class="pagination-link" aria-label="Bayesian Statistics">
        <span class="nav-page-text"><span class="chapter-number">30</span>&nbsp; <span class="chapter-title">Bayesian Statistics</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb110" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb110-1"><a href="#cb110-1" aria-hidden="true" tabindex="-1"></a><span class="fu"># Core Concepts in Statistical Learning {#sec-statistical-learning}</span></span>
<span id="cb110-2"><a href="#cb110-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-5"><a href="#cb110-5" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb110-6"><a href="#cb110-6" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: false</span></span>
<span id="cb110-7"><a href="#cb110-7" aria-hidden="true" tabindex="-1"></a><span class="co">#| message: false</span></span>
<span id="cb110-8"><a href="#cb110-8" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tidyverse)</span>
<span id="cb110-9"><a href="#cb110-9" aria-hidden="true" tabindex="-1"></a><span class="fu">theme_set</span>(<span class="fu">theme_minimal</span>())</span>
<span id="cb110-10"><a href="#cb110-10" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb110-11"><a href="#cb110-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-12"><a href="#cb110-12" aria-hidden="true" tabindex="-1"></a><span class="fu">## From Inference to Prediction</span></span>
<span id="cb110-13"><a href="#cb110-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-14"><a href="#cb110-14" aria-hidden="true" tabindex="-1"></a>Traditional statistics emphasizes inference—understanding relationships, testing hypotheses, and quantifying uncertainty. Statistical learning (or machine learning) shifts focus toward prediction—building models that accurately predict outcomes for new data <span class="co">[</span><span class="ot">@hastie2009elements</span><span class="co">]</span>.</span>
<span id="cb110-15"><a href="#cb110-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-16"><a href="#cb110-16" aria-hidden="true" tabindex="-1"></a>Both approaches use similar mathematical tools, but the goals differ. In inference, we want to understand the true relationship between variables. In prediction, we want accurate predictions, even if the model does not perfectly capture the underlying mechanism.</span>
<span id="cb110-17"><a href="#cb110-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-18"><a href="#cb110-18" aria-hidden="true" tabindex="-1"></a><span class="fu">## The Overfitting Problem</span></span>
<span id="cb110-19"><a href="#cb110-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-20"><a href="#cb110-20" aria-hidden="true" tabindex="-1"></a>Models are built to fit training data as closely as possible. A linear regression minimizes squared errors; a logistic regression maximizes likelihood. But models that fit training data too well often predict poorly on new data.</span>
<span id="cb110-21"><a href="#cb110-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-22"><a href="#cb110-22" aria-hidden="true" tabindex="-1"></a>**Overfitting** occurs when a model captures noise specific to the training data rather than the true underlying pattern. Complex models with many parameters are especially susceptible.</span>
<span id="cb110-23"><a href="#cb110-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-24"><a href="#cb110-24" aria-hidden="true" tabindex="-1"></a>The solution is to evaluate models on data they have not seen—held-out test data or through cross-validation.</span>
<span id="cb110-25"><a href="#cb110-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-26"><a href="#cb110-26" aria-hidden="true" tabindex="-1"></a><span class="fu">## Loss Functions: Quantifying Prediction Error</span></span>
<span id="cb110-27"><a href="#cb110-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-28"><a href="#cb110-28" aria-hidden="true" tabindex="-1"></a>A **loss function** (or **cost function**) measures how wrong a prediction is. It quantifies the penalty for predicting $\hat{y}$ when the true value is $y$.</span>
<span id="cb110-29"><a href="#cb110-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-30"><a href="#cb110-30" aria-hidden="true" tabindex="-1"></a><span class="fu">### Common Loss Functions for Regression</span></span>
<span id="cb110-31"><a href="#cb110-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-32"><a href="#cb110-32" aria-hidden="true" tabindex="-1"></a>**Squared Error Loss** (L2): The most common loss for continuous outcomes:</span>
<span id="cb110-33"><a href="#cb110-33" aria-hidden="true" tabindex="-1"></a>$$L(y, \hat{y}) = (y - \hat{y})^2$$</span>
<span id="cb110-34"><a href="#cb110-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-35"><a href="#cb110-35" aria-hidden="true" tabindex="-1"></a>Squaring penalizes large errors more heavily than small ones. Linear regression minimizes the sum of squared errors (SSE or RSS).</span>
<span id="cb110-36"><a href="#cb110-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-37"><a href="#cb110-37" aria-hidden="true" tabindex="-1"></a>**Absolute Error Loss** (L1): Less sensitive to outliers:</span>
<span id="cb110-38"><a href="#cb110-38" aria-hidden="true" tabindex="-1"></a>$$L(y, \hat{y}) = |y - \hat{y}|$$</span>
<span id="cb110-39"><a href="#cb110-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-40"><a href="#cb110-40" aria-hidden="true" tabindex="-1"></a>**Mean Squared Error (MSE)** and **Root Mean Squared Error (RMSE)** are averages across all predictions:</span>
<span id="cb110-41"><a href="#cb110-41" aria-hidden="true" tabindex="-1"></a>$$\text{MSE} = \frac{1}{n}\sum_{i=1}^n (y_i - \hat{y}_i)^2, \quad \text{RMSE} = \sqrt{\text{MSE}}$$</span>
<span id="cb110-42"><a href="#cb110-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-43"><a href="#cb110-43" aria-hidden="true" tabindex="-1"></a><span class="fu">### Common Loss Functions for Classification</span></span>
<span id="cb110-44"><a href="#cb110-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-45"><a href="#cb110-45" aria-hidden="true" tabindex="-1"></a>**0-1 Loss**: The simplest classification loss—1 if wrong, 0 if correct:</span>
<span id="cb110-46"><a href="#cb110-46" aria-hidden="true" tabindex="-1"></a>$$L(y, \hat{y}) = \mathbb{I}(y \neq \hat{y})$$</span>
<span id="cb110-47"><a href="#cb110-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-48"><a href="#cb110-48" aria-hidden="true" tabindex="-1"></a>The average 0-1 loss is the **error rate**; one minus the error rate is **accuracy**.</span>
<span id="cb110-49"><a href="#cb110-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-50"><a href="#cb110-50" aria-hidden="true" tabindex="-1"></a>**Log Loss** (Cross-Entropy): Used when we have predicted probabilities $\hat{p}$:</span>
<span id="cb110-51"><a href="#cb110-51" aria-hidden="true" tabindex="-1"></a>$$L(y, \hat{p}) = -<span class="co">[</span><span class="ot">y \log(\hat{p}) + (1-y) \log(1-\hat{p})</span><span class="co">]</span>$$</span>
<span id="cb110-52"><a href="#cb110-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-53"><a href="#cb110-53" aria-hidden="true" tabindex="-1"></a>Log loss penalizes confident wrong predictions severely—predicting probability 0.99 for the wrong class incurs much larger loss than predicting 0.6.</span>
<span id="cb110-54"><a href="#cb110-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-57"><a href="#cb110-57" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb110-58"><a href="#cb110-58" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-loss-functions</span></span>
<span id="cb110-59"><a href="#cb110-59" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "Comparison of squared loss (penalizes large errors heavily) versus absolute loss (more robust to outliers)"</span></span>
<span id="cb110-60"><a href="#cb110-60" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-width: 8</span></span>
<span id="cb110-61"><a href="#cb110-61" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-height: 4</span></span>
<span id="cb110-62"><a href="#cb110-62" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">2</span>))</span>
<span id="cb110-63"><a href="#cb110-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-64"><a href="#cb110-64" aria-hidden="true" tabindex="-1"></a><span class="co"># Regression loss functions</span></span>
<span id="cb110-65"><a href="#cb110-65" aria-hidden="true" tabindex="-1"></a>errors <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="sc">-</span><span class="dv">3</span>, <span class="dv">3</span>, <span class="at">length.out =</span> <span class="dv">100</span>)</span>
<span id="cb110-66"><a href="#cb110-66" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(errors, errors<span class="sc">^</span><span class="dv">2</span>, <span class="at">type =</span> <span class="st">"l"</span>, <span class="at">col =</span> <span class="st">"blue"</span>, <span class="at">lwd =</span> <span class="dv">2</span>,</span>
<span id="cb110-67"><a href="#cb110-67" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">"Prediction Error (y - ŷ)"</span>, <span class="at">ylab =</span> <span class="st">"Loss"</span>,</span>
<span id="cb110-68"><a href="#cb110-68" aria-hidden="true" tabindex="-1"></a>     <span class="at">main =</span> <span class="st">"Regression Loss Functions"</span>)</span>
<span id="cb110-69"><a href="#cb110-69" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(errors, <span class="fu">abs</span>(errors), <span class="at">col =</span> <span class="st">"red"</span>, <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb110-70"><a href="#cb110-70" aria-hidden="true" tabindex="-1"></a><span class="fu">legend</span>(<span class="st">"top"</span>, <span class="fu">c</span>(<span class="st">"Squared (L2)"</span>, <span class="st">"Absolute (L1)"</span>),</span>
<span id="cb110-71"><a href="#cb110-71" aria-hidden="true" tabindex="-1"></a>       <span class="at">col =</span> <span class="fu">c</span>(<span class="st">"blue"</span>, <span class="st">"red"</span>), <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb110-72"><a href="#cb110-72" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-73"><a href="#cb110-73" aria-hidden="true" tabindex="-1"></a><span class="co"># Classification log loss</span></span>
<span id="cb110-74"><a href="#cb110-74" aria-hidden="true" tabindex="-1"></a>p <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="fl">0.01</span>, <span class="fl">0.99</span>, <span class="at">length.out =</span> <span class="dv">100</span>)</span>
<span id="cb110-75"><a href="#cb110-75" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(p, <span class="sc">-</span><span class="fu">log</span>(p), <span class="at">type =</span> <span class="st">"l"</span>, <span class="at">col =</span> <span class="st">"blue"</span>, <span class="at">lwd =</span> <span class="dv">2</span>,</span>
<span id="cb110-76"><a href="#cb110-76" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">"Predicted Probability for True Class"</span>, <span class="at">ylab =</span> <span class="st">"Log Loss"</span>,</span>
<span id="cb110-77"><a href="#cb110-77" aria-hidden="true" tabindex="-1"></a>     <span class="at">main =</span> <span class="st">"Classification Log Loss"</span>)</span>
<span id="cb110-78"><a href="#cb110-78" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">v =</span> <span class="fl">0.5</span>, <span class="at">lty =</span> <span class="dv">2</span>, <span class="at">col =</span> <span class="st">"gray"</span>)</span>
<span id="cb110-79"><a href="#cb110-79" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb110-80"><a href="#cb110-80" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-81"><a href="#cb110-81" aria-hidden="true" tabindex="-1"></a><span class="fu">### Why Loss Functions Matter</span></span>
<span id="cb110-82"><a href="#cb110-82" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-83"><a href="#cb110-83" aria-hidden="true" tabindex="-1"></a>Different loss functions lead to different optimal predictions:</span>
<span id="cb110-84"><a href="#cb110-84" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-85"><a href="#cb110-85" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Squared loss** → optimal prediction is the **mean**</span>
<span id="cb110-86"><a href="#cb110-86" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Absolute loss** → optimal prediction is the **median**</span>
<span id="cb110-87"><a href="#cb110-87" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**0-1 loss** → optimal prediction is the **mode** (most frequent class)</span>
<span id="cb110-88"><a href="#cb110-88" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-89"><a href="#cb110-89" aria-hidden="true" tabindex="-1"></a>The choice of loss function should reflect how errors affect your application. Medical diagnosis may weight false negatives (missed disease) more heavily than false positives.</span>
<span id="cb110-90"><a href="#cb110-90" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-91"><a href="#cb110-91" aria-hidden="true" tabindex="-1"></a><span class="fu">## Cross-Validation</span></span>
<span id="cb110-92"><a href="#cb110-92" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-93"><a href="#cb110-93" aria-hidden="true" tabindex="-1"></a><span class="fu">### The Problem: Training Error vs. Test Error</span></span>
<span id="cb110-94"><a href="#cb110-94" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-95"><a href="#cb110-95" aria-hidden="true" tabindex="-1"></a>A fundamental insight of statistical learning is that **training error** (how well we fit the data used to build the model) is an overly optimistic estimate of **test error** (how well we predict new data).</span>
<span id="cb110-96"><a href="#cb110-96" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-99"><a href="#cb110-99" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb110-100"><a href="#cb110-100" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-train-test-error</span></span>
<span id="cb110-101"><a href="#cb110-101" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "Training error always decreases with model complexity, but test error eventually increases due to overfitting. The optimal model minimizes test error."</span></span>
<span id="cb110-102"><a href="#cb110-102" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-width: 8</span></span>
<span id="cb110-103"><a href="#cb110-103" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-height: 5</span></span>
<span id="cb110-104"><a href="#cb110-104" aria-hidden="true" tabindex="-1"></a><span class="co"># Demonstrate training vs test error</span></span>
<span id="cb110-105"><a href="#cb110-105" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">42</span>)</span>
<span id="cb110-106"><a href="#cb110-106" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">100</span></span>
<span id="cb110-107"><a href="#cb110-107" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">sort</span>(<span class="fu">runif</span>(n, <span class="dv">0</span>, <span class="dv">10</span>))</span>
<span id="cb110-108"><a href="#cb110-108" aria-hidden="true" tabindex="-1"></a>y_true <span class="ot">&lt;-</span> <span class="fu">sin</span>(x) <span class="sc">+</span> <span class="fl">0.5</span> <span class="sc">*</span> <span class="fu">cos</span>(<span class="fl">0.5</span> <span class="sc">*</span> x)</span>
<span id="cb110-109"><a href="#cb110-109" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> y_true <span class="sc">+</span> <span class="fu">rnorm</span>(n, <span class="at">sd =</span> <span class="fl">0.3</span>)</span>
<span id="cb110-110"><a href="#cb110-110" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-111"><a href="#cb110-111" aria-hidden="true" tabindex="-1"></a><span class="co"># Split into training and test</span></span>
<span id="cb110-112"><a href="#cb110-112" aria-hidden="true" tabindex="-1"></a>train_idx <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="dv">1</span><span class="sc">:</span>n, <span class="dv">70</span>)</span>
<span id="cb110-113"><a href="#cb110-113" aria-hidden="true" tabindex="-1"></a>train_data <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">x =</span> x[train_idx], <span class="at">y =</span> y[train_idx])</span>
<span id="cb110-114"><a href="#cb110-114" aria-hidden="true" tabindex="-1"></a>test_data <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">x =</span> x[<span class="sc">-</span>train_idx], <span class="at">y =</span> y[<span class="sc">-</span>train_idx])</span>
<span id="cb110-115"><a href="#cb110-115" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-116"><a href="#cb110-116" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit polynomials of increasing degree</span></span>
<span id="cb110-117"><a href="#cb110-117" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(splines)</span>
<span id="cb110-118"><a href="#cb110-118" aria-hidden="true" tabindex="-1"></a>degrees <span class="ot">&lt;-</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">15</span></span>
<span id="cb110-119"><a href="#cb110-119" aria-hidden="true" tabindex="-1"></a>train_error <span class="ot">&lt;-</span> test_error <span class="ot">&lt;-</span> <span class="fu">numeric</span>(<span class="fu">length</span>(degrees))</span>
<span id="cb110-120"><a href="#cb110-120" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-121"><a href="#cb110-121" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="fu">seq_along</span>(degrees)) {</span>
<span id="cb110-122"><a href="#cb110-122" aria-hidden="true" tabindex="-1"></a>  d <span class="ot">&lt;-</span> degrees[i]</span>
<span id="cb110-123"><a href="#cb110-123" aria-hidden="true" tabindex="-1"></a>  fit <span class="ot">&lt;-</span> <span class="fu">lm</span>(y <span class="sc">~</span> <span class="fu">poly</span>(x, d), <span class="at">data =</span> train_data)</span>
<span id="cb110-124"><a href="#cb110-124" aria-hidden="true" tabindex="-1"></a>  train_error[i] <span class="ot">&lt;-</span> <span class="fu">mean</span>((train_data<span class="sc">$</span>y <span class="sc">-</span> <span class="fu">predict</span>(fit, train_data))<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb110-125"><a href="#cb110-125" aria-hidden="true" tabindex="-1"></a>  test_error[i] <span class="ot">&lt;-</span> <span class="fu">mean</span>((test_data<span class="sc">$</span>y <span class="sc">-</span> <span class="fu">predict</span>(fit, test_data))<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb110-126"><a href="#cb110-126" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb110-127"><a href="#cb110-127" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-128"><a href="#cb110-128" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot</span></span>
<span id="cb110-129"><a href="#cb110-129" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(degrees, train_error, <span class="at">type =</span> <span class="st">"b"</span>, <span class="at">pch =</span> <span class="dv">19</span>, <span class="at">col =</span> <span class="st">"blue"</span>,</span>
<span id="cb110-130"><a href="#cb110-130" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">"Model Complexity (Polynomial Degree)"</span>,</span>
<span id="cb110-131"><a href="#cb110-131" aria-hidden="true" tabindex="-1"></a>     <span class="at">ylab =</span> <span class="st">"Mean Squared Error"</span>,</span>
<span id="cb110-132"><a href="#cb110-132" aria-hidden="true" tabindex="-1"></a>     <span class="at">main =</span> <span class="st">"Training vs Test Error"</span>, <span class="at">ylim =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="fu">max</span>(test_error)))</span>
<span id="cb110-133"><a href="#cb110-133" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(degrees, test_error, <span class="at">type =</span> <span class="st">"b"</span>, <span class="at">pch =</span> <span class="dv">19</span>, <span class="at">col =</span> <span class="st">"red"</span>)</span>
<span id="cb110-134"><a href="#cb110-134" aria-hidden="true" tabindex="-1"></a><span class="fu">legend</span>(<span class="st">"topright"</span>, <span class="fu">c</span>(<span class="st">"Training Error"</span>, <span class="st">"Test Error"</span>),</span>
<span id="cb110-135"><a href="#cb110-135" aria-hidden="true" tabindex="-1"></a>       <span class="at">col =</span> <span class="fu">c</span>(<span class="st">"blue"</span>, <span class="st">"red"</span>), <span class="at">pch =</span> <span class="dv">19</span>, <span class="at">lty =</span> <span class="dv">1</span>)</span>
<span id="cb110-136"><a href="#cb110-136" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">v =</span> degrees[<span class="fu">which.min</span>(test_error)], <span class="at">lty =</span> <span class="dv">2</span>, <span class="at">col =</span> <span class="st">"gray"</span>)</span>
<span id="cb110-137"><a href="#cb110-137" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb110-138"><a href="#cb110-138" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-139"><a href="#cb110-139" aria-hidden="true" tabindex="-1"></a>Notice that training error keeps decreasing as complexity increases, eventually reaching near zero. But test error follows a U-shape—it decreases initially as the model captures true patterns, then increases as the model starts fitting noise.</span>
<span id="cb110-140"><a href="#cb110-140" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-141"><a href="#cb110-141" aria-hidden="true" tabindex="-1"></a><span class="fu">### K-Fold Cross-Validation</span></span>
<span id="cb110-142"><a href="#cb110-142" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-143"><a href="#cb110-143" aria-hidden="true" tabindex="-1"></a>Cross-validation estimates how well a model will generalize to new data without requiring a separate test set.</span>
<span id="cb110-144"><a href="#cb110-144" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-145"><a href="#cb110-145" aria-hidden="true" tabindex="-1"></a>**K-fold cross-validation**:</span>
<span id="cb110-146"><a href="#cb110-146" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>Split data into k roughly equal parts (folds)</span>
<span id="cb110-147"><a href="#cb110-147" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>For each fold: train on k-1 folds, test on the held-out fold</span>
<span id="cb110-148"><a href="#cb110-148" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>Average performance across all folds</span>
<span id="cb110-149"><a href="#cb110-149" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-152"><a href="#cb110-152" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb110-153"><a href="#cb110-153" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-cross-validation</span></span>
<span id="cb110-154"><a href="#cb110-154" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "K-fold cross-validation: each fold takes turns being the test set"</span></span>
<span id="cb110-155"><a href="#cb110-155" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-width: 7</span></span>
<span id="cb110-156"><a href="#cb110-156" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-height: 5</span></span>
<span id="cb110-157"><a href="#cb110-157" aria-hidden="true" tabindex="-1"></a><span class="co"># Simple CV example with linear regression</span></span>
<span id="cb110-158"><a href="#cb110-158" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(boot)</span>
<span id="cb110-159"><a href="#cb110-159" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-160"><a href="#cb110-160" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate data</span></span>
<span id="cb110-161"><a href="#cb110-161" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">42</span>)</span>
<span id="cb110-162"><a href="#cb110-162" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(<span class="dv">100</span>)</span>
<span id="cb110-163"><a href="#cb110-163" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> <span class="dv">2</span> <span class="sc">+</span> <span class="dv">3</span><span class="sc">*</span>x <span class="sc">+</span> <span class="fu">rnorm</span>(<span class="dv">100</span>)</span>
<span id="cb110-164"><a href="#cb110-164" aria-hidden="true" tabindex="-1"></a>data <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(x, y)</span>
<span id="cb110-165"><a href="#cb110-165" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-166"><a href="#cb110-166" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit model and perform CV</span></span>
<span id="cb110-167"><a href="#cb110-167" aria-hidden="true" tabindex="-1"></a>model <span class="ot">&lt;-</span> <span class="fu">glm</span>(y <span class="sc">~</span> x, <span class="at">data =</span> data)</span>
<span id="cb110-168"><a href="#cb110-168" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-169"><a href="#cb110-169" aria-hidden="true" tabindex="-1"></a><span class="co"># 10-fold cross-validation</span></span>
<span id="cb110-170"><a href="#cb110-170" aria-hidden="true" tabindex="-1"></a>cv_result <span class="ot">&lt;-</span> <span class="fu">cv.glm</span>(data, model, <span class="at">K =</span> <span class="dv">10</span>)</span>
<span id="cb110-171"><a href="#cb110-171" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"CV estimate of prediction error:"</span>, <span class="fu">round</span>(cv_result<span class="sc">$</span>delta[<span class="dv">1</span>], <span class="dv">3</span>), <span class="st">"</span><span class="sc">\n</span><span class="st">"</span>)</span>
<span id="cb110-172"><a href="#cb110-172" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb110-173"><a href="#cb110-173" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-174"><a href="#cb110-174" aria-hidden="true" tabindex="-1"></a>**Leave-one-out cross-validation (LOOCV)** is k-fold with k = n: each observation is held out once. More computationally expensive but lower variance.</span>
<span id="cb110-175"><a href="#cb110-175" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-176"><a href="#cb110-176" aria-hidden="true" tabindex="-1"></a><span class="fu">### Bootstrap for Error Estimation</span></span>
<span id="cb110-177"><a href="#cb110-177" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-178"><a href="#cb110-178" aria-hidden="true" tabindex="-1"></a>The **bootstrap** can also estimate prediction error. The approach:</span>
<span id="cb110-179"><a href="#cb110-179" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-180"><a href="#cb110-180" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>Draw a bootstrap sample (n observations with replacement)</span>
<span id="cb110-181"><a href="#cb110-181" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>Fit the model on the bootstrap sample</span>
<span id="cb110-182"><a href="#cb110-182" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>Evaluate on observations NOT selected (the "out-of-bag" observations)</span>
<span id="cb110-183"><a href="#cb110-183" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>Repeat and average</span>
<span id="cb110-184"><a href="#cb110-184" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-185"><a href="#cb110-185" aria-hidden="true" tabindex="-1"></a>This is similar to cross-validation but uses the natural ~37% of observations left out of each bootstrap sample.</span>
<span id="cb110-186"><a href="#cb110-186" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-189"><a href="#cb110-189" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb110-190"><a href="#cb110-190" aria-hidden="true" tabindex="-1"></a><span class="co"># Bootstrap estimate of prediction error</span></span>
<span id="cb110-191"><a href="#cb110-191" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb110-192"><a href="#cb110-192" aria-hidden="true" tabindex="-1"></a>n_boot <span class="ot">&lt;-</span> <span class="dv">100</span></span>
<span id="cb110-193"><a href="#cb110-193" aria-hidden="true" tabindex="-1"></a>boot_errors <span class="ot">&lt;-</span> <span class="fu">numeric</span>(n_boot)</span>
<span id="cb110-194"><a href="#cb110-194" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-195"><a href="#cb110-195" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (b <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>n_boot) {</span>
<span id="cb110-196"><a href="#cb110-196" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Bootstrap sample</span></span>
<span id="cb110-197"><a href="#cb110-197" aria-hidden="true" tabindex="-1"></a>  boot_idx <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="dv">1</span><span class="sc">:</span><span class="fu">nrow</span>(data), <span class="at">replace =</span> <span class="cn">TRUE</span>)</span>
<span id="cb110-198"><a href="#cb110-198" aria-hidden="true" tabindex="-1"></a>  oob_idx <span class="ot">&lt;-</span> <span class="fu">setdiff</span>(<span class="dv">1</span><span class="sc">:</span><span class="fu">nrow</span>(data), <span class="fu">unique</span>(boot_idx))</span>
<span id="cb110-199"><a href="#cb110-199" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-200"><a href="#cb110-200" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span> (<span class="fu">length</span>(oob_idx) <span class="sc">&gt;</span> <span class="dv">0</span>) {</span>
<span id="cb110-201"><a href="#cb110-201" aria-hidden="true" tabindex="-1"></a>    fit <span class="ot">&lt;-</span> <span class="fu">lm</span>(y <span class="sc">~</span> x, <span class="at">data =</span> data[boot_idx, ])</span>
<span id="cb110-202"><a href="#cb110-202" aria-hidden="true" tabindex="-1"></a>    boot_errors[b] <span class="ot">&lt;-</span> <span class="fu">mean</span>((data<span class="sc">$</span>y[oob_idx] <span class="sc">-</span> <span class="fu">predict</span>(fit, data[oob_idx, ]))<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb110-203"><a href="#cb110-203" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb110-204"><a href="#cb110-204" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb110-205"><a href="#cb110-205" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-206"><a href="#cb110-206" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"Bootstrap estimate of prediction error:"</span>, <span class="fu">round</span>(<span class="fu">mean</span>(boot_errors), <span class="dv">3</span>), <span class="st">"</span><span class="sc">\n</span><span class="st">"</span>)</span>
<span id="cb110-207"><a href="#cb110-207" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb110-208"><a href="#cb110-208" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-209"><a href="#cb110-209" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip}</span>
<span id="cb110-210"><a href="#cb110-210" aria-hidden="true" tabindex="-1"></a><span class="fu">## Choosing a CV Strategy</span></span>
<span id="cb110-211"><a href="#cb110-211" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-212"><a href="#cb110-212" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**k = 5 or k = 10**: Standard choices that balance bias and variance</span>
<span id="cb110-213"><a href="#cb110-213" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**LOOCV (k = n)**: Low bias but high variance; expensive for large n</span>
<span id="cb110-214"><a href="#cb110-214" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Bootstrap**: Useful when you also want confidence intervals</span>
<span id="cb110-215"><a href="#cb110-215" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Repeated CV**: Run k-fold multiple times with different splits for more stable estimates</span>
<span id="cb110-216"><a href="#cb110-216" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb110-217"><a href="#cb110-217" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-218"><a href="#cb110-218" aria-hidden="true" tabindex="-1"></a><span class="fu">## Bias-Variance Tradeoff</span></span>
<span id="cb110-219"><a href="#cb110-219" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-220"><a href="#cb110-220" aria-hidden="true" tabindex="-1"></a>Prediction error has two components:</span>
<span id="cb110-221"><a href="#cb110-221" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-222"><a href="#cb110-222" aria-hidden="true" tabindex="-1"></a>**Bias**: Error from approximating a complex reality with a simpler model. Simple models have high bias—they may miss important patterns.</span>
<span id="cb110-223"><a href="#cb110-223" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-224"><a href="#cb110-224" aria-hidden="true" tabindex="-1"></a>**Variance**: Error from sensitivity to training data. Complex models have high variance—they change substantially with different training samples.</span>
<span id="cb110-225"><a href="#cb110-225" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-226"><a href="#cb110-226" aria-hidden="true" tabindex="-1"></a>The best predictions come from models that balance bias and variance. As model complexity increases, bias decreases but variance increases. The optimal complexity minimizes total prediction error.</span>
<span id="cb110-227"><a href="#cb110-227" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-228"><a href="#cb110-228" aria-hidden="true" tabindex="-1"></a><span class="fu">## Regularization: Controlling Model Complexity</span></span>
<span id="cb110-229"><a href="#cb110-229" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-230"><a href="#cb110-230" aria-hidden="true" tabindex="-1"></a>**Regularization** addresses overfitting by adding a penalty term that discourages complex models. This is particularly important when you have many predictors relative to observations, or when predictors are correlated.</span>
<span id="cb110-231"><a href="#cb110-231" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-232"><a href="#cb110-232" aria-hidden="true" tabindex="-1"></a><span class="fu">### The Regularization Idea</span></span>
<span id="cb110-233"><a href="#cb110-233" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-234"><a href="#cb110-234" aria-hidden="true" tabindex="-1"></a>Standard linear regression minimizes the sum of squared residuals (RSS):</span>
<span id="cb110-235"><a href="#cb110-235" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-236"><a href="#cb110-236" aria-hidden="true" tabindex="-1"></a>$$\text{RSS} = \sum_{i=1}^n (y_i - \beta_0 - \sum_{j=1}^p \beta_j x_{ij})^2$$</span>
<span id="cb110-237"><a href="#cb110-237" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-238"><a href="#cb110-238" aria-hidden="true" tabindex="-1"></a>Regularized regression adds a penalty term $\lambda P(\beta)$ that shrinks coefficients toward zero:</span>
<span id="cb110-239"><a href="#cb110-239" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-240"><a href="#cb110-240" aria-hidden="true" tabindex="-1"></a>$$\text{Minimize: } \text{RSS} + \lambda P(\beta)$$</span>
<span id="cb110-241"><a href="#cb110-241" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-242"><a href="#cb110-242" aria-hidden="true" tabindex="-1"></a>The **regularization parameter** $\lambda$ controls the strength of the penalty:</span>
<span id="cb110-243"><a href="#cb110-243" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\lambda = 0$: No penalty, equivalent to ordinary least squares</span>
<span id="cb110-244"><a href="#cb110-244" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\lambda \to \infty$: Very strong penalty, coefficients shrink toward zero</span>
<span id="cb110-245"><a href="#cb110-245" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-246"><a href="#cb110-246" aria-hidden="true" tabindex="-1"></a><span class="fu">### Ridge Regression (L2 Penalty)</span></span>
<span id="cb110-247"><a href="#cb110-247" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-248"><a href="#cb110-248" aria-hidden="true" tabindex="-1"></a>**Ridge regression** <span class="co">[</span><span class="ot">@hoerl1970ridge</span><span class="co">]</span> uses the sum of squared coefficients as the penalty:</span>
<span id="cb110-249"><a href="#cb110-249" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-250"><a href="#cb110-250" aria-hidden="true" tabindex="-1"></a>$$P(\beta) = \sum_{j=1}^p \beta_j^2$$</span>
<span id="cb110-251"><a href="#cb110-251" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-252"><a href="#cb110-252" aria-hidden="true" tabindex="-1"></a>This shrinks all coefficients toward zero but never exactly to zero. Ridge is particularly effective when predictors are correlated (multicollinearity).</span>
<span id="cb110-253"><a href="#cb110-253" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-256"><a href="#cb110-256" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb110-257"><a href="#cb110-257" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-ridge-path</span></span>
<span id="cb110-258"><a href="#cb110-258" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "Ridge regression coefficient paths: as lambda increases, coefficients shrink toward zero but never reach exactly zero"</span></span>
<span id="cb110-259"><a href="#cb110-259" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-width: 8</span></span>
<span id="cb110-260"><a href="#cb110-260" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-height: 5</span></span>
<span id="cb110-261"><a href="#cb110-261" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(glmnet)</span>
<span id="cb110-262"><a href="#cb110-262" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-263"><a href="#cb110-263" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate sample data with correlated predictors</span></span>
<span id="cb110-264"><a href="#cb110-264" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">42</span>)</span>
<span id="cb110-265"><a href="#cb110-265" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">100</span></span>
<span id="cb110-266"><a href="#cb110-266" aria-hidden="true" tabindex="-1"></a>p <span class="ot">&lt;-</span> <span class="dv">10</span></span>
<span id="cb110-267"><a href="#cb110-267" aria-hidden="true" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">rnorm</span>(n <span class="sc">*</span> p), n, p)</span>
<span id="cb110-268"><a href="#cb110-268" aria-hidden="true" tabindex="-1"></a><span class="co"># Create correlated predictors</span></span>
<span id="cb110-269"><a href="#cb110-269" aria-hidden="true" tabindex="-1"></a>X[, <span class="dv">2</span>] <span class="ot">&lt;-</span> X[, <span class="dv">1</span>] <span class="sc">+</span> <span class="fu">rnorm</span>(n, <span class="at">sd =</span> <span class="fl">0.5</span>)</span>
<span id="cb110-270"><a href="#cb110-270" aria-hidden="true" tabindex="-1"></a>X[, <span class="dv">3</span>] <span class="ot">&lt;-</span> X[, <span class="dv">1</span>] <span class="sc">+</span> <span class="fu">rnorm</span>(n, <span class="at">sd =</span> <span class="fl">0.5</span>)</span>
<span id="cb110-271"><a href="#cb110-271" aria-hidden="true" tabindex="-1"></a>true_beta <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">3</span>, <span class="sc">-</span><span class="dv">2</span>, <span class="fl">1.5</span>, <span class="fu">rep</span>(<span class="dv">0</span>, p <span class="sc">-</span> <span class="dv">3</span>))</span>
<span id="cb110-272"><a href="#cb110-272" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> X <span class="sc">%*%</span> true_beta <span class="sc">+</span> <span class="fu">rnorm</span>(n)</span>
<span id="cb110-273"><a href="#cb110-273" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-274"><a href="#cb110-274" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit ridge regression across lambda values</span></span>
<span id="cb110-275"><a href="#cb110-275" aria-hidden="true" tabindex="-1"></a>ridge_fit <span class="ot">&lt;-</span> <span class="fu">glmnet</span>(X, y, <span class="at">alpha =</span> <span class="dv">0</span>)  <span class="co"># alpha = 0 for ridge</span></span>
<span id="cb110-276"><a href="#cb110-276" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-277"><a href="#cb110-277" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot coefficient paths</span></span>
<span id="cb110-278"><a href="#cb110-278" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(ridge_fit, <span class="at">xvar =</span> <span class="st">"lambda"</span>, <span class="at">main =</span> <span class="st">"Ridge Regression Coefficients"</span>)</span>
<span id="cb110-279"><a href="#cb110-279" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb110-280"><a href="#cb110-280" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-281"><a href="#cb110-281" aria-hidden="true" tabindex="-1"></a><span class="fu">### Lasso Regression (L1 Penalty)</span></span>
<span id="cb110-282"><a href="#cb110-282" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-283"><a href="#cb110-283" aria-hidden="true" tabindex="-1"></a>**Lasso** (Least Absolute Shrinkage and Selection Operator) <span class="co">[</span><span class="ot">@tibshirani1996regression</span><span class="co">]</span> uses the sum of absolute values as the penalty:</span>
<span id="cb110-284"><a href="#cb110-284" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-285"><a href="#cb110-285" aria-hidden="true" tabindex="-1"></a>$$P(\beta) = \sum_{j=1}^p |\beta_j|$$</span>
<span id="cb110-286"><a href="#cb110-286" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-287"><a href="#cb110-287" aria-hidden="true" tabindex="-1"></a>Unlike ridge, lasso can shrink coefficients exactly to zero, effectively performing **variable selection**. This produces sparse models that are easier to interpret.</span>
<span id="cb110-288"><a href="#cb110-288" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-291"><a href="#cb110-291" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb110-292"><a href="#cb110-292" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-lasso-path</span></span>
<span id="cb110-293"><a href="#cb110-293" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "Lasso regression coefficient paths: as lambda increases, coefficients shrink and some become exactly zero (variable selection)"</span></span>
<span id="cb110-294"><a href="#cb110-294" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-width: 8</span></span>
<span id="cb110-295"><a href="#cb110-295" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-height: 5</span></span>
<span id="cb110-296"><a href="#cb110-296" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit lasso regression</span></span>
<span id="cb110-297"><a href="#cb110-297" aria-hidden="true" tabindex="-1"></a>lasso_fit <span class="ot">&lt;-</span> <span class="fu">glmnet</span>(X, y, <span class="at">alpha =</span> <span class="dv">1</span>)  <span class="co"># alpha = 1 for lasso</span></span>
<span id="cb110-298"><a href="#cb110-298" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-299"><a href="#cb110-299" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(lasso_fit, <span class="at">xvar =</span> <span class="st">"lambda"</span>, <span class="at">main =</span> <span class="st">"Lasso Regression Coefficients"</span>)</span>
<span id="cb110-300"><a href="#cb110-300" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb110-301"><a href="#cb110-301" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-302"><a href="#cb110-302" aria-hidden="true" tabindex="-1"></a><span class="fu">### Elastic Net: Combining Ridge and Lasso</span></span>
<span id="cb110-303"><a href="#cb110-303" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-304"><a href="#cb110-304" aria-hidden="true" tabindex="-1"></a>**Elastic net** combines both penalties:</span>
<span id="cb110-305"><a href="#cb110-305" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-306"><a href="#cb110-306" aria-hidden="true" tabindex="-1"></a>$$P(\beta) = \alpha \sum_{j=1}^p |\beta_j| + (1-\alpha) \sum_{j=1}^p \beta_j^2$$</span>
<span id="cb110-307"><a href="#cb110-307" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-308"><a href="#cb110-308" aria-hidden="true" tabindex="-1"></a>The mixing parameter $\alpha$ controls the balance:</span>
<span id="cb110-309"><a href="#cb110-309" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\alpha = 0$: Pure ridge</span>
<span id="cb110-310"><a href="#cb110-310" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\alpha = 1$: Pure lasso</span>
<span id="cb110-311"><a href="#cb110-311" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$0 &lt; \alpha &lt; 1$: Combination</span>
<span id="cb110-312"><a href="#cb110-312" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-313"><a href="#cb110-313" aria-hidden="true" tabindex="-1"></a>Elastic net is often preferred when predictors are correlated—it tends to select groups of correlated variables together.</span>
<span id="cb110-314"><a href="#cb110-314" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-315"><a href="#cb110-315" aria-hidden="true" tabindex="-1"></a><span class="fu">### Choosing Lambda with Cross-Validation</span></span>
<span id="cb110-316"><a href="#cb110-316" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-317"><a href="#cb110-317" aria-hidden="true" tabindex="-1"></a>The regularization parameter $\lambda$ is typically chosen by cross-validation:</span>
<span id="cb110-318"><a href="#cb110-318" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-321"><a href="#cb110-321" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb110-322"><a href="#cb110-322" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-cv-lambda</span></span>
<span id="cb110-323"><a href="#cb110-323" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "Cross-validation to select optimal lambda: the left dashed line marks the minimum error, the right marks the most regularized model within one standard error"</span></span>
<span id="cb110-324"><a href="#cb110-324" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-width: 8</span></span>
<span id="cb110-325"><a href="#cb110-325" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-height: 5</span></span>
<span id="cb110-326"><a href="#cb110-326" aria-hidden="true" tabindex="-1"></a><span class="co"># Cross-validation for lasso</span></span>
<span id="cb110-327"><a href="#cb110-327" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb110-328"><a href="#cb110-328" aria-hidden="true" tabindex="-1"></a>cv_lasso <span class="ot">&lt;-</span> <span class="fu">cv.glmnet</span>(X, y, <span class="at">alpha =</span> <span class="dv">1</span>)</span>
<span id="cb110-329"><a href="#cb110-329" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-330"><a href="#cb110-330" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot cross-validation results</span></span>
<span id="cb110-331"><a href="#cb110-331" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(cv_lasso)</span>
<span id="cb110-332"><a href="#cb110-332" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-333"><a href="#cb110-333" aria-hidden="true" tabindex="-1"></a><span class="co"># Optimal lambda values</span></span>
<span id="cb110-334"><a href="#cb110-334" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"Lambda with minimum CV error:"</span>, <span class="fu">round</span>(cv_lasso<span class="sc">$</span>lambda.min, <span class="dv">4</span>), <span class="st">"</span><span class="sc">\n</span><span class="st">"</span>)</span>
<span id="cb110-335"><a href="#cb110-335" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"Lambda within 1 SE of minimum:"</span>, <span class="fu">round</span>(cv_lasso<span class="sc">$</span>lambda<span class="fl">.1</span>se, <span class="dv">4</span>), <span class="st">"</span><span class="sc">\n</span><span class="st">"</span>)</span>
<span id="cb110-336"><a href="#cb110-336" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb110-337"><a href="#cb110-337" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-338"><a href="#cb110-338" aria-hidden="true" tabindex="-1"></a>The <span class="in">`lambda.1se`</span> (one standard error rule) often provides a more parsimonious model with nearly as good performance as the minimum.</span>
<span id="cb110-339"><a href="#cb110-339" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-340"><a href="#cb110-340" aria-hidden="true" tabindex="-1"></a><span class="fu">### Comparing Regularization Methods</span></span>
<span id="cb110-341"><a href="#cb110-341" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-344"><a href="#cb110-344" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb110-345"><a href="#cb110-345" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit models with optimal lambda</span></span>
<span id="cb110-346"><a href="#cb110-346" aria-hidden="true" tabindex="-1"></a>ridge_cv <span class="ot">&lt;-</span> <span class="fu">cv.glmnet</span>(X, y, <span class="at">alpha =</span> <span class="dv">0</span>)</span>
<span id="cb110-347"><a href="#cb110-347" aria-hidden="true" tabindex="-1"></a>lasso_cv <span class="ot">&lt;-</span> <span class="fu">cv.glmnet</span>(X, y, <span class="at">alpha =</span> <span class="dv">1</span>)</span>
<span id="cb110-348"><a href="#cb110-348" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-349"><a href="#cb110-349" aria-hidden="true" tabindex="-1"></a><span class="co"># Extract coefficients</span></span>
<span id="cb110-350"><a href="#cb110-350" aria-hidden="true" tabindex="-1"></a>coef_ols <span class="ot">&lt;-</span> <span class="fu">coef</span>(<span class="fu">lm</span>(y <span class="sc">~</span> X))</span>
<span id="cb110-351"><a href="#cb110-351" aria-hidden="true" tabindex="-1"></a>coef_ridge <span class="ot">&lt;-</span> <span class="fu">coef</span>(ridge_cv, <span class="at">s =</span> <span class="st">"lambda.1se"</span>)</span>
<span id="cb110-352"><a href="#cb110-352" aria-hidden="true" tabindex="-1"></a>coef_lasso <span class="ot">&lt;-</span> <span class="fu">coef</span>(lasso_cv, <span class="at">s =</span> <span class="st">"lambda.1se"</span>)</span>
<span id="cb110-353"><a href="#cb110-353" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-354"><a href="#cb110-354" aria-hidden="true" tabindex="-1"></a><span class="co"># Compare (excluding intercept)</span></span>
<span id="cb110-355"><a href="#cb110-355" aria-hidden="true" tabindex="-1"></a>comparison <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(</span>
<span id="cb110-356"><a href="#cb110-356" aria-hidden="true" tabindex="-1"></a>  <span class="at">True =</span> <span class="fu">c</span>(<span class="cn">NA</span>, true_beta),</span>
<span id="cb110-357"><a href="#cb110-357" aria-hidden="true" tabindex="-1"></a>  <span class="at">OLS =</span> <span class="fu">as.vector</span>(coef_ols),</span>
<span id="cb110-358"><a href="#cb110-358" aria-hidden="true" tabindex="-1"></a>  <span class="at">Ridge =</span> <span class="fu">as.vector</span>(coef_ridge),</span>
<span id="cb110-359"><a href="#cb110-359" aria-hidden="true" tabindex="-1"></a>  <span class="at">Lasso =</span> <span class="fu">as.vector</span>(coef_lasso)</span>
<span id="cb110-360"><a href="#cb110-360" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb110-361"><a href="#cb110-361" aria-hidden="true" tabindex="-1"></a><span class="fu">rownames</span>(comparison) <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">"Intercept"</span>, <span class="fu">paste0</span>(<span class="st">"X"</span>, <span class="dv">1</span><span class="sc">:</span>p))</span>
<span id="cb110-362"><a href="#cb110-362" aria-hidden="true" tabindex="-1"></a><span class="fu">round</span>(comparison, <span class="dv">3</span>)</span>
<span id="cb110-363"><a href="#cb110-363" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb110-364"><a href="#cb110-364" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-365"><a href="#cb110-365" aria-hidden="true" tabindex="-1"></a>Notice that lasso correctly identifies the zero coefficients (variables 4-10), while ridge shrinks them but doesn't eliminate them.</span>
<span id="cb110-366"><a href="#cb110-366" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-367"><a href="#cb110-367" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip}</span>
<span id="cb110-368"><a href="#cb110-368" aria-hidden="true" tabindex="-1"></a><span class="fu">## When to Use Each Method</span></span>
<span id="cb110-369"><a href="#cb110-369" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-370"><a href="#cb110-370" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Ridge**: When you believe all predictors are relevant and want to handle multicollinearity</span>
<span id="cb110-371"><a href="#cb110-371" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Lasso**: When you want automatic variable selection and a sparse model</span>
<span id="cb110-372"><a href="#cb110-372" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Elastic Net**: When predictors are correlated and you want both selection and grouping</span>
<span id="cb110-373"><a href="#cb110-373" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-374"><a href="#cb110-374" aria-hidden="true" tabindex="-1"></a>**Important**: Always standardize predictors before applying regularization, as the penalty treats all coefficients equally. The <span class="in">`glmnet`</span> function does this automatically by default.</span>
<span id="cb110-375"><a href="#cb110-375" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb110-376"><a href="#cb110-376" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-377"><a href="#cb110-377" aria-hidden="true" tabindex="-1"></a><span class="fu">## Smoothing: From Simple Averages to Flexible Curves</span></span>
<span id="cb110-378"><a href="#cb110-378" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-379"><a href="#cb110-379" aria-hidden="true" tabindex="-1"></a>When the relationship between a predictor and outcome is non-linear, we need methods more flexible than linear regression. **Smoothing** methods estimate curves by averaging nearby observations, allowing the data to reveal its own pattern.</span>
<span id="cb110-380"><a href="#cb110-380" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-381"><a href="#cb110-381" aria-hidden="true" tabindex="-1"></a><span class="fu">### Bin Smoothing</span></span>
<span id="cb110-382"><a href="#cb110-382" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-383"><a href="#cb110-383" aria-hidden="true" tabindex="-1"></a>The simplest smoothing approach is **bin smoothing** (also called **binning**): divide the predictor into intervals (bins) and estimate the outcome as the average within each bin.</span>
<span id="cb110-384"><a href="#cb110-384" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-387"><a href="#cb110-387" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb110-388"><a href="#cb110-388" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-bin-smoothing</span></span>
<span id="cb110-389"><a href="#cb110-389" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "Bin smoothing divides data into intervals and estimates each segment as the mean of points in that bin"</span></span>
<span id="cb110-390"><a href="#cb110-390" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-width: 8</span></span>
<span id="cb110-391"><a href="#cb110-391" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-height: 5</span></span>
<span id="cb110-392"><a href="#cb110-392" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate non-linear data</span></span>
<span id="cb110-393"><a href="#cb110-393" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">42</span>)</span>
<span id="cb110-394"><a href="#cb110-394" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">200</span></span>
<span id="cb110-395"><a href="#cb110-395" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">runif</span>(n, <span class="dv">0</span>, <span class="dv">10</span>)</span>
<span id="cb110-396"><a href="#cb110-396" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> <span class="fu">sin</span>(x) <span class="sc">+</span> <span class="fu">rnorm</span>(n, <span class="at">sd =</span> <span class="fl">0.3</span>)</span>
<span id="cb110-397"><a href="#cb110-397" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-398"><a href="#cb110-398" aria-hidden="true" tabindex="-1"></a><span class="co"># Bin smoothing with different bin widths</span></span>
<span id="cb110-399"><a href="#cb110-399" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">2</span>))</span>
<span id="cb110-400"><a href="#cb110-400" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-401"><a href="#cb110-401" aria-hidden="true" tabindex="-1"></a><span class="co"># Narrow bins</span></span>
<span id="cb110-402"><a href="#cb110-402" aria-hidden="true" tabindex="-1"></a>n_bins <span class="ot">&lt;-</span> <span class="dv">20</span></span>
<span id="cb110-403"><a href="#cb110-403" aria-hidden="true" tabindex="-1"></a>breaks <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="fu">min</span>(x), <span class="fu">max</span>(x), <span class="at">length.out =</span> n_bins <span class="sc">+</span> <span class="dv">1</span>)</span>
<span id="cb110-404"><a href="#cb110-404" aria-hidden="true" tabindex="-1"></a>bin_means <span class="ot">&lt;-</span> <span class="fu">sapply</span>(<span class="dv">1</span><span class="sc">:</span>n_bins, <span class="cf">function</span>(i) {</span>
<span id="cb110-405"><a href="#cb110-405" aria-hidden="true" tabindex="-1"></a>  in_bin <span class="ot">&lt;-</span> x <span class="sc">&gt;=</span> breaks[i] <span class="sc">&amp;</span> x <span class="sc">&lt;</span> breaks[i <span class="sc">+</span> <span class="dv">1</span>]</span>
<span id="cb110-406"><a href="#cb110-406" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span> (<span class="fu">sum</span>(in_bin) <span class="sc">&gt;</span> <span class="dv">0</span>) <span class="fu">mean</span>(y[in_bin]) <span class="cf">else</span> <span class="cn">NA</span></span>
<span id="cb110-407"><a href="#cb110-407" aria-hidden="true" tabindex="-1"></a>})</span>
<span id="cb110-408"><a href="#cb110-408" aria-hidden="true" tabindex="-1"></a>bin_centers <span class="ot">&lt;-</span> (breaks[<span class="sc">-</span><span class="dv">1</span>] <span class="sc">+</span> breaks[<span class="sc">-</span>(n_bins <span class="sc">+</span> <span class="dv">1</span>)]) <span class="sc">/</span> <span class="dv">2</span></span>
<span id="cb110-409"><a href="#cb110-409" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-410"><a href="#cb110-410" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(x, y, <span class="at">pch =</span> <span class="dv">16</span>, <span class="at">col =</span> <span class="st">"gray60"</span>, <span class="at">main =</span> <span class="st">"Narrow bins (20)"</span>)</span>
<span id="cb110-411"><a href="#cb110-411" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span>(bin_centers, bin_means, <span class="at">col =</span> <span class="st">"blue"</span>, <span class="at">pch =</span> <span class="dv">19</span>, <span class="at">cex =</span> <span class="fl">1.5</span>)</span>
<span id="cb110-412"><a href="#cb110-412" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(bin_centers, bin_means, <span class="at">col =</span> <span class="st">"blue"</span>, <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb110-413"><a href="#cb110-413" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-414"><a href="#cb110-414" aria-hidden="true" tabindex="-1"></a><span class="co"># Wide bins</span></span>
<span id="cb110-415"><a href="#cb110-415" aria-hidden="true" tabindex="-1"></a>n_bins <span class="ot">&lt;-</span> <span class="dv">5</span></span>
<span id="cb110-416"><a href="#cb110-416" aria-hidden="true" tabindex="-1"></a>breaks <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="fu">min</span>(x), <span class="fu">max</span>(x), <span class="at">length.out =</span> n_bins <span class="sc">+</span> <span class="dv">1</span>)</span>
<span id="cb110-417"><a href="#cb110-417" aria-hidden="true" tabindex="-1"></a>bin_means <span class="ot">&lt;-</span> <span class="fu">sapply</span>(<span class="dv">1</span><span class="sc">:</span>n_bins, <span class="cf">function</span>(i) {</span>
<span id="cb110-418"><a href="#cb110-418" aria-hidden="true" tabindex="-1"></a>  in_bin <span class="ot">&lt;-</span> x <span class="sc">&gt;=</span> breaks[i] <span class="sc">&amp;</span> x <span class="sc">&lt;</span> breaks[i <span class="sc">+</span> <span class="dv">1</span>]</span>
<span id="cb110-419"><a href="#cb110-419" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span> (<span class="fu">sum</span>(in_bin) <span class="sc">&gt;</span> <span class="dv">0</span>) <span class="fu">mean</span>(y[in_bin]) <span class="cf">else</span> <span class="cn">NA</span></span>
<span id="cb110-420"><a href="#cb110-420" aria-hidden="true" tabindex="-1"></a>})</span>
<span id="cb110-421"><a href="#cb110-421" aria-hidden="true" tabindex="-1"></a>bin_centers <span class="ot">&lt;-</span> (breaks[<span class="sc">-</span><span class="dv">1</span>] <span class="sc">+</span> breaks[<span class="sc">-</span>(n_bins <span class="sc">+</span> <span class="dv">1</span>)]) <span class="sc">/</span> <span class="dv">2</span></span>
<span id="cb110-422"><a href="#cb110-422" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-423"><a href="#cb110-423" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(x, y, <span class="at">pch =</span> <span class="dv">16</span>, <span class="at">col =</span> <span class="st">"gray60"</span>, <span class="at">main =</span> <span class="st">"Wide bins (5)"</span>)</span>
<span id="cb110-424"><a href="#cb110-424" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span>(bin_centers, bin_means, <span class="at">col =</span> <span class="st">"red"</span>, <span class="at">pch =</span> <span class="dv">19</span>, <span class="at">cex =</span> <span class="fl">1.5</span>)</span>
<span id="cb110-425"><a href="#cb110-425" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(bin_centers, bin_means, <span class="at">col =</span> <span class="st">"red"</span>, <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb110-426"><a href="#cb110-426" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb110-427"><a href="#cb110-427" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-428"><a href="#cb110-428" aria-hidden="true" tabindex="-1"></a>Bin smoothing illustrates the **bias-variance tradeoff** in smoothing:</span>
<span id="cb110-429"><a href="#cb110-429" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-430"><a href="#cb110-430" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Narrow bins**: Capture local variation (low bias) but are noisy (high variance)</span>
<span id="cb110-431"><a href="#cb110-431" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Wide bins**: Smooth over noise (low variance) but may miss true curvature (high bias)</span>
<span id="cb110-432"><a href="#cb110-432" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-433"><a href="#cb110-433" aria-hidden="true" tabindex="-1"></a>The main limitation of bin smoothing is the **discontinuity** at bin boundaries—the estimate jumps from one bin to the next.</span>
<span id="cb110-434"><a href="#cb110-434" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-435"><a href="#cb110-435" aria-hidden="true" tabindex="-1"></a><span class="fu">### Kernel Smoothing</span></span>
<span id="cb110-436"><a href="#cb110-436" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-437"><a href="#cb110-437" aria-hidden="true" tabindex="-1"></a>**Kernel smoothing** improves on binning by using weighted averages, where closer points receive more weight. This creates smooth, continuous estimates.</span>
<span id="cb110-438"><a href="#cb110-438" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-439"><a href="#cb110-439" aria-hidden="true" tabindex="-1"></a>The estimate at any point $x_0$ is:</span>
<span id="cb110-440"><a href="#cb110-440" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-441"><a href="#cb110-441" aria-hidden="true" tabindex="-1"></a>$$\hat{f}(x_0) = \frac{\sum_{i=1}^n K\left(\frac{x_i - x_0}{h}\right) y_i}{\sum_{i=1}^n K\left(\frac{x_i - x_0}{h}\right)}$$</span>
<span id="cb110-442"><a href="#cb110-442" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-443"><a href="#cb110-443" aria-hidden="true" tabindex="-1"></a>where $K$ is a **kernel function** (typically Gaussian or Epanechnikov) and $h$ is the **bandwidth** controlling smoothness.</span>
<span id="cb110-444"><a href="#cb110-444" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-447"><a href="#cb110-447" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb110-448"><a href="#cb110-448" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-kernel-smoothing</span></span>
<span id="cb110-449"><a href="#cb110-449" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "Kernel smoothing uses weighted averages with Gaussian weights, creating smooth estimates"</span></span>
<span id="cb110-450"><a href="#cb110-450" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-width: 9</span></span>
<span id="cb110-451"><a href="#cb110-451" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-height: 4</span></span>
<span id="cb110-452"><a href="#cb110-452" aria-hidden="true" tabindex="-1"></a><span class="co"># Kernel smoothing function</span></span>
<span id="cb110-453"><a href="#cb110-453" aria-hidden="true" tabindex="-1"></a>kernel_smooth <span class="ot">&lt;-</span> <span class="cf">function</span>(x0, x, y, bandwidth) {</span>
<span id="cb110-454"><a href="#cb110-454" aria-hidden="true" tabindex="-1"></a>  weights <span class="ot">&lt;-</span> <span class="fu">dnorm</span>(x, <span class="at">mean =</span> x0, <span class="at">sd =</span> bandwidth)</span>
<span id="cb110-455"><a href="#cb110-455" aria-hidden="true" tabindex="-1"></a>  <span class="fu">sum</span>(weights <span class="sc">*</span> y) <span class="sc">/</span> <span class="fu">sum</span>(weights)</span>
<span id="cb110-456"><a href="#cb110-456" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb110-457"><a href="#cb110-457" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-458"><a href="#cb110-458" aria-hidden="true" tabindex="-1"></a><span class="co"># Apply to grid of points</span></span>
<span id="cb110-459"><a href="#cb110-459" aria-hidden="true" tabindex="-1"></a>x_grid <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="dv">0</span>, <span class="dv">10</span>, <span class="at">length.out =</span> <span class="dv">200</span>)</span>
<span id="cb110-460"><a href="#cb110-460" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-461"><a href="#cb110-461" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">3</span>))</span>
<span id="cb110-462"><a href="#cb110-462" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-463"><a href="#cb110-463" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (bw <span class="cf">in</span> <span class="fu">c</span>(<span class="fl">0.2</span>, <span class="fl">0.5</span>, <span class="fl">1.0</span>)) {</span>
<span id="cb110-464"><a href="#cb110-464" aria-hidden="true" tabindex="-1"></a>  y_smooth <span class="ot">&lt;-</span> <span class="fu">sapply</span>(x_grid, <span class="cf">function</span>(x0) <span class="fu">kernel_smooth</span>(x0, x, y, bw))</span>
<span id="cb110-465"><a href="#cb110-465" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-466"><a href="#cb110-466" aria-hidden="true" tabindex="-1"></a>  <span class="fu">plot</span>(x, y, <span class="at">pch =</span> <span class="dv">16</span>, <span class="at">col =</span> <span class="st">"gray60"</span>, <span class="at">main =</span> <span class="fu">paste</span>(<span class="st">"Bandwidth ="</span>, bw))</span>
<span id="cb110-467"><a href="#cb110-467" aria-hidden="true" tabindex="-1"></a>  <span class="fu">lines</span>(x_grid, y_smooth, <span class="at">col =</span> <span class="st">"blue"</span>, <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb110-468"><a href="#cb110-468" aria-hidden="true" tabindex="-1"></a>  <span class="fu">lines</span>(x_grid, <span class="fu">sin</span>(x_grid), <span class="at">col =</span> <span class="st">"red"</span>, <span class="at">lwd =</span> <span class="dv">2</span>, <span class="at">lty =</span> <span class="dv">2</span>)</span>
<span id="cb110-469"><a href="#cb110-469" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb110-470"><a href="#cb110-470" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb110-471"><a href="#cb110-471" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-472"><a href="#cb110-472" aria-hidden="true" tabindex="-1"></a>The **bandwidth** parameter plays the same role as the number of bins:</span>
<span id="cb110-473"><a href="#cb110-473" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-474"><a href="#cb110-474" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Small bandwidth**: More local, follows the data closely (risk of overfitting)</span>
<span id="cb110-475"><a href="#cb110-475" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Large bandwidth**: More global, smoother curve (risk of over-smoothing)</span>
<span id="cb110-476"><a href="#cb110-476" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-477"><a href="#cb110-477" aria-hidden="true" tabindex="-1"></a>Kernel smoothing eliminates the discontinuity problem of bin smoothing while retaining its intuitive local-averaging interpretation.</span>
<span id="cb110-478"><a href="#cb110-478" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-479"><a href="#cb110-479" aria-hidden="true" tabindex="-1"></a><span class="fu">## Splines: Flexible Curve Fitting</span></span>
<span id="cb110-480"><a href="#cb110-480" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-481"><a href="#cb110-481" aria-hidden="true" tabindex="-1"></a>While LOESS provides local smoothing, **splines** offer a more structured approach to fitting flexible curves. A spline is a piecewise polynomial function that joins smoothly at points called **knots**.</span>
<span id="cb110-482"><a href="#cb110-482" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-483"><a href="#cb110-483" aria-hidden="true" tabindex="-1"></a><span class="fu">### Why Splines?</span></span>
<span id="cb110-484"><a href="#cb110-484" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-485"><a href="#cb110-485" aria-hidden="true" tabindex="-1"></a>Linear regression assumes a straight-line relationship, which is often too restrictive. We could fit polynomial regression (e.g., $y = \beta_0 + \beta_1 x + \beta_2 x^2 + \beta_3 x^3$), but polynomials can behave erratically, especially at the edges of the data.</span>
<span id="cb110-486"><a href="#cb110-486" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-487"><a href="#cb110-487" aria-hidden="true" tabindex="-1"></a>Splines provide flexibility while maintaining smooth, well-behaved curves.</span>
<span id="cb110-488"><a href="#cb110-488" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-489"><a href="#cb110-489" aria-hidden="true" tabindex="-1"></a><span class="fu">### Regression Splines</span></span>
<span id="cb110-490"><a href="#cb110-490" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-491"><a href="#cb110-491" aria-hidden="true" tabindex="-1"></a>**Regression splines** fit piecewise polynomials at fixed knot locations. The <span class="in">`splines`</span> package provides basis functions for incorporating splines into linear models:</span>
<span id="cb110-492"><a href="#cb110-492" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-495"><a href="#cb110-495" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb110-496"><a href="#cb110-496" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-spline-comparison</span></span>
<span id="cb110-497"><a href="#cb110-497" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "Comparison of linear, polynomial, and spline fits for non-linear data"</span></span>
<span id="cb110-498"><a href="#cb110-498" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-width: 9</span></span>
<span id="cb110-499"><a href="#cb110-499" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-height: 4</span></span>
<span id="cb110-500"><a href="#cb110-500" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(splines)</span>
<span id="cb110-501"><a href="#cb110-501" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-502"><a href="#cb110-502" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate non-linear data</span></span>
<span id="cb110-503"><a href="#cb110-503" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">42</span>)</span>
<span id="cb110-504"><a href="#cb110-504" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="dv">0</span>, <span class="dv">10</span>, <span class="at">length.out =</span> <span class="dv">100</span>)</span>
<span id="cb110-505"><a href="#cb110-505" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> <span class="fu">sin</span>(x) <span class="sc">+</span> <span class="fl">0.5</span> <span class="sc">*</span> <span class="fu">cos</span>(<span class="dv">2</span><span class="sc">*</span>x) <span class="sc">+</span> <span class="fu">rnorm</span>(<span class="dv">100</span>, <span class="at">sd =</span> <span class="fl">0.3</span>)</span>
<span id="cb110-506"><a href="#cb110-506" aria-hidden="true" tabindex="-1"></a>data <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(x, y)</span>
<span id="cb110-507"><a href="#cb110-507" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-508"><a href="#cb110-508" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit different models</span></span>
<span id="cb110-509"><a href="#cb110-509" aria-hidden="true" tabindex="-1"></a>fit_linear <span class="ot">&lt;-</span> <span class="fu">lm</span>(y <span class="sc">~</span> x, <span class="at">data =</span> data)</span>
<span id="cb110-510"><a href="#cb110-510" aria-hidden="true" tabindex="-1"></a>fit_poly <span class="ot">&lt;-</span> <span class="fu">lm</span>(y <span class="sc">~</span> <span class="fu">poly</span>(x, <span class="dv">5</span>), <span class="at">data =</span> data)</span>
<span id="cb110-511"><a href="#cb110-511" aria-hidden="true" tabindex="-1"></a>fit_spline <span class="ot">&lt;-</span> <span class="fu">lm</span>(y <span class="sc">~</span> <span class="fu">bs</span>(x, <span class="at">df =</span> <span class="dv">6</span>), <span class="at">data =</span> data)  <span class="co"># B-spline with 6 df</span></span>
<span id="cb110-512"><a href="#cb110-512" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-513"><a href="#cb110-513" aria-hidden="true" tabindex="-1"></a><span class="co"># Predictions</span></span>
<span id="cb110-514"><a href="#cb110-514" aria-hidden="true" tabindex="-1"></a>data<span class="sc">$</span>pred_linear <span class="ot">&lt;-</span> <span class="fu">predict</span>(fit_linear)</span>
<span id="cb110-515"><a href="#cb110-515" aria-hidden="true" tabindex="-1"></a>data<span class="sc">$</span>pred_poly <span class="ot">&lt;-</span> <span class="fu">predict</span>(fit_poly)</span>
<span id="cb110-516"><a href="#cb110-516" aria-hidden="true" tabindex="-1"></a>data<span class="sc">$</span>pred_spline <span class="ot">&lt;-</span> <span class="fu">predict</span>(fit_spline)</span>
<span id="cb110-517"><a href="#cb110-517" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-518"><a href="#cb110-518" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">3</span>))</span>
<span id="cb110-519"><a href="#cb110-519" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-520"><a href="#cb110-520" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(x, y, <span class="at">pch =</span> <span class="dv">16</span>, <span class="at">col =</span> <span class="st">"gray60"</span>, <span class="at">main =</span> <span class="st">"Linear"</span>)</span>
<span id="cb110-521"><a href="#cb110-521" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(x, data<span class="sc">$</span>pred_linear, <span class="at">col =</span> <span class="st">"red"</span>, <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb110-522"><a href="#cb110-522" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-523"><a href="#cb110-523" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(x, y, <span class="at">pch =</span> <span class="dv">16</span>, <span class="at">col =</span> <span class="st">"gray60"</span>, <span class="at">main =</span> <span class="st">"Polynomial (degree 5)"</span>)</span>
<span id="cb110-524"><a href="#cb110-524" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(x, data<span class="sc">$</span>pred_poly, <span class="at">col =</span> <span class="st">"blue"</span>, <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb110-525"><a href="#cb110-525" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-526"><a href="#cb110-526" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(x, y, <span class="at">pch =</span> <span class="dv">16</span>, <span class="at">col =</span> <span class="st">"gray60"</span>, <span class="at">main =</span> <span class="st">"Spline (6 df)"</span>)</span>
<span id="cb110-527"><a href="#cb110-527" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(x, data<span class="sc">$</span>pred_spline, <span class="at">col =</span> <span class="st">"darkgreen"</span>, <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb110-528"><a href="#cb110-528" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb110-529"><a href="#cb110-529" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-530"><a href="#cb110-530" aria-hidden="true" tabindex="-1"></a><span class="fu">### Natural Splines</span></span>
<span id="cb110-531"><a href="#cb110-531" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-532"><a href="#cb110-532" aria-hidden="true" tabindex="-1"></a>**Natural splines** add the constraint that the function is linear beyond the boundary knots. This prevents the wild behavior that polynomials often exhibit at the edges:</span>
<span id="cb110-533"><a href="#cb110-533" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-536"><a href="#cb110-536" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb110-537"><a href="#cb110-537" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-natural-spline</span></span>
<span id="cb110-538"><a href="#cb110-538" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "Natural splines constrain the fit to be linear beyond the data boundaries, reducing edge effects"</span></span>
<span id="cb110-539"><a href="#cb110-539" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-width: 7</span></span>
<span id="cb110-540"><a href="#cb110-540" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-height: 5</span></span>
<span id="cb110-541"><a href="#cb110-541" aria-hidden="true" tabindex="-1"></a><span class="co"># Compare B-spline and natural spline</span></span>
<span id="cb110-542"><a href="#cb110-542" aria-hidden="true" tabindex="-1"></a>fit_bs <span class="ot">&lt;-</span> <span class="fu">lm</span>(y <span class="sc">~</span> <span class="fu">bs</span>(x, <span class="at">df =</span> <span class="dv">6</span>), <span class="at">data =</span> data)</span>
<span id="cb110-543"><a href="#cb110-543" aria-hidden="true" tabindex="-1"></a>fit_ns <span class="ot">&lt;-</span> <span class="fu">lm</span>(y <span class="sc">~</span> <span class="fu">ns</span>(x, <span class="at">df =</span> <span class="dv">6</span>), <span class="at">data =</span> data)</span>
<span id="cb110-544"><a href="#cb110-544" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-545"><a href="#cb110-545" aria-hidden="true" tabindex="-1"></a><span class="co"># Extend prediction range to see edge behavior</span></span>
<span id="cb110-546"><a href="#cb110-546" aria-hidden="true" tabindex="-1"></a>x_ext <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="sc">-</span><span class="dv">2</span>, <span class="dv">12</span>, <span class="at">length.out =</span> <span class="dv">200</span>)</span>
<span id="cb110-547"><a href="#cb110-547" aria-hidden="true" tabindex="-1"></a>pred_bs <span class="ot">&lt;-</span> <span class="fu">predict</span>(fit_bs, <span class="at">newdata =</span> <span class="fu">data.frame</span>(<span class="at">x =</span> x_ext))</span>
<span id="cb110-548"><a href="#cb110-548" aria-hidden="true" tabindex="-1"></a>pred_ns <span class="ot">&lt;-</span> <span class="fu">predict</span>(fit_ns, <span class="at">newdata =</span> <span class="fu">data.frame</span>(<span class="at">x =</span> x_ext))</span>
<span id="cb110-549"><a href="#cb110-549" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-550"><a href="#cb110-550" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(x, y, <span class="at">pch =</span> <span class="dv">16</span>, <span class="at">col =</span> <span class="st">"gray60"</span>, <span class="at">xlim =</span> <span class="fu">c</span>(<span class="sc">-</span><span class="dv">2</span>, <span class="dv">12</span>), <span class="at">ylim =</span> <span class="fu">c</span>(<span class="sc">-</span><span class="dv">3</span>, <span class="dv">3</span>),</span>
<span id="cb110-551"><a href="#cb110-551" aria-hidden="true" tabindex="-1"></a>     <span class="at">main =</span> <span class="st">"B-spline vs Natural Spline at Boundaries"</span>)</span>
<span id="cb110-552"><a href="#cb110-552" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(x_ext, pred_bs, <span class="at">col =</span> <span class="st">"blue"</span>, <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb110-553"><a href="#cb110-553" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(x_ext, pred_ns, <span class="at">col =</span> <span class="st">"darkgreen"</span>, <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb110-554"><a href="#cb110-554" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">v =</span> <span class="fu">range</span>(x), <span class="at">lty =</span> <span class="dv">2</span>, <span class="at">col =</span> <span class="st">"gray"</span>)</span>
<span id="cb110-555"><a href="#cb110-555" aria-hidden="true" tabindex="-1"></a><span class="fu">legend</span>(<span class="st">"topright"</span>, <span class="fu">c</span>(<span class="st">"B-spline"</span>, <span class="st">"Natural spline"</span>, <span class="st">"Data range"</span>),</span>
<span id="cb110-556"><a href="#cb110-556" aria-hidden="true" tabindex="-1"></a>       <span class="at">col =</span> <span class="fu">c</span>(<span class="st">"blue"</span>, <span class="st">"darkgreen"</span>, <span class="st">"gray"</span>), <span class="at">lty =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">2</span>), <span class="at">lwd =</span> <span class="fu">c</span>(<span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">1</span>))</span>
<span id="cb110-557"><a href="#cb110-557" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb110-558"><a href="#cb110-558" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-559"><a href="#cb110-559" aria-hidden="true" tabindex="-1"></a><span class="fu">### Smoothing Splines</span></span>
<span id="cb110-560"><a href="#cb110-560" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-561"><a href="#cb110-561" aria-hidden="true" tabindex="-1"></a>**Smoothing splines** take a different approach: instead of pre-specifying knots, they place a knot at every data point and control smoothness through a penalty on the second derivative:</span>
<span id="cb110-562"><a href="#cb110-562" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-563"><a href="#cb110-563" aria-hidden="true" tabindex="-1"></a>$$\text{Minimize: } \sum_{i=1}^n (y_i - f(x_i))^2 + \lambda \int f''(x)^2 dx$$</span>
<span id="cb110-564"><a href="#cb110-564" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-565"><a href="#cb110-565" aria-hidden="true" tabindex="-1"></a>The smoothing parameter $\lambda$ is typically chosen by cross-validation:</span>
<span id="cb110-566"><a href="#cb110-566" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-569"><a href="#cb110-569" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb110-570"><a href="#cb110-570" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-smoothing-spline</span></span>
<span id="cb110-571"><a href="#cb110-571" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "Smoothing spline with automatic cross-validation selection of the smoothing parameter"</span></span>
<span id="cb110-572"><a href="#cb110-572" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-width: 7</span></span>
<span id="cb110-573"><a href="#cb110-573" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-height: 5</span></span>
<span id="cb110-574"><a href="#cb110-574" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit smoothing spline with cross-validation</span></span>
<span id="cb110-575"><a href="#cb110-575" aria-hidden="true" tabindex="-1"></a>smooth_fit <span class="ot">&lt;-</span> <span class="fu">smooth.spline</span>(x, y, <span class="at">cv =</span> <span class="cn">TRUE</span>)</span>
<span id="cb110-576"><a href="#cb110-576" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-577"><a href="#cb110-577" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(x, y, <span class="at">pch =</span> <span class="dv">16</span>, <span class="at">col =</span> <span class="st">"gray60"</span>, <span class="at">main =</span> <span class="st">"Smoothing Spline"</span>)</span>
<span id="cb110-578"><a href="#cb110-578" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(smooth_fit, <span class="at">col =</span> <span class="st">"purple"</span>, <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb110-579"><a href="#cb110-579" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"Optimal degrees of freedom:"</span>, <span class="fu">round</span>(smooth_fit<span class="sc">$</span>df, <span class="dv">2</span>), <span class="st">"</span><span class="sc">\n</span><span class="st">"</span>)</span>
<span id="cb110-580"><a href="#cb110-580" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb110-581"><a href="#cb110-581" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-582"><a href="#cb110-582" aria-hidden="true" tabindex="-1"></a>::: {.callout-note}</span>
<span id="cb110-583"><a href="#cb110-583" aria-hidden="true" tabindex="-1"></a><span class="fu">## Choosing the Right Approach</span></span>
<span id="cb110-584"><a href="#cb110-584" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-585"><a href="#cb110-585" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Regression splines (bs, ns)**: When you want to include splines in a regression model with other predictors</span>
<span id="cb110-586"><a href="#cb110-586" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Natural splines**: When extrapolation behavior matters</span>
<span id="cb110-587"><a href="#cb110-587" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Smoothing splines**: For exploratory smoothing with automatic tuning</span>
<span id="cb110-588"><a href="#cb110-588" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**LOESS**: For local, non-parametric smoothing (especially useful for visualization)</span>
<span id="cb110-589"><a href="#cb110-589" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb110-590"><a href="#cb110-590" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-591"><a href="#cb110-591" aria-hidden="true" tabindex="-1"></a><span class="fu">## LOESS: Flexible Non-Parametric Smoothing</span></span>
<span id="cb110-592"><a href="#cb110-592" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-593"><a href="#cb110-593" aria-hidden="true" tabindex="-1"></a>**LOESS** (Locally Estimated Scatterplot Smoothing) <span class="co">[</span><span class="ot">@cleveland1979robust</span><span class="co">]</span> fits local regressions to subsets of data, weighted by distance from each point.</span>
<span id="cb110-594"><a href="#cb110-594" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-597"><a href="#cb110-597" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb110-598"><a href="#cb110-598" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-loess-comparison</span></span>
<span id="cb110-599"><a href="#cb110-599" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "Comparison of linear regression and LOESS smoothing for non-linear data"</span></span>
<span id="cb110-600"><a href="#cb110-600" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-width: 7</span></span>
<span id="cb110-601"><a href="#cb110-601" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-height: 5</span></span>
<span id="cb110-602"><a href="#cb110-602" aria-hidden="true" tabindex="-1"></a><span class="co"># Compare linear regression and LOESS</span></span>
<span id="cb110-603"><a href="#cb110-603" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb110-604"><a href="#cb110-604" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="dv">0</span>, <span class="dv">4</span><span class="sc">*</span>pi, <span class="at">length.out =</span> <span class="dv">100</span>)</span>
<span id="cb110-605"><a href="#cb110-605" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> <span class="fu">sin</span>(x) <span class="sc">+</span> <span class="fu">rnorm</span>(<span class="dv">100</span>, <span class="at">sd =</span> <span class="fl">0.3</span>)</span>
<span id="cb110-606"><a href="#cb110-606" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-607"><a href="#cb110-607" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(x, y, <span class="at">pch =</span> <span class="dv">16</span>, <span class="at">col =</span> <span class="st">"gray60"</span>, <span class="at">main =</span> <span class="st">"Linear vs LOESS"</span>)</span>
<span id="cb110-608"><a href="#cb110-608" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="fu">lm</span>(y <span class="sc">~</span> x), <span class="at">col =</span> <span class="st">"red"</span>, <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb110-609"><a href="#cb110-609" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(x, <span class="fu">predict</span>(<span class="fu">loess</span>(y <span class="sc">~</span> x, <span class="at">span =</span> <span class="fl">0.3</span>)), <span class="at">col =</span> <span class="st">"blue"</span>, <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb110-610"><a href="#cb110-610" aria-hidden="true" tabindex="-1"></a><span class="fu">legend</span>(<span class="st">"topright"</span>, <span class="fu">c</span>(<span class="st">"Linear"</span>, <span class="st">"LOESS"</span>), <span class="at">col =</span> <span class="fu">c</span>(<span class="st">"red"</span>, <span class="st">"blue"</span>), <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb110-611"><a href="#cb110-611" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb110-612"><a href="#cb110-612" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-613"><a href="#cb110-613" aria-hidden="true" tabindex="-1"></a>The **span** parameter controls smoothness: smaller values fit more locally (more flexible), larger values average more broadly (smoother).</span>
<span id="cb110-614"><a href="#cb110-614" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-615"><a href="#cb110-615" aria-hidden="true" tabindex="-1"></a><span class="fu">## Classification</span></span>
<span id="cb110-616"><a href="#cb110-616" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-617"><a href="#cb110-617" aria-hidden="true" tabindex="-1"></a>When the response is categorical, we have a classification problem rather than regression. The goal is to predict which category an observation belongs to.</span>
<span id="cb110-618"><a href="#cb110-618" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-619"><a href="#cb110-619" aria-hidden="true" tabindex="-1"></a>**Logistic regression** produces probabilities that can be converted to class predictions.</span>
<span id="cb110-620"><a href="#cb110-620" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-621"><a href="#cb110-621" aria-hidden="true" tabindex="-1"></a>**Decision trees** recursively partition the feature space based on simple rules.</span>
<span id="cb110-622"><a href="#cb110-622" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-623"><a href="#cb110-623" aria-hidden="true" tabindex="-1"></a>**Random forests** combine many decision trees for more robust predictions.</span>
<span id="cb110-624"><a href="#cb110-624" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-625"><a href="#cb110-625" aria-hidden="true" tabindex="-1"></a><span class="fu">## K-Nearest Neighbors</span></span>
<span id="cb110-626"><a href="#cb110-626" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-627"><a href="#cb110-627" aria-hidden="true" tabindex="-1"></a>**K-nearest neighbors (kNN)** is one of the simplest and most intuitive classification algorithms. To classify a new observation, kNN finds the k closest observations in the training data and assigns the most common class among those neighbors.</span>
<span id="cb110-628"><a href="#cb110-628" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-631"><a href="#cb110-631" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb110-632"><a href="#cb110-632" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-knn-concept</span></span>
<span id="cb110-633"><a href="#cb110-633" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "K-nearest neighbors classification: the new point (star) is classified based on its nearest neighbors"</span></span>
<span id="cb110-634"><a href="#cb110-634" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-width: 8</span></span>
<span id="cb110-635"><a href="#cb110-635" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-height: 5</span></span>
<span id="cb110-636"><a href="#cb110-636" aria-hidden="true" tabindex="-1"></a><span class="co"># Simulate two-class data</span></span>
<span id="cb110-637"><a href="#cb110-637" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">42</span>)</span>
<span id="cb110-638"><a href="#cb110-638" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">100</span></span>
<span id="cb110-639"><a href="#cb110-639" aria-hidden="true" tabindex="-1"></a>class1 <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(</span>
<span id="cb110-640"><a href="#cb110-640" aria-hidden="true" tabindex="-1"></a>  <span class="at">x1 =</span> <span class="fu">rnorm</span>(n<span class="sc">/</span><span class="dv">2</span>, <span class="at">mean =</span> <span class="dv">2</span>, <span class="at">sd =</span> <span class="dv">1</span>),</span>
<span id="cb110-641"><a href="#cb110-641" aria-hidden="true" tabindex="-1"></a>  <span class="at">x2 =</span> <span class="fu">rnorm</span>(n<span class="sc">/</span><span class="dv">2</span>, <span class="at">mean =</span> <span class="dv">2</span>, <span class="at">sd =</span> <span class="dv">1</span>),</span>
<span id="cb110-642"><a href="#cb110-642" aria-hidden="true" tabindex="-1"></a>  <span class="at">class =</span> <span class="st">"A"</span></span>
<span id="cb110-643"><a href="#cb110-643" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb110-644"><a href="#cb110-644" aria-hidden="true" tabindex="-1"></a>class2 <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(</span>
<span id="cb110-645"><a href="#cb110-645" aria-hidden="true" tabindex="-1"></a>  <span class="at">x1 =</span> <span class="fu">rnorm</span>(n<span class="sc">/</span><span class="dv">2</span>, <span class="at">mean =</span> <span class="dv">4</span>, <span class="at">sd =</span> <span class="dv">1</span>),</span>
<span id="cb110-646"><a href="#cb110-646" aria-hidden="true" tabindex="-1"></a>  <span class="at">x2 =</span> <span class="fu">rnorm</span>(n<span class="sc">/</span><span class="dv">2</span>, <span class="at">mean =</span> <span class="dv">4</span>, <span class="at">sd =</span> <span class="dv">1</span>),</span>
<span id="cb110-647"><a href="#cb110-647" aria-hidden="true" tabindex="-1"></a>  <span class="at">class =</span> <span class="st">"B"</span></span>
<span id="cb110-648"><a href="#cb110-648" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb110-649"><a href="#cb110-649" aria-hidden="true" tabindex="-1"></a>train_data <span class="ot">&lt;-</span> <span class="fu">rbind</span>(class1, class2)</span>
<span id="cb110-650"><a href="#cb110-650" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-651"><a href="#cb110-651" aria-hidden="true" tabindex="-1"></a><span class="co"># New point to classify</span></span>
<span id="cb110-652"><a href="#cb110-652" aria-hidden="true" tabindex="-1"></a>new_point <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">x1 =</span> <span class="fl">3.2</span>, <span class="at">x2 =</span> <span class="fl">3.5</span>)</span>
<span id="cb110-653"><a href="#cb110-653" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-654"><a href="#cb110-654" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot</span></span>
<span id="cb110-655"><a href="#cb110-655" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(train_data<span class="sc">$</span>x1, train_data<span class="sc">$</span>x2,</span>
<span id="cb110-656"><a href="#cb110-656" aria-hidden="true" tabindex="-1"></a>     <span class="at">col =</span> <span class="fu">ifelse</span>(train_data<span class="sc">$</span>class <span class="sc">==</span> <span class="st">"A"</span>, <span class="st">"blue"</span>, <span class="st">"red"</span>),</span>
<span id="cb110-657"><a href="#cb110-657" aria-hidden="true" tabindex="-1"></a>     <span class="at">pch =</span> <span class="dv">16</span>, <span class="at">xlab =</span> <span class="st">"Feature 1"</span>, <span class="at">ylab =</span> <span class="st">"Feature 2"</span>,</span>
<span id="cb110-658"><a href="#cb110-658" aria-hidden="true" tabindex="-1"></a>     <span class="at">main =</span> <span class="st">"K-Nearest Neighbors (k=5)"</span>)</span>
<span id="cb110-659"><a href="#cb110-659" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span>(new_point<span class="sc">$</span>x1, new_point<span class="sc">$</span>x2, <span class="at">pch =</span> <span class="dv">8</span>, <span class="at">cex =</span> <span class="dv">2</span>, <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb110-660"><a href="#cb110-660" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-661"><a href="#cb110-661" aria-hidden="true" tabindex="-1"></a><span class="co"># Find 5 nearest neighbors</span></span>
<span id="cb110-662"><a href="#cb110-662" aria-hidden="true" tabindex="-1"></a>distances <span class="ot">&lt;-</span> <span class="fu">sqrt</span>((train_data<span class="sc">$</span>x1 <span class="sc">-</span> new_point<span class="sc">$</span>x1)<span class="sc">^</span><span class="dv">2</span> <span class="sc">+</span></span>
<span id="cb110-663"><a href="#cb110-663" aria-hidden="true" tabindex="-1"></a>                  (train_data<span class="sc">$</span>x2 <span class="sc">-</span> new_point<span class="sc">$</span>x2)<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb110-664"><a href="#cb110-664" aria-hidden="true" tabindex="-1"></a>nearest <span class="ot">&lt;-</span> <span class="fu">order</span>(distances)[<span class="dv">1</span><span class="sc">:</span><span class="dv">5</span>]</span>
<span id="cb110-665"><a href="#cb110-665" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-666"><a href="#cb110-666" aria-hidden="true" tabindex="-1"></a><span class="co"># Draw circles around nearest neighbors</span></span>
<span id="cb110-667"><a href="#cb110-667" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span>(train_data<span class="sc">$</span>x1[nearest], train_data<span class="sc">$</span>x2[nearest],</span>
<span id="cb110-668"><a href="#cb110-668" aria-hidden="true" tabindex="-1"></a>       <span class="at">cex =</span> <span class="dv">2</span>, <span class="at">col =</span> <span class="fu">ifelse</span>(train_data<span class="sc">$</span>class[nearest] <span class="sc">==</span> <span class="st">"A"</span>, <span class="st">"blue"</span>, <span class="st">"red"</span>))</span>
<span id="cb110-669"><a href="#cb110-669" aria-hidden="true" tabindex="-1"></a><span class="fu">legend</span>(<span class="st">"topleft"</span>, <span class="fu">c</span>(<span class="st">"Class A"</span>, <span class="st">"Class B"</span>, <span class="st">"New point"</span>),</span>
<span id="cb110-670"><a href="#cb110-670" aria-hidden="true" tabindex="-1"></a>       <span class="at">col =</span> <span class="fu">c</span>(<span class="st">"blue"</span>, <span class="st">"red"</span>, <span class="st">"black"</span>), <span class="at">pch =</span> <span class="fu">c</span>(<span class="dv">16</span>, <span class="dv">16</span>, <span class="dv">8</span>))</span>
<span id="cb110-671"><a href="#cb110-671" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb110-672"><a href="#cb110-672" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-673"><a href="#cb110-673" aria-hidden="true" tabindex="-1"></a>The choice of **k** is critical and illustrates the bias-variance tradeoff:</span>
<span id="cb110-674"><a href="#cb110-674" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-675"><a href="#cb110-675" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Small k** (e.g., k=1): Very flexible, low bias but high variance. The decision boundary is jagged and sensitive to individual training points—prone to overfitting.</span>
<span id="cb110-676"><a href="#cb110-676" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Large k**: Smoother decision boundary, higher bias but lower variance. May miss local patterns—prone to underfitting.</span>
<span id="cb110-677"><a href="#cb110-677" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-680"><a href="#cb110-680" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb110-681"><a href="#cb110-681" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-knn-k-comparison</span></span>
<span id="cb110-682"><a href="#cb110-682" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "Effect of k on kNN classification: small k creates complex boundaries (potential overfitting), large k creates smooth boundaries (potential underfitting)"</span></span>
<span id="cb110-683"><a href="#cb110-683" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-width: 9</span></span>
<span id="cb110-684"><a href="#cb110-684" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-height: 4</span></span>
<span id="cb110-685"><a href="#cb110-685" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(class)</span>
<span id="cb110-686"><a href="#cb110-686" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-687"><a href="#cb110-687" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a grid for visualization</span></span>
<span id="cb110-688"><a href="#cb110-688" aria-hidden="true" tabindex="-1"></a>x1_grid <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="dv">0</span>, <span class="dv">6</span>, <span class="at">length.out =</span> <span class="dv">100</span>)</span>
<span id="cb110-689"><a href="#cb110-689" aria-hidden="true" tabindex="-1"></a>x2_grid <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="dv">0</span>, <span class="dv">6</span>, <span class="at">length.out =</span> <span class="dv">100</span>)</span>
<span id="cb110-690"><a href="#cb110-690" aria-hidden="true" tabindex="-1"></a>grid <span class="ot">&lt;-</span> <span class="fu">expand.grid</span>(<span class="at">x1 =</span> x1_grid, <span class="at">x2 =</span> x2_grid)</span>
<span id="cb110-691"><a href="#cb110-691" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-692"><a href="#cb110-692" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">3</span>))</span>
<span id="cb110-693"><a href="#cb110-693" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-694"><a href="#cb110-694" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (k_val <span class="cf">in</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">15</span>, <span class="dv">50</span>)) {</span>
<span id="cb110-695"><a href="#cb110-695" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Predict on grid</span></span>
<span id="cb110-696"><a href="#cb110-696" aria-hidden="true" tabindex="-1"></a>  pred <span class="ot">&lt;-</span> <span class="fu">knn</span>(<span class="at">train =</span> train_data[, <span class="dv">1</span><span class="sc">:</span><span class="dv">2</span>],</span>
<span id="cb110-697"><a href="#cb110-697" aria-hidden="true" tabindex="-1"></a>              <span class="at">test =</span> grid,</span>
<span id="cb110-698"><a href="#cb110-698" aria-hidden="true" tabindex="-1"></a>              <span class="at">cl =</span> train_data<span class="sc">$</span>class,</span>
<span id="cb110-699"><a href="#cb110-699" aria-hidden="true" tabindex="-1"></a>              <span class="at">k =</span> k_val)</span>
<span id="cb110-700"><a href="#cb110-700" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-701"><a href="#cb110-701" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Plot decision regions</span></span>
<span id="cb110-702"><a href="#cb110-702" aria-hidden="true" tabindex="-1"></a>  <span class="fu">plot</span>(grid<span class="sc">$</span>x1, grid<span class="sc">$</span>x2, <span class="at">col =</span> <span class="fu">ifelse</span>(pred <span class="sc">==</span> <span class="st">"A"</span>,</span>
<span id="cb110-703"><a href="#cb110-703" aria-hidden="true" tabindex="-1"></a>       <span class="fu">rgb</span>(<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">1</span>, <span class="fl">0.1</span>), <span class="fu">rgb</span>(<span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="fl">0.1</span>)),</span>
<span id="cb110-704"><a href="#cb110-704" aria-hidden="true" tabindex="-1"></a>       <span class="at">pch =</span> <span class="dv">15</span>, <span class="at">cex =</span> <span class="fl">0.5</span>, <span class="at">xlab =</span> <span class="st">"Feature 1"</span>, <span class="at">ylab =</span> <span class="st">"Feature 2"</span>,</span>
<span id="cb110-705"><a href="#cb110-705" aria-hidden="true" tabindex="-1"></a>       <span class="at">main =</span> <span class="fu">paste</span>(<span class="st">"k ="</span>, k_val))</span>
<span id="cb110-706"><a href="#cb110-706" aria-hidden="true" tabindex="-1"></a>  <span class="fu">points</span>(train_data<span class="sc">$</span>x1, train_data<span class="sc">$</span>x2,</span>
<span id="cb110-707"><a href="#cb110-707" aria-hidden="true" tabindex="-1"></a>         <span class="at">col =</span> <span class="fu">ifelse</span>(train_data<span class="sc">$</span>class <span class="sc">==</span> <span class="st">"A"</span>, <span class="st">"blue"</span>, <span class="st">"red"</span>), <span class="at">pch =</span> <span class="dv">16</span>)</span>
<span id="cb110-708"><a href="#cb110-708" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb110-709"><a href="#cb110-709" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb110-710"><a href="#cb110-710" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-711"><a href="#cb110-711" aria-hidden="true" tabindex="-1"></a><span class="fu">### Selecting k with Cross-Validation</span></span>
<span id="cb110-712"><a href="#cb110-712" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-713"><a href="#cb110-713" aria-hidden="true" tabindex="-1"></a>We choose k by evaluating classification accuracy across different values using cross-validation:</span>
<span id="cb110-714"><a href="#cb110-714" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-717"><a href="#cb110-717" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb110-718"><a href="#cb110-718" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-knn-cv</span></span>
<span id="cb110-719"><a href="#cb110-719" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "Cross-validation accuracy for different values of k: accuracy on training data decreases with k, but test accuracy peaks at intermediate values"</span></span>
<span id="cb110-720"><a href="#cb110-720" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-width: 7</span></span>
<span id="cb110-721"><a href="#cb110-721" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-height: 5</span></span>
<span id="cb110-722"><a href="#cb110-722" aria-hidden="true" tabindex="-1"></a><span class="co"># Evaluate different k values</span></span>
<span id="cb110-723"><a href="#cb110-723" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb110-724"><a href="#cb110-724" aria-hidden="true" tabindex="-1"></a>k_values <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="dv">1</span>, <span class="dv">50</span>, <span class="at">by =</span> <span class="dv">2</span>)</span>
<span id="cb110-725"><a href="#cb110-725" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-726"><a href="#cb110-726" aria-hidden="true" tabindex="-1"></a><span class="co"># Simple holdout validation</span></span>
<span id="cb110-727"><a href="#cb110-727" aria-hidden="true" tabindex="-1"></a>test_idx <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="dv">1</span><span class="sc">:</span><span class="fu">nrow</span>(train_data), <span class="dv">30</span>)</span>
<span id="cb110-728"><a href="#cb110-728" aria-hidden="true" tabindex="-1"></a>train_subset <span class="ot">&lt;-</span> train_data[<span class="sc">-</span>test_idx, ]</span>
<span id="cb110-729"><a href="#cb110-729" aria-hidden="true" tabindex="-1"></a>test_subset <span class="ot">&lt;-</span> train_data[test_idx, ]</span>
<span id="cb110-730"><a href="#cb110-730" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-731"><a href="#cb110-731" aria-hidden="true" tabindex="-1"></a>accuracy <span class="ot">&lt;-</span> <span class="fu">sapply</span>(k_values, <span class="cf">function</span>(k) {</span>
<span id="cb110-732"><a href="#cb110-732" aria-hidden="true" tabindex="-1"></a>  pred <span class="ot">&lt;-</span> <span class="fu">knn</span>(<span class="at">train =</span> train_subset[, <span class="dv">1</span><span class="sc">:</span><span class="dv">2</span>],</span>
<span id="cb110-733"><a href="#cb110-733" aria-hidden="true" tabindex="-1"></a>              <span class="at">test =</span> test_subset[, <span class="dv">1</span><span class="sc">:</span><span class="dv">2</span>],</span>
<span id="cb110-734"><a href="#cb110-734" aria-hidden="true" tabindex="-1"></a>              <span class="at">cl =</span> train_subset<span class="sc">$</span>class,</span>
<span id="cb110-735"><a href="#cb110-735" aria-hidden="true" tabindex="-1"></a>              <span class="at">k =</span> k)</span>
<span id="cb110-736"><a href="#cb110-736" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mean</span>(pred <span class="sc">==</span> test_subset<span class="sc">$</span>class)</span>
<span id="cb110-737"><a href="#cb110-737" aria-hidden="true" tabindex="-1"></a>})</span>
<span id="cb110-738"><a href="#cb110-738" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-739"><a href="#cb110-739" aria-hidden="true" tabindex="-1"></a>train_accuracy <span class="ot">&lt;-</span> <span class="fu">sapply</span>(k_values, <span class="cf">function</span>(k) {</span>
<span id="cb110-740"><a href="#cb110-740" aria-hidden="true" tabindex="-1"></a>  pred <span class="ot">&lt;-</span> <span class="fu">knn</span>(<span class="at">train =</span> train_subset[, <span class="dv">1</span><span class="sc">:</span><span class="dv">2</span>],</span>
<span id="cb110-741"><a href="#cb110-741" aria-hidden="true" tabindex="-1"></a>              <span class="at">test =</span> train_subset[, <span class="dv">1</span><span class="sc">:</span><span class="dv">2</span>],</span>
<span id="cb110-742"><a href="#cb110-742" aria-hidden="true" tabindex="-1"></a>              <span class="at">cl =</span> train_subset<span class="sc">$</span>class,</span>
<span id="cb110-743"><a href="#cb110-743" aria-hidden="true" tabindex="-1"></a>              <span class="at">k =</span> k)</span>
<span id="cb110-744"><a href="#cb110-744" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mean</span>(pred <span class="sc">==</span> train_subset<span class="sc">$</span>class)</span>
<span id="cb110-745"><a href="#cb110-745" aria-hidden="true" tabindex="-1"></a>})</span>
<span id="cb110-746"><a href="#cb110-746" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-747"><a href="#cb110-747" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(k_values, train_accuracy, <span class="at">type =</span> <span class="st">"l"</span>, <span class="at">col =</span> <span class="st">"blue"</span>, <span class="at">lwd =</span> <span class="dv">2</span>,</span>
<span id="cb110-748"><a href="#cb110-748" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">"k (number of neighbors)"</span>, <span class="at">ylab =</span> <span class="st">"Accuracy"</span>,</span>
<span id="cb110-749"><a href="#cb110-749" aria-hidden="true" tabindex="-1"></a>     <span class="at">main =</span> <span class="st">"Training vs Test Accuracy"</span>, <span class="at">ylim =</span> <span class="fu">c</span>(<span class="fl">0.5</span>, <span class="dv">1</span>))</span>
<span id="cb110-750"><a href="#cb110-750" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(k_values, accuracy, <span class="at">col =</span> <span class="st">"red"</span>, <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb110-751"><a href="#cb110-751" aria-hidden="true" tabindex="-1"></a><span class="fu">legend</span>(<span class="st">"bottomright"</span>, <span class="fu">c</span>(<span class="st">"Training"</span>, <span class="st">"Test"</span>), <span class="at">col =</span> <span class="fu">c</span>(<span class="st">"blue"</span>, <span class="st">"red"</span>), <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb110-752"><a href="#cb110-752" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb110-753"><a href="#cb110-753" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-754"><a href="#cb110-754" aria-hidden="true" tabindex="-1"></a>Notice that training accuracy is perfect (1.0) when k=1—each point is its own nearest neighbor. But test accuracy tells the true story of generalization performance.</span>
<span id="cb110-755"><a href="#cb110-755" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-756"><a href="#cb110-756" aria-hidden="true" tabindex="-1"></a><span class="fu">## Confusion Matrices</span></span>
<span id="cb110-757"><a href="#cb110-757" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-758"><a href="#cb110-758" aria-hidden="true" tabindex="-1"></a>Classification performance is evaluated with a **confusion matrix**:</span>
<span id="cb110-759"><a href="#cb110-759" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-760"><a href="#cb110-760" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span>  <span class="pp">|</span> Predicted Positive <span class="pp">|</span> Predicted Negative <span class="pp">|</span></span>
<span id="cb110-761"><a href="#cb110-761" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span>:--<span class="pp">|</span>:--:<span class="pp">|</span>:--:<span class="pp">|</span></span>
<span id="cb110-762"><a href="#cb110-762" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> Actual Positive <span class="pp">|</span> True Positive (TP) <span class="pp">|</span> False Negative (FN) <span class="pp">|</span></span>
<span id="cb110-763"><a href="#cb110-763" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> Actual Negative <span class="pp">|</span> False Positive (FP) <span class="pp">|</span> True Negative (TN) <span class="pp">|</span></span>
<span id="cb110-764"><a href="#cb110-764" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-765"><a href="#cb110-765" aria-hidden="true" tabindex="-1"></a>Key metrics:</span>
<span id="cb110-766"><a href="#cb110-766" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Accuracy**: (TP + TN) / Total</span>
<span id="cb110-767"><a href="#cb110-767" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Sensitivity** (Recall): TP / (TP + FN) — how many positives were caught</span>
<span id="cb110-768"><a href="#cb110-768" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Specificity**: TN / (TN + FP) — how many negatives were correctly identified</span>
<span id="cb110-769"><a href="#cb110-769" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Precision**: TP / (TP + FP) — among positive predictions, how many were correct</span>
<span id="cb110-770"><a href="#cb110-770" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-771"><a href="#cb110-771" aria-hidden="true" tabindex="-1"></a><span class="fu">### The Problem with Accuracy</span></span>
<span id="cb110-772"><a href="#cb110-772" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-773"><a href="#cb110-773" aria-hidden="true" tabindex="-1"></a>Accuracy can be misleading with **imbalanced classes**. If 95% of emails are legitimate, a classifier that labels everything as "not spam" achieves 95% accuracy while being completely useless for its intended purpose.</span>
<span id="cb110-774"><a href="#cb110-774" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-777"><a href="#cb110-777" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb110-778"><a href="#cb110-778" aria-hidden="true" tabindex="-1"></a><span class="co"># Imbalanced class example</span></span>
<span id="cb110-779"><a href="#cb110-779" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">42</span>)</span>
<span id="cb110-780"><a href="#cb110-780" aria-hidden="true" tabindex="-1"></a><span class="co"># 95% negative, 5% positive (e.g., rare disease screening)</span></span>
<span id="cb110-781"><a href="#cb110-781" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">1000</span></span>
<span id="cb110-782"><a href="#cb110-782" aria-hidden="true" tabindex="-1"></a>actual <span class="ot">&lt;-</span> <span class="fu">factor</span>(<span class="fu">c</span>(<span class="fu">rep</span>(<span class="st">"Negative"</span>, <span class="dv">950</span>), <span class="fu">rep</span>(<span class="st">"Positive"</span>, <span class="dv">50</span>)))</span>
<span id="cb110-783"><a href="#cb110-783" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-784"><a href="#cb110-784" aria-hidden="true" tabindex="-1"></a><span class="co"># Naive classifier: always predict negative</span></span>
<span id="cb110-785"><a href="#cb110-785" aria-hidden="true" tabindex="-1"></a>naive_pred <span class="ot">&lt;-</span> <span class="fu">factor</span>(<span class="fu">rep</span>(<span class="st">"Negative"</span>, n), <span class="at">levels =</span> <span class="fu">c</span>(<span class="st">"Negative"</span>, <span class="st">"Positive"</span>))</span>
<span id="cb110-786"><a href="#cb110-786" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-787"><a href="#cb110-787" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate metrics</span></span>
<span id="cb110-788"><a href="#cb110-788" aria-hidden="true" tabindex="-1"></a>TP <span class="ot">&lt;-</span> <span class="fu">sum</span>(naive_pred <span class="sc">==</span> <span class="st">"Positive"</span> <span class="sc">&amp;</span> actual <span class="sc">==</span> <span class="st">"Positive"</span>)</span>
<span id="cb110-789"><a href="#cb110-789" aria-hidden="true" tabindex="-1"></a>TN <span class="ot">&lt;-</span> <span class="fu">sum</span>(naive_pred <span class="sc">==</span> <span class="st">"Negative"</span> <span class="sc">&amp;</span> actual <span class="sc">==</span> <span class="st">"Negative"</span>)</span>
<span id="cb110-790"><a href="#cb110-790" aria-hidden="true" tabindex="-1"></a>FP <span class="ot">&lt;-</span> <span class="fu">sum</span>(naive_pred <span class="sc">==</span> <span class="st">"Positive"</span> <span class="sc">&amp;</span> actual <span class="sc">==</span> <span class="st">"Negative"</span>)</span>
<span id="cb110-791"><a href="#cb110-791" aria-hidden="true" tabindex="-1"></a>FN <span class="ot">&lt;-</span> <span class="fu">sum</span>(naive_pred <span class="sc">==</span> <span class="st">"Negative"</span> <span class="sc">&amp;</span> actual <span class="sc">==</span> <span class="st">"Positive"</span>)</span>
<span id="cb110-792"><a href="#cb110-792" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-793"><a href="#cb110-793" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"Accuracy:"</span>, (TP <span class="sc">+</span> TN) <span class="sc">/</span> n, <span class="st">"</span><span class="sc">\n</span><span class="st">"</span>)</span>
<span id="cb110-794"><a href="#cb110-794" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"Sensitivity (Recall):"</span>, TP <span class="sc">/</span> (TP <span class="sc">+</span> FN), <span class="st">"</span><span class="sc">\n</span><span class="st">"</span>)</span>
<span id="cb110-795"><a href="#cb110-795" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"The classifier catches 0% of positive cases!</span><span class="sc">\n</span><span class="st">"</span>)</span>
<span id="cb110-796"><a href="#cb110-796" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb110-797"><a href="#cb110-797" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-798"><a href="#cb110-798" aria-hidden="true" tabindex="-1"></a><span class="fu">### F1 Score and Balanced Accuracy</span></span>
<span id="cb110-799"><a href="#cb110-799" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-800"><a href="#cb110-800" aria-hidden="true" tabindex="-1"></a>For imbalanced data, better metrics include:</span>
<span id="cb110-801"><a href="#cb110-801" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-802"><a href="#cb110-802" aria-hidden="true" tabindex="-1"></a>**F1 Score**: The harmonic mean of precision and recall, balancing both concerns:</span>
<span id="cb110-803"><a href="#cb110-803" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-804"><a href="#cb110-804" aria-hidden="true" tabindex="-1"></a>$$F_1 = 2 \cdot \frac{\text{Precision} \cdot \text{Recall}}{\text{Precision} + \text{Recall}} = \frac{2 \cdot TP}{2 \cdot TP + FP + FN}$$</span>
<span id="cb110-805"><a href="#cb110-805" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-806"><a href="#cb110-806" aria-hidden="true" tabindex="-1"></a>**Balanced Accuracy**: The average of sensitivity and specificity:</span>
<span id="cb110-807"><a href="#cb110-807" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-808"><a href="#cb110-808" aria-hidden="true" tabindex="-1"></a>$$\text{Balanced Accuracy} = \frac{\text{Sensitivity} + \text{Specificity}}{2}$$</span>
<span id="cb110-809"><a href="#cb110-809" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-812"><a href="#cb110-812" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb110-813"><a href="#cb110-813" aria-hidden="true" tabindex="-1"></a><span class="co"># Better classifier for the imbalanced data</span></span>
<span id="cb110-814"><a href="#cb110-814" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb110-815"><a href="#cb110-815" aria-hidden="true" tabindex="-1"></a><span class="co"># Suppose we have a model that catches 80% of positives but has some false positives</span></span>
<span id="cb110-816"><a href="#cb110-816" aria-hidden="true" tabindex="-1"></a>better_pred <span class="ot">&lt;-</span> actual  <span class="co"># Start with actual</span></span>
<span id="cb110-817"><a href="#cb110-817" aria-hidden="true" tabindex="-1"></a><span class="co"># Correctly identify 80% of positives</span></span>
<span id="cb110-818"><a href="#cb110-818" aria-hidden="true" tabindex="-1"></a>pos_idx <span class="ot">&lt;-</span> <span class="fu">which</span>(actual <span class="sc">==</span> <span class="st">"Positive"</span>)</span>
<span id="cb110-819"><a href="#cb110-819" aria-hidden="true" tabindex="-1"></a>neg_idx <span class="ot">&lt;-</span> <span class="fu">which</span>(actual <span class="sc">==</span> <span class="st">"Negative"</span>)</span>
<span id="cb110-820"><a href="#cb110-820" aria-hidden="true" tabindex="-1"></a>better_pred[<span class="fu">sample</span>(pos_idx, <span class="dv">10</span>)] <span class="ot">&lt;-</span> <span class="st">"Negative"</span>  <span class="co"># Miss 10 of 50 positives (20%)</span></span>
<span id="cb110-821"><a href="#cb110-821" aria-hidden="true" tabindex="-1"></a>better_pred[<span class="fu">sample</span>(neg_idx, <span class="dv">50</span>)] <span class="ot">&lt;-</span> <span class="st">"Positive"</span>  <span class="co"># 50 false positives</span></span>
<span id="cb110-822"><a href="#cb110-822" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-823"><a href="#cb110-823" aria-hidden="true" tabindex="-1"></a><span class="co"># Confusion matrix</span></span>
<span id="cb110-824"><a href="#cb110-824" aria-hidden="true" tabindex="-1"></a>TP <span class="ot">&lt;-</span> <span class="fu">sum</span>(better_pred <span class="sc">==</span> <span class="st">"Positive"</span> <span class="sc">&amp;</span> actual <span class="sc">==</span> <span class="st">"Positive"</span>)</span>
<span id="cb110-825"><a href="#cb110-825" aria-hidden="true" tabindex="-1"></a>TN <span class="ot">&lt;-</span> <span class="fu">sum</span>(better_pred <span class="sc">==</span> <span class="st">"Negative"</span> <span class="sc">&amp;</span> actual <span class="sc">==</span> <span class="st">"Negative"</span>)</span>
<span id="cb110-826"><a href="#cb110-826" aria-hidden="true" tabindex="-1"></a>FP <span class="ot">&lt;-</span> <span class="fu">sum</span>(better_pred <span class="sc">==</span> <span class="st">"Positive"</span> <span class="sc">&amp;</span> actual <span class="sc">==</span> <span class="st">"Negative"</span>)</span>
<span id="cb110-827"><a href="#cb110-827" aria-hidden="true" tabindex="-1"></a>FN <span class="ot">&lt;-</span> <span class="fu">sum</span>(better_pred <span class="sc">==</span> <span class="st">"Negative"</span> <span class="sc">&amp;</span> actual <span class="sc">==</span> <span class="st">"Positive"</span>)</span>
<span id="cb110-828"><a href="#cb110-828" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-829"><a href="#cb110-829" aria-hidden="true" tabindex="-1"></a>precision <span class="ot">&lt;-</span> TP <span class="sc">/</span> (TP <span class="sc">+</span> FP)</span>
<span id="cb110-830"><a href="#cb110-830" aria-hidden="true" tabindex="-1"></a>recall <span class="ot">&lt;-</span> TP <span class="sc">/</span> (TP <span class="sc">+</span> FN)  <span class="co"># Sensitivity</span></span>
<span id="cb110-831"><a href="#cb110-831" aria-hidden="true" tabindex="-1"></a>specificity <span class="ot">&lt;-</span> TN <span class="sc">/</span> (TN <span class="sc">+</span> FP)</span>
<span id="cb110-832"><a href="#cb110-832" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-833"><a href="#cb110-833" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate metrics</span></span>
<span id="cb110-834"><a href="#cb110-834" aria-hidden="true" tabindex="-1"></a>accuracy <span class="ot">&lt;-</span> (TP <span class="sc">+</span> TN) <span class="sc">/</span> n</span>
<span id="cb110-835"><a href="#cb110-835" aria-hidden="true" tabindex="-1"></a>f1 <span class="ot">&lt;-</span> <span class="dv">2</span> <span class="sc">*</span> precision <span class="sc">*</span> recall <span class="sc">/</span> (precision <span class="sc">+</span> recall)</span>
<span id="cb110-836"><a href="#cb110-836" aria-hidden="true" tabindex="-1"></a>balanced_acc <span class="ot">&lt;-</span> (recall <span class="sc">+</span> specificity) <span class="sc">/</span> <span class="dv">2</span></span>
<span id="cb110-837"><a href="#cb110-837" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-838"><a href="#cb110-838" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"Accuracy:"</span>, <span class="fu">round</span>(accuracy, <span class="dv">3</span>), <span class="st">"</span><span class="sc">\n</span><span class="st">"</span>)</span>
<span id="cb110-839"><a href="#cb110-839" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"Precision:"</span>, <span class="fu">round</span>(precision, <span class="dv">3</span>), <span class="st">"</span><span class="sc">\n</span><span class="st">"</span>)</span>
<span id="cb110-840"><a href="#cb110-840" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"Recall (Sensitivity):"</span>, <span class="fu">round</span>(recall, <span class="dv">3</span>), <span class="st">"</span><span class="sc">\n</span><span class="st">"</span>)</span>
<span id="cb110-841"><a href="#cb110-841" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"F1 Score:"</span>, <span class="fu">round</span>(f1, <span class="dv">3</span>), <span class="st">"</span><span class="sc">\n</span><span class="st">"</span>)</span>
<span id="cb110-842"><a href="#cb110-842" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"Balanced Accuracy:"</span>, <span class="fu">round</span>(balanced_acc, <span class="dv">3</span>), <span class="st">"</span><span class="sc">\n</span><span class="st">"</span>)</span>
<span id="cb110-843"><a href="#cb110-843" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb110-844"><a href="#cb110-844" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-845"><a href="#cb110-845" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip}</span>
<span id="cb110-846"><a href="#cb110-846" aria-hidden="true" tabindex="-1"></a><span class="fu">## Which Metric to Use?</span></span>
<span id="cb110-847"><a href="#cb110-847" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-848"><a href="#cb110-848" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Accuracy**: Only when classes are balanced</span>
<span id="cb110-849"><a href="#cb110-849" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**F1 Score**: When you care about both precision and recall equally</span>
<span id="cb110-850"><a href="#cb110-850" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Sensitivity/Recall**: When missing positives is costly (disease screening)</span>
<span id="cb110-851"><a href="#cb110-851" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Precision**: When false positives are costly (spam filtering)</span>
<span id="cb110-852"><a href="#cb110-852" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Balanced Accuracy**: Quick summary for imbalanced data</span>
<span id="cb110-853"><a href="#cb110-853" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb110-854"><a href="#cb110-854" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-855"><a href="#cb110-855" aria-hidden="true" tabindex="-1"></a><span class="fu">## ROC Curves and AUC</span></span>
<span id="cb110-856"><a href="#cb110-856" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-857"><a href="#cb110-857" aria-hidden="true" tabindex="-1"></a>Many classifiers output probabilities rather than hard class labels. By varying the **threshold** for classifying as positive, we trade off sensitivity against specificity.</span>
<span id="cb110-858"><a href="#cb110-858" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-859"><a href="#cb110-859" aria-hidden="true" tabindex="-1"></a>The **Receiver Operating Characteristic (ROC) curve** plots sensitivity (true positive rate) against 1 - specificity (false positive rate) at all possible thresholds.</span>
<span id="cb110-860"><a href="#cb110-860" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-863"><a href="#cb110-863" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb110-864"><a href="#cb110-864" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-roc-curve</span></span>
<span id="cb110-865"><a href="#cb110-865" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "ROC curve showing the tradeoff between sensitivity and false positive rate; the dashed diagonal represents random guessing"</span></span>
<span id="cb110-866"><a href="#cb110-866" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-width: 7</span></span>
<span id="cb110-867"><a href="#cb110-867" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-height: 6</span></span>
<span id="cb110-868"><a href="#cb110-868" aria-hidden="true" tabindex="-1"></a><span class="co"># Simulate a classifier with probabilities</span></span>
<span id="cb110-869"><a href="#cb110-869" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">42</span>)</span>
<span id="cb110-870"><a href="#cb110-870" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">500</span></span>
<span id="cb110-871"><a href="#cb110-871" aria-hidden="true" tabindex="-1"></a>actual <span class="ot">&lt;-</span> <span class="fu">factor</span>(<span class="fu">c</span>(<span class="fu">rep</span>(<span class="dv">1</span>, <span class="dv">100</span>), <span class="fu">rep</span>(<span class="dv">0</span>, <span class="dv">400</span>)))  <span class="co"># 20% positive</span></span>
<span id="cb110-872"><a href="#cb110-872" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-873"><a href="#cb110-873" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate predicted probabilities (imperfect classifier)</span></span>
<span id="cb110-874"><a href="#cb110-874" aria-hidden="true" tabindex="-1"></a>probs <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fu">rbeta</span>(<span class="dv">100</span>, <span class="dv">3</span>, <span class="dv">2</span>),   <span class="co"># Positives: higher probs</span></span>
<span id="cb110-875"><a href="#cb110-875" aria-hidden="true" tabindex="-1"></a>           <span class="fu">rbeta</span>(<span class="dv">400</span>, <span class="dv">2</span>, <span class="dv">3</span>))   <span class="co"># Negatives: lower probs</span></span>
<span id="cb110-876"><a href="#cb110-876" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-877"><a href="#cb110-877" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate ROC curve manually</span></span>
<span id="cb110-878"><a href="#cb110-878" aria-hidden="true" tabindex="-1"></a>thresholds <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="at">by =</span> <span class="fl">0.01</span>)</span>
<span id="cb110-879"><a href="#cb110-879" aria-hidden="true" tabindex="-1"></a>roc_data <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(</span>
<span id="cb110-880"><a href="#cb110-880" aria-hidden="true" tabindex="-1"></a>  <span class="at">threshold =</span> thresholds,</span>
<span id="cb110-881"><a href="#cb110-881" aria-hidden="true" tabindex="-1"></a>  <span class="at">TPR =</span> <span class="fu">sapply</span>(thresholds, <span class="cf">function</span>(t) {</span>
<span id="cb110-882"><a href="#cb110-882" aria-hidden="true" tabindex="-1"></a>    pred <span class="ot">&lt;-</span> <span class="fu">ifelse</span>(probs <span class="sc">&gt;=</span> t, <span class="dv">1</span>, <span class="dv">0</span>)</span>
<span id="cb110-883"><a href="#cb110-883" aria-hidden="true" tabindex="-1"></a>    <span class="fu">sum</span>(pred <span class="sc">==</span> <span class="dv">1</span> <span class="sc">&amp;</span> actual <span class="sc">==</span> <span class="dv">1</span>) <span class="sc">/</span> <span class="fu">sum</span>(actual <span class="sc">==</span> <span class="dv">1</span>)</span>
<span id="cb110-884"><a href="#cb110-884" aria-hidden="true" tabindex="-1"></a>  }),</span>
<span id="cb110-885"><a href="#cb110-885" aria-hidden="true" tabindex="-1"></a>  <span class="at">FPR =</span> <span class="fu">sapply</span>(thresholds, <span class="cf">function</span>(t) {</span>
<span id="cb110-886"><a href="#cb110-886" aria-hidden="true" tabindex="-1"></a>    pred <span class="ot">&lt;-</span> <span class="fu">ifelse</span>(probs <span class="sc">&gt;=</span> t, <span class="dv">1</span>, <span class="dv">0</span>)</span>
<span id="cb110-887"><a href="#cb110-887" aria-hidden="true" tabindex="-1"></a>    <span class="fu">sum</span>(pred <span class="sc">==</span> <span class="dv">1</span> <span class="sc">&amp;</span> actual <span class="sc">==</span> <span class="dv">0</span>) <span class="sc">/</span> <span class="fu">sum</span>(actual <span class="sc">==</span> <span class="dv">0</span>)</span>
<span id="cb110-888"><a href="#cb110-888" aria-hidden="true" tabindex="-1"></a>  })</span>
<span id="cb110-889"><a href="#cb110-889" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb110-890"><a href="#cb110-890" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-891"><a href="#cb110-891" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot ROC curve</span></span>
<span id="cb110-892"><a href="#cb110-892" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(roc_data<span class="sc">$</span>FPR, roc_data<span class="sc">$</span>TPR, <span class="at">type =</span> <span class="st">"l"</span>, <span class="at">lwd =</span> <span class="dv">2</span>, <span class="at">col =</span> <span class="st">"blue"</span>,</span>
<span id="cb110-893"><a href="#cb110-893" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">"False Positive Rate (1 - Specificity)"</span>,</span>
<span id="cb110-894"><a href="#cb110-894" aria-hidden="true" tabindex="-1"></a>     <span class="at">ylab =</span> <span class="st">"True Positive Rate (Sensitivity)"</span>,</span>
<span id="cb110-895"><a href="#cb110-895" aria-hidden="true" tabindex="-1"></a>     <span class="at">main =</span> <span class="st">"ROC Curve"</span>)</span>
<span id="cb110-896"><a href="#cb110-896" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="at">lty =</span> <span class="dv">2</span>, <span class="at">col =</span> <span class="st">"gray"</span>)  <span class="co"># Random classifier line</span></span>
<span id="cb110-897"><a href="#cb110-897" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-898"><a href="#cb110-898" aria-hidden="true" tabindex="-1"></a><span class="co"># Add points for specific thresholds</span></span>
<span id="cb110-899"><a href="#cb110-899" aria-hidden="true" tabindex="-1"></a>highlight <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fl">0.3</span>, <span class="fl">0.5</span>, <span class="fl">0.7</span>)</span>
<span id="cb110-900"><a href="#cb110-900" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (t <span class="cf">in</span> highlight) {</span>
<span id="cb110-901"><a href="#cb110-901" aria-hidden="true" tabindex="-1"></a>  idx <span class="ot">&lt;-</span> <span class="fu">which.min</span>(<span class="fu">abs</span>(roc_data<span class="sc">$</span>threshold <span class="sc">-</span> t))</span>
<span id="cb110-902"><a href="#cb110-902" aria-hidden="true" tabindex="-1"></a>  <span class="fu">points</span>(roc_data<span class="sc">$</span>FPR[idx], roc_data<span class="sc">$</span>TPR[idx], <span class="at">pch =</span> <span class="dv">19</span>, <span class="at">cex =</span> <span class="fl">1.5</span>)</span>
<span id="cb110-903"><a href="#cb110-903" aria-hidden="true" tabindex="-1"></a>  <span class="fu">text</span>(roc_data<span class="sc">$</span>FPR[idx] <span class="sc">+</span> <span class="fl">0.05</span>, roc_data<span class="sc">$</span>TPR[idx],</span>
<span id="cb110-904"><a href="#cb110-904" aria-hidden="true" tabindex="-1"></a>       <span class="fu">paste</span>(<span class="st">"t ="</span>, t), <span class="at">cex =</span> <span class="fl">0.8</span>)</span>
<span id="cb110-905"><a href="#cb110-905" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb110-906"><a href="#cb110-906" aria-hidden="true" tabindex="-1"></a><span class="fu">legend</span>(<span class="st">"bottomright"</span>, <span class="fu">c</span>(<span class="st">"ROC Curve"</span>, <span class="st">"Random Classifier"</span>),</span>
<span id="cb110-907"><a href="#cb110-907" aria-hidden="true" tabindex="-1"></a>       <span class="at">col =</span> <span class="fu">c</span>(<span class="st">"blue"</span>, <span class="st">"gray"</span>), <span class="at">lty =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">2</span>), <span class="at">lwd =</span> <span class="fu">c</span>(<span class="dv">2</span>, <span class="dv">1</span>))</span>
<span id="cb110-908"><a href="#cb110-908" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb110-909"><a href="#cb110-909" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-910"><a href="#cb110-910" aria-hidden="true" tabindex="-1"></a><span class="fu">### Area Under the Curve (AUC)</span></span>
<span id="cb110-911"><a href="#cb110-911" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-912"><a href="#cb110-912" aria-hidden="true" tabindex="-1"></a>The **AUC** (Area Under the ROC Curve) summarizes classifier performance in a single number:</span>
<span id="cb110-913"><a href="#cb110-913" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-914"><a href="#cb110-914" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**AUC = 0.5**: No better than random guessing</span>
<span id="cb110-915"><a href="#cb110-915" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**AUC = 1.0**: Perfect classification</span>
<span id="cb110-916"><a href="#cb110-916" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**AUC &gt; 0.8**: Generally considered good</span>
<span id="cb110-917"><a href="#cb110-917" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-920"><a href="#cb110-920" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb110-921"><a href="#cb110-921" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate AUC using trapezoidal rule</span></span>
<span id="cb110-922"><a href="#cb110-922" aria-hidden="true" tabindex="-1"></a>auc <span class="ot">&lt;-</span> <span class="fu">sum</span>(<span class="fu">diff</span>(roc_data<span class="sc">$</span>FPR[<span class="fu">order</span>(roc_data<span class="sc">$</span>FPR)]) <span class="sc">*</span></span>
<span id="cb110-923"><a href="#cb110-923" aria-hidden="true" tabindex="-1"></a>           (<span class="fu">head</span>(roc_data<span class="sc">$</span>TPR[<span class="fu">order</span>(roc_data<span class="sc">$</span>FPR)], <span class="sc">-</span><span class="dv">1</span>) <span class="sc">+</span></span>
<span id="cb110-924"><a href="#cb110-924" aria-hidden="true" tabindex="-1"></a>            <span class="fu">tail</span>(roc_data<span class="sc">$</span>TPR[<span class="fu">order</span>(roc_data<span class="sc">$</span>FPR)], <span class="sc">-</span><span class="dv">1</span>)) <span class="sc">/</span> <span class="dv">2</span>)</span>
<span id="cb110-925"><a href="#cb110-925" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"AUC:"</span>, <span class="fu">round</span>(<span class="fu">abs</span>(auc), <span class="dv">3</span>), <span class="st">"</span><span class="sc">\n</span><span class="st">"</span>)</span>
<span id="cb110-926"><a href="#cb110-926" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb110-927"><a href="#cb110-927" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-928"><a href="#cb110-928" aria-hidden="true" tabindex="-1"></a><span class="fu">### Comparing Classifiers with ROC</span></span>
<span id="cb110-929"><a href="#cb110-929" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-930"><a href="#cb110-930" aria-hidden="true" tabindex="-1"></a>ROC curves allow direct comparison of classifiers:</span>
<span id="cb110-931"><a href="#cb110-931" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-934"><a href="#cb110-934" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb110-935"><a href="#cb110-935" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-roc-comparison</span></span>
<span id="cb110-936"><a href="#cb110-936" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "Comparing classifiers using ROC curves: higher curves (larger AUC) indicate better performance"</span></span>
<span id="cb110-937"><a href="#cb110-937" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-width: 7</span></span>
<span id="cb110-938"><a href="#cb110-938" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-height: 6</span></span>
<span id="cb110-939"><a href="#cb110-939" aria-hidden="true" tabindex="-1"></a><span class="co"># Simulate three classifiers of varying quality</span></span>
<span id="cb110-940"><a href="#cb110-940" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">42</span>)</span>
<span id="cb110-941"><a href="#cb110-941" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-942"><a href="#cb110-942" aria-hidden="true" tabindex="-1"></a><span class="co"># Good classifier</span></span>
<span id="cb110-943"><a href="#cb110-943" aria-hidden="true" tabindex="-1"></a>probs_good <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fu">rbeta</span>(<span class="dv">100</span>, <span class="dv">4</span>, <span class="fl">1.5</span>), <span class="fu">rbeta</span>(<span class="dv">400</span>, <span class="fl">1.5</span>, <span class="dv">4</span>))</span>
<span id="cb110-944"><a href="#cb110-944" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-945"><a href="#cb110-945" aria-hidden="true" tabindex="-1"></a><span class="co"># Medium classifier (our original)</span></span>
<span id="cb110-946"><a href="#cb110-946" aria-hidden="true" tabindex="-1"></a>probs_medium <span class="ot">&lt;-</span> probs</span>
<span id="cb110-947"><a href="#cb110-947" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-948"><a href="#cb110-948" aria-hidden="true" tabindex="-1"></a><span class="co"># Poor classifier</span></span>
<span id="cb110-949"><a href="#cb110-949" aria-hidden="true" tabindex="-1"></a>probs_poor <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fu">rbeta</span>(<span class="dv">100</span>, <span class="dv">2</span>, <span class="dv">2</span>), <span class="fu">rbeta</span>(<span class="dv">400</span>, <span class="dv">2</span>, <span class="dv">2</span>))</span>
<span id="cb110-950"><a href="#cb110-950" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-951"><a href="#cb110-951" aria-hidden="true" tabindex="-1"></a><span class="co"># Function to calculate ROC data</span></span>
<span id="cb110-952"><a href="#cb110-952" aria-hidden="true" tabindex="-1"></a>calc_roc <span class="ot">&lt;-</span> <span class="cf">function</span>(probs, actual) {</span>
<span id="cb110-953"><a href="#cb110-953" aria-hidden="true" tabindex="-1"></a>  thresholds <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="at">by =</span> <span class="fl">0.01</span>)</span>
<span id="cb110-954"><a href="#cb110-954" aria-hidden="true" tabindex="-1"></a>  <span class="fu">data.frame</span>(</span>
<span id="cb110-955"><a href="#cb110-955" aria-hidden="true" tabindex="-1"></a>    <span class="at">TPR =</span> <span class="fu">sapply</span>(thresholds, <span class="cf">function</span>(t) {</span>
<span id="cb110-956"><a href="#cb110-956" aria-hidden="true" tabindex="-1"></a>      pred <span class="ot">&lt;-</span> <span class="fu">ifelse</span>(probs <span class="sc">&gt;=</span> t, <span class="dv">1</span>, <span class="dv">0</span>)</span>
<span id="cb110-957"><a href="#cb110-957" aria-hidden="true" tabindex="-1"></a>      <span class="fu">sum</span>(pred <span class="sc">==</span> <span class="dv">1</span> <span class="sc">&amp;</span> actual <span class="sc">==</span> <span class="dv">1</span>) <span class="sc">/</span> <span class="fu">sum</span>(actual <span class="sc">==</span> <span class="dv">1</span>)</span>
<span id="cb110-958"><a href="#cb110-958" aria-hidden="true" tabindex="-1"></a>    }),</span>
<span id="cb110-959"><a href="#cb110-959" aria-hidden="true" tabindex="-1"></a>    <span class="at">FPR =</span> <span class="fu">sapply</span>(thresholds, <span class="cf">function</span>(t) {</span>
<span id="cb110-960"><a href="#cb110-960" aria-hidden="true" tabindex="-1"></a>      pred <span class="ot">&lt;-</span> <span class="fu">ifelse</span>(probs <span class="sc">&gt;=</span> t, <span class="dv">1</span>, <span class="dv">0</span>)</span>
<span id="cb110-961"><a href="#cb110-961" aria-hidden="true" tabindex="-1"></a>      <span class="fu">sum</span>(pred <span class="sc">==</span> <span class="dv">1</span> <span class="sc">&amp;</span> actual <span class="sc">==</span> <span class="dv">0</span>) <span class="sc">/</span> <span class="fu">sum</span>(actual <span class="sc">==</span> <span class="dv">0</span>)</span>
<span id="cb110-962"><a href="#cb110-962" aria-hidden="true" tabindex="-1"></a>    })</span>
<span id="cb110-963"><a href="#cb110-963" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb110-964"><a href="#cb110-964" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb110-965"><a href="#cb110-965" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-966"><a href="#cb110-966" aria-hidden="true" tabindex="-1"></a>roc_good <span class="ot">&lt;-</span> <span class="fu">calc_roc</span>(probs_good, actual)</span>
<span id="cb110-967"><a href="#cb110-967" aria-hidden="true" tabindex="-1"></a>roc_medium <span class="ot">&lt;-</span> <span class="fu">calc_roc</span>(probs_medium, actual)</span>
<span id="cb110-968"><a href="#cb110-968" aria-hidden="true" tabindex="-1"></a>roc_poor <span class="ot">&lt;-</span> <span class="fu">calc_roc</span>(probs_poor, actual)</span>
<span id="cb110-969"><a href="#cb110-969" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-970"><a href="#cb110-970" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot comparison</span></span>
<span id="cb110-971"><a href="#cb110-971" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(roc_good<span class="sc">$</span>FPR, roc_good<span class="sc">$</span>TPR, <span class="at">type =</span> <span class="st">"l"</span>, <span class="at">lwd =</span> <span class="dv">2</span>, <span class="at">col =</span> <span class="st">"darkgreen"</span>,</span>
<span id="cb110-972"><a href="#cb110-972" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">"False Positive Rate"</span>, <span class="at">ylab =</span> <span class="st">"True Positive Rate"</span>,</span>
<span id="cb110-973"><a href="#cb110-973" aria-hidden="true" tabindex="-1"></a>     <span class="at">main =</span> <span class="st">"ROC Curve Comparison"</span>)</span>
<span id="cb110-974"><a href="#cb110-974" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(roc_medium<span class="sc">$</span>FPR, roc_medium<span class="sc">$</span>TPR, <span class="at">lwd =</span> <span class="dv">2</span>, <span class="at">col =</span> <span class="st">"blue"</span>)</span>
<span id="cb110-975"><a href="#cb110-975" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(roc_poor<span class="sc">$</span>FPR, roc_poor<span class="sc">$</span>TPR, <span class="at">lwd =</span> <span class="dv">2</span>, <span class="at">col =</span> <span class="st">"red"</span>)</span>
<span id="cb110-976"><a href="#cb110-976" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="at">lty =</span> <span class="dv">2</span>, <span class="at">col =</span> <span class="st">"gray"</span>)</span>
<span id="cb110-977"><a href="#cb110-977" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-978"><a href="#cb110-978" aria-hidden="true" tabindex="-1"></a><span class="fu">legend</span>(<span class="st">"bottomright"</span>,</span>
<span id="cb110-979"><a href="#cb110-979" aria-hidden="true" tabindex="-1"></a>       <span class="fu">c</span>(<span class="st">"Good (AUC ≈ 0.90)"</span>, <span class="st">"Medium (AUC ≈ 0.75)"</span>, <span class="st">"Poor (AUC ≈ 0.50)"</span>),</span>
<span id="cb110-980"><a href="#cb110-980" aria-hidden="true" tabindex="-1"></a>       <span class="at">col =</span> <span class="fu">c</span>(<span class="st">"darkgreen"</span>, <span class="st">"blue"</span>, <span class="st">"red"</span>), <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb110-981"><a href="#cb110-981" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb110-982"><a href="#cb110-982" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-983"><a href="#cb110-983" aria-hidden="true" tabindex="-1"></a><span class="fu">### Precision-Recall Curves</span></span>
<span id="cb110-984"><a href="#cb110-984" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-985"><a href="#cb110-985" aria-hidden="true" tabindex="-1"></a>For highly imbalanced data, **precision-recall curves** can be more informative than ROC curves because they focus on the minority (positive) class:</span>
<span id="cb110-986"><a href="#cb110-986" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-989"><a href="#cb110-989" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb110-990"><a href="#cb110-990" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-pr-curve</span></span>
<span id="cb110-991"><a href="#cb110-991" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "Precision-recall curve for imbalanced classification; the horizontal dashed line shows baseline precision (proportion of positives)"</span></span>
<span id="cb110-992"><a href="#cb110-992" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-width: 7</span></span>
<span id="cb110-993"><a href="#cb110-993" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-height: 5</span></span>
<span id="cb110-994"><a href="#cb110-994" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate precision-recall curve</span></span>
<span id="cb110-995"><a href="#cb110-995" aria-hidden="true" tabindex="-1"></a>pr_data <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(</span>
<span id="cb110-996"><a href="#cb110-996" aria-hidden="true" tabindex="-1"></a>  <span class="at">threshold =</span> thresholds,</span>
<span id="cb110-997"><a href="#cb110-997" aria-hidden="true" tabindex="-1"></a>  <span class="at">precision =</span> <span class="fu">sapply</span>(thresholds, <span class="cf">function</span>(t) {</span>
<span id="cb110-998"><a href="#cb110-998" aria-hidden="true" tabindex="-1"></a>    pred <span class="ot">&lt;-</span> <span class="fu">ifelse</span>(probs <span class="sc">&gt;=</span> t, <span class="dv">1</span>, <span class="dv">0</span>)</span>
<span id="cb110-999"><a href="#cb110-999" aria-hidden="true" tabindex="-1"></a>    tp <span class="ot">&lt;-</span> <span class="fu">sum</span>(pred <span class="sc">==</span> <span class="dv">1</span> <span class="sc">&amp;</span> actual <span class="sc">==</span> <span class="dv">1</span>)</span>
<span id="cb110-1000"><a href="#cb110-1000" aria-hidden="true" tabindex="-1"></a>    fp <span class="ot">&lt;-</span> <span class="fu">sum</span>(pred <span class="sc">==</span> <span class="dv">1</span> <span class="sc">&amp;</span> actual <span class="sc">==</span> <span class="dv">0</span>)</span>
<span id="cb110-1001"><a href="#cb110-1001" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> (tp <span class="sc">+</span> fp <span class="sc">==</span> <span class="dv">0</span>) <span class="fu">return</span>(<span class="cn">NA</span>)</span>
<span id="cb110-1002"><a href="#cb110-1002" aria-hidden="true" tabindex="-1"></a>    tp <span class="sc">/</span> (tp <span class="sc">+</span> fp)</span>
<span id="cb110-1003"><a href="#cb110-1003" aria-hidden="true" tabindex="-1"></a>  }),</span>
<span id="cb110-1004"><a href="#cb110-1004" aria-hidden="true" tabindex="-1"></a>  <span class="at">recall =</span> <span class="fu">sapply</span>(thresholds, <span class="cf">function</span>(t) {</span>
<span id="cb110-1005"><a href="#cb110-1005" aria-hidden="true" tabindex="-1"></a>    pred <span class="ot">&lt;-</span> <span class="fu">ifelse</span>(probs <span class="sc">&gt;=</span> t, <span class="dv">1</span>, <span class="dv">0</span>)</span>
<span id="cb110-1006"><a href="#cb110-1006" aria-hidden="true" tabindex="-1"></a>    <span class="fu">sum</span>(pred <span class="sc">==</span> <span class="dv">1</span> <span class="sc">&amp;</span> actual <span class="sc">==</span> <span class="dv">1</span>) <span class="sc">/</span> <span class="fu">sum</span>(actual <span class="sc">==</span> <span class="dv">1</span>)</span>
<span id="cb110-1007"><a href="#cb110-1007" aria-hidden="true" tabindex="-1"></a>  })</span>
<span id="cb110-1008"><a href="#cb110-1008" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb110-1009"><a href="#cb110-1009" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-1010"><a href="#cb110-1010" aria-hidden="true" tabindex="-1"></a><span class="co"># Remove NA values</span></span>
<span id="cb110-1011"><a href="#cb110-1011" aria-hidden="true" tabindex="-1"></a>pr_data <span class="ot">&lt;-</span> pr_data[<span class="sc">!</span><span class="fu">is.na</span>(pr_data<span class="sc">$</span>precision), ]</span>
<span id="cb110-1012"><a href="#cb110-1012" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-1013"><a href="#cb110-1013" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(pr_data<span class="sc">$</span>recall, pr_data<span class="sc">$</span>precision, <span class="at">type =</span> <span class="st">"l"</span>, <span class="at">lwd =</span> <span class="dv">2</span>, <span class="at">col =</span> <span class="st">"purple"</span>,</span>
<span id="cb110-1014"><a href="#cb110-1014" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">"Recall (Sensitivity)"</span>, <span class="at">ylab =</span> <span class="st">"Precision"</span>,</span>
<span id="cb110-1015"><a href="#cb110-1015" aria-hidden="true" tabindex="-1"></a>     <span class="at">main =</span> <span class="st">"Precision-Recall Curve"</span>, <span class="at">ylim =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">1</span>))</span>
<span id="cb110-1016"><a href="#cb110-1016" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">h =</span> <span class="fu">mean</span>(actual <span class="sc">==</span> <span class="dv">1</span>), <span class="at">lty =</span> <span class="dv">2</span>, <span class="at">col =</span> <span class="st">"gray"</span>)  <span class="co"># Baseline</span></span>
<span id="cb110-1017"><a href="#cb110-1017" aria-hidden="true" tabindex="-1"></a><span class="fu">legend</span>(<span class="st">"topright"</span>, <span class="fu">c</span>(<span class="st">"PR Curve"</span>, <span class="st">"Baseline (random)"</span>),</span>
<span id="cb110-1018"><a href="#cb110-1018" aria-hidden="true" tabindex="-1"></a>       <span class="at">col =</span> <span class="fu">c</span>(<span class="st">"purple"</span>, <span class="st">"gray"</span>), <span class="at">lty =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">2</span>), <span class="at">lwd =</span> <span class="fu">c</span>(<span class="dv">2</span>, <span class="dv">1</span>))</span>
<span id="cb110-1019"><a href="#cb110-1019" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb110-1020"><a href="#cb110-1020" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-1021"><a href="#cb110-1021" aria-hidden="true" tabindex="-1"></a><span class="fu">## The Curse of Dimensionality</span></span>
<span id="cb110-1022"><a href="#cb110-1022" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-1023"><a href="#cb110-1023" aria-hidden="true" tabindex="-1"></a>We described how methods such as LDA and QDA are not meant to be used with many predictors $p$ because the number of parameters that we need to estimate becomes too large. Kernel methods such as kNN or local regression do not have model parameters to estimate. However, they also face a challenge when multiple predictors are used due to what is referred to as the **curse of dimensionality**. The *dimension* here refers to the fact that when we have $p$ predictors, the distance between two observations is computed in $p$-dimensional space.</span>
<span id="cb110-1024"><a href="#cb110-1024" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-1025"><a href="#cb110-1025" aria-hidden="true" tabindex="-1"></a>A useful way of understanding the curse of dimensionality is by considering how large we have to make a span/neighborhood/window to include a given percentage of the data. Remember that with larger neighborhoods, our methods lose flexibility.</span>
<span id="cb110-1026"><a href="#cb110-1026" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-1027"><a href="#cb110-1027" aria-hidden="true" tabindex="-1"></a>For example, suppose we have one continuous predictor with equally spaced points in the <span class="co">[</span><span class="ot">0,1</span><span class="co">]</span> interval and we want to create windows that include 1/10th of data. Then it's easy to see that our windows have to be of size 0.1.</span>
<span id="cb110-1028"><a href="#cb110-1028" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-1029"><a href="#cb110-1029" aria-hidden="true" tabindex="-1"></a>Now, for two predictors, if we decide to keep the neighborhood just as small (10% for each dimension), we include only 1 point. If we want to include 10% of the data, then we need to increase the size of each side of the square to $\sqrt{.10} \approx .316$.</span>
<span id="cb110-1030"><a href="#cb110-1030" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-1031"><a href="#cb110-1031" aria-hidden="true" tabindex="-1"></a>Using the same logic, if we want to include 10% of the data in a three-dimensional space, then the side of each cube is $\sqrt<span class="co">[</span><span class="ot">3</span><span class="co">]</span>{.10} \approx 0.464$. In general, to include 10% of the data in a case with $p$ dimensions, we need an interval with each side of size $\sqrt<span class="co">[</span><span class="ot">p</span><span class="co">]</span>{.10}$ of the total. This proportion gets close to 1 quickly, and if the proportion is 1 it means we include all the data and are no longer smoothing.</span>
<span id="cb110-1032"><a href="#cb110-1032" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-1035"><a href="#cb110-1035" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb110-1036"><a href="#cb110-1036" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-curse-dimensionality</span></span>
<span id="cb110-1037"><a href="#cb110-1037" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "As dimensions increase, the neighborhood size needed to include a fixed proportion of data grows rapidly. By 100 dimensions, any 'local' neighborhood must span nearly the entire data range."</span></span>
<span id="cb110-1038"><a href="#cb110-1038" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-width: 7</span></span>
<span id="cb110-1039"><a href="#cb110-1039" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-height: 5</span></span>
<span id="cb110-1040"><a href="#cb110-1040" aria-hidden="true" tabindex="-1"></a>p <span class="ot">&lt;-</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">100</span></span>
<span id="cb110-1041"><a href="#cb110-1041" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(p, .<span class="dv">1</span><span class="sc">^</span>(<span class="dv">1</span><span class="sc">/</span>p), <span class="at">type =</span> <span class="st">"l"</span>, <span class="at">lwd =</span> <span class="dv">2</span>, <span class="at">col =</span> <span class="st">"steelblue"</span>,</span>
<span id="cb110-1042"><a href="#cb110-1042" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">"Number of Dimensions (p)"</span>,</span>
<span id="cb110-1043"><a href="#cb110-1043" aria-hidden="true" tabindex="-1"></a>     <span class="at">ylab =</span> <span class="st">"Side Length to Include 10% of Data"</span>,</span>
<span id="cb110-1044"><a href="#cb110-1044" aria-hidden="true" tabindex="-1"></a>     <span class="at">main =</span> <span class="st">"The Curse of Dimensionality"</span>,</span>
<span id="cb110-1045"><a href="#cb110-1045" aria-hidden="true" tabindex="-1"></a>     <span class="at">ylim =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">1</span>))</span>
<span id="cb110-1046"><a href="#cb110-1046" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">h =</span> <span class="dv">1</span>, <span class="at">lty =</span> <span class="dv">2</span>, <span class="at">col =</span> <span class="st">"gray50"</span>)</span>
<span id="cb110-1047"><a href="#cb110-1047" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb110-1048"><a href="#cb110-1048" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-1049"><a href="#cb110-1049" aria-hidden="true" tabindex="-1"></a>By the time we reach 100 predictors, the neighborhood is no longer very local, as each side covers almost the entire dataset.</span>
<span id="cb110-1050"><a href="#cb110-1050" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-1051"><a href="#cb110-1051" aria-hidden="true" tabindex="-1"></a>This motivates the use of methods that adapt to higher dimensions while still producing interpretable models. Decision trees and random forests are examples of such methods.</span>
<span id="cb110-1052"><a href="#cb110-1052" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-1053"><a href="#cb110-1053" aria-hidden="true" tabindex="-1"></a><span class="fu">## Decision Trees (CART)</span></span>
<span id="cb110-1054"><a href="#cb110-1054" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-1055"><a href="#cb110-1055" aria-hidden="true" tabindex="-1"></a>**Classification and Regression Trees (CART)** make predictions by recursively partitioning the feature space into regions. At each node, the algorithm asks a yes/no question about a single feature, splitting observations into two groups. The process continues until a stopping criterion is met.</span>
<span id="cb110-1056"><a href="#cb110-1056" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-1057"><a href="#cb110-1057" aria-hidden="true" tabindex="-1"></a><span class="fu">### Motivating Example: Olive Oil Classification</span></span>
<span id="cb110-1058"><a href="#cb110-1058" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-1059"><a href="#cb110-1059" aria-hidden="true" tabindex="-1"></a>To motivate decision trees, consider a dataset that includes the breakdown of olive oil composition into 8 fatty acids:</span>
<span id="cb110-1060"><a href="#cb110-1060" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-1063"><a href="#cb110-1063" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb110-1064"><a href="#cb110-1064" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(dslabs)</span>
<span id="cb110-1065"><a href="#cb110-1065" aria-hidden="true" tabindex="-1"></a><span class="fu">data</span>(<span class="st">"olive"</span>)</span>
<span id="cb110-1066"><a href="#cb110-1066" aria-hidden="true" tabindex="-1"></a><span class="fu">names</span>(olive)</span>
<span id="cb110-1067"><a href="#cb110-1067" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb110-1068"><a href="#cb110-1068" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-1069"><a href="#cb110-1069" aria-hidden="true" tabindex="-1"></a>We will try to predict the region of origin using the fatty acid composition values as predictors.</span>
<span id="cb110-1070"><a href="#cb110-1070" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-1073"><a href="#cb110-1073" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb110-1074"><a href="#cb110-1074" aria-hidden="true" tabindex="-1"></a><span class="fu">table</span>(olive<span class="sc">$</span>region)</span>
<span id="cb110-1075"><a href="#cb110-1075" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb110-1076"><a href="#cb110-1076" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-1077"><a href="#cb110-1077" aria-hidden="true" tabindex="-1"></a>We remove the <span class="in">`area`</span> column because we won't use it as a predictor.</span>
<span id="cb110-1078"><a href="#cb110-1078" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-1081"><a href="#cb110-1081" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb110-1082"><a href="#cb110-1082" aria-hidden="true" tabindex="-1"></a>olive <span class="ot">&lt;-</span> <span class="fu">select</span>(olive, <span class="sc">-</span>area)</span>
<span id="cb110-1083"><a href="#cb110-1083" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb110-1084"><a href="#cb110-1084" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-1085"><a href="#cb110-1085" aria-hidden="true" tabindex="-1"></a>If we examine the distribution of each predictor stratified by region, we see that eicosenoic is only present in Southern Italy and that linoleic separates Northern Italy from Sardinia:</span>
<span id="cb110-1086"><a href="#cb110-1086" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-1089"><a href="#cb110-1089" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb110-1090"><a href="#cb110-1090" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-olive-eda</span></span>
<span id="cb110-1091"><a href="#cb110-1091" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "Distribution of fatty acid composition by region. Some predictors like eicosenoic and linoleic clearly separate regions."</span></span>
<span id="cb110-1092"><a href="#cb110-1092" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-width: 10</span></span>
<span id="cb110-1093"><a href="#cb110-1093" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-height: 6</span></span>
<span id="cb110-1094"><a href="#cb110-1094" aria-hidden="true" tabindex="-1"></a>olive <span class="sc">%&gt;%</span></span>
<span id="cb110-1095"><a href="#cb110-1095" aria-hidden="true" tabindex="-1"></a>  <span class="fu">pivot_longer</span>(<span class="sc">-</span>region, <span class="at">names_to =</span> <span class="st">"fatty_acid"</span>, <span class="at">values_to =</span> <span class="st">"percentage"</span>) <span class="sc">%&gt;%</span></span>
<span id="cb110-1096"><a href="#cb110-1096" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(region, percentage, <span class="at">fill =</span> region)) <span class="sc">+</span></span>
<span id="cb110-1097"><a href="#cb110-1097" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_boxplot</span>() <span class="sc">+</span></span>
<span id="cb110-1098"><a href="#cb110-1098" aria-hidden="true" tabindex="-1"></a>  <span class="fu">facet_wrap</span>(<span class="sc">~</span>fatty_acid, <span class="at">scales =</span> <span class="st">"free"</span>, <span class="at">ncol =</span> <span class="dv">4</span>) <span class="sc">+</span></span>
<span id="cb110-1099"><a href="#cb110-1099" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">axis.text.x =</span> <span class="fu">element_blank</span>(), <span class="at">legend.position =</span> <span class="st">"bottom"</span>) <span class="sc">+</span></span>
<span id="cb110-1100"><a href="#cb110-1100" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">"Fatty Acid Composition by Region"</span>)</span>
<span id="cb110-1101"><a href="#cb110-1101" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb110-1102"><a href="#cb110-1102" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-1103"><a href="#cb110-1103" aria-hidden="true" tabindex="-1"></a>This implies that we should be able to build an algorithm that predicts perfectly. We can see this clearly by plotting the values for eicosenoic and linoleic:</span>
<span id="cb110-1104"><a href="#cb110-1104" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-1107"><a href="#cb110-1107" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb110-1108"><a href="#cb110-1108" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-olive-two-predictors</span></span>
<span id="cb110-1109"><a href="#cb110-1109" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "With just two predictors, we can draw decision boundaries that perfectly separate the regions"</span></span>
<span id="cb110-1110"><a href="#cb110-1110" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-width: 7</span></span>
<span id="cb110-1111"><a href="#cb110-1111" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-height: 5</span></span>
<span id="cb110-1112"><a href="#cb110-1112" aria-hidden="true" tabindex="-1"></a>olive <span class="sc">%&gt;%</span></span>
<span id="cb110-1113"><a href="#cb110-1113" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(eicosenoic, linoleic, <span class="at">color =</span> region)) <span class="sc">+</span></span>
<span id="cb110-1114"><a href="#cb110-1114" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>() <span class="sc">+</span></span>
<span id="cb110-1115"><a href="#cb110-1115" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_vline</span>(<span class="at">xintercept =</span> <span class="fl">0.065</span>, <span class="at">lty =</span> <span class="dv">2</span>) <span class="sc">+</span></span>
<span id="cb110-1116"><a href="#cb110-1116" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_segment</span>(<span class="at">x =</span> <span class="sc">-</span><span class="fl">0.2</span>, <span class="at">y =</span> <span class="fl">10.54</span>, <span class="at">xend =</span> <span class="fl">0.065</span>, <span class="at">yend =</span> <span class="fl">10.54</span>,</span>
<span id="cb110-1117"><a href="#cb110-1117" aria-hidden="true" tabindex="-1"></a>               <span class="at">color =</span> <span class="st">"black"</span>, <span class="at">lty =</span> <span class="dv">2</span>) <span class="sc">+</span></span>
<span id="cb110-1118"><a href="#cb110-1118" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">"Perfect Classification with Simple Rules"</span>)</span>
<span id="cb110-1119"><a href="#cb110-1119" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb110-1120"><a href="#cb110-1120" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-1121"><a href="#cb110-1121" aria-hidden="true" tabindex="-1"></a>By eye, we can construct a prediction rule that partitions the predictor space so that each partition contains outcomes of only one category:</span>
<span id="cb110-1122"><a href="#cb110-1122" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-1123"><a href="#cb110-1123" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>If eicosenoic &gt; 0.065, predict Southern Italy</span>
<span id="cb110-1124"><a href="#cb110-1124" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>If not, then if linoleic &gt; 10.535, predict Sardinia</span>
<span id="cb110-1125"><a href="#cb110-1125" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Otherwise, predict Northern Italy</span>
<span id="cb110-1126"><a href="#cb110-1126" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-1127"><a href="#cb110-1127" aria-hidden="true" tabindex="-1"></a>This is exactly what a decision tree does—it learns these rules from data. A tree is basically a flow chart of yes/no questions. The general idea is to use data to create these trees with predictions at the ends, referred to as **nodes**.</span>
<span id="cb110-1128"><a href="#cb110-1128" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-1129"><a href="#cb110-1129" aria-hidden="true" tabindex="-1"></a><span class="fu">### How Trees Work</span></span>
<span id="cb110-1130"><a href="#cb110-1130" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-1131"><a href="#cb110-1131" aria-hidden="true" tabindex="-1"></a>The key idea: find the split that best separates the data at each step.</span>
<span id="cb110-1132"><a href="#cb110-1132" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-1133"><a href="#cb110-1133" aria-hidden="true" tabindex="-1"></a>Regression and decision trees operate by predicting an outcome variable $Y$ by partitioning the predictors. We partition the predictor space into $J$ non-overlapping regions, $R_1, R_2, \ldots, R_J$, and then for any predictor $x$ that falls within region $R_j$, we estimate $f(x)$ with the average (for regression) or majority class (for classification) of the training observations in that region.</span>
<span id="cb110-1134"><a href="#cb110-1134" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-1135"><a href="#cb110-1135" aria-hidden="true" tabindex="-1"></a>Trees create partitions recursively. We start with one partition, the entire predictor space. After the first step we have two partitions. After the second step we split one of these partitions into two and have three partitions, then four, and so on.</span>
<span id="cb110-1136"><a href="#cb110-1136" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-1137"><a href="#cb110-1137" aria-hidden="true" tabindex="-1"></a>Once we select a partition to split, we find a predictor $j$ and value $s$ that define two new partitions:</span>
<span id="cb110-1138"><a href="#cb110-1138" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-1139"><a href="#cb110-1139" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb110-1140"><a href="#cb110-1140" aria-hidden="true" tabindex="-1"></a>R_1(j,s) = <span class="sc">\{</span>\mathbf{x} \mid x_j &lt; s<span class="sc">\}</span> \mbox{  and  } R_2(j,s) = <span class="sc">\{</span>\mathbf{x} \mid x_j \geq s<span class="sc">\}</span></span>
<span id="cb110-1141"><a href="#cb110-1141" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb110-1142"><a href="#cb110-1142" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-1143"><a href="#cb110-1143" aria-hidden="true" tabindex="-1"></a>But how do we pick $j$ and $s$? We find the pair that minimizes our loss function.</span>
<span id="cb110-1144"><a href="#cb110-1144" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-1145"><a href="#cb110-1145" aria-hidden="true" tabindex="-1"></a>For **regression trees**, we minimize the **residual sum of squares (RSS)**:</span>
<span id="cb110-1146"><a href="#cb110-1146" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-1147"><a href="#cb110-1147" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb110-1148"><a href="#cb110-1148" aria-hidden="true" tabindex="-1"></a>\sum_{i:\, x_i \in R_1(j,s)} (y_i - \hat{y}_{R_1})^2 +</span>
<span id="cb110-1149"><a href="#cb110-1149" aria-hidden="true" tabindex="-1"></a>\sum_{i:\, x_i \in R_2(j,s)} (y_i - \hat{y}_{R_2})^2</span>
<span id="cb110-1150"><a href="#cb110-1150" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb110-1151"><a href="#cb110-1151" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-1152"><a href="#cb110-1152" aria-hidden="true" tabindex="-1"></a>where $\hat{y}_{R_1}$ and $\hat{y}_{R_2}$ are the average outcomes in each region.</span>
<span id="cb110-1153"><a href="#cb110-1153" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-1154"><a href="#cb110-1154" aria-hidden="true" tabindex="-1"></a>For **classification trees**, we use the **Gini impurity**:</span>
<span id="cb110-1155"><a href="#cb110-1155" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-1156"><a href="#cb110-1156" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb110-1157"><a href="#cb110-1157" aria-hidden="true" tabindex="-1"></a>\mbox{Gini}(j) = \sum_{k=1}^K \hat{p}_{j,k}(1-\hat{p}_{j,k})</span>
<span id="cb110-1158"><a href="#cb110-1158" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb110-1159"><a href="#cb110-1159" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-1160"><a href="#cb110-1160" aria-hidden="true" tabindex="-1"></a>where $\hat{p}_{j,k}$ is the proportion of observations in partition $j$ that are of class $k$. A pure node (all one class) has Gini = 0.</span>
<span id="cb110-1161"><a href="#cb110-1161" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-1162"><a href="#cb110-1162" aria-hidden="true" tabindex="-1"></a>**Entropy** is a very similar quantity:</span>
<span id="cb110-1163"><a href="#cb110-1163" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-1164"><a href="#cb110-1164" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb110-1165"><a href="#cb110-1165" aria-hidden="true" tabindex="-1"></a>\mbox{Entropy}(j) = -\sum_{k=1}^K \hat{p}_{j,k}\log(\hat{p}_{j,k})</span>
<span id="cb110-1166"><a href="#cb110-1166" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb110-1167"><a href="#cb110-1167" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-1168"><a href="#cb110-1168" aria-hidden="true" tabindex="-1"></a>(with $0 \times \log(0)$ defined as 0). Both Gini and entropy are 0 for perfectly pure nodes and increase as the class distribution becomes more mixed.</span>
<span id="cb110-1169"><a href="#cb110-1169" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-1172"><a href="#cb110-1172" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb110-1173"><a href="#cb110-1173" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-cart-classification</span></span>
<span id="cb110-1174"><a href="#cb110-1174" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "A CART decision tree for classifying iris species. Each node shows the predicted class, proportion of observations, and the splitting rule."</span></span>
<span id="cb110-1175"><a href="#cb110-1175" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-width: 8</span></span>
<span id="cb110-1176"><a href="#cb110-1176" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-height: 6</span></span>
<span id="cb110-1177"><a href="#cb110-1177" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(rpart)</span>
<span id="cb110-1178"><a href="#cb110-1178" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(rpart.plot)</span>
<span id="cb110-1179"><a href="#cb110-1179" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-1180"><a href="#cb110-1180" aria-hidden="true" tabindex="-1"></a><span class="co"># Build a classification tree</span></span>
<span id="cb110-1181"><a href="#cb110-1181" aria-hidden="true" tabindex="-1"></a><span class="fu">data</span>(iris)</span>
<span id="cb110-1182"><a href="#cb110-1182" aria-hidden="true" tabindex="-1"></a>tree_class <span class="ot">&lt;-</span> <span class="fu">rpart</span>(Species <span class="sc">~</span> ., <span class="at">data =</span> iris, <span class="at">method =</span> <span class="st">"class"</span>)</span>
<span id="cb110-1183"><a href="#cb110-1183" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-1184"><a href="#cb110-1184" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualize with rpart.plot</span></span>
<span id="cb110-1185"><a href="#cb110-1185" aria-hidden="true" tabindex="-1"></a><span class="fu">rpart.plot</span>(tree_class, <span class="at">extra =</span> <span class="dv">104</span>, <span class="at">box.palette =</span> <span class="st">"RdYlGn"</span>,</span>
<span id="cb110-1186"><a href="#cb110-1186" aria-hidden="true" tabindex="-1"></a>           <span class="at">main =</span> <span class="st">"Classification Tree for Iris Species"</span>)</span>
<span id="cb110-1187"><a href="#cb110-1187" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb110-1188"><a href="#cb110-1188" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-1189"><a href="#cb110-1189" aria-hidden="true" tabindex="-1"></a><span class="fu">### Interpreting Tree Output</span></span>
<span id="cb110-1190"><a href="#cb110-1190" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-1191"><a href="#cb110-1191" aria-hidden="true" tabindex="-1"></a>The tree visualization shows:</span>
<span id="cb110-1192"><a href="#cb110-1192" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-1193"><a href="#cb110-1193" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Node prediction**: The predicted class (or value for regression)</span>
<span id="cb110-1194"><a href="#cb110-1194" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Split rule**: The feature and threshold used to split</span>
<span id="cb110-1195"><a href="#cb110-1195" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Proportions**: Distribution of classes at each node</span>
<span id="cb110-1196"><a href="#cb110-1196" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Sample size**: Number of observations reaching each node</span>
<span id="cb110-1197"><a href="#cb110-1197" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-1200"><a href="#cb110-1200" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb110-1201"><a href="#cb110-1201" aria-hidden="true" tabindex="-1"></a><span class="co"># Detailed tree summary</span></span>
<span id="cb110-1202"><a href="#cb110-1202" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(tree_class, <span class="at">cp =</span> <span class="fl">0.1</span>)</span>
<span id="cb110-1203"><a href="#cb110-1203" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb110-1204"><a href="#cb110-1204" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-1205"><a href="#cb110-1205" aria-hidden="true" tabindex="-1"></a><span class="fu">### Regression Trees</span></span>
<span id="cb110-1206"><a href="#cb110-1206" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-1207"><a href="#cb110-1207" aria-hidden="true" tabindex="-1"></a>When the outcome is continuous, we call the method a **regression tree**. To illustrate, we will use poll data from the 2008 presidential election where we try to estimate the conditional expectation of poll margin $Y$ given day $x$.</span>
<span id="cb110-1208"><a href="#cb110-1208" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-1211"><a href="#cb110-1211" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb110-1212"><a href="#cb110-1212" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-polls-2008-data</span></span>
<span id="cb110-1213"><a href="#cb110-1213" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "2008 presidential poll data: margin (Obama - McCain) over time"</span></span>
<span id="cb110-1214"><a href="#cb110-1214" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-width: 7</span></span>
<span id="cb110-1215"><a href="#cb110-1215" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-height: 5</span></span>
<span id="cb110-1216"><a href="#cb110-1216" aria-hidden="true" tabindex="-1"></a><span class="fu">data</span>(<span class="st">"polls_2008"</span>)</span>
<span id="cb110-1217"><a href="#cb110-1217" aria-hidden="true" tabindex="-1"></a><span class="fu">qplot</span>(day, margin, <span class="at">data =</span> polls_2008) <span class="sc">+</span></span>
<span id="cb110-1218"><a href="#cb110-1218" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">"2008 Presidential Poll Data"</span>, <span class="at">x =</span> <span class="st">"Days before election"</span>, <span class="at">y =</span> <span class="st">"Poll margin"</span>)</span>
<span id="cb110-1219"><a href="#cb110-1219" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb110-1220"><a href="#cb110-1220" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-1221"><a href="#cb110-1221" aria-hidden="true" tabindex="-1"></a>Let's fit a regression tree using the <span class="in">`rpart`</span> function:</span>
<span id="cb110-1222"><a href="#cb110-1222" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-1225"><a href="#cb110-1225" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb110-1226"><a href="#cb110-1226" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-polls-tree</span></span>
<span id="cb110-1227"><a href="#cb110-1227" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "Regression tree for poll data showing where the algorithm decided to split"</span></span>
<span id="cb110-1228"><a href="#cb110-1228" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-width: 8</span></span>
<span id="cb110-1229"><a href="#cb110-1229" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-height: 5</span></span>
<span id="cb110-1230"><a href="#cb110-1230" aria-hidden="true" tabindex="-1"></a>fit <span class="ot">&lt;-</span> <span class="fu">rpart</span>(margin <span class="sc">~</span> ., <span class="at">data =</span> polls_2008)</span>
<span id="cb110-1231"><a href="#cb110-1231" aria-hidden="true" tabindex="-1"></a>rafalib<span class="sc">::</span><span class="fu">mypar</span>()</span>
<span id="cb110-1232"><a href="#cb110-1232" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(fit, <span class="at">margin =</span> <span class="fl">0.1</span>)</span>
<span id="cb110-1233"><a href="#cb110-1233" aria-hidden="true" tabindex="-1"></a><span class="fu">text</span>(fit, <span class="at">cex =</span> <span class="fl">0.75</span>)</span>
<span id="cb110-1234"><a href="#cb110-1234" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb110-1235"><a href="#cb110-1235" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-1236"><a href="#cb110-1236" aria-hidden="true" tabindex="-1"></a>The tree shows that the first split is made at day 39.5, then further splits occur at days 86.5, 49.5, 117.5, and so on. The final estimate $\hat{f}(x)$ is a step function:</span>
<span id="cb110-1237"><a href="#cb110-1237" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-1240"><a href="#cb110-1240" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb110-1241"><a href="#cb110-1241" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-polls-tree-fit</span></span>
<span id="cb110-1242"><a href="#cb110-1242" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "Regression tree predictions for poll data create a step function"</span></span>
<span id="cb110-1243"><a href="#cb110-1243" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-width: 7</span></span>
<span id="cb110-1244"><a href="#cb110-1244" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-height: 5</span></span>
<span id="cb110-1245"><a href="#cb110-1245" aria-hidden="true" tabindex="-1"></a>polls_2008 <span class="sc">%&gt;%</span></span>
<span id="cb110-1246"><a href="#cb110-1246" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">y_hat =</span> <span class="fu">predict</span>(fit)) <span class="sc">%&gt;%</span></span>
<span id="cb110-1247"><a href="#cb110-1247" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>() <span class="sc">+</span></span>
<span id="cb110-1248"><a href="#cb110-1248" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="fu">aes</span>(day, margin)) <span class="sc">+</span></span>
<span id="cb110-1249"><a href="#cb110-1249" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_step</span>(<span class="fu">aes</span>(day, y_hat), <span class="at">col =</span> <span class="st">"red"</span>, <span class="at">linewidth =</span> <span class="dv">1</span>) <span class="sc">+</span></span>
<span id="cb110-1250"><a href="#cb110-1250" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">"Regression Tree Fit"</span>, <span class="at">x =</span> <span class="st">"Day"</span>, <span class="at">y =</span> <span class="st">"Poll margin"</span>)</span>
<span id="cb110-1251"><a href="#cb110-1251" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb110-1252"><a href="#cb110-1252" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-1253"><a href="#cb110-1253" aria-hidden="true" tabindex="-1"></a>Trees can also be applied to multiple predictors:</span>
<span id="cb110-1254"><a href="#cb110-1254" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-1257"><a href="#cb110-1257" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb110-1258"><a href="#cb110-1258" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-cart-regression</span></span>
<span id="cb110-1259"><a href="#cb110-1259" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "A regression tree predicting car fuel efficiency (mpg) from weight and horsepower"</span></span>
<span id="cb110-1260"><a href="#cb110-1260" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-width: 8</span></span>
<span id="cb110-1261"><a href="#cb110-1261" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-height: 5</span></span>
<span id="cb110-1262"><a href="#cb110-1262" aria-hidden="true" tabindex="-1"></a><span class="co"># Build a regression tree</span></span>
<span id="cb110-1263"><a href="#cb110-1263" aria-hidden="true" tabindex="-1"></a>tree_reg <span class="ot">&lt;-</span> <span class="fu">rpart</span>(mpg <span class="sc">~</span> wt <span class="sc">+</span> hp <span class="sc">+</span> cyl, <span class="at">data =</span> mtcars, <span class="at">method =</span> <span class="st">"anova"</span>)</span>
<span id="cb110-1264"><a href="#cb110-1264" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-1265"><a href="#cb110-1265" aria-hidden="true" tabindex="-1"></a><span class="fu">rpart.plot</span>(tree_reg, <span class="at">extra =</span> <span class="dv">101</span>, <span class="at">box.palette =</span> <span class="st">"Blues"</span>,</span>
<span id="cb110-1266"><a href="#cb110-1266" aria-hidden="true" tabindex="-1"></a>           <span class="at">main =</span> <span class="st">"Regression Tree for MPG"</span>)</span>
<span id="cb110-1267"><a href="#cb110-1267" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb110-1268"><a href="#cb110-1268" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-1269"><a href="#cb110-1269" aria-hidden="true" tabindex="-1"></a><span class="fu">### The Decision Boundary</span></span>
<span id="cb110-1270"><a href="#cb110-1270" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-1271"><a href="#cb110-1271" aria-hidden="true" tabindex="-1"></a>Trees partition the feature space into rectangular regions:</span>
<span id="cb110-1272"><a href="#cb110-1272" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-1275"><a href="#cb110-1275" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb110-1276"><a href="#cb110-1276" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-tree-boundary</span></span>
<span id="cb110-1277"><a href="#cb110-1277" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "Decision tree partition of the feature space. Each rectangular region is assigned to a class based on the majority vote of training points in that region."</span></span>
<span id="cb110-1278"><a href="#cb110-1278" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-width: 8</span></span>
<span id="cb110-1279"><a href="#cb110-1279" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-height: 6</span></span>
<span id="cb110-1280"><a href="#cb110-1280" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualize decision boundary for 2D case</span></span>
<span id="cb110-1281"><a href="#cb110-1281" aria-hidden="true" tabindex="-1"></a>tree_2d <span class="ot">&lt;-</span> <span class="fu">rpart</span>(Species <span class="sc">~</span> Petal.Length <span class="sc">+</span> Petal.Width, <span class="at">data =</span> iris, <span class="at">method =</span> <span class="st">"class"</span>)</span>
<span id="cb110-1282"><a href="#cb110-1282" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-1283"><a href="#cb110-1283" aria-hidden="true" tabindex="-1"></a><span class="co"># Create grid for prediction</span></span>
<span id="cb110-1284"><a href="#cb110-1284" aria-hidden="true" tabindex="-1"></a>petal_length_seq <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="dv">0</span>, <span class="dv">7</span>, <span class="at">length.out =</span> <span class="dv">200</span>)</span>
<span id="cb110-1285"><a href="#cb110-1285" aria-hidden="true" tabindex="-1"></a>petal_width_seq <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="dv">0</span>, <span class="dv">3</span>, <span class="at">length.out =</span> <span class="dv">200</span>)</span>
<span id="cb110-1286"><a href="#cb110-1286" aria-hidden="true" tabindex="-1"></a>grid <span class="ot">&lt;-</span> <span class="fu">expand.grid</span>(<span class="at">Petal.Length =</span> petal_length_seq,</span>
<span id="cb110-1287"><a href="#cb110-1287" aria-hidden="true" tabindex="-1"></a>                    <span class="at">Petal.Width =</span> petal_width_seq)</span>
<span id="cb110-1288"><a href="#cb110-1288" aria-hidden="true" tabindex="-1"></a>grid<span class="sc">$</span>pred <span class="ot">&lt;-</span> <span class="fu">predict</span>(tree_2d, grid, <span class="at">type =</span> <span class="st">"class"</span>)</span>
<span id="cb110-1289"><a href="#cb110-1289" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-1290"><a href="#cb110-1290" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot</span></span>
<span id="cb110-1291"><a href="#cb110-1291" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(grid<span class="sc">$</span>Petal.Length, grid<span class="sc">$</span>Petal.Width,</span>
<span id="cb110-1292"><a href="#cb110-1292" aria-hidden="true" tabindex="-1"></a>     <span class="at">col =</span> <span class="fu">c</span>(<span class="fu">rgb</span>(<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="fl">0.1</span>), <span class="fu">rgb</span>(<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="fl">0.1</span>), <span class="fu">rgb</span>(<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="fl">0.1</span>))[grid<span class="sc">$</span>pred],</span>
<span id="cb110-1293"><a href="#cb110-1293" aria-hidden="true" tabindex="-1"></a>     <span class="at">pch =</span> <span class="dv">15</span>, <span class="at">cex =</span> <span class="fl">0.3</span>,</span>
<span id="cb110-1294"><a href="#cb110-1294" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">"Petal Length"</span>, <span class="at">ylab =</span> <span class="st">"Petal Width"</span>,</span>
<span id="cb110-1295"><a href="#cb110-1295" aria-hidden="true" tabindex="-1"></a>     <span class="at">main =</span> <span class="st">"Decision Tree Boundaries"</span>)</span>
<span id="cb110-1296"><a href="#cb110-1296" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span>(iris<span class="sc">$</span>Petal.Length, iris<span class="sc">$</span>Petal.Width,</span>
<span id="cb110-1297"><a href="#cb110-1297" aria-hidden="true" tabindex="-1"></a>       <span class="at">col =</span> <span class="fu">c</span>(<span class="st">"red"</span>, <span class="st">"green"</span>, <span class="st">"blue"</span>)[iris<span class="sc">$</span>Species],</span>
<span id="cb110-1298"><a href="#cb110-1298" aria-hidden="true" tabindex="-1"></a>       <span class="at">pch =</span> <span class="dv">19</span>, <span class="at">cex =</span> <span class="fl">0.8</span>)</span>
<span id="cb110-1299"><a href="#cb110-1299" aria-hidden="true" tabindex="-1"></a><span class="fu">legend</span>(<span class="st">"topleft"</span>, <span class="fu">levels</span>(iris<span class="sc">$</span>Species),</span>
<span id="cb110-1300"><a href="#cb110-1300" aria-hidden="true" tabindex="-1"></a>       <span class="at">col =</span> <span class="fu">c</span>(<span class="st">"red"</span>, <span class="st">"green"</span>, <span class="st">"blue"</span>), <span class="at">pch =</span> <span class="dv">19</span>)</span>
<span id="cb110-1301"><a href="#cb110-1301" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb110-1302"><a href="#cb110-1302" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-1303"><a href="#cb110-1303" aria-hidden="true" tabindex="-1"></a><span class="fu">### Controlling Tree Complexity</span></span>
<span id="cb110-1304"><a href="#cb110-1304" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-1305"><a href="#cb110-1305" aria-hidden="true" tabindex="-1"></a>Trees easily overfit—they can keep splitting until each leaf contains a single observation. Several parameters control complexity:</span>
<span id="cb110-1306"><a href="#cb110-1306" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-1307"><a href="#cb110-1307" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**cp (complexity parameter)**: Every time we split and define two new partitions, our training set RSS decreases. The RSS must improve by a factor of cp for the new partition to be added. Large values of cp force the algorithm to stop earlier, resulting in fewer nodes.</span>
<span id="cb110-1308"><a href="#cb110-1308" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**minsplit**: Minimum observations required in a partition before attempting to split further. The default in <span class="in">`rpart`</span> is 20.</span>
<span id="cb110-1309"><a href="#cb110-1309" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**minbucket**: Minimum number of observations in each terminal node (leaf). Defaults to <span class="in">`round(minsplit/3)`</span>.</span>
<span id="cb110-1310"><a href="#cb110-1310" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**maxdepth**: Maximum depth of the tree</span>
<span id="cb110-1311"><a href="#cb110-1311" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-1312"><a href="#cb110-1312" aria-hidden="true" tabindex="-1"></a>If we set <span class="in">`cp = 0`</span> and <span class="in">`minsplit = 2`</span>, our prediction becomes as flexible as possible and simply memorizes the training data—a clear case of overfitting:</span>
<span id="cb110-1313"><a href="#cb110-1313" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-1316"><a href="#cb110-1316" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb110-1317"><a href="#cb110-1317" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-polls-overfit</span></span>
<span id="cb110-1318"><a href="#cb110-1318" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "With cp=0 and minsplit=2, the tree overfits by memorizing every point in the training data"</span></span>
<span id="cb110-1319"><a href="#cb110-1319" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-width: 7</span></span>
<span id="cb110-1320"><a href="#cb110-1320" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-height: 5</span></span>
<span id="cb110-1321"><a href="#cb110-1321" aria-hidden="true" tabindex="-1"></a>fit_overfit <span class="ot">&lt;-</span> <span class="fu">rpart</span>(margin <span class="sc">~</span> ., <span class="at">data =</span> polls_2008,</span>
<span id="cb110-1322"><a href="#cb110-1322" aria-hidden="true" tabindex="-1"></a>                      <span class="at">control =</span> <span class="fu">rpart.control</span>(<span class="at">cp =</span> <span class="dv">0</span>, <span class="at">minsplit =</span> <span class="dv">2</span>))</span>
<span id="cb110-1323"><a href="#cb110-1323" aria-hidden="true" tabindex="-1"></a>polls_2008 <span class="sc">%&gt;%</span></span>
<span id="cb110-1324"><a href="#cb110-1324" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">y_hat =</span> <span class="fu">predict</span>(fit_overfit)) <span class="sc">%&gt;%</span></span>
<span id="cb110-1325"><a href="#cb110-1325" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>() <span class="sc">+</span></span>
<span id="cb110-1326"><a href="#cb110-1326" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="fu">aes</span>(day, margin)) <span class="sc">+</span></span>
<span id="cb110-1327"><a href="#cb110-1327" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_step</span>(<span class="fu">aes</span>(day, y_hat), <span class="at">col =</span> <span class="st">"red"</span>) <span class="sc">+</span></span>
<span id="cb110-1328"><a href="#cb110-1328" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">"Overfitting with cp = 0"</span>, <span class="at">x =</span> <span class="st">"Day"</span>, <span class="at">y =</span> <span class="st">"Poll margin"</span>)</span>
<span id="cb110-1329"><a href="#cb110-1329" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb110-1330"><a href="#cb110-1330" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-1331"><a href="#cb110-1331" aria-hidden="true" tabindex="-1"></a>The larger these values are, the more data is averaged to compute a predictor, reducing variability but restricting flexibility. We use cross-validation to select the optimal balance.</span>
<span id="cb110-1332"><a href="#cb110-1332" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-1335"><a href="#cb110-1335" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb110-1336"><a href="#cb110-1336" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-tree-complexity</span></span>
<span id="cb110-1337"><a href="#cb110-1337" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "Effect of the complexity parameter on tree structure: smaller cp allows more splits and greater complexity"</span></span>
<span id="cb110-1338"><a href="#cb110-1338" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-width: 9</span></span>
<span id="cb110-1339"><a href="#cb110-1339" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-height: 4</span></span>
<span id="cb110-1340"><a href="#cb110-1340" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">3</span>))</span>
<span id="cb110-1341"><a href="#cb110-1341" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-1342"><a href="#cb110-1342" aria-hidden="true" tabindex="-1"></a><span class="co"># Simple tree (high cp)</span></span>
<span id="cb110-1343"><a href="#cb110-1343" aria-hidden="true" tabindex="-1"></a>tree_simple <span class="ot">&lt;-</span> <span class="fu">rpart</span>(Species <span class="sc">~</span> ., <span class="at">data =</span> iris, <span class="at">cp =</span> <span class="fl">0.1</span>)</span>
<span id="cb110-1344"><a href="#cb110-1344" aria-hidden="true" tabindex="-1"></a><span class="fu">rpart.plot</span>(tree_simple, <span class="at">main =</span> <span class="st">"cp = 0.1 (simple)"</span>)</span>
<span id="cb110-1345"><a href="#cb110-1345" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-1346"><a href="#cb110-1346" aria-hidden="true" tabindex="-1"></a><span class="co"># Medium tree</span></span>
<span id="cb110-1347"><a href="#cb110-1347" aria-hidden="true" tabindex="-1"></a>tree_medium <span class="ot">&lt;-</span> <span class="fu">rpart</span>(Species <span class="sc">~</span> ., <span class="at">data =</span> iris, <span class="at">cp =</span> <span class="fl">0.02</span>)</span>
<span id="cb110-1348"><a href="#cb110-1348" aria-hidden="true" tabindex="-1"></a><span class="fu">rpart.plot</span>(tree_medium, <span class="at">main =</span> <span class="st">"cp = 0.02 (medium)"</span>)</span>
<span id="cb110-1349"><a href="#cb110-1349" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-1350"><a href="#cb110-1350" aria-hidden="true" tabindex="-1"></a><span class="co"># Complex tree (low cp)</span></span>
<span id="cb110-1351"><a href="#cb110-1351" aria-hidden="true" tabindex="-1"></a>tree_complex <span class="ot">&lt;-</span> <span class="fu">rpart</span>(Species <span class="sc">~</span> ., <span class="at">data =</span> iris, <span class="at">cp =</span> <span class="fl">0.001</span>)</span>
<span id="cb110-1352"><a href="#cb110-1352" aria-hidden="true" tabindex="-1"></a><span class="fu">rpart.plot</span>(tree_complex, <span class="at">main =</span> <span class="st">"cp = 0.001 (complex)"</span>)</span>
<span id="cb110-1353"><a href="#cb110-1353" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb110-1354"><a href="#cb110-1354" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-1355"><a href="#cb110-1355" aria-hidden="true" tabindex="-1"></a><span class="fu">### Pruning with Cross-Validation</span></span>
<span id="cb110-1356"><a href="#cb110-1356" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-1357"><a href="#cb110-1357" aria-hidden="true" tabindex="-1"></a>The optimal complexity is typically chosen by cross-validation:</span>
<span id="cb110-1358"><a href="#cb110-1358" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-1361"><a href="#cb110-1361" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb110-1362"><a href="#cb110-1362" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-tree-cv</span></span>
<span id="cb110-1363"><a href="#cb110-1363" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "Cross-validation error as a function of tree complexity. The dashed line shows one standard error above the minimum, often used to select a simpler tree."</span></span>
<span id="cb110-1364"><a href="#cb110-1364" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-width: 7</span></span>
<span id="cb110-1365"><a href="#cb110-1365" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-height: 5</span></span>
<span id="cb110-1366"><a href="#cb110-1366" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit full tree</span></span>
<span id="cb110-1367"><a href="#cb110-1367" aria-hidden="true" tabindex="-1"></a>full_tree <span class="ot">&lt;-</span> <span class="fu">rpart</span>(Species <span class="sc">~</span> ., <span class="at">data =</span> iris, <span class="at">cp =</span> <span class="fl">0.001</span>)</span>
<span id="cb110-1368"><a href="#cb110-1368" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-1369"><a href="#cb110-1369" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot CV error vs complexity</span></span>
<span id="cb110-1370"><a href="#cb110-1370" aria-hidden="true" tabindex="-1"></a><span class="fu">plotcp</span>(full_tree)</span>
<span id="cb110-1371"><a href="#cb110-1371" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-1372"><a href="#cb110-1372" aria-hidden="true" tabindex="-1"></a><span class="co"># Print CP table</span></span>
<span id="cb110-1373"><a href="#cb110-1373" aria-hidden="true" tabindex="-1"></a><span class="fu">printcp</span>(full_tree)</span>
<span id="cb110-1374"><a href="#cb110-1374" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-1375"><a href="#cb110-1375" aria-hidden="true" tabindex="-1"></a><span class="co"># Prune to optimal cp</span></span>
<span id="cb110-1376"><a href="#cb110-1376" aria-hidden="true" tabindex="-1"></a>best_cp <span class="ot">&lt;-</span> full_tree<span class="sc">$</span>cptable[<span class="fu">which.min</span>(full_tree<span class="sc">$</span>cptable[, <span class="st">"xerror"</span>]), <span class="st">"CP"</span>]</span>
<span id="cb110-1377"><a href="#cb110-1377" aria-hidden="true" tabindex="-1"></a>pruned_tree <span class="ot">&lt;-</span> <span class="fu">prune</span>(full_tree, <span class="at">cp =</span> best_cp)</span>
<span id="cb110-1378"><a href="#cb110-1378" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb110-1379"><a href="#cb110-1379" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-1380"><a href="#cb110-1380" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip}</span>
<span id="cb110-1381"><a href="#cb110-1381" aria-hidden="true" tabindex="-1"></a><span class="fu">## Advantages and Disadvantages of Trees</span></span>
<span id="cb110-1382"><a href="#cb110-1382" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-1383"><a href="#cb110-1383" aria-hidden="true" tabindex="-1"></a>**Advantages:**</span>
<span id="cb110-1384"><a href="#cb110-1384" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Highly interpretable—easy to explain to non-statisticians</span>
<span id="cb110-1385"><a href="#cb110-1385" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Handle both numeric and categorical predictors</span>
<span id="cb110-1386"><a href="#cb110-1386" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Capture non-linear relationships and interactions automatically</span>
<span id="cb110-1387"><a href="#cb110-1387" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Robust to outliers and don't require feature scaling</span>
<span id="cb110-1388"><a href="#cb110-1388" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-1389"><a href="#cb110-1389" aria-hidden="true" tabindex="-1"></a>**Disadvantages:**</span>
<span id="cb110-1390"><a href="#cb110-1390" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>High variance—small changes in data can produce very different trees</span>
<span id="cb110-1391"><a href="#cb110-1391" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Prone to overfitting without careful tuning</span>
<span id="cb110-1392"><a href="#cb110-1392" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Axis-aligned splits can't capture diagonal relationships efficiently</span>
<span id="cb110-1393"><a href="#cb110-1393" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Generally lower predictive accuracy than ensemble methods</span>
<span id="cb110-1394"><a href="#cb110-1394" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb110-1395"><a href="#cb110-1395" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-1396"><a href="#cb110-1396" aria-hidden="true" tabindex="-1"></a><span class="fu">## Random Forests</span></span>
<span id="cb110-1397"><a href="#cb110-1397" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-1398"><a href="#cb110-1398" aria-hidden="true" tabindex="-1"></a>**Random forests** [@breiman2001random] are a **very popular** machine learning approach that addresses the shortcomings of decision trees using a clever idea. The goal is to improve prediction performance and reduce instability by *averaging* multiple decision trees (a forest of trees constructed with randomness). It has two features that help accomplish this.</span>
<span id="cb110-1399"><a href="#cb110-1399" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-1400"><a href="#cb110-1400" aria-hidden="true" tabindex="-1"></a>The first step is **bootstrap aggregation** or **bagging**. The general idea is to generate many predictors, each using regression or classification trees, and then form a final prediction based on the average prediction of all these trees. To assure that the individual trees are not the same, we use the bootstrap to induce randomness. These two features combined explain the name: the bootstrap makes the individual trees **randomly** different, and the combination of trees is the **forest**.</span>
<span id="cb110-1401"><a href="#cb110-1401" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-1402"><a href="#cb110-1402" aria-hidden="true" tabindex="-1"></a><span class="fu">### The Random Forest Algorithm</span></span>
<span id="cb110-1403"><a href="#cb110-1403" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-1404"><a href="#cb110-1404" aria-hidden="true" tabindex="-1"></a>The specific steps are:</span>
<span id="cb110-1405"><a href="#cb110-1405" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-1406"><a href="#cb110-1406" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>Build $B$ decision trees using the training set. We refer to the fitted models as $T_1, T_2, \dots, T_B$.</span>
<span id="cb110-1407"><a href="#cb110-1407" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>For every observation in the test set, form a prediction $\hat{y}_j$ using tree $T_j$.</span>
<span id="cb110-1408"><a href="#cb110-1408" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>For continuous outcomes, form a final prediction with the average $\hat{y} = \frac{1}{B} \sum_{j=1}^B \hat{y}_j$. For categorical data classification, predict $\hat{y}$ with majority vote (most frequent class among $\hat{y}_1, \dots, \hat{y}_B$).</span>
<span id="cb110-1409"><a href="#cb110-1409" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-1410"><a href="#cb110-1410" aria-hidden="true" tabindex="-1"></a>To create $T_j, \, j=1,\ldots,B$ from the training set:</span>
<span id="cb110-1411"><a href="#cb110-1411" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-1412"><a href="#cb110-1412" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>Create a bootstrap training set by sampling $N$ observations from the training set **with replacement**. This is the first way to induce randomness.</span>
<span id="cb110-1413"><a href="#cb110-1413" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>At each split, consider only a random subset of $m$ features (typically $m = \sqrt{p}$ for classification, $m = p/3$ for regression). This reduces correlation between trees in the forest, thereby improving prediction accuracy.</span>
<span id="cb110-1414"><a href="#cb110-1414" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-1415"><a href="#cb110-1415" aria-hidden="true" tabindex="-1"></a>The randomness serves two purposes:</span>
<span id="cb110-1416"><a href="#cb110-1416" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Bagging** reduces variance by averaging many noisy but unbiased trees</span>
<span id="cb110-1417"><a href="#cb110-1417" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Random feature selection** decorrelates the trees, making the average more effective</span>
<span id="cb110-1418"><a href="#cb110-1418" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-1419"><a href="#cb110-1419" aria-hidden="true" tabindex="-1"></a><span class="fu">### Why Averaging Produces Smooth Estimates</span></span>
<span id="cb110-1420"><a href="#cb110-1420" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-1421"><a href="#cb110-1421" aria-hidden="true" tabindex="-1"></a>A key insight is that the average of many step functions can be smooth. Let's illustrate with the polls data:</span>
<span id="cb110-1422"><a href="#cb110-1422" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-1425"><a href="#cb110-1425" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb110-1426"><a href="#cb110-1426" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-rf-polls-smooth</span></span>
<span id="cb110-1427"><a href="#cb110-1427" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "Random forest predictions are much smoother than single trees because averaging many step functions produces a smooth curve"</span></span>
<span id="cb110-1428"><a href="#cb110-1428" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-width: 9</span></span>
<span id="cb110-1429"><a href="#cb110-1429" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-height: 4</span></span>
<span id="cb110-1430"><a href="#cb110-1430" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">2</span>))</span>
<span id="cb110-1431"><a href="#cb110-1431" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-1432"><a href="#cb110-1432" aria-hidden="true" tabindex="-1"></a><span class="co"># Single tree (from before)</span></span>
<span id="cb110-1433"><a href="#cb110-1433" aria-hidden="true" tabindex="-1"></a>fit_tree <span class="ot">&lt;-</span> <span class="fu">rpart</span>(margin <span class="sc">~</span> ., <span class="at">data =</span> polls_2008)</span>
<span id="cb110-1434"><a href="#cb110-1434" aria-hidden="true" tabindex="-1"></a>polls_2008 <span class="sc">%&gt;%</span></span>
<span id="cb110-1435"><a href="#cb110-1435" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">y_hat =</span> <span class="fu">predict</span>(fit_tree)) <span class="sc">%&gt;%</span></span>
<span id="cb110-1436"><a href="#cb110-1436" aria-hidden="true" tabindex="-1"></a>  <span class="fu">with</span>(<span class="fu">plot</span>(day, margin, <span class="at">pch =</span> <span class="dv">16</span>, <span class="at">cex =</span> <span class="fl">0.5</span>,</span>
<span id="cb110-1437"><a href="#cb110-1437" aria-hidden="true" tabindex="-1"></a>            <span class="at">main =</span> <span class="st">"Single Regression Tree"</span>))</span>
<span id="cb110-1438"><a href="#cb110-1438" aria-hidden="true" tabindex="-1"></a>polls_2008 <span class="sc">%&gt;%</span></span>
<span id="cb110-1439"><a href="#cb110-1439" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">y_hat =</span> <span class="fu">predict</span>(fit_tree)) <span class="sc">%&gt;%</span></span>
<span id="cb110-1440"><a href="#cb110-1440" aria-hidden="true" tabindex="-1"></a>  <span class="fu">with</span>(<span class="fu">lines</span>(day, y_hat, <span class="at">col =</span> <span class="st">"red"</span>, <span class="at">type =</span> <span class="st">"s"</span>, <span class="at">lwd =</span> <span class="dv">2</span>))</span>
<span id="cb110-1441"><a href="#cb110-1441" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-1442"><a href="#cb110-1442" aria-hidden="true" tabindex="-1"></a><span class="co"># Random forest</span></span>
<span id="cb110-1443"><a href="#cb110-1443" aria-hidden="true" tabindex="-1"></a>fit_rf <span class="ot">&lt;-</span> <span class="fu">randomForest</span>(margin <span class="sc">~</span> ., <span class="at">data =</span> polls_2008)</span>
<span id="cb110-1444"><a href="#cb110-1444" aria-hidden="true" tabindex="-1"></a>polls_2008 <span class="sc">%&gt;%</span></span>
<span id="cb110-1445"><a href="#cb110-1445" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">y_hat =</span> <span class="fu">predict</span>(fit_rf, <span class="at">newdata =</span> polls_2008)) <span class="sc">%&gt;%</span></span>
<span id="cb110-1446"><a href="#cb110-1446" aria-hidden="true" tabindex="-1"></a>  <span class="fu">with</span>(<span class="fu">plot</span>(day, margin, <span class="at">pch =</span> <span class="dv">16</span>, <span class="at">cex =</span> <span class="fl">0.5</span>,</span>
<span id="cb110-1447"><a href="#cb110-1447" aria-hidden="true" tabindex="-1"></a>            <span class="at">main =</span> <span class="st">"Random Forest"</span>))</span>
<span id="cb110-1448"><a href="#cb110-1448" aria-hidden="true" tabindex="-1"></a>polls_2008 <span class="sc">%&gt;%</span></span>
<span id="cb110-1449"><a href="#cb110-1449" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">y_hat =</span> <span class="fu">predict</span>(fit_rf, <span class="at">newdata =</span> polls_2008)) <span class="sc">%&gt;%</span></span>
<span id="cb110-1450"><a href="#cb110-1450" aria-hidden="true" tabindex="-1"></a>  <span class="fu">with</span>(<span class="fu">lines</span>(day, y_hat, <span class="at">col =</span> <span class="st">"blue"</span>, <span class="at">lwd =</span> <span class="dv">2</span>))</span>
<span id="cb110-1451"><a href="#cb110-1451" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb110-1452"><a href="#cb110-1452" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-1453"><a href="#cb110-1453" aria-hidden="true" tabindex="-1"></a>Notice that the random forest estimate is much smoother than what we achieved with the single regression tree. This is possible because the average of many step functions can be smooth—each bootstrap sample produces a slightly different tree, and their average traces out a smooth curve.</span>
<span id="cb110-1454"><a href="#cb110-1454" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-1455"><a href="#cb110-1455" aria-hidden="true" tabindex="-1"></a><span class="fu">### Random Forests in R</span></span>
<span id="cb110-1456"><a href="#cb110-1456" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-1459"><a href="#cb110-1459" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb110-1460"><a href="#cb110-1460" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-rf-model</span></span>
<span id="cb110-1461"><a href="#cb110-1461" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "Random forest OOB error rate decreasing as more trees are added"</span></span>
<span id="cb110-1462"><a href="#cb110-1462" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-width: 7</span></span>
<span id="cb110-1463"><a href="#cb110-1463" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-height: 5</span></span>
<span id="cb110-1464"><a href="#cb110-1464" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(randomForest)</span>
<span id="cb110-1465"><a href="#cb110-1465" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">42</span>)</span>
<span id="cb110-1466"><a href="#cb110-1466" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-1467"><a href="#cb110-1467" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit random forest</span></span>
<span id="cb110-1468"><a href="#cb110-1468" aria-hidden="true" tabindex="-1"></a>rf_model <span class="ot">&lt;-</span> <span class="fu">randomForest</span>(Species <span class="sc">~</span> ., <span class="at">data =</span> iris,</span>
<span id="cb110-1469"><a href="#cb110-1469" aria-hidden="true" tabindex="-1"></a>                          <span class="at">ntree =</span> <span class="dv">500</span>,       <span class="co"># Number of trees</span></span>
<span id="cb110-1470"><a href="#cb110-1470" aria-hidden="true" tabindex="-1"></a>                          <span class="at">mtry =</span> <span class="dv">2</span>,          <span class="co"># Features tried at each split</span></span>
<span id="cb110-1471"><a href="#cb110-1471" aria-hidden="true" tabindex="-1"></a>                          <span class="at">importance =</span> <span class="cn">TRUE</span>)  <span class="co"># Calculate variable importance</span></span>
<span id="cb110-1472"><a href="#cb110-1472" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-1473"><a href="#cb110-1473" aria-hidden="true" tabindex="-1"></a><span class="co"># Model summary</span></span>
<span id="cb110-1474"><a href="#cb110-1474" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(rf_model)</span>
<span id="cb110-1475"><a href="#cb110-1475" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-1476"><a href="#cb110-1476" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot error vs number of trees</span></span>
<span id="cb110-1477"><a href="#cb110-1477" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(rf_model, <span class="at">main =</span> <span class="st">"Random Forest: Error vs. Number of Trees"</span>)</span>
<span id="cb110-1478"><a href="#cb110-1478" aria-hidden="true" tabindex="-1"></a><span class="fu">legend</span>(<span class="st">"topright"</span>, <span class="fu">colnames</span>(rf_model<span class="sc">$</span>err.rate), <span class="at">col =</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">4</span>, <span class="at">lty =</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">4</span>)</span>
<span id="cb110-1479"><a href="#cb110-1479" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb110-1480"><a href="#cb110-1480" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-1481"><a href="#cb110-1481" aria-hidden="true" tabindex="-1"></a><span class="fu">### Out-of-Bag (OOB) Error</span></span>
<span id="cb110-1482"><a href="#cb110-1482" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-1483"><a href="#cb110-1483" aria-hidden="true" tabindex="-1"></a>Each bootstrap sample uses about 63% of observations. The remaining 37% (out-of-bag samples) provide a built-in test set:</span>
<span id="cb110-1484"><a href="#cb110-1484" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-1487"><a href="#cb110-1487" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb110-1488"><a href="#cb110-1488" aria-hidden="true" tabindex="-1"></a><span class="co"># OOB confusion matrix</span></span>
<span id="cb110-1489"><a href="#cb110-1489" aria-hidden="true" tabindex="-1"></a>rf_model<span class="sc">$</span>confusion</span>
<span id="cb110-1490"><a href="#cb110-1490" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-1491"><a href="#cb110-1491" aria-hidden="true" tabindex="-1"></a><span class="co"># OOB error rate</span></span>
<span id="cb110-1492"><a href="#cb110-1492" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"OOB Error Rate:"</span>, <span class="fu">round</span>(<span class="dv">1</span> <span class="sc">-</span> <span class="fu">sum</span>(<span class="fu">diag</span>(rf_model<span class="sc">$</span>confusion[,<span class="dv">1</span><span class="sc">:</span><span class="dv">3</span>])) <span class="sc">/</span></span>
<span id="cb110-1493"><a href="#cb110-1493" aria-hidden="true" tabindex="-1"></a>                              <span class="fu">sum</span>(rf_model<span class="sc">$</span>confusion[,<span class="dv">1</span><span class="sc">:</span><span class="dv">3</span>]), <span class="dv">3</span>), <span class="st">"</span><span class="sc">\n</span><span class="st">"</span>)</span>
<span id="cb110-1494"><a href="#cb110-1494" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb110-1495"><a href="#cb110-1495" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-1496"><a href="#cb110-1496" aria-hidden="true" tabindex="-1"></a>OOB error is nearly as accurate as cross-validation but comes "for free" during training.</span>
<span id="cb110-1497"><a href="#cb110-1497" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-1498"><a href="#cb110-1498" aria-hidden="true" tabindex="-1"></a><span class="fu">### Variable Importance</span></span>
<span id="cb110-1499"><a href="#cb110-1499" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-1500"><a href="#cb110-1500" aria-hidden="true" tabindex="-1"></a>Random forests provide measures of how important each predictor is:</span>
<span id="cb110-1501"><a href="#cb110-1501" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-1504"><a href="#cb110-1504" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb110-1505"><a href="#cb110-1505" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-rf-importance</span></span>
<span id="cb110-1506"><a href="#cb110-1506" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "Variable importance from random forest: Mean Decrease Accuracy measures how much removing a variable hurts prediction; Mean Decrease Gini measures the total reduction in node impurity"</span></span>
<span id="cb110-1507"><a href="#cb110-1507" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-width: 8</span></span>
<span id="cb110-1508"><a href="#cb110-1508" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-height: 5</span></span>
<span id="cb110-1509"><a href="#cb110-1509" aria-hidden="true" tabindex="-1"></a><span class="co"># Variable importance plot</span></span>
<span id="cb110-1510"><a href="#cb110-1510" aria-hidden="true" tabindex="-1"></a><span class="fu">varImpPlot</span>(rf_model, <span class="at">main =</span> <span class="st">"Variable Importance"</span>)</span>
<span id="cb110-1511"><a href="#cb110-1511" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-1512"><a href="#cb110-1512" aria-hidden="true" tabindex="-1"></a><span class="co"># Numeric importance values</span></span>
<span id="cb110-1513"><a href="#cb110-1513" aria-hidden="true" tabindex="-1"></a><span class="fu">importance</span>(rf_model)</span>
<span id="cb110-1514"><a href="#cb110-1514" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb110-1515"><a href="#cb110-1515" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-1516"><a href="#cb110-1516" aria-hidden="true" tabindex="-1"></a>**Mean Decrease Accuracy**: For each tree, predictions are made on OOB samples. Then the values of variable $j$ are randomly permuted, and predictions are made again. The decrease in accuracy from permutation measures importance.</span>
<span id="cb110-1517"><a href="#cb110-1517" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-1518"><a href="#cb110-1518" aria-hidden="true" tabindex="-1"></a>**Mean Decrease Gini**: Total decrease in Gini impurity from splits on variable $j$, averaged over all trees.</span>
<span id="cb110-1519"><a href="#cb110-1519" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-1520"><a href="#cb110-1520" aria-hidden="true" tabindex="-1"></a><span class="fu">### Tuning Random Forests</span></span>
<span id="cb110-1521"><a href="#cb110-1521" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-1522"><a href="#cb110-1522" aria-hidden="true" tabindex="-1"></a>Key parameters to tune:</span>
<span id="cb110-1523"><a href="#cb110-1523" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-1524"><a href="#cb110-1524" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**ntree**: Number of trees (more is generally better, but with diminishing returns)</span>
<span id="cb110-1525"><a href="#cb110-1525" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**mtry**: Number of features considered at each split</span>
<span id="cb110-1526"><a href="#cb110-1526" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**nodesize**: Minimum size of terminal nodes</span>
<span id="cb110-1527"><a href="#cb110-1527" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-1530"><a href="#cb110-1530" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb110-1531"><a href="#cb110-1531" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-rf-tuning</span></span>
<span id="cb110-1532"><a href="#cb110-1532" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "Random forest OOB error as a function of mtry (number of features considered at each split)"</span></span>
<span id="cb110-1533"><a href="#cb110-1533" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-width: 7</span></span>
<span id="cb110-1534"><a href="#cb110-1534" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-height: 5</span></span>
<span id="cb110-1535"><a href="#cb110-1535" aria-hidden="true" tabindex="-1"></a><span class="co"># Tune mtry</span></span>
<span id="cb110-1536"><a href="#cb110-1536" aria-hidden="true" tabindex="-1"></a>oob_error <span class="ot">&lt;-</span> <span class="fu">sapply</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">4</span>, <span class="cf">function</span>(m) {</span>
<span id="cb110-1537"><a href="#cb110-1537" aria-hidden="true" tabindex="-1"></a>  rf <span class="ot">&lt;-</span> <span class="fu">randomForest</span>(Species <span class="sc">~</span> ., <span class="at">data =</span> iris, <span class="at">mtry =</span> m, <span class="at">ntree =</span> <span class="dv">200</span>)</span>
<span id="cb110-1538"><a href="#cb110-1538" aria-hidden="true" tabindex="-1"></a>  rf<span class="sc">$</span>err.rate[<span class="dv">200</span>, <span class="st">"OOB"</span>]</span>
<span id="cb110-1539"><a href="#cb110-1539" aria-hidden="true" tabindex="-1"></a>})</span>
<span id="cb110-1540"><a href="#cb110-1540" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-1541"><a href="#cb110-1541" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">4</span>, oob_error, <span class="at">type =</span> <span class="st">"b"</span>, <span class="at">pch =</span> <span class="dv">19</span>,</span>
<span id="cb110-1542"><a href="#cb110-1542" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">"mtry (features at each split)"</span>,</span>
<span id="cb110-1543"><a href="#cb110-1543" aria-hidden="true" tabindex="-1"></a>     <span class="at">ylab =</span> <span class="st">"OOB Error Rate"</span>,</span>
<span id="cb110-1544"><a href="#cb110-1544" aria-hidden="true" tabindex="-1"></a>     <span class="at">main =</span> <span class="st">"Tuning mtry Parameter"</span>)</span>
<span id="cb110-1545"><a href="#cb110-1545" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb110-1546"><a href="#cb110-1546" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-1547"><a href="#cb110-1547" aria-hidden="true" tabindex="-1"></a><span class="fu">### Random Forest for Regression</span></span>
<span id="cb110-1548"><a href="#cb110-1548" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-1551"><a href="#cb110-1551" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb110-1552"><a href="#cb110-1552" aria-hidden="true" tabindex="-1"></a><span class="co"># Regression random forest</span></span>
<span id="cb110-1553"><a href="#cb110-1553" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">42</span>)</span>
<span id="cb110-1554"><a href="#cb110-1554" aria-hidden="true" tabindex="-1"></a>rf_reg <span class="ot">&lt;-</span> <span class="fu">randomForest</span>(mpg <span class="sc">~</span> ., <span class="at">data =</span> mtcars, <span class="at">ntree =</span> <span class="dv">500</span>, <span class="at">importance =</span> <span class="cn">TRUE</span>)</span>
<span id="cb110-1555"><a href="#cb110-1555" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-1556"><a href="#cb110-1556" aria-hidden="true" tabindex="-1"></a><span class="co"># Performance</span></span>
<span id="cb110-1557"><a href="#cb110-1557" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"Variance explained:"</span>, <span class="fu">round</span>(rf_reg<span class="sc">$</span>rsq[<span class="dv">500</span>] <span class="sc">*</span> <span class="dv">100</span>, <span class="dv">1</span>), <span class="st">"%</span><span class="sc">\n</span><span class="st">"</span>)</span>
<span id="cb110-1558"><a href="#cb110-1558" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"MSE:"</span>, <span class="fu">round</span>(rf_reg<span class="sc">$</span>mse[<span class="dv">500</span>], <span class="dv">2</span>), <span class="st">"</span><span class="sc">\n</span><span class="st">"</span>)</span>
<span id="cb110-1559"><a href="#cb110-1559" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-1560"><a href="#cb110-1560" aria-hidden="true" tabindex="-1"></a><span class="co"># Variable importance for regression</span></span>
<span id="cb110-1561"><a href="#cb110-1561" aria-hidden="true" tabindex="-1"></a><span class="fu">varImpPlot</span>(rf_reg, <span class="at">main =</span> <span class="st">"Variable Importance for MPG Prediction"</span>)</span>
<span id="cb110-1562"><a href="#cb110-1562" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb110-1563"><a href="#cb110-1563" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-1564"><a href="#cb110-1564" aria-hidden="true" tabindex="-1"></a><span class="fu">## Support Vector Machines (SVM)</span></span>
<span id="cb110-1565"><a href="#cb110-1565" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-1566"><a href="#cb110-1566" aria-hidden="true" tabindex="-1"></a>**Support Vector Machines** <span class="co">[</span><span class="ot">@cortes1995support</span><span class="co">]</span> find the hyperplane that best separates classes by maximizing the margin—the distance between the boundary and the nearest points from each class.</span>
<span id="cb110-1567"><a href="#cb110-1567" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-1568"><a href="#cb110-1568" aria-hidden="true" tabindex="-1"></a><span class="fu">### The Maximum Margin Classifier</span></span>
<span id="cb110-1569"><a href="#cb110-1569" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-1570"><a href="#cb110-1570" aria-hidden="true" tabindex="-1"></a>For linearly separable data, infinitely many lines could separate the classes. SVM chooses the line with the largest margin:</span>
<span id="cb110-1571"><a href="#cb110-1571" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-1574"><a href="#cb110-1574" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb110-1575"><a href="#cb110-1575" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-svm-concept</span></span>
<span id="cb110-1576"><a href="#cb110-1576" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "Support Vector Machine concept: the decision boundary (solid line) maximizes the margin (distance to nearest points). Support vectors are the points on the margin boundaries."</span></span>
<span id="cb110-1577"><a href="#cb110-1577" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-width: 7</span></span>
<span id="cb110-1578"><a href="#cb110-1578" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-height: 6</span></span>
<span id="cb110-1579"><a href="#cb110-1579" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(e1071)</span>
<span id="cb110-1580"><a href="#cb110-1580" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-1581"><a href="#cb110-1581" aria-hidden="true" tabindex="-1"></a><span class="co"># Create simple 2D data</span></span>
<span id="cb110-1582"><a href="#cb110-1582" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">42</span>)</span>
<span id="cb110-1583"><a href="#cb110-1583" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">40</span></span>
<span id="cb110-1584"><a href="#cb110-1584" aria-hidden="true" tabindex="-1"></a>x1 <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fu">rnorm</span>(n<span class="sc">/</span><span class="dv">2</span>, <span class="at">mean =</span> <span class="dv">0</span>), <span class="fu">rnorm</span>(n<span class="sc">/</span><span class="dv">2</span>, <span class="at">mean =</span> <span class="dv">3</span>))</span>
<span id="cb110-1585"><a href="#cb110-1585" aria-hidden="true" tabindex="-1"></a>x2 <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fu">rnorm</span>(n<span class="sc">/</span><span class="dv">2</span>, <span class="at">mean =</span> <span class="dv">0</span>), <span class="fu">rnorm</span>(n<span class="sc">/</span><span class="dv">2</span>, <span class="at">mean =</span> <span class="dv">3</span>))</span>
<span id="cb110-1586"><a href="#cb110-1586" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> <span class="fu">factor</span>(<span class="fu">c</span>(<span class="fu">rep</span>(<span class="sc">-</span><span class="dv">1</span>, n<span class="sc">/</span><span class="dv">2</span>), <span class="fu">rep</span>(<span class="dv">1</span>, n<span class="sc">/</span><span class="dv">2</span>)))</span>
<span id="cb110-1587"><a href="#cb110-1587" aria-hidden="true" tabindex="-1"></a>svm_data <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(x1, x2, y)</span>
<span id="cb110-1588"><a href="#cb110-1588" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-1589"><a href="#cb110-1589" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit linear SVM</span></span>
<span id="cb110-1590"><a href="#cb110-1590" aria-hidden="true" tabindex="-1"></a>svm_linear <span class="ot">&lt;-</span> <span class="fu">svm</span>(y <span class="sc">~</span> x1 <span class="sc">+</span> x2, <span class="at">data =</span> svm_data, <span class="at">kernel =</span> <span class="st">"linear"</span>,</span>
<span id="cb110-1591"><a href="#cb110-1591" aria-hidden="true" tabindex="-1"></a>                   <span class="at">cost =</span> <span class="dv">10</span>, <span class="at">scale =</span> <span class="cn">FALSE</span>)</span>
<span id="cb110-1592"><a href="#cb110-1592" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-1593"><a href="#cb110-1593" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot</span></span>
<span id="cb110-1594"><a href="#cb110-1594" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(svm_linear, svm_data, x1 <span class="sc">~</span> x2,</span>
<span id="cb110-1595"><a href="#cb110-1595" aria-hidden="true" tabindex="-1"></a>     <span class="at">col =</span> <span class="fu">c</span>(<span class="st">"lightblue"</span>, <span class="st">"lightpink"</span>),</span>
<span id="cb110-1596"><a href="#cb110-1596" aria-hidden="true" tabindex="-1"></a>     <span class="at">symbolPalette =</span> <span class="fu">c</span>(<span class="st">"blue"</span>, <span class="st">"red"</span>),</span>
<span id="cb110-1597"><a href="#cb110-1597" aria-hidden="true" tabindex="-1"></a>     <span class="at">svSymbol =</span> <span class="st">"x"</span>, <span class="at">dataSymbol =</span> <span class="st">"o"</span>)</span>
<span id="cb110-1598"><a href="#cb110-1598" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb110-1599"><a href="#cb110-1599" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-1600"><a href="#cb110-1600" aria-hidden="true" tabindex="-1"></a><span class="fu">### Soft Margin and the Cost Parameter</span></span>
<span id="cb110-1601"><a href="#cb110-1601" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-1602"><a href="#cb110-1602" aria-hidden="true" tabindex="-1"></a>Real data is rarely perfectly separable. **Soft margin** SVM allows some points to violate the margin, controlled by the cost parameter $C$:</span>
<span id="cb110-1603"><a href="#cb110-1603" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-1604"><a href="#cb110-1604" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**High C**: Small margin, few violations (may overfit)</span>
<span id="cb110-1605"><a href="#cb110-1605" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Low C**: Large margin, more violations (may underfit)</span>
<span id="cb110-1606"><a href="#cb110-1606" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-1609"><a href="#cb110-1609" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb110-1610"><a href="#cb110-1610" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-svm-cost</span></span>
<span id="cb110-1611"><a href="#cb110-1611" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "Effect of cost parameter on SVM decision boundary: low cost allows more margin violations (smoother boundary), high cost enforces stricter separation"</span></span>
<span id="cb110-1612"><a href="#cb110-1612" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-width: 9</span></span>
<span id="cb110-1613"><a href="#cb110-1613" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-height: 4</span></span>
<span id="cb110-1614"><a href="#cb110-1614" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">3</span>))</span>
<span id="cb110-1615"><a href="#cb110-1615" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-1616"><a href="#cb110-1616" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (cost_val <span class="cf">in</span> <span class="fu">c</span>(<span class="fl">0.1</span>, <span class="dv">1</span>, <span class="dv">100</span>)) {</span>
<span id="cb110-1617"><a href="#cb110-1617" aria-hidden="true" tabindex="-1"></a>  svm_fit <span class="ot">&lt;-</span> <span class="fu">svm</span>(y <span class="sc">~</span> x1 <span class="sc">+</span> x2, <span class="at">data =</span> svm_data, <span class="at">kernel =</span> <span class="st">"linear"</span>,</span>
<span id="cb110-1618"><a href="#cb110-1618" aria-hidden="true" tabindex="-1"></a>                  <span class="at">cost =</span> cost_val, <span class="at">scale =</span> <span class="cn">FALSE</span>)</span>
<span id="cb110-1619"><a href="#cb110-1619" aria-hidden="true" tabindex="-1"></a>  <span class="fu">plot</span>(svm_fit, svm_data, x1 <span class="sc">~</span> x2,</span>
<span id="cb110-1620"><a href="#cb110-1620" aria-hidden="true" tabindex="-1"></a>       <span class="at">col =</span> <span class="fu">c</span>(<span class="st">"lightblue"</span>, <span class="st">"lightpink"</span>),</span>
<span id="cb110-1621"><a href="#cb110-1621" aria-hidden="true" tabindex="-1"></a>       <span class="at">main =</span> <span class="fu">paste</span>(<span class="st">"Cost ="</span>, cost_val))</span>
<span id="cb110-1622"><a href="#cb110-1622" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb110-1623"><a href="#cb110-1623" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb110-1624"><a href="#cb110-1624" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-1625"><a href="#cb110-1625" aria-hidden="true" tabindex="-1"></a><span class="fu">### The Kernel Trick</span></span>
<span id="cb110-1626"><a href="#cb110-1626" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-1627"><a href="#cb110-1627" aria-hidden="true" tabindex="-1"></a>For non-linear boundaries, SVM uses **kernels** to implicitly map data to higher dimensions where classes become linearly separable:</span>
<span id="cb110-1628"><a href="#cb110-1628" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-1629"><a href="#cb110-1629" aria-hidden="true" tabindex="-1"></a>**Common kernels:**</span>
<span id="cb110-1630"><a href="#cb110-1630" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-1631"><a href="#cb110-1631" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Linear**: $K(x, x') = x \cdot x'$ (no transformation)</span>
<span id="cb110-1632"><a href="#cb110-1632" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Polynomial**: $K(x, x') = (1 + x \cdot x')^d$</span>
<span id="cb110-1633"><a href="#cb110-1633" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Radial Basis Function (RBF)**: $K(x, x') = \exp(-\gamma <span class="sc">\|</span>x - x'<span class="sc">\|</span>^2)$</span>
<span id="cb110-1634"><a href="#cb110-1634" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-1637"><a href="#cb110-1637" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb110-1638"><a href="#cb110-1638" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-svm-kernels</span></span>
<span id="cb110-1639"><a href="#cb110-1639" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "Different SVM kernels produce different decision boundaries: linear kernels give straight lines, polynomial and RBF kernels can capture non-linear patterns"</span></span>
<span id="cb110-1640"><a href="#cb110-1640" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-width: 9</span></span>
<span id="cb110-1641"><a href="#cb110-1641" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-height: 4</span></span>
<span id="cb110-1642"><a href="#cb110-1642" aria-hidden="true" tabindex="-1"></a><span class="co"># Create non-linear data</span></span>
<span id="cb110-1643"><a href="#cb110-1643" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb110-1644"><a href="#cb110-1644" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">200</span></span>
<span id="cb110-1645"><a href="#cb110-1645" aria-hidden="true" tabindex="-1"></a>r1 <span class="ot">&lt;-</span> <span class="fu">runif</span>(n<span class="sc">/</span><span class="dv">2</span>, <span class="dv">0</span>, <span class="dv">2</span>)</span>
<span id="cb110-1646"><a href="#cb110-1646" aria-hidden="true" tabindex="-1"></a>theta1 <span class="ot">&lt;-</span> <span class="fu">runif</span>(n<span class="sc">/</span><span class="dv">2</span>, <span class="dv">0</span>, <span class="dv">2</span><span class="sc">*</span>pi)</span>
<span id="cb110-1647"><a href="#cb110-1647" aria-hidden="true" tabindex="-1"></a>r2 <span class="ot">&lt;-</span> <span class="fu">runif</span>(n<span class="sc">/</span><span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">5</span>)</span>
<span id="cb110-1648"><a href="#cb110-1648" aria-hidden="true" tabindex="-1"></a>theta2 <span class="ot">&lt;-</span> <span class="fu">runif</span>(n<span class="sc">/</span><span class="dv">2</span>, <span class="dv">0</span>, <span class="dv">2</span><span class="sc">*</span>pi)</span>
<span id="cb110-1649"><a href="#cb110-1649" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-1650"><a href="#cb110-1650" aria-hidden="true" tabindex="-1"></a>x1 <span class="ot">&lt;-</span> <span class="fu">c</span>(r1 <span class="sc">*</span> <span class="fu">cos</span>(theta1), r2 <span class="sc">*</span> <span class="fu">cos</span>(theta2))</span>
<span id="cb110-1651"><a href="#cb110-1651" aria-hidden="true" tabindex="-1"></a>x2 <span class="ot">&lt;-</span> <span class="fu">c</span>(r1 <span class="sc">*</span> <span class="fu">sin</span>(theta1), r2 <span class="sc">*</span> <span class="fu">sin</span>(theta2))</span>
<span id="cb110-1652"><a href="#cb110-1652" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> <span class="fu">factor</span>(<span class="fu">c</span>(<span class="fu">rep</span>(<span class="st">"inner"</span>, n<span class="sc">/</span><span class="dv">2</span>), <span class="fu">rep</span>(<span class="st">"outer"</span>, n<span class="sc">/</span><span class="dv">2</span>)))</span>
<span id="cb110-1653"><a href="#cb110-1653" aria-hidden="true" tabindex="-1"></a>circle_data <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(x1, x2, y)</span>
<span id="cb110-1654"><a href="#cb110-1654" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-1655"><a href="#cb110-1655" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">3</span>))</span>
<span id="cb110-1656"><a href="#cb110-1656" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-1657"><a href="#cb110-1657" aria-hidden="true" tabindex="-1"></a><span class="co"># Linear (fails)</span></span>
<span id="cb110-1658"><a href="#cb110-1658" aria-hidden="true" tabindex="-1"></a>svm_lin <span class="ot">&lt;-</span> <span class="fu">svm</span>(y <span class="sc">~</span> ., <span class="at">data =</span> circle_data, <span class="at">kernel =</span> <span class="st">"linear"</span>)</span>
<span id="cb110-1659"><a href="#cb110-1659" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(svm_lin, circle_data, <span class="at">col =</span> <span class="fu">c</span>(<span class="st">"lightblue"</span>, <span class="st">"lightpink"</span>),</span>
<span id="cb110-1660"><a href="#cb110-1660" aria-hidden="true" tabindex="-1"></a>     <span class="at">main =</span> <span class="st">"Linear Kernel"</span>)</span>
<span id="cb110-1661"><a href="#cb110-1661" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-1662"><a href="#cb110-1662" aria-hidden="true" tabindex="-1"></a><span class="co"># Polynomial</span></span>
<span id="cb110-1663"><a href="#cb110-1663" aria-hidden="true" tabindex="-1"></a>svm_poly <span class="ot">&lt;-</span> <span class="fu">svm</span>(y <span class="sc">~</span> ., <span class="at">data =</span> circle_data, <span class="at">kernel =</span> <span class="st">"polynomial"</span>, <span class="at">degree =</span> <span class="dv">2</span>)</span>
<span id="cb110-1664"><a href="#cb110-1664" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(svm_poly, circle_data, <span class="at">col =</span> <span class="fu">c</span>(<span class="st">"lightblue"</span>, <span class="st">"lightpink"</span>),</span>
<span id="cb110-1665"><a href="#cb110-1665" aria-hidden="true" tabindex="-1"></a>     <span class="at">main =</span> <span class="st">"Polynomial Kernel (d=2)"</span>)</span>
<span id="cb110-1666"><a href="#cb110-1666" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-1667"><a href="#cb110-1667" aria-hidden="true" tabindex="-1"></a><span class="co"># RBF</span></span>
<span id="cb110-1668"><a href="#cb110-1668" aria-hidden="true" tabindex="-1"></a>svm_rbf <span class="ot">&lt;-</span> <span class="fu">svm</span>(y <span class="sc">~</span> ., <span class="at">data =</span> circle_data, <span class="at">kernel =</span> <span class="st">"radial"</span>, <span class="at">gamma =</span> <span class="fl">0.5</span>)</span>
<span id="cb110-1669"><a href="#cb110-1669" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(svm_rbf, circle_data, <span class="at">col =</span> <span class="fu">c</span>(<span class="st">"lightblue"</span>, <span class="st">"lightpink"</span>),</span>
<span id="cb110-1670"><a href="#cb110-1670" aria-hidden="true" tabindex="-1"></a>     <span class="at">main =</span> <span class="st">"RBF Kernel"</span>)</span>
<span id="cb110-1671"><a href="#cb110-1671" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb110-1672"><a href="#cb110-1672" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-1673"><a href="#cb110-1673" aria-hidden="true" tabindex="-1"></a><span class="fu">### Tuning SVM with Cross-Validation</span></span>
<span id="cb110-1674"><a href="#cb110-1674" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-1675"><a href="#cb110-1675" aria-hidden="true" tabindex="-1"></a>The key parameters to tune are:</span>
<span id="cb110-1676"><a href="#cb110-1676" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**cost**: Penalty for margin violations</span>
<span id="cb110-1677"><a href="#cb110-1677" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**gamma**: For RBF kernel, controls the "reach" of each training example</span>
<span id="cb110-1678"><a href="#cb110-1678" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-1681"><a href="#cb110-1681" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb110-1682"><a href="#cb110-1682" aria-hidden="true" tabindex="-1"></a><span class="co"># Tune SVM using cross-validation</span></span>
<span id="cb110-1683"><a href="#cb110-1683" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">42</span>)</span>
<span id="cb110-1684"><a href="#cb110-1684" aria-hidden="true" tabindex="-1"></a>tune_result <span class="ot">&lt;-</span> <span class="fu">tune</span>(svm, Species <span class="sc">~</span> ., <span class="at">data =</span> iris,</span>
<span id="cb110-1685"><a href="#cb110-1685" aria-hidden="true" tabindex="-1"></a>                     <span class="at">kernel =</span> <span class="st">"radial"</span>,</span>
<span id="cb110-1686"><a href="#cb110-1686" aria-hidden="true" tabindex="-1"></a>                     <span class="at">ranges =</span> <span class="fu">list</span>(</span>
<span id="cb110-1687"><a href="#cb110-1687" aria-hidden="true" tabindex="-1"></a>                       <span class="at">cost =</span> <span class="fu">c</span>(<span class="fl">0.1</span>, <span class="dv">1</span>, <span class="dv">10</span>, <span class="dv">100</span>),</span>
<span id="cb110-1688"><a href="#cb110-1688" aria-hidden="true" tabindex="-1"></a>                       <span class="at">gamma =</span> <span class="fu">c</span>(<span class="fl">0.01</span>, <span class="fl">0.1</span>, <span class="fl">0.5</span>, <span class="dv">1</span>)</span>
<span id="cb110-1689"><a href="#cb110-1689" aria-hidden="true" tabindex="-1"></a>                     ))</span>
<span id="cb110-1690"><a href="#cb110-1690" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-1691"><a href="#cb110-1691" aria-hidden="true" tabindex="-1"></a><span class="co"># Best parameters</span></span>
<span id="cb110-1692"><a href="#cb110-1692" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"Best parameters:</span><span class="sc">\n</span><span class="st">"</span>)</span>
<span id="cb110-1693"><a href="#cb110-1693" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(tune_result<span class="sc">$</span>best.parameters)</span>
<span id="cb110-1694"><a href="#cb110-1694" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-1695"><a href="#cb110-1695" aria-hidden="true" tabindex="-1"></a><span class="co"># Best model performance</span></span>
<span id="cb110-1696"><a href="#cb110-1696" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"</span><span class="sc">\n</span><span class="st">Best model error:"</span>, <span class="fu">round</span>(tune_result<span class="sc">$</span>best.performance, <span class="dv">3</span>), <span class="st">"</span><span class="sc">\n</span><span class="st">"</span>)</span>
<span id="cb110-1697"><a href="#cb110-1697" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-1698"><a href="#cb110-1698" aria-hidden="true" tabindex="-1"></a><span class="co"># Use best model</span></span>
<span id="cb110-1699"><a href="#cb110-1699" aria-hidden="true" tabindex="-1"></a>best_svm <span class="ot">&lt;-</span> tune_result<span class="sc">$</span>best.model</span>
<span id="cb110-1700"><a href="#cb110-1700" aria-hidden="true" tabindex="-1"></a><span class="fu">table</span>(<span class="at">Predicted =</span> <span class="fu">predict</span>(best_svm), <span class="at">Actual =</span> iris<span class="sc">$</span>Species)</span>
<span id="cb110-1701"><a href="#cb110-1701" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb110-1702"><a href="#cb110-1702" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-1703"><a href="#cb110-1703" aria-hidden="true" tabindex="-1"></a><span class="fu">### Multiclass SVM</span></span>
<span id="cb110-1704"><a href="#cb110-1704" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-1705"><a href="#cb110-1705" aria-hidden="true" tabindex="-1"></a>SVM is inherently binary, but extends to multiple classes via:</span>
<span id="cb110-1706"><a href="#cb110-1706" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-1707"><a href="#cb110-1707" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**One-vs-One**: Fit $\binom{K}{2}$ classifiers for all pairs of classes; classify by voting</span>
<span id="cb110-1708"><a href="#cb110-1708" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**One-vs-All**: Fit $K$ classifiers (each class vs. rest); classify to highest-scoring class</span>
<span id="cb110-1709"><a href="#cb110-1709" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-1710"><a href="#cb110-1710" aria-hidden="true" tabindex="-1"></a>R's <span class="in">`svm()`</span> uses one-vs-one by default.</span>
<span id="cb110-1711"><a href="#cb110-1711" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-1712"><a href="#cb110-1712" aria-hidden="true" tabindex="-1"></a><span class="fu">### Support Vector Regression (SVR)</span></span>
<span id="cb110-1713"><a href="#cb110-1713" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-1714"><a href="#cb110-1714" aria-hidden="true" tabindex="-1"></a>SVMs can also be used for regression problems. **Support Vector Regression** works by fitting a tube of width $\epsilon$ around the data—points inside the tube contribute no loss, while points outside are penalized.</span>
<span id="cb110-1715"><a href="#cb110-1715" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-1716"><a href="#cb110-1716" aria-hidden="true" tabindex="-1"></a>The key idea is that instead of minimizing squared errors (like in linear regression), SVR minimizes how much predictions deviate beyond a tolerance margin $\epsilon$:</span>
<span id="cb110-1717"><a href="#cb110-1717" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-1718"><a href="#cb110-1718" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb110-1719"><a href="#cb110-1719" aria-hidden="true" tabindex="-1"></a>L_\epsilon(y, \hat{y}) = \begin{cases} 0 &amp; \text{if } |y - \hat{y}| \leq \epsilon <span class="sc">\\</span> |y - \hat{y}| - \epsilon &amp; \text{otherwise} \end{cases}</span>
<span id="cb110-1720"><a href="#cb110-1720" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb110-1721"><a href="#cb110-1721" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-1722"><a href="#cb110-1722" aria-hidden="true" tabindex="-1"></a>This is called the **$\epsilon$-insensitive loss function**.</span>
<span id="cb110-1723"><a href="#cb110-1723" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-1726"><a href="#cb110-1726" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb110-1727"><a href="#cb110-1727" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-svr</span></span>
<span id="cb110-1728"><a href="#cb110-1728" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "Support Vector Regression fits a tube around the data. Points within the tube (width epsilon) have zero loss."</span></span>
<span id="cb110-1729"><a href="#cb110-1729" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-width: 8</span></span>
<span id="cb110-1730"><a href="#cb110-1730" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-height: 5</span></span>
<span id="cb110-1731"><a href="#cb110-1731" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate non-linear data</span></span>
<span id="cb110-1732"><a href="#cb110-1732" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">42</span>)</span>
<span id="cb110-1733"><a href="#cb110-1733" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">100</span></span>
<span id="cb110-1734"><a href="#cb110-1734" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="dv">0</span>, <span class="dv">4</span><span class="sc">*</span>pi, <span class="at">length.out =</span> n)</span>
<span id="cb110-1735"><a href="#cb110-1735" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> <span class="fu">sin</span>(x) <span class="sc">+</span> <span class="fu">rnorm</span>(n, <span class="at">sd =</span> <span class="fl">0.3</span>)</span>
<span id="cb110-1736"><a href="#cb110-1736" aria-hidden="true" tabindex="-1"></a>svr_data <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">x =</span> x, <span class="at">y =</span> y)</span>
<span id="cb110-1737"><a href="#cb110-1737" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-1738"><a href="#cb110-1738" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit SVR with RBF kernel</span></span>
<span id="cb110-1739"><a href="#cb110-1739" aria-hidden="true" tabindex="-1"></a>svr_model <span class="ot">&lt;-</span> <span class="fu">svm</span>(y <span class="sc">~</span> x, <span class="at">data =</span> svr_data, <span class="at">kernel =</span> <span class="st">"radial"</span>,</span>
<span id="cb110-1740"><a href="#cb110-1740" aria-hidden="true" tabindex="-1"></a>                  <span class="at">epsilon =</span> <span class="fl">0.3</span>, <span class="at">cost =</span> <span class="dv">10</span>)</span>
<span id="cb110-1741"><a href="#cb110-1741" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-1742"><a href="#cb110-1742" aria-hidden="true" tabindex="-1"></a><span class="co"># Predictions</span></span>
<span id="cb110-1743"><a href="#cb110-1743" aria-hidden="true" tabindex="-1"></a>svr_data<span class="sc">$</span>pred <span class="ot">&lt;-</span> <span class="fu">predict</span>(svr_model, svr_data)</span>
<span id="cb110-1744"><a href="#cb110-1744" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-1745"><a href="#cb110-1745" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot</span></span>
<span id="cb110-1746"><a href="#cb110-1746" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(x, y, <span class="at">pch =</span> <span class="dv">16</span>, <span class="at">col =</span> <span class="st">"gray60"</span>, <span class="at">main =</span> <span class="st">"Support Vector Regression"</span>)</span>
<span id="cb110-1747"><a href="#cb110-1747" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(x, svr_data<span class="sc">$</span>pred, <span class="at">col =</span> <span class="st">"blue"</span>, <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb110-1748"><a href="#cb110-1748" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(x, <span class="fu">sin</span>(x), <span class="at">col =</span> <span class="st">"red"</span>, <span class="at">lwd =</span> <span class="dv">2</span>, <span class="at">lty =</span> <span class="dv">2</span>)</span>
<span id="cb110-1749"><a href="#cb110-1749" aria-hidden="true" tabindex="-1"></a><span class="fu">legend</span>(<span class="st">"topright"</span>, <span class="fu">c</span>(<span class="st">"SVR fit"</span>, <span class="st">"True function"</span>),</span>
<span id="cb110-1750"><a href="#cb110-1750" aria-hidden="true" tabindex="-1"></a>       <span class="at">col =</span> <span class="fu">c</span>(<span class="st">"blue"</span>, <span class="st">"red"</span>), <span class="at">lty =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">2</span>), <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb110-1751"><a href="#cb110-1751" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb110-1752"><a href="#cb110-1752" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-1753"><a href="#cb110-1753" aria-hidden="true" tabindex="-1"></a>Key SVR parameters:</span>
<span id="cb110-1754"><a href="#cb110-1754" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-1755"><a href="#cb110-1755" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**epsilon**: Width of the insensitive tube. Larger values give smoother fits.</span>
<span id="cb110-1756"><a href="#cb110-1756" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**cost (C)**: Penalty for points outside the tube. Higher C fits the data more closely.</span>
<span id="cb110-1757"><a href="#cb110-1757" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**kernel**: As with classification, RBF kernels can capture non-linear patterns.</span>
<span id="cb110-1758"><a href="#cb110-1758" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-1761"><a href="#cb110-1761" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb110-1762"><a href="#cb110-1762" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-svr-epsilon</span></span>
<span id="cb110-1763"><a href="#cb110-1763" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "Effect of epsilon on SVR: larger epsilon creates wider tubes and smoother fits"</span></span>
<span id="cb110-1764"><a href="#cb110-1764" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-width: 9</span></span>
<span id="cb110-1765"><a href="#cb110-1765" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-height: 4</span></span>
<span id="cb110-1766"><a href="#cb110-1766" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">3</span>))</span>
<span id="cb110-1767"><a href="#cb110-1767" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-1768"><a href="#cb110-1768" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (eps <span class="cf">in</span> <span class="fu">c</span>(<span class="fl">0.1</span>, <span class="fl">0.3</span>, <span class="fl">0.5</span>)) {</span>
<span id="cb110-1769"><a href="#cb110-1769" aria-hidden="true" tabindex="-1"></a>  svr_fit <span class="ot">&lt;-</span> <span class="fu">svm</span>(y <span class="sc">~</span> x, <span class="at">data =</span> svr_data, <span class="at">kernel =</span> <span class="st">"radial"</span>,</span>
<span id="cb110-1770"><a href="#cb110-1770" aria-hidden="true" tabindex="-1"></a>                  <span class="at">epsilon =</span> eps, <span class="at">cost =</span> <span class="dv">10</span>)</span>
<span id="cb110-1771"><a href="#cb110-1771" aria-hidden="true" tabindex="-1"></a>  <span class="fu">plot</span>(x, y, <span class="at">pch =</span> <span class="dv">16</span>, <span class="at">col =</span> <span class="st">"gray60"</span>, <span class="at">main =</span> <span class="fu">paste</span>(<span class="st">"epsilon ="</span>, eps))</span>
<span id="cb110-1772"><a href="#cb110-1772" aria-hidden="true" tabindex="-1"></a>  <span class="fu">lines</span>(x, <span class="fu">predict</span>(svr_fit), <span class="at">col =</span> <span class="st">"blue"</span>, <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb110-1773"><a href="#cb110-1773" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb110-1774"><a href="#cb110-1774" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb110-1775"><a href="#cb110-1775" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-1776"><a href="#cb110-1776" aria-hidden="true" tabindex="-1"></a>::: {.callout-note}</span>
<span id="cb110-1777"><a href="#cb110-1777" aria-hidden="true" tabindex="-1"></a><span class="fu">## SVM vs. Other Methods</span></span>
<span id="cb110-1778"><a href="#cb110-1778" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-1779"><a href="#cb110-1779" aria-hidden="true" tabindex="-1"></a>**Advantages of SVM:**</span>
<span id="cb110-1780"><a href="#cb110-1780" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Effective in high-dimensional spaces (even when dimensions &gt; samples)</span>
<span id="cb110-1781"><a href="#cb110-1781" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Memory efficient (uses only support vectors)</span>
<span id="cb110-1782"><a href="#cb110-1782" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Versatile through different kernels</span>
<span id="cb110-1783"><a href="#cb110-1783" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-1784"><a href="#cb110-1784" aria-hidden="true" tabindex="-1"></a>**Disadvantages:**</span>
<span id="cb110-1785"><a href="#cb110-1785" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Doesn't provide probability estimates directly (though they can be computed)</span>
<span id="cb110-1786"><a href="#cb110-1786" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Sensitive to feature scaling—always standardize!</span>
<span id="cb110-1787"><a href="#cb110-1787" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Can be slow on very large datasets</span>
<span id="cb110-1788"><a href="#cb110-1788" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Kernel and parameter selection can be tricky</span>
<span id="cb110-1789"><a href="#cb110-1789" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb110-1790"><a href="#cb110-1790" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-1791"><a href="#cb110-1791" aria-hidden="true" tabindex="-1"></a><span class="fu">## Comparing Classification Methods</span></span>
<span id="cb110-1792"><a href="#cb110-1792" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-1793"><a href="#cb110-1793" aria-hidden="true" tabindex="-1"></a>Different methods have different strengths:</span>
<span id="cb110-1794"><a href="#cb110-1794" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-1795"><a href="#cb110-1795" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> Method <span class="pp">|</span> Interpretability <span class="pp">|</span> Handles Non-linearity <span class="pp">|</span> Speed <span class="pp">|</span> Best For <span class="pp">|</span></span>
<span id="cb110-1796"><a href="#cb110-1796" aria-hidden="true" tabindex="-1"></a><span class="pp">|:-------|:-----------------|:----------------------|:------|:---------|</span></span>
<span id="cb110-1797"><a href="#cb110-1797" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> kNN <span class="pp">|</span> Low <span class="pp">|</span> Yes (inherently) <span class="pp">|</span> Slow for large data <span class="pp">|</span> Simple problems, few features <span class="pp">|</span></span>
<span id="cb110-1798"><a href="#cb110-1798" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> Decision Tree <span class="pp">|</span> High <span class="pp">|</span> Yes <span class="pp">|</span> Fast <span class="pp">|</span> Interpretability needed <span class="pp">|</span></span>
<span id="cb110-1799"><a href="#cb110-1799" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> Random Forest <span class="pp">|</span> Medium <span class="pp">|</span> Yes <span class="pp">|</span> Moderate <span class="pp">|</span> General purpose, variable importance <span class="pp">|</span></span>
<span id="cb110-1800"><a href="#cb110-1800" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> SVM <span class="pp">|</span> Low <span class="pp">|</span> Yes (with kernels) <span class="pp">|</span> Moderate <span class="pp">|</span> High-dimensional data <span class="pp">|</span></span>
<span id="cb110-1801"><a href="#cb110-1801" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> Logistic Regression <span class="pp">|</span> High <span class="pp">|</span> No (needs feature engineering) <span class="pp">|</span> Fast <span class="pp">|</span> Probability estimates, inference <span class="pp">|</span></span>
<span id="cb110-1802"><a href="#cb110-1802" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-1803"><a href="#cb110-1803" aria-hidden="true" tabindex="-1"></a><span class="fu">## Practical Workflow</span></span>
<span id="cb110-1804"><a href="#cb110-1804" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-1805"><a href="#cb110-1805" aria-hidden="true" tabindex="-1"></a>A typical statistical learning workflow:</span>
<span id="cb110-1806"><a href="#cb110-1806" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-1807"><a href="#cb110-1807" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**Split data** into training and test sets</span>
<span id="cb110-1808"><a href="#cb110-1808" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**Explore** the training data</span>
<span id="cb110-1809"><a href="#cb110-1809" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**Build candidate models** with different algorithms or parameters</span>
<span id="cb110-1810"><a href="#cb110-1810" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>**Evaluate** using cross-validation on training data</span>
<span id="cb110-1811"><a href="#cb110-1811" aria-hidden="true" tabindex="-1"></a><span class="ss">5. </span>**Select** the best model</span>
<span id="cb110-1812"><a href="#cb110-1812" aria-hidden="true" tabindex="-1"></a><span class="ss">6. </span>**Final evaluation** on held-out test data</span>
<span id="cb110-1813"><a href="#cb110-1813" aria-hidden="true" tabindex="-1"></a><span class="ss">7. </span>**Report** honest estimates of performance</span>
<span id="cb110-1814"><a href="#cb110-1814" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-1815"><a href="#cb110-1815" aria-hidden="true" tabindex="-1"></a>Never use test data for model building or selection—that defeats the purpose of holding it out.</span>
<span id="cb110-1816"><a href="#cb110-1816" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-1817"><a href="#cb110-1817" aria-hidden="true" tabindex="-1"></a><span class="fu">## When to Use Statistical Learning</span></span>
<span id="cb110-1818"><a href="#cb110-1818" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-1819"><a href="#cb110-1819" aria-hidden="true" tabindex="-1"></a>Statistical learning excels when:</span>
<span id="cb110-1820"><a href="#cb110-1820" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Prediction is the primary goal</span>
<span id="cb110-1821"><a href="#cb110-1821" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Relationships are complex or non-linear</span>
<span id="cb110-1822"><a href="#cb110-1822" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>You have substantial data</span>
<span id="cb110-1823"><a href="#cb110-1823" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Interpretability is less critical</span>
<span id="cb110-1824"><a href="#cb110-1824" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-1825"><a href="#cb110-1825" aria-hidden="true" tabindex="-1"></a>Traditional statistical methods may be preferable when:</span>
<span id="cb110-1826"><a href="#cb110-1826" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Understanding relationships matters more than prediction</span>
<span id="cb110-1827"><a href="#cb110-1827" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Sample sizes are small</span>
<span id="cb110-1828"><a href="#cb110-1828" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>You need confidence intervals and hypothesis tests</span>
<span id="cb110-1829"><a href="#cb110-1829" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Interpretability is essential</span>
<span id="cb110-1830"><a href="#cb110-1830" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-1831"><a href="#cb110-1831" aria-hidden="true" tabindex="-1"></a><span class="fu">## Connection to Dimensionality Reduction</span></span>
<span id="cb110-1832"><a href="#cb110-1832" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-1833"><a href="#cb110-1833" aria-hidden="true" tabindex="-1"></a>High-dimensional data often benefit from dimensionality reduction before applying statistical learning methods. Techniques like PCA, clustering, and discriminant analysis are covered in detail in @sec-dimensionality-reduction.</span>
<span id="cb110-1834"><a href="#cb110-1834" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-1835"><a href="#cb110-1835" aria-hidden="true" tabindex="-1"></a><span class="fu">## Exercises</span></span>
<span id="cb110-1836"><a href="#cb110-1836" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-1837"><a href="#cb110-1837" aria-hidden="true" tabindex="-1"></a>::: {.callout-note}</span>
<span id="cb110-1838"><a href="#cb110-1838" aria-hidden="true" tabindex="-1"></a><span class="fu">### Exercise SL.1: Decision Trees and Random Forests</span></span>
<span id="cb110-1839"><a href="#cb110-1839" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-1840"><a href="#cb110-1840" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>Create a simple dataset where the outcome grows 0.75 units on average for every increase in a predictor:</span>
<span id="cb110-1841"><a href="#cb110-1841" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-1844"><a href="#cb110-1844" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb110-1845"><a href="#cb110-1845" aria-hidden="true" tabindex="-1"></a><span class="co">#| eval: false</span></span>
<span id="cb110-1846"><a href="#cb110-1846" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">1000</span></span>
<span id="cb110-1847"><a href="#cb110-1847" aria-hidden="true" tabindex="-1"></a>sigma <span class="ot">&lt;-</span> <span class="fl">0.25</span></span>
<span id="cb110-1848"><a href="#cb110-1848" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(n, <span class="dv">0</span>, <span class="dv">1</span>)</span>
<span id="cb110-1849"><a href="#cb110-1849" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> <span class="fl">0.75</span> <span class="sc">*</span> x <span class="sc">+</span> <span class="fu">rnorm</span>(n, <span class="dv">0</span>, sigma)</span>
<span id="cb110-1850"><a href="#cb110-1850" aria-hidden="true" tabindex="-1"></a>dat <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">x =</span> x, <span class="at">y =</span> y)</span>
<span id="cb110-1851"><a href="#cb110-1851" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb110-1852"><a href="#cb110-1852" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-1853"><a href="#cb110-1853" aria-hidden="true" tabindex="-1"></a>Use <span class="in">`rpart`</span> to fit a regression tree and save the result to <span class="in">`fit`</span>.</span>
<span id="cb110-1854"><a href="#cb110-1854" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-1855"><a href="#cb110-1855" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>Plot the final tree so that you can see where the partitions occurred.</span>
<span id="cb110-1856"><a href="#cb110-1856" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-1857"><a href="#cb110-1857" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>Make a scatterplot of <span class="in">`y`</span> versus <span class="in">`x`</span> along with the predicted values based on the fit.</span>
<span id="cb110-1858"><a href="#cb110-1858" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-1859"><a href="#cb110-1859" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>Now model with a random forest instead of a regression tree using <span class="in">`randomForest`</span> from the __randomForest__ package, and remake the scatterplot with the prediction line.</span>
<span id="cb110-1860"><a href="#cb110-1860" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-1861"><a href="#cb110-1861" aria-hidden="true" tabindex="-1"></a><span class="ss">5. </span>Use the function <span class="in">`plot`</span> to see if the random forest has converged or if we need more trees.</span>
<span id="cb110-1862"><a href="#cb110-1862" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-1863"><a href="#cb110-1863" aria-hidden="true" tabindex="-1"></a><span class="ss">6. </span>It seems that the default values for the random forest result in an estimate that is too flexible (not smooth). Re-run the random forest but this time with <span class="in">`nodesize`</span> set at 50 and <span class="in">`maxnodes`</span> set at 25. Remake the plot.</span>
<span id="cb110-1864"><a href="#cb110-1864" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-1865"><a href="#cb110-1865" aria-hidden="true" tabindex="-1"></a><span class="ss">7. </span>We see that this yields smoother results. Let's use the <span class="in">`train`</span> function to help us pick these values. From the __caret__ manual we see that we can't tune the `maxnodes` parameter or the `nodesize` argument with `randomForest`, so we will use the __Rborist__ package and tune the <span class="in">`minNode`</span> argument. Use the <span class="in">`train`</span> function to try values <span class="in">`minNode &lt;- seq(5, 250, 25)`</span>. See which value minimizes the estimated RMSE.</span>
<span id="cb110-1866"><a href="#cb110-1866" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-1867"><a href="#cb110-1867" aria-hidden="true" tabindex="-1"></a><span class="ss">8. </span>Make a scatterplot along with the prediction from the best fitted model.</span>
<span id="cb110-1868"><a href="#cb110-1868" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb110-1869"><a href="#cb110-1869" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-1870"><a href="#cb110-1870" aria-hidden="true" tabindex="-1"></a>::: {.callout-note}</span>
<span id="cb110-1871"><a href="#cb110-1871" aria-hidden="true" tabindex="-1"></a><span class="fu">### Exercise SL.2: Classification Trees</span></span>
<span id="cb110-1872"><a href="#cb110-1872" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-1873"><a href="#cb110-1873" aria-hidden="true" tabindex="-1"></a><span class="ss">9. </span>Use the <span class="in">`rpart`</span> function to fit a classification tree to the <span class="in">`tissue_gene_expression`</span> dataset. Use the <span class="in">`train`</span> function to estimate the accuracy. Try out <span class="in">`cp`</span> values of <span class="in">`seq(0, 0.05, 0.01)`</span>. Plot the accuracy to report the results of the best model.</span>
<span id="cb110-1874"><a href="#cb110-1874" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-1877"><a href="#cb110-1877" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb110-1878"><a href="#cb110-1878" aria-hidden="true" tabindex="-1"></a><span class="co">#| eval: false</span></span>
<span id="cb110-1879"><a href="#cb110-1879" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(dslabs)</span>
<span id="cb110-1880"><a href="#cb110-1880" aria-hidden="true" tabindex="-1"></a><span class="fu">data</span>(<span class="st">"tissue_gene_expression"</span>)</span>
<span id="cb110-1881"><a href="#cb110-1881" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb110-1882"><a href="#cb110-1882" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-1883"><a href="#cb110-1883" aria-hidden="true" tabindex="-1"></a><span class="ss">10. </span>Study the confusion matrix for the best fitting classification tree. What do you observe happening for placenta?</span>
<span id="cb110-1884"><a href="#cb110-1884" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-1885"><a href="#cb110-1885" aria-hidden="true" tabindex="-1"></a><span class="ss">11. </span>Notice that placentas are called endometrium more often than placenta. Note also that the number of placentas is just six, and that, by default, <span class="in">`rpart`</span> requires 20 observations before splitting a node. Thus it is not possible with these parameters to have a node in which placentas are the majority. Rerun the above analysis but this time permit <span class="in">`rpart`</span> to split any node by using the argument <span class="in">`control = rpart.control(minsplit = 0)`</span>. Does the accuracy increase? Look at the confusion matrix again.</span>
<span id="cb110-1886"><a href="#cb110-1886" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-1887"><a href="#cb110-1887" aria-hidden="true" tabindex="-1"></a><span class="ss">12. </span>Plot the tree from the best fitting model obtained in exercise 11.</span>
<span id="cb110-1888"><a href="#cb110-1888" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-1889"><a href="#cb110-1889" aria-hidden="true" tabindex="-1"></a><span class="ss">13. </span>We can see that with just six genes, we are able to predict the tissue type. Now let's see if we can do even better with a random forest. Use the <span class="in">`train`</span> function and the <span class="in">`rf`</span> method to train a random forest. Try out values of <span class="in">`mtry`</span> ranging from, at least, <span class="in">`seq(50, 200, 25)`</span>. What <span class="in">`mtry`</span> value maximizes accuracy? To permit small <span class="in">`nodesize`</span> to grow as we did with the classification trees, use the following argument: <span class="in">`nodesize = 1`</span>. This will take several seconds to run. If you want to test it out, try using smaller values with <span class="in">`ntree`</span>. Set the seed to 1990.</span>
<span id="cb110-1890"><a href="#cb110-1890" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-1891"><a href="#cb110-1891" aria-hidden="true" tabindex="-1"></a><span class="ss">14. </span>Use the function <span class="in">`varImp`</span> on the output of <span class="in">`train`</span> and save it to an object called <span class="in">`imp`</span>.</span>
<span id="cb110-1892"><a href="#cb110-1892" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-1893"><a href="#cb110-1893" aria-hidden="true" tabindex="-1"></a><span class="ss">15. </span>The <span class="in">`rpart`</span> model we ran above produced a tree that used just six predictors. Extracting the predictor names is not straightforward, but can be done. If the output of the call to train was <span class="in">`fit_rpart`</span>, we can extract the names like this:</span>
<span id="cb110-1894"><a href="#cb110-1894" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-1897"><a href="#cb110-1897" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb110-1898"><a href="#cb110-1898" aria-hidden="true" tabindex="-1"></a><span class="co">#| eval: false</span></span>
<span id="cb110-1899"><a href="#cb110-1899" aria-hidden="true" tabindex="-1"></a>ind <span class="ot">&lt;-</span> <span class="sc">!</span>(fit_rpart<span class="sc">$</span>finalModel<span class="sc">$</span>frame<span class="sc">$</span>var <span class="sc">==</span> <span class="st">"&lt;leaf&gt;"</span>)</span>
<span id="cb110-1900"><a href="#cb110-1900" aria-hidden="true" tabindex="-1"></a>tree_terms <span class="ot">&lt;-</span></span>
<span id="cb110-1901"><a href="#cb110-1901" aria-hidden="true" tabindex="-1"></a>  fit_rpart<span class="sc">$</span>finalModel<span class="sc">$</span>frame<span class="sc">$</span>var[ind] <span class="sc">%&gt;%</span></span>
<span id="cb110-1902"><a href="#cb110-1902" aria-hidden="true" tabindex="-1"></a>  <span class="fu">unique</span>() <span class="sc">%&gt;%</span></span>
<span id="cb110-1903"><a href="#cb110-1903" aria-hidden="true" tabindex="-1"></a>  <span class="fu">as.character</span>()</span>
<span id="cb110-1904"><a href="#cb110-1904" aria-hidden="true" tabindex="-1"></a>tree_terms</span>
<span id="cb110-1905"><a href="#cb110-1905" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb110-1906"><a href="#cb110-1906" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-1907"><a href="#cb110-1907" aria-hidden="true" tabindex="-1"></a>What is the variable importance in the random forest call for these predictors? Where do they rank?</span>
<span id="cb110-1908"><a href="#cb110-1908" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-1909"><a href="#cb110-1909" aria-hidden="true" tabindex="-1"></a><span class="ss">16. </span>Advanced: Extract the top 50 predictors based on importance, take a subset of <span class="in">`x`</span> with just these predictors and apply the function <span class="in">`heatmap`</span> to see how these genes behave across the tissues. We will introduce the <span class="in">`heatmap`</span> function in @sec-clustering.</span>
<span id="cb110-1910"><a href="#cb110-1910" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb110-1911"><a href="#cb110-1911" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-1912"><a href="#cb110-1912" aria-hidden="true" tabindex="-1"></a><span class="fu">## Summary</span></span>
<span id="cb110-1913"><a href="#cb110-1913" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-1914"><a href="#cb110-1914" aria-hidden="true" tabindex="-1"></a>Statistical learning provides powerful tools for prediction and pattern discovery:</span>
<span id="cb110-1915"><a href="#cb110-1915" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-1916"><a href="#cb110-1916" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Overfitting** is the central challenge—models that fit training data too well predict poorly</span>
<span id="cb110-1917"><a href="#cb110-1917" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Loss functions** quantify prediction error (squared loss for regression, log loss for classification)</span>
<span id="cb110-1918"><a href="#cb110-1918" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Cross-validation** provides honest estimates of predictive performance</span>
<span id="cb110-1919"><a href="#cb110-1919" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>Training error is always optimistic; test error reveals true performance</span>
<span id="cb110-1920"><a href="#cb110-1920" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>K-fold CV and bootstrap estimate generalization error</span>
<span id="cb110-1921"><a href="#cb110-1921" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>The **bias-variance tradeoff** governs model complexity choices</span>
<span id="cb110-1922"><a href="#cb110-1922" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Regularization** (ridge, lasso, elastic net) controls overfitting by penalizing model complexity</span>
<span id="cb110-1923"><a href="#cb110-1923" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>Ridge shrinks coefficients but keeps all predictors</span>
<span id="cb110-1924"><a href="#cb110-1924" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>Lasso performs variable selection by shrinking some coefficients to zero</span>
<span id="cb110-1925"><a href="#cb110-1925" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>Cross-validation selects the optimal regularization strength</span>
<span id="cb110-1926"><a href="#cb110-1926" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Smoothing methods** estimate flexible curves from data</span>
<span id="cb110-1927"><a href="#cb110-1927" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>Bin smoothing divides data into intervals</span>
<span id="cb110-1928"><a href="#cb110-1928" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>Kernel smoothing uses weighted averages for continuous estimates</span>
<span id="cb110-1929"><a href="#cb110-1929" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>Splines fit piecewise polynomials with controlled smoothness</span>
<span id="cb110-1930"><a href="#cb110-1930" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>LOESS fits local regressions weighted by distance</span>
<span id="cb110-1931"><a href="#cb110-1931" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**K-nearest neighbors** illustrates how hyperparameters control model complexity</span>
<span id="cb110-1932"><a href="#cb110-1932" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Decision trees (CART)** recursively partition data using simple rules</span>
<span id="cb110-1933"><a href="#cb110-1933" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>Highly interpretable but prone to overfitting</span>
<span id="cb110-1934"><a href="#cb110-1934" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>Controlled via complexity parameters and pruning</span>
<span id="cb110-1935"><a href="#cb110-1935" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Random forests** combine many trees for robust predictions</span>
<span id="cb110-1936"><a href="#cb110-1936" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>Bagging and random feature selection reduce variance</span>
<span id="cb110-1937"><a href="#cb110-1937" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>Out-of-bag error provides built-in validation</span>
<span id="cb110-1938"><a href="#cb110-1938" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>Variable importance measures identify key predictors</span>
<span id="cb110-1939"><a href="#cb110-1939" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Support vector machines** find maximum-margin decision boundaries</span>
<span id="cb110-1940"><a href="#cb110-1940" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>Kernel trick enables non-linear classification</span>
<span id="cb110-1941"><a href="#cb110-1941" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>Support vector regression (SVR) extends to continuous outcomes</span>
<span id="cb110-1942"><a href="#cb110-1942" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>Effective in high-dimensional spaces</span>
<span id="cb110-1943"><a href="#cb110-1943" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Confusion matrices** summarize classification performance with metrics like accuracy, sensitivity, and precision</span>
<span id="cb110-1944"><a href="#cb110-1944" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**F1 score and balanced accuracy** are better metrics for imbalanced data</span>
<span id="cb110-1945"><a href="#cb110-1945" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**ROC curves and AUC** allow comparison of classifiers across all thresholds</span>
<span id="cb110-1946"><a href="#cb110-1946" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Precision-recall curves** are preferred for highly imbalanced problems</span>
<span id="cb110-1947"><a href="#cb110-1947" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>The choice between traditional statistics and machine learning depends on goals</span>
<span id="cb110-1948"><a href="#cb110-1948" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-1949"><a href="#cb110-1949" aria-hidden="true" tabindex="-1"></a><span class="fu">## Additional Resources</span></span>
<span id="cb110-1950"><a href="#cb110-1950" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-1951"><a href="#cb110-1951" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>@james2023islr - The standard introduction to statistical learning</span>
<span id="cb110-1952"><a href="#cb110-1952" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>@thulin2025msr - Modern perspectives on statistics with R</span>
<span id="cb110-1953"><a href="#cb110-1953" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>@crawley2007r - Practical statistical methods in R</span>
</code><button title="Copy to Clipboard" class="code-copy-button" data-in-quarto-modal=""><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p>Statistics for Biosciences and Bioengineering</p>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    </div>
    <div class="nav-footer-right">
<p>Built with <a href="https://quarto.org/">Quarto</a></p>
</div>
  </div>
</footer>




</body></html>