<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.30">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>34&nbsp; Decision Trees and Random Forests – Statistics for the Biosciences and Bioengineering</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../chapters/34-svm.html" rel="next">
<link href="../chapters/32-classification-methods.html" rel="prev">
<script src="../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-de070a7b0ab54f8780927367ac907214.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap-485d01fc63b59abcd3ee1bf1e8e2748d.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar floating quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../chapters/28-intro-statistical-learning.html">Statistical Learning</a></li><li class="breadcrumb-item"><a href="../chapters/33-trees-forests.html"><span class="chapter-number">34</span>&nbsp; <span class="chapter-title">Decision Trees and Random Forests</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../">Statistics for the Biosciences and Bioengineering</a> 
        <div class="sidebar-tools-main">
  <a href="" class="quarto-reader-toggle quarto-navigation-tool px-1" onclick="window.quartoToggleReader(); return false;" title="Toggle reader mode">
  <div class="quarto-reader-toggle-btn">
  <i class="bi"></i>
  </div>
</a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Why This Book?</span></span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">Core Data Science Tools</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/01-introduction.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Introduction</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/02-installing-tools.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Installing Core Tools</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/03-unix-command-line.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Unix and the Command Line</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/04-r-rstudio.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">R and RStudio</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/05-markdown-latex.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Markdown and LaTeX</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true">
 <span class="menu-text">Data Exploration</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/06-tidy-data.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Tidy Data</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/07-data-wrangling.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Data Wrangling with dplyr</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/08-exploratory-data-analysis.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Exploratory Data Analysis</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/09-data-visualization.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Data Visualization</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true">
 <span class="menu-text">Probability and Distributions</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/10-probability-foundations.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Foundations of Probability</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/11-discrete-distributions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Discrete Probability Distributions</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/12-continuous-distributions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Continuous Probability Distributions</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/13-sampling-estimation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Sampling and Parameter Estimation</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true">
 <span class="menu-text">Statistical Inference</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/14-experimental-design.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Experimental Design Principles</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/15-hypothesis-testing.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Hypothesis Testing</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/16-t-tests.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">T-Tests</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/17-nonparametric-tests.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Nonparametric Tests</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/18-bootstrapping.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">Resampling Methods</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="true">
 <span class="menu-text">Linear Models</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/19-what-are-models.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">What are Models?</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/20-correlation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">21</span>&nbsp; <span class="chapter-title">Correlation</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/21-simple-linear-regression.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">22</span>&nbsp; <span class="chapter-title">Simple Linear Regression</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/22-residual-analysis.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">23</span>&nbsp; <span class="chapter-title">Residual Analysis</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/23-statistical-power.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">24</span>&nbsp; <span class="chapter-title">Statistical Power</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/24-multiple-regression.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">25</span>&nbsp; <span class="chapter-title">Multiple Regression</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/25-single-factor-anova.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">26</span>&nbsp; <span class="chapter-title">Single Factor ANOVA</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/26-multifactor-anova.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">27</span>&nbsp; <span class="chapter-title">Multi-Factor ANOVA</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/27-glm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">28</span>&nbsp; <span class="chapter-title">Generalized Linear Models</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="true">
 <span class="menu-text">Statistical Learning</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-6" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/28-intro-statistical-learning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">29</span>&nbsp; <span class="chapter-title">Introduction to Statistical Learning</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/29-model-validation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">30</span>&nbsp; <span class="chapter-title">Model Validation and the Bias-Variance Tradeoff</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/30-regularization.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">31</span>&nbsp; <span class="chapter-title">Regularization Methods</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/31-smoothing.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">32</span>&nbsp; <span class="chapter-title">Smoothing and Non-Parametric Regression</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/32-classification-methods.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">33</span>&nbsp; <span class="chapter-title">Classification and Performance Metrics</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/33-trees-forests.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">34</span>&nbsp; <span class="chapter-title">Decision Trees and Random Forests</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/34-svm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">35</span>&nbsp; <span class="chapter-title">Support Vector Machines</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/35-clustering.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">36</span>&nbsp; <span class="chapter-title">Clustering</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/36-dimensionality-reduction.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">37</span>&nbsp; <span class="chapter-title">Dimensionality Reduction and Multivariate Methods</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/37-tsne-umap.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">38</span>&nbsp; <span class="chapter-title">Non-Linear Dimensionality Reduction: t-SNE and UMAP</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/38-bayesian-statistics.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">39</span>&nbsp; <span class="chapter-title">Bayesian Statistics</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/39-deep-learning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">40</span>&nbsp; <span class="chapter-title">Introduction to Deep Learning</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/40-high-performance-computing.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">41</span>&nbsp; <span class="chapter-title">High Performance Computing</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="true">
 <span class="menu-text">Scientific Communication</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-7" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/41-presenting-results.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">42</span>&nbsp; <span class="chapter-title">Presenting Statistical Results</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-8" role="navigation" aria-expanded="true">
 <span class="menu-text">Historical Context</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-8" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-8" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/A1-eugenics-history.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">43</span>&nbsp; <span class="chapter-title">The Eugenics History of Statistics</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-9" role="navigation" aria-expanded="true">
 <span class="menu-text">Appendices</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-9" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-9" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/A2-keyboard-shortcuts.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">44</span>&nbsp; <span class="chapter-title">Keyboard Shortcuts Reference</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/A3-unix-reference.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">45</span>&nbsp; <span class="chapter-title">Unix Command Reference</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/A4-r-reference.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">46</span>&nbsp; <span class="chapter-title">R Command Reference</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/A5-quarto-reference.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">47</span>&nbsp; <span class="chapter-title">Quarto Markdown Reference</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/A6-latex-reference.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">48</span>&nbsp; <span class="chapter-title">LaTeX Command Reference</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/A7-greek-letters.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">49</span>&nbsp; <span class="chapter-title">Greek Letters in Mathematics and Statistics</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/A8-probability-distributions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">50</span>&nbsp; <span class="chapter-title">Common Probability Distributions</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/A9-sampling-distributions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">51</span>&nbsp; <span class="chapter-title">Sampling Distributions in Hypothesis Testing</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/A10-matrix-algebra.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">52</span>&nbsp; <span class="chapter-title">Matrix Algebra Fundamentals</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#introduction-to-tree-based-methods" id="toc-introduction-to-tree-based-methods" class="nav-link active" data-scroll-target="#introduction-to-tree-based-methods"><span class="header-section-number">34.1</span> Introduction to Tree-Based Methods</a></li>
  <li><a href="#decision-trees-cart" id="toc-decision-trees-cart" class="nav-link" data-scroll-target="#decision-trees-cart"><span class="header-section-number">34.2</span> Decision Trees (CART)</a>
  <ul class="collapse">
  <li><a href="#motivating-example-olive-oil-classification" id="toc-motivating-example-olive-oil-classification" class="nav-link" data-scroll-target="#motivating-example-olive-oil-classification">Motivating Example: Olive Oil Classification</a></li>
  <li><a href="#how-trees-work" id="toc-how-trees-work" class="nav-link" data-scroll-target="#how-trees-work">How Trees Work</a></li>
  <li><a href="#building-a-classification-tree" id="toc-building-a-classification-tree" class="nav-link" data-scroll-target="#building-a-classification-tree">Building a Classification Tree</a></li>
  <li><a href="#interpreting-tree-output" id="toc-interpreting-tree-output" class="nav-link" data-scroll-target="#interpreting-tree-output">Interpreting Tree Output</a></li>
  <li><a href="#regression-trees" id="toc-regression-trees" class="nav-link" data-scroll-target="#regression-trees">Regression Trees</a></li>
  <li><a href="#decision-boundaries" id="toc-decision-boundaries" class="nav-link" data-scroll-target="#decision-boundaries">Decision Boundaries</a></li>
  <li><a href="#controlling-tree-complexity" id="toc-controlling-tree-complexity" class="nav-link" data-scroll-target="#controlling-tree-complexity">Controlling Tree Complexity</a></li>
  <li><a href="#overfitting-example" id="toc-overfitting-example" class="nav-link" data-scroll-target="#overfitting-example">Overfitting Example</a></li>
  <li><a href="#pruning-with-cross-validation" id="toc-pruning-with-cross-validation" class="nav-link" data-scroll-target="#pruning-with-cross-validation">Pruning with Cross-Validation</a></li>
  </ul></li>
  <li><a href="#random-forests" id="toc-random-forests" class="nav-link" data-scroll-target="#random-forests"><span class="header-section-number">34.3</span> Random Forests</a>
  <ul class="collapse">
  <li><a href="#the-random-forest-algorithm" id="toc-the-random-forest-algorithm" class="nav-link" data-scroll-target="#the-random-forest-algorithm">The Random Forest Algorithm</a></li>
  <li><a href="#random-forests-in-r" id="toc-random-forests-in-r" class="nav-link" data-scroll-target="#random-forests-in-r">Random Forests in R</a></li>
  <li><a href="#out-of-bag-oob-error" id="toc-out-of-bag-oob-error" class="nav-link" data-scroll-target="#out-of-bag-oob-error">Out-of-Bag (OOB) Error</a></li>
  <li><a href="#variable-importance" id="toc-variable-importance" class="nav-link" data-scroll-target="#variable-importance">Variable Importance</a></li>
  <li><a href="#tuning-random-forests" id="toc-tuning-random-forests" class="nav-link" data-scroll-target="#tuning-random-forests">Tuning Random Forests</a></li>
  <li><a href="#random-forest-for-regression" id="toc-random-forest-for-regression" class="nav-link" data-scroll-target="#random-forest-for-regression">Random Forest for Regression</a></li>
  </ul></li>
  <li><a href="#comparing-trees-and-forests" id="toc-comparing-trees-and-forests" class="nav-link" data-scroll-target="#comparing-trees-and-forests"><span class="header-section-number">34.4</span> Comparing Trees and Forests</a>
  <ul class="collapse">
  <li><a href="#when-to-use-each-method" id="toc-when-to-use-each-method" class="nav-link" data-scroll-target="#when-to-use-each-method">When to Use Each Method</a></li>
  </ul></li>
  <li><a href="#exercises" id="toc-exercises" class="nav-link" data-scroll-target="#exercises"><span class="header-section-number">34.5</span> Exercises</a></li>
  <li><a href="#summary" id="toc-summary" class="nav-link" data-scroll-target="#summary"><span class="header-section-number">34.6</span> Summary</a></li>
  <li><a href="#additional-resources" id="toc-additional-resources" class="nav-link" data-scroll-target="#additional-resources"><span class="header-section-number">34.7</span> Additional Resources</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../chapters/28-intro-statistical-learning.html">Statistical Learning</a></li><li class="breadcrumb-item"><a href="../chapters/33-trees-forests.html"><span class="chapter-number">34</span>&nbsp; <span class="chapter-title">Decision Trees and Random Forests</span></a></li></ol></nav>
<div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title"><span id="sec-trees-forests" class="quarto-section-identifier"><span class="chapter-number">34</span>&nbsp; <span class="chapter-title">Decision Trees and Random Forests</span></span></h1><button type="button" class="btn code-tools-button dropdown-toggle" id="quarto-code-tools-menu" data-bs-toggle="dropdown" aria-expanded="false"><i class="bi"></i> Code</button><ul class="dropdown-menu dropdown-menu-end" aria-labelelledby="quarto-code-tools-menu"><li><a id="quarto-show-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Show All Code</a></li><li><a id="quarto-hide-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Hide All Code</a></li><li><hr class="dropdown-divider"></li><li><a id="quarto-view-source" class="dropdown-item" href="javascript:void(0)" role="button">View Source</a></li></ul></div></div>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<section id="introduction-to-tree-based-methods" class="level2" data-number="34.1">
<h2 data-number="34.1" class="anchored" data-anchor-id="introduction-to-tree-based-methods"><span class="header-section-number">34.1</span> Introduction to Tree-Based Methods</h2>
<p><strong>Decision trees</strong> are a fundamentally different approach to prediction. Instead of fitting a smooth function or finding nearest neighbors, trees partition the feature space into rectangular regions and make a simple prediction within each region.</p>
<p>Trees are popular for several compelling reasons. They are highly interpretable—you can draw a tree as a flow chart and explain it to non-statisticians who may never have taken a statistics course. They handle both numeric and categorical predictors naturally, without requiring dummy variables or other transformations. They capture non-linear relationships and interactions automatically through their recursive splitting structure. And they are robust to outliers and don’t require feature scaling, since only the rank order of values matters for splitting.</p>
<p>However, individual trees have high variance and are prone to overfitting. <strong>Random forests</strong> address this limitation by averaging many trees together, dramatically improving prediction accuracy at the cost of some interpretability.</p>
<p><a href="#fig-decision-tree-example" class="quarto-xref">Figure&nbsp;<span>34.1</span></a> shows a bioengineering application of decision trees: predicting musculoskeletal injury risk based on physiological factors. The tree structure makes the decision process transparent—you can trace the path from root to leaf and understand exactly why a particular prediction was made. Each internal node asks a yes/no question (Is BMI ≥ 30? Is exercise frequency ≥ 3x/week?), and each leaf node provides a prediction with an associated probability.</p>
<div id="fig-decision-tree-example" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-decision-tree-example-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../images/ch33/ch33_decision_tree_example.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:90.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-decision-tree-example-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;34.1: A decision tree for predicting musculoskeletal injury risk using bioengineering variables. The tree partitions patients based on BMI, exercise frequency, and age, assigning each leaf node an injury risk probability. This interpretability—being able to trace exactly how a prediction is made—is a key advantage of tree-based methods.
</figcaption>
</figure>
</div>
</section>
<section id="decision-trees-cart" class="level2" data-number="34.2">
<h2 data-number="34.2" class="anchored" data-anchor-id="decision-trees-cart"><span class="header-section-number">34.2</span> Decision Trees (CART)</h2>
<p><strong>Classification and Regression Trees (CART)</strong> make predictions by recursively partitioning the feature space. At each node, the algorithm asks a yes/no question about a single feature, splitting observations into two groups.</p>
<section id="motivating-example-olive-oil-classification" class="level3">
<h3 class="anchored" data-anchor-id="motivating-example-olive-oil-classification">Motivating Example: Olive Oil Classification</h3>
<p>Consider a dataset of olive oil samples from three regions of Italy, with measurements of 8 fatty acids:</p>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="fu">data</span>(<span class="st">"olive"</span>)</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="fu">names</span>(olive)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code> [1] "region"      "area"        "palmitic"    "palmitoleic" "stearic"    
 [6] "oleic"       "linoleic"    "linolenic"   "arachidic"   "eicosenoic" </code></pre>
</div>
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="fu">table</span>(olive<span class="sc">$</span>region)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>
Northern Italy       Sardinia Southern Italy 
           151             98            323 </code></pre>
</div>
</div>
<p>We want to predict the region of origin from fatty acid composition. Let’s examine how some predictors separate regions:</p>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>olive <span class="ot">&lt;-</span> <span class="fu">select</span>(olive, <span class="sc">-</span>area)</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">2</span>))</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a><span class="fu">boxplot</span>(eicosenoic <span class="sc">~</span> region, <span class="at">data =</span> olive, <span class="at">col =</span> <span class="fu">c</span>(<span class="st">"coral"</span>, <span class="st">"lightgreen"</span>, <span class="st">"lightblue"</span>),</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>        <span class="at">main =</span> <span class="st">"Eicosenoic by Region"</span>)</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a><span class="fu">boxplot</span>(linoleic <span class="sc">~</span> region, <span class="at">data =</span> olive, <span class="at">col =</span> <span class="fu">c</span>(<span class="st">"coral"</span>, <span class="st">"lightgreen"</span>, <span class="st">"lightblue"</span>),</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>        <span class="at">main =</span> <span class="st">"Linoleic by Region"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div id="fig-olive-eda" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-olive-eda-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="33-trees-forests_files/figure-html/fig-olive-eda-1.png" class="img-fluid figure-img" width="864">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-olive-eda-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;34.2: Distribution of two fatty acids by region. These predictors clearly separate the regions.
</figcaption>
</figure>
</div>
</div>
</div>
<p>Notice that eicosenoic is only present in Southern Italy, and linoleic separates Northern Italy from Sardinia. We can visualize this separation:</p>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>olive <span class="sc">%&gt;%</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(eicosenoic, linoleic, <span class="at">color =</span> region)) <span class="sc">+</span></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>() <span class="sc">+</span></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_vline</span>(<span class="at">xintercept =</span> <span class="fl">0.065</span>, <span class="at">lty =</span> <span class="dv">2</span>) <span class="sc">+</span></span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_segment</span>(<span class="at">x =</span> <span class="sc">-</span><span class="fl">0.2</span>, <span class="at">y =</span> <span class="fl">10.54</span>, <span class="at">xend =</span> <span class="fl">0.065</span>, <span class="at">yend =</span> <span class="fl">10.54</span>,</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>               <span class="at">color =</span> <span class="st">"black"</span>, <span class="at">lty =</span> <span class="dv">2</span>) <span class="sc">+</span></span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">"Perfect Classification with Simple Rules"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div id="fig-olive-two-predictors" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-olive-two-predictors-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="33-trees-forests_files/figure-html/fig-olive-two-predictors-1.png" class="img-fluid figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-olive-two-predictors-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;34.3: With just two predictors, simple rules can perfectly separate the regions
</figcaption>
</figure>
</div>
</div>
</div>
<p>By eye, we can construct a simple prediction rule: if eicosenoic is greater than 0.065, predict Southern Italy. If not, check linoleic—if it’s greater than 10.535, predict Sardinia; otherwise, predict Northern Italy.</p>
<p>This is exactly what a decision tree learns from data—a flow chart of yes/no questions that partition the feature space into regions, each assigned to a single class.</p>
</section>
<section id="how-trees-work" class="level3">
<h3 class="anchored" data-anchor-id="how-trees-work">How Trees Work</h3>
<p>Trees create partitions recursively. We start with one partition (the entire dataset). After the first split we have two partitions, then four, and so on.</p>
<p>For each split, we find a predictor <span class="math inline">\(j\)</span> and value <span class="math inline">\(s\)</span> that define two new partitions:</p>
<p><span class="math display">\[R_1(j,s) = \{x : x_j &lt; s\} \quad \text{and} \quad R_2(j,s) = \{x : x_j \geq s\}\]</span></p>
<p><strong>For regression trees</strong>, we choose the split that minimizes the <strong>residual sum of squares (RSS)</strong>:</p>
<p><span class="math display">\[\sum_{i: x_i \in R_1} (y_i - \hat{y}_{R_1})^2 + \sum_{i: x_i \in R_2} (y_i - \hat{y}_{R_2})^2\]</span></p>
<p><strong>For classification trees</strong>, we use <strong>Gini impurity</strong>:</p>
<p><span class="math display">\[\text{Gini}(j) = \sum_{k=1}^K \hat{p}_{j,k}(1-\hat{p}_{j,k})\]</span></p>
<p>where <span class="math inline">\(\hat{p}_{j,k}\)</span> is the proportion of observations in partition <span class="math inline">\(j\)</span> that are of class <span class="math inline">\(k\)</span>. A pure node (all one class) has Gini = 0.</p>
</section>
<section id="building-a-classification-tree" class="level3">
<h3 class="anchored" data-anchor-id="building-a-classification-tree">Building a Classification Tree</h3>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Build a classification tree</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="fu">data</span>(iris)</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>tree_class <span class="ot">&lt;-</span> <span class="fu">rpart</span>(Species <span class="sc">~</span> ., <span class="at">data =</span> iris, <span class="at">method =</span> <span class="st">"class"</span>)</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualize with rpart.plot</span></span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a><span class="fu">rpart.plot</span>(tree_class, <span class="at">extra =</span> <span class="dv">104</span>, <span class="at">box.palette =</span> <span class="st">"RdYlGn"</span>,</span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>           <span class="at">main =</span> <span class="st">"Classification Tree for Iris Species"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div id="fig-cart-classification" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-cart-classification-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="33-trees-forests_files/figure-html/fig-cart-classification-1.png" class="img-fluid figure-img" width="768">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-cart-classification-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;34.4: A CART decision tree for classifying iris species. Each node shows the predicted class and the splitting rule.
</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="interpreting-tree-output" class="level3">
<h3 class="anchored" data-anchor-id="interpreting-tree-output">Interpreting Tree Output</h3>
<p>The tree visualization conveys several pieces of information at each node. The <strong>node prediction</strong> shows the class that would be assigned to observations reaching that node—this is simply the most common class in that region of the feature space. The <strong>split rule</strong> shows which feature and threshold value are used to partition observations into the left and right child nodes. The <strong>proportions</strong> display the distribution of classes among observations at that node, helping you see how “pure” each region is. Finally, the <strong>sample size</strong> indicates what percentage of the training data reaches each node, showing which branches contain most of the data.</p>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Detailed tree summary</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(tree_class, <span class="at">cp =</span> <span class="fl">0.1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Call:
rpart(formula = Species ~ ., data = iris, method = "class")
  n= 150 

    CP nsplit rel error xerror       xstd
1 0.50      0      1.00   1.20 0.04898979
2 0.44      1      0.50   0.69 0.06104097
3 0.01      2      0.06   0.09 0.02908608

Variable importance
 Petal.Width Petal.Length Sepal.Length  Sepal.Width 
          34           31           21           14 

Node number 1: 150 observations,    complexity param=0.5
  predicted class=setosa      expected loss=0.6666667  P(node) =1
    class counts:    50    50    50
   probabilities: 0.333 0.333 0.333 
  left son=2 (50 obs) right son=3 (100 obs)
  Primary splits:
      Petal.Length &lt; 2.45 to the left,  improve=50.00000, (0 missing)
      Petal.Width  &lt; 0.8  to the left,  improve=50.00000, (0 missing)
      Sepal.Length &lt; 5.45 to the left,  improve=34.16405, (0 missing)
      Sepal.Width  &lt; 3.35 to the right, improve=19.03851, (0 missing)
  Surrogate splits:
      Petal.Width  &lt; 0.8  to the left,  agree=1.000, adj=1.00, (0 split)
      Sepal.Length &lt; 5.45 to the left,  agree=0.920, adj=0.76, (0 split)
      Sepal.Width  &lt; 3.35 to the right, agree=0.833, adj=0.50, (0 split)

Node number 2: 50 observations
  predicted class=setosa      expected loss=0  P(node) =0.3333333
    class counts:    50     0     0
   probabilities: 1.000 0.000 0.000 

Node number 3: 100 observations,    complexity param=0.44
  predicted class=versicolor  expected loss=0.5  P(node) =0.6666667
    class counts:     0    50    50
   probabilities: 0.000 0.500 0.500 
  left son=6 (54 obs) right son=7 (46 obs)
  Primary splits:
      Petal.Width  &lt; 1.75 to the left,  improve=38.969400, (0 missing)
      Petal.Length &lt; 4.75 to the left,  improve=37.353540, (0 missing)
      Sepal.Length &lt; 6.15 to the left,  improve=10.686870, (0 missing)
      Sepal.Width  &lt; 2.45 to the left,  improve= 3.555556, (0 missing)
  Surrogate splits:
      Petal.Length &lt; 4.75 to the left,  agree=0.91, adj=0.804, (0 split)
      Sepal.Length &lt; 6.15 to the left,  agree=0.73, adj=0.413, (0 split)
      Sepal.Width  &lt; 2.95 to the left,  agree=0.67, adj=0.283, (0 split)

Node number 6: 54 observations
  predicted class=versicolor  expected loss=0.09259259  P(node) =0.36
    class counts:     0    49     5
   probabilities: 0.000 0.907 0.093 

Node number 7: 46 observations
  predicted class=virginica   expected loss=0.02173913  P(node) =0.3066667
    class counts:     0     1    45
   probabilities: 0.000 0.022 0.978 </code></pre>
</div>
</div>
</section>
<section id="regression-trees" class="level3">
<h3 class="anchored" data-anchor-id="regression-trees">Regression Trees</h3>
<p>When the outcome is continuous, we call the method a <strong>regression tree</strong>:</p>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="fu">data</span>(<span class="st">"polls_2008"</span>)</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>fit <span class="ot">&lt;-</span> <span class="fu">rpart</span>(margin <span class="sc">~</span> ., <span class="at">data =</span> polls_2008)</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>rafalib<span class="sc">::</span><span class="fu">mypar</span>()</span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(fit, <span class="at">margin =</span> <span class="fl">0.1</span>)</span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a><span class="fu">text</span>(fit, <span class="at">cex =</span> <span class="fl">0.75</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div id="fig-polls-tree" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-polls-tree-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="33-trees-forests_files/figure-html/fig-polls-tree-1.png" class="img-fluid figure-img" width="768">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-polls-tree-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;34.5: Regression tree for poll data. The tree partitions time into regions with constant predictions.
</figcaption>
</figure>
</div>
</div>
</div>
<p>The tree creates a step function:</p>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>polls_2008 <span class="sc">%&gt;%</span></span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">y_hat =</span> <span class="fu">predict</span>(fit)) <span class="sc">%&gt;%</span></span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>() <span class="sc">+</span></span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="fu">aes</span>(day, margin)) <span class="sc">+</span></span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_step</span>(<span class="fu">aes</span>(day, y_hat), <span class="at">col =</span> <span class="st">"red"</span>, <span class="at">linewidth =</span> <span class="dv">1</span>) <span class="sc">+</span></span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">"Regression Tree Fit"</span>, <span class="at">x =</span> <span class="st">"Day"</span>, <span class="at">y =</span> <span class="st">"Poll margin"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div id="fig-polls-tree-fit" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-polls-tree-fit-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="33-trees-forests_files/figure-html/fig-polls-tree-fit-1.png" class="img-fluid figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-polls-tree-fit-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;34.6: Regression tree predictions form a step function
</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="decision-boundaries" class="level3">
<h3 class="anchored" data-anchor-id="decision-boundaries">Decision Boundaries</h3>
<p>Trees partition the feature space into rectangular regions:</p>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualize decision boundary for 2D case</span></span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>tree_2d <span class="ot">&lt;-</span> <span class="fu">rpart</span>(Species <span class="sc">~</span> Petal.Length <span class="sc">+</span> Petal.Width, <span class="at">data =</span> iris, <span class="at">method =</span> <span class="st">"class"</span>)</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Create grid for prediction</span></span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a>petal_length_seq <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="dv">0</span>, <span class="dv">7</span>, <span class="at">length.out =</span> <span class="dv">200</span>)</span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a>petal_width_seq <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="dv">0</span>, <span class="dv">3</span>, <span class="at">length.out =</span> <span class="dv">200</span>)</span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a>grid <span class="ot">&lt;-</span> <span class="fu">expand.grid</span>(<span class="at">Petal.Length =</span> petal_length_seq,</span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a>                    <span class="at">Petal.Width =</span> petal_width_seq)</span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a>grid<span class="sc">$</span>pred <span class="ot">&lt;-</span> <span class="fu">predict</span>(tree_2d, grid, <span class="at">type =</span> <span class="st">"class"</span>)</span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-11"><a href="#cb12-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot</span></span>
<span id="cb12-12"><a href="#cb12-12" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(grid<span class="sc">$</span>Petal.Length, grid<span class="sc">$</span>Petal.Width,</span>
<span id="cb12-13"><a href="#cb12-13" aria-hidden="true" tabindex="-1"></a>     <span class="at">col =</span> <span class="fu">c</span>(<span class="fu">rgb</span>(<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="fl">0.1</span>), <span class="fu">rgb</span>(<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="fl">0.1</span>), <span class="fu">rgb</span>(<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="fl">0.1</span>))[grid<span class="sc">$</span>pred],</span>
<span id="cb12-14"><a href="#cb12-14" aria-hidden="true" tabindex="-1"></a>     <span class="at">pch =</span> <span class="dv">15</span>, <span class="at">cex =</span> <span class="fl">0.3</span>,</span>
<span id="cb12-15"><a href="#cb12-15" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">"Petal Length"</span>, <span class="at">ylab =</span> <span class="st">"Petal Width"</span>,</span>
<span id="cb12-16"><a href="#cb12-16" aria-hidden="true" tabindex="-1"></a>     <span class="at">main =</span> <span class="st">"Decision Tree Boundaries"</span>)</span>
<span id="cb12-17"><a href="#cb12-17" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span>(iris<span class="sc">$</span>Petal.Length, iris<span class="sc">$</span>Petal.Width,</span>
<span id="cb12-18"><a href="#cb12-18" aria-hidden="true" tabindex="-1"></a>       <span class="at">col =</span> <span class="fu">c</span>(<span class="st">"red"</span>, <span class="st">"green"</span>, <span class="st">"blue"</span>)[iris<span class="sc">$</span>Species],</span>
<span id="cb12-19"><a href="#cb12-19" aria-hidden="true" tabindex="-1"></a>       <span class="at">pch =</span> <span class="dv">19</span>, <span class="at">cex =</span> <span class="fl">0.8</span>)</span>
<span id="cb12-20"><a href="#cb12-20" aria-hidden="true" tabindex="-1"></a><span class="fu">legend</span>(<span class="st">"topleft"</span>, <span class="fu">levels</span>(iris<span class="sc">$</span>Species),</span>
<span id="cb12-21"><a href="#cb12-21" aria-hidden="true" tabindex="-1"></a>       <span class="at">col =</span> <span class="fu">c</span>(<span class="st">"red"</span>, <span class="st">"green"</span>, <span class="st">"blue"</span>), <span class="at">pch =</span> <span class="dv">19</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div id="fig-tree-boundary" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-tree-boundary-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="33-trees-forests_files/figure-html/fig-tree-boundary-1.png" class="img-fluid figure-img" width="768">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-tree-boundary-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;34.7: Decision tree partition of feature space. Each rectangular region is assigned to a class.
</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="controlling-tree-complexity" class="level3">
<h3 class="anchored" data-anchor-id="controlling-tree-complexity">Controlling Tree Complexity</h3>
<p>Trees easily overfit—without constraints, they can keep splitting until each leaf contains a single observation, perfectly memorizing the training data. Several parameters control complexity to prevent this.</p>
<p>The <strong>complexity parameter (cp)</strong> requires that each split must improve the model’s fit by at least this factor, measured as a proportion of the initial error. Larger values of cp produce simpler trees with fewer splits. The <strong>minsplit</strong> parameter sets the minimum number of observations required in a node before the algorithm will attempt to split it (the default is 20). Similarly, <strong>minbucket</strong> sets the minimum number of observations allowed in each terminal (leaf) node. Finally, <strong>maxdepth</strong> directly limits how deep the tree can grow, restricting the number of sequential splitting decisions.</p>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">3</span>))</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (cp_val <span class="cf">in</span> <span class="fu">c</span>(<span class="fl">0.1</span>, <span class="fl">0.02</span>, <span class="fl">0.001</span>)) {</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>  tree <span class="ot">&lt;-</span> <span class="fu">rpart</span>(Species <span class="sc">~</span> ., <span class="at">data =</span> iris, <span class="at">cp =</span> cp_val)</span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">rpart.plot</span>(tree, <span class="at">main =</span> <span class="fu">paste</span>(<span class="st">"cp ="</span>, cp_val))</span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div id="fig-tree-complexity" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-tree-complexity-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="33-trees-forests_files/figure-html/fig-tree-complexity-1.png" class="img-fluid figure-img" width="864">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-tree-complexity-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;34.8: Effect of complexity parameter on tree structure: smaller cp allows more complexity
</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="overfitting-example" class="level3">
<h3 class="anchored" data-anchor-id="overfitting-example">Overfitting Example</h3>
<p>Setting cp = 0 and minsplit = 2 causes severe overfitting:</p>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a>fit_overfit <span class="ot">&lt;-</span> <span class="fu">rpart</span>(margin <span class="sc">~</span> ., <span class="at">data =</span> polls_2008,</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>                      <span class="at">control =</span> <span class="fu">rpart.control</span>(<span class="at">cp =</span> <span class="dv">0</span>, <span class="at">minsplit =</span> <span class="dv">2</span>))</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a>polls_2008 <span class="sc">%&gt;%</span></span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">y_hat =</span> <span class="fu">predict</span>(fit_overfit)) <span class="sc">%&gt;%</span></span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>() <span class="sc">+</span></span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="fu">aes</span>(day, margin)) <span class="sc">+</span></span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_step</span>(<span class="fu">aes</span>(day, y_hat), <span class="at">col =</span> <span class="st">"red"</span>) <span class="sc">+</span></span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">"Overfitting with cp = 0"</span>, <span class="at">x =</span> <span class="st">"Day"</span>, <span class="at">y =</span> <span class="st">"Poll margin"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div id="fig-polls-overfit" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-polls-overfit-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="33-trees-forests_files/figure-html/fig-polls-overfit-1.png" class="img-fluid figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-polls-overfit-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;34.9: With cp=0 and minsplit=2, the tree overfits by memorizing every point
</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="pruning-with-cross-validation" class="level3">
<h3 class="anchored" data-anchor-id="pruning-with-cross-validation">Pruning with Cross-Validation</h3>
<p>We use cross-validation to select optimal complexity:</p>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit full tree</span></span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>full_tree <span class="ot">&lt;-</span> <span class="fu">rpart</span>(Species <span class="sc">~</span> ., <span class="at">data =</span> iris, <span class="at">cp =</span> <span class="fl">0.001</span>)</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot CV error vs complexity</span></span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a><span class="fu">plotcp</span>(full_tree)</span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Print CP table</span></span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a><span class="fu">printcp</span>(full_tree)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>
Classification tree:
rpart(formula = Species ~ ., data = iris, cp = 0.001)

Variables actually used in tree construction:
[1] Petal.Length Petal.Width 

Root node error: 100/150 = 0.66667

n= 150 

     CP nsplit rel error xerror     xstd
1 0.500      0      1.00   1.17 0.050735
2 0.440      1      0.50   0.78 0.061188
3 0.001      2      0.06   0.08 0.027520</code></pre>
</div>
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Prune to optimal cp</span></span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>best_cp <span class="ot">&lt;-</span> full_tree<span class="sc">$</span>cptable[<span class="fu">which.min</span>(full_tree<span class="sc">$</span>cptable[, <span class="st">"xerror"</span>]), <span class="st">"CP"</span>]</span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a>pruned_tree <span class="ot">&lt;-</span> <span class="fu">prune</span>(full_tree, <span class="at">cp =</span> best_cp)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div id="fig-tree-cv" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-tree-cv-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="33-trees-forests_files/figure-html/fig-tree-cv-1.png" class="img-fluid figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-tree-cv-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;34.10: Cross-validation error vs.&nbsp;tree complexity. The dashed line shows one SE above minimum.
</figcaption>
</figure>
</div>
</div>
</div>
</section>
</section>
<section id="random-forests" class="level2" data-number="34.3">
<h2 data-number="34.3" class="anchored" data-anchor-id="random-forests"><span class="header-section-number">34.3</span> Random Forests</h2>
<p><strong>Random forests</strong> <span class="citation" data-cites="breiman2001random">(<a href="../references.html#ref-breiman2001random" role="doc-biblioref">Breiman 2001</a>)</span> address trees’ high variance by averaging many trees. The key insight: averaging many noisy but unbiased estimators reduces variance.</p>
<section id="the-random-forest-algorithm" class="level3">
<h3 class="anchored" data-anchor-id="the-random-forest-algorithm">The Random Forest Algorithm</h3>
<p>The random forest algorithm builds <span class="math inline">\(B\)</span> decision trees using the training set, but with two sources of randomness that make the trees different from one another.</p>
<p>First, each tree is trained on a <strong>bootstrap sample</strong>—a sample of <span class="math inline">\(n\)</span> observations drawn with replacement from the training data. This means each tree sees a slightly different dataset. Second, at each split, the algorithm considers only a random subset of <span class="math inline">\(m\)</span> features rather than all available predictors. This “random feature selection” prevents all trees from splitting on the same strong predictors and produces more diverse trees.</p>
<p>For prediction, the forest combines the predictions of all <span class="math inline">\(B\)</span> trees. In classification, the final prediction is the <strong>majority vote</strong> across all trees—whichever class is predicted most often wins. In regression, the final prediction is simply the <strong>average</strong> of all individual tree predictions.</p>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">2</span>))</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Single tree</span></span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a>fit_tree <span class="ot">&lt;-</span> <span class="fu">rpart</span>(margin <span class="sc">~</span> ., <span class="at">data =</span> polls_2008)</span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a>polls_2008 <span class="sc">%&gt;%</span></span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">y_hat =</span> <span class="fu">predict</span>(fit_tree)) <span class="sc">%&gt;%</span></span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">with</span>(<span class="fu">plot</span>(day, margin, <span class="at">pch =</span> <span class="dv">16</span>, <span class="at">cex =</span> <span class="fl">0.5</span>,</span>
<span id="cb18-8"><a href="#cb18-8" aria-hidden="true" tabindex="-1"></a>            <span class="at">main =</span> <span class="st">"Single Regression Tree"</span>))</span>
<span id="cb18-9"><a href="#cb18-9" aria-hidden="true" tabindex="-1"></a>polls_2008 <span class="sc">%&gt;%</span></span>
<span id="cb18-10"><a href="#cb18-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">y_hat =</span> <span class="fu">predict</span>(fit_tree)) <span class="sc">%&gt;%</span></span>
<span id="cb18-11"><a href="#cb18-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">with</span>(<span class="fu">lines</span>(day, y_hat, <span class="at">col =</span> <span class="st">"red"</span>, <span class="at">type =</span> <span class="st">"s"</span>, <span class="at">lwd =</span> <span class="dv">2</span>))</span>
<span id="cb18-12"><a href="#cb18-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-13"><a href="#cb18-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Random forest</span></span>
<span id="cb18-14"><a href="#cb18-14" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">42</span>)</span>
<span id="cb18-15"><a href="#cb18-15" aria-hidden="true" tabindex="-1"></a>fit_rf <span class="ot">&lt;-</span> <span class="fu">randomForest</span>(margin <span class="sc">~</span> ., <span class="at">data =</span> polls_2008)</span>
<span id="cb18-16"><a href="#cb18-16" aria-hidden="true" tabindex="-1"></a>polls_2008 <span class="sc">%&gt;%</span></span>
<span id="cb18-17"><a href="#cb18-17" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">y_hat =</span> <span class="fu">predict</span>(fit_rf, <span class="at">newdata =</span> polls_2008)) <span class="sc">%&gt;%</span></span>
<span id="cb18-18"><a href="#cb18-18" aria-hidden="true" tabindex="-1"></a>  <span class="fu">with</span>(<span class="fu">plot</span>(day, margin, <span class="at">pch =</span> <span class="dv">16</span>, <span class="at">cex =</span> <span class="fl">0.5</span>,</span>
<span id="cb18-19"><a href="#cb18-19" aria-hidden="true" tabindex="-1"></a>            <span class="at">main =</span> <span class="st">"Random Forest"</span>))</span>
<span id="cb18-20"><a href="#cb18-20" aria-hidden="true" tabindex="-1"></a>polls_2008 <span class="sc">%&gt;%</span></span>
<span id="cb18-21"><a href="#cb18-21" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">y_hat =</span> <span class="fu">predict</span>(fit_rf, <span class="at">newdata =</span> polls_2008)) <span class="sc">%&gt;%</span></span>
<span id="cb18-22"><a href="#cb18-22" aria-hidden="true" tabindex="-1"></a>  <span class="fu">with</span>(<span class="fu">lines</span>(day, y_hat, <span class="at">col =</span> <span class="st">"blue"</span>, <span class="at">lwd =</span> <span class="dv">2</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div id="fig-rf-smoothing" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-rf-smoothing-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="33-trees-forests_files/figure-html/fig-rf-smoothing-1.png" class="img-fluid figure-img" width="864">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-rf-smoothing-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;34.11: Random forest predictions are smoother than single trees because averaging step functions produces smooth curves
</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="random-forests-in-r" class="level3">
<h3 class="anchored" data-anchor-id="random-forests-in-r">Random Forests in R</h3>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">42</span>)</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit random forest</span></span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a>rf_model <span class="ot">&lt;-</span> <span class="fu">randomForest</span>(Species <span class="sc">~</span> ., <span class="at">data =</span> iris,</span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a>                          <span class="at">ntree =</span> <span class="dv">500</span>,       <span class="co"># Number of trees</span></span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a>                          <span class="at">mtry =</span> <span class="dv">2</span>,          <span class="co"># Features tried at each split</span></span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a>                          <span class="at">importance =</span> <span class="cn">TRUE</span>)  <span class="co"># Calculate variable importance</span></span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-9"><a href="#cb19-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Model summary</span></span>
<span id="cb19-10"><a href="#cb19-10" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(rf_model)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
 randomForest(formula = Species ~ ., data = iris, ntree = 500,      mtry = 2, importance = TRUE) 
               Type of random forest: classification
                     Number of trees: 500
No. of variables tried at each split: 2

        OOB estimate of  error rate: 4%
Confusion matrix:
           setosa versicolor virginica class.error
setosa         50          0         0        0.00
versicolor      0         47         3        0.06
virginica       0          3        47        0.06</code></pre>
</div>
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot error vs number of trees</span></span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(rf_model, <span class="at">main =</span> <span class="st">"Random Forest: Error vs. Number of Trees"</span>)</span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a><span class="fu">legend</span>(<span class="st">"topright"</span>, <span class="fu">colnames</span>(rf_model<span class="sc">$</span>err.rate), <span class="at">col =</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">4</span>, <span class="at">lty =</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">4</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div id="fig-rf-model" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-rf-model-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="33-trees-forests_files/figure-html/fig-rf-model-1.png" class="img-fluid figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-rf-model-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;34.12: Random forest OOB error rate decreasing as more trees are added
</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="out-of-bag-oob-error" class="level3">
<h3 class="anchored" data-anchor-id="out-of-bag-oob-error">Out-of-Bag (OOB) Error</h3>
<p>Each bootstrap sample uses about 63% of observations. The remaining 37% (<strong>out-of-bag</strong> samples) provide a built-in test set:</p>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="co"># OOB confusion matrix</span></span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a>rf_model<span class="sc">$</span>confusion</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>           setosa versicolor virginica class.error
setosa         50          0         0        0.00
versicolor      0         47         3        0.06
virginica       0          3        47        0.06</code></pre>
</div>
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="co"># OOB error rate</span></span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"OOB Error Rate:"</span>, <span class="fu">round</span>(<span class="dv">1</span> <span class="sc">-</span> <span class="fu">sum</span>(<span class="fu">diag</span>(rf_model<span class="sc">$</span>confusion[,<span class="dv">1</span><span class="sc">:</span><span class="dv">3</span>])) <span class="sc">/</span></span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a>                              <span class="fu">sum</span>(rf_model<span class="sc">$</span>confusion[,<span class="dv">1</span><span class="sc">:</span><span class="dv">3</span>]), <span class="dv">3</span>), <span class="st">"</span><span class="sc">\n</span><span class="st">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>OOB Error Rate: 0.04 </code></pre>
</div>
</div>
<p>OOB error is nearly as accurate as cross-validation but comes “for free” during training.</p>
</section>
<section id="variable-importance" class="level3">
<h3 class="anchored" data-anchor-id="variable-importance">Variable Importance</h3>
<p>Random forests provide measures of predictor importance:</p>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Variable importance plot</span></span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a><span class="fu">varImpPlot</span>(rf_model, <span class="at">main =</span> <span class="st">"Variable Importance"</span>)</span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-4"><a href="#cb26-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Numeric importance values</span></span>
<span id="cb26-5"><a href="#cb26-5" aria-hidden="true" tabindex="-1"></a><span class="fu">importance</span>(rf_model)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>                setosa versicolor virginica MeanDecreaseAccuracy
Sepal.Length  6.202069  8.1714623  7.112845            10.837857
Sepal.Width   4.389926 -0.0394507  4.433121             4.505871
Petal.Length 22.142310 32.6681049 28.411695            33.420235
Petal.Width  22.452771 32.9325603 30.673079            33.808242
             MeanDecreaseGini
Sepal.Length         9.273382
Sepal.Width          2.178884
Petal.Length        43.873860
Petal.Width         43.879867</code></pre>
</div>
<div class="cell-output-display">
<div id="fig-rf-importance" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-rf-importance-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="33-trees-forests_files/figure-html/fig-rf-importance-1.png" class="img-fluid figure-img" width="768">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-rf-importance-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;34.13: Variable importance: Mean Decrease Accuracy measures how much removing a variable hurts prediction; Mean Decrease Gini measures total reduction in node impurity
</figcaption>
</figure>
</div>
</div>
</div>
<p><strong>Mean Decrease Accuracy</strong>: For each tree, predictions are made on OOB samples. Then variable <span class="math inline">\(j\)</span> is randomly permuted and predictions are made again. The decrease in accuracy measures importance.</p>
<p><strong>Mean Decrease Gini</strong>: Total decrease in Gini impurity from splits on variable <span class="math inline">\(j\)</span>, averaged over all trees.</p>
</section>
<section id="tuning-random-forests" class="level3">
<h3 class="anchored" data-anchor-id="tuning-random-forests">Tuning Random Forests</h3>
<p>Random forests have several key parameters that can be tuned to optimize performance.</p>
<p>The <strong>ntree</strong> parameter specifies the number of trees in the forest. Generally, more trees are better—the error decreases and stabilizes as trees are added—but with diminishing returns beyond a certain point. The computational cost grows linearly with the number of trees, so practical considerations often limit this to a few hundred or thousand trees.</p>
<p>The <strong>mtry</strong> parameter determines how many features are randomly considered at each split. Smaller values increase the diversity among trees (reducing correlation), while larger values allow each tree to use the strongest predictors more often. The <strong>nodesize</strong> parameter controls the minimum size of terminal nodes, affecting how deep trees can grow. Smaller values allow trees to grow deeper and capture more complex patterns, but may overfit.</p>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Tune mtry</span></span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a>oob_error <span class="ot">&lt;-</span> <span class="fu">sapply</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">4</span>, <span class="cf">function</span>(m) {</span>
<span id="cb28-3"><a href="#cb28-3" aria-hidden="true" tabindex="-1"></a>  rf <span class="ot">&lt;-</span> <span class="fu">randomForest</span>(Species <span class="sc">~</span> ., <span class="at">data =</span> iris, <span class="at">mtry =</span> m, <span class="at">ntree =</span> <span class="dv">200</span>)</span>
<span id="cb28-4"><a href="#cb28-4" aria-hidden="true" tabindex="-1"></a>  rf<span class="sc">$</span>err.rate[<span class="dv">200</span>, <span class="st">"OOB"</span>]</span>
<span id="cb28-5"><a href="#cb28-5" aria-hidden="true" tabindex="-1"></a>})</span>
<span id="cb28-6"><a href="#cb28-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-7"><a href="#cb28-7" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">4</span>, oob_error, <span class="at">type =</span> <span class="st">"b"</span>, <span class="at">pch =</span> <span class="dv">19</span>,</span>
<span id="cb28-8"><a href="#cb28-8" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">"mtry (features at each split)"</span>,</span>
<span id="cb28-9"><a href="#cb28-9" aria-hidden="true" tabindex="-1"></a>     <span class="at">ylab =</span> <span class="st">"OOB Error Rate"</span>,</span>
<span id="cb28-10"><a href="#cb28-10" aria-hidden="true" tabindex="-1"></a>     <span class="at">main =</span> <span class="st">"Tuning mtry Parameter"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div id="fig-rf-tuning" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-rf-tuning-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="33-trees-forests_files/figure-html/fig-rf-tuning-1.png" class="img-fluid figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-rf-tuning-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;34.14: Random forest OOB error as a function of mtry
</figcaption>
</figure>
</div>
</div>
</div>
<p>Default values for mtry are often reasonable starting points. For classification problems, the default is <span class="math inline">\(m = \sqrt{p}\)</span>, the square root of the number of predictors. For regression problems, the default is <span class="math inline">\(m = p/3\)</span>, one-third of the predictors. These defaults strike a balance between allowing strong predictors to contribute while maintaining diversity among trees.</p>
</section>
<section id="random-forest-for-regression" class="level3">
<h3 class="anchored" data-anchor-id="random-forest-for-regression">Random Forest for Regression</h3>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Regression random forest</span></span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">42</span>)</span>
<span id="cb29-3"><a href="#cb29-3" aria-hidden="true" tabindex="-1"></a>rf_reg <span class="ot">&lt;-</span> <span class="fu">randomForest</span>(mpg <span class="sc">~</span> ., <span class="at">data =</span> mtcars, <span class="at">ntree =</span> <span class="dv">500</span>, <span class="at">importance =</span> <span class="cn">TRUE</span>)</span>
<span id="cb29-4"><a href="#cb29-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-5"><a href="#cb29-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Performance</span></span>
<span id="cb29-6"><a href="#cb29-6" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"Variance explained:"</span>, <span class="fu">round</span>(rf_reg<span class="sc">$</span>rsq[<span class="dv">500</span>] <span class="sc">*</span> <span class="dv">100</span>, <span class="dv">1</span>), <span class="st">"%</span><span class="sc">\n</span><span class="st">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Variance explained: 83.8 %</code></pre>
</div>
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"MSE:"</span>, <span class="fu">round</span>(rf_reg<span class="sc">$</span>mse[<span class="dv">500</span>], <span class="dv">2</span>), <span class="st">"</span><span class="sc">\n</span><span class="st">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>MSE: 5.71 </code></pre>
</div>
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb33"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Variable importance</span></span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a><span class="fu">varImpPlot</span>(rf_reg, <span class="at">main =</span> <span class="st">"Variable Importance for MPG Prediction"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="33-trees-forests_files/figure-html/unnamed-chunk-5-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
</section>
</section>
<section id="comparing-trees-and-forests" class="level2" data-number="34.4">
<h2 data-number="34.4" class="anchored" data-anchor-id="comparing-trees-and-forests"><span class="header-section-number">34.4</span> Comparing Trees and Forests</h2>
<table class="caption-top table">
<thead>
<tr class="header">
<th style="text-align: left;">Aspect</th>
<th style="text-align: left;">Decision Tree</th>
<th style="text-align: left;">Random Forest</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><strong>Interpretability</strong></td>
<td style="text-align: left;">High</td>
<td style="text-align: left;">Medium</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Variance</strong></td>
<td style="text-align: left;">High</td>
<td style="text-align: left;">Low</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>Bias</strong></td>
<td style="text-align: left;">Medium</td>
<td style="text-align: left;">Medium-Low</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Overfitting</strong></td>
<td style="text-align: left;">Prone</td>
<td style="text-align: left;">Resistant</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>Speed</strong></td>
<td style="text-align: left;">Fast</td>
<td style="text-align: left;">Slower</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Variable importance</strong></td>
<td style="text-align: left;">Limited</td>
<td style="text-align: left;">Built-in</td>
</tr>
</tbody>
</table>
<section id="when-to-use-each-method" class="level3">
<h3 class="anchored" data-anchor-id="when-to-use-each-method">When to Use Each Method</h3>
<p>The choice between decision trees and random forests depends on your priorities.</p>
<p><strong>Decision trees</strong> are the right choice when interpretability is paramount. If you need to explain the model to stakeholders—clinicians, regulators, or business managers—a single tree can be drawn as a simple flow chart that anyone can follow. Trees are also valuable during exploratory analysis, when you want to understand which variables matter and how they interact before committing to a more complex model.</p>
<p><strong>Random forests</strong> are preferred when prediction accuracy is the primary goal. They consistently outperform single trees across a wide range of problems. They work best when you have sufficient data to support averaging many trees, and when you want built-in variable importance measures to understand which predictors contribute most to predictions. The tradeoff is that random forests are less interpretable—you can no longer point to a single decision rule and explain exactly why a prediction was made.</p>
</section>
</section>
<section id="exercises" class="level2" data-number="34.5">
<h2 data-number="34.5" class="anchored" data-anchor-id="exercises"><span class="header-section-number">34.5</span> Exercises</h2>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Exercise TF.1: Decision Trees
</div>
</div>
<div class="callout-body-container callout-body">
<ol type="1">
<li>Create a simple dataset:</li>
</ol>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb34"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">1000</span></span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a>sigma <span class="ot">&lt;-</span> <span class="fl">0.25</span></span>
<span id="cb34-3"><a href="#cb34-3" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(n, <span class="dv">0</span>, <span class="dv">1</span>)</span>
<span id="cb34-4"><a href="#cb34-4" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> <span class="fl">0.75</span> <span class="sc">*</span> x <span class="sc">+</span> <span class="fu">rnorm</span>(n, <span class="dv">0</span>, sigma)</span>
<span id="cb34-5"><a href="#cb34-5" aria-hidden="true" tabindex="-1"></a>dat <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">x =</span> x, <span class="at">y =</span> y)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<ol start="2" type="1">
<li><p>Use <code>rpart</code> to fit a regression tree. Plot the tree.</p></li>
<li><p>Make a scatterplot of y vs x with the predicted values overlaid.</p></li>
<li><p>Try different values of <code>cp</code>. How does tree complexity change?</p></li>
</ol>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Exercise TF.2: Random Forests
</div>
</div>
<div class="callout-body-container callout-body">
<ol start="5" type="1">
<li><p>Using the same data, fit a random forest with <code>randomForest</code>.</p></li>
<li><p>Plot the predictions. How do they differ from the single tree?</p></li>
<li><p>Use <code>plot(rf)</code> to check if the forest has converged.</p></li>
<li><p>Experiment with <code>nodesize</code> and <code>maxnodes</code> to control smoothness.</p></li>
</ol>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Exercise TF.3: Classification Trees
</div>
</div>
<div class="callout-body-container callout-body">
<ol start="9" type="1">
<li>Use the <code>tissue_gene_expression</code> dataset:</li>
</ol>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb35"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(dslabs)</span>
<span id="cb35-2"><a href="#cb35-2" aria-hidden="true" tabindex="-1"></a><span class="fu">data</span>(<span class="st">"tissue_gene_expression"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<ol start="10" type="1">
<li><p>Fit a classification tree using <code>caret::train</code> with <code>method = "rpart"</code>. Use cross-validation to select optimal <code>cp</code>.</p></li>
<li><p>Examine the confusion matrix. Which tissues are most often confused?</p></li>
<li><p>Compare to a random forest. Does it improve accuracy?</p></li>
</ol>
</div>
</div>
</section>
<section id="summary" class="level2" data-number="34.6">
<h2 data-number="34.6" class="anchored" data-anchor-id="summary"><span class="header-section-number">34.6</span> Summary</h2>
<p>This chapter introduced <strong>decision trees</strong> and <strong>random forests</strong>, two of the most widely used machine learning methods.</p>
<p><strong>Decision trees (CART)</strong> recursively partition the data using simple yes/no rules based on predictor values. Their greatest strength is interpretability—a tree can be drawn as a flow chart that anyone can understand. They handle non-linear relationships and interactions automatically through their splitting structure, without requiring you to specify these features in advance. However, individual trees are prone to overfitting without careful tuning. The complexity parameter (cp), minsplit, and maxdepth control how complex a tree can become, and cross-validation helps select the optimal level of complexity.</p>
<p><strong>Random forests</strong> combine many trees to achieve robust prediction. By training each tree on a bootstrap sample and considering only a random subset of features at each split, the algorithm produces diverse trees whose errors tend to cancel out when averaged. The out-of-bag (OOB) error provides a built-in estimate of generalization performance without needing a separate validation set. Variable importance measures identify which predictors contribute most to predictions.</p>
<p>The fundamental tradeoff is between interpretability and accuracy. Individual trees have high variance—small changes in the training data can produce very different trees—but they are easy to interpret. Random forests reduce variance through averaging, achieving substantially better prediction accuracy, but sacrifice the simple interpretability of a single tree. For maximum interpretability, use a carefully pruned decision tree. For maximum prediction accuracy, use a random forest.</p>
</section>
<section id="additional-resources" class="level2" data-number="34.7">
<h2 data-number="34.7" class="anchored" data-anchor-id="additional-resources"><span class="header-section-number">34.7</span> Additional Resources</h2>
<ul>
<li><span class="citation" data-cites="james2023islr">James et al. (<a href="../references.html#ref-james2023islr" role="doc-biblioref">2023</a>)</span> - Comprehensive treatment of tree-based methods</li>
<li><span class="citation" data-cites="hastie2009elements">Hastie, Tibshirani, and Friedman (<a href="../references.html#ref-hastie2009elements" role="doc-biblioref">2009</a>)</span> - Theoretical foundations</li>
<li><span class="citation" data-cites="breiman2001random">Breiman (<a href="../references.html#ref-breiman2001random" role="doc-biblioref">2001</a>)</span> - Original random forest paper</li>
<li><code>rpart</code> and <code>randomForest</code> package documentation</li>
</ul>


<!-- -->

<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list" style="display: none">
<div id="ref-breiman2001random" class="csl-entry" role="listitem">
Breiman, Leo. 2001. <span>“Random Forests.”</span> <em>Machine Learning</em> 45 (1): 5–32.
</div>
<div id="ref-hastie2009elements" class="csl-entry" role="listitem">
Hastie, Trevor, Robert Tibshirani, and Jerome Friedman. 2009. <em>The Elements of Statistical Learning: Data Mining, Inference, and Prediction</em>. 2nd ed. New York: Springer.
</div>
<div id="ref-james2023islr" class="csl-entry" role="listitem">
James, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2023. <em>An Introduction to Statistical Learning with Applications in r</em>. 2nd ed. Springer. <a href="https://www.statlearning.com">https://www.statlearning.com</a>.
</div>
</div>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
    const viewSource = window.document.getElementById('quarto-view-source') ||
                       window.document.getElementById('quarto-code-tools-source');
    if (viewSource) {
      const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
      viewSource.addEventListener("click", function(e) {
        if (sourceUrl) {
          // rstudio viewer pane
          if (/\bcapabilities=\b/.test(window.location)) {
            window.open(sourceUrl);
          } else {
            window.location.href = sourceUrl;
          }
        } else {
          const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
          modal.show();
        }
        return false;
      });
    }
    function toggleCodeHandler(show) {
      return function(e) {
        const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
        for (let i=0; i<detailsSrc.length; i++) {
          const details = detailsSrc[i].parentElement;
          if (show) {
            details.open = true;
          } else {
            details.removeAttribute("open");
          }
        }
        const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
        const fromCls = show ? "hidden" : "unhidden";
        const toCls = show ? "unhidden" : "hidden";
        for (let i=0; i<cellCodeDivs.length; i++) {
          const codeDiv = cellCodeDivs[i];
          if (codeDiv.classList.contains(fromCls)) {
            codeDiv.classList.remove(fromCls);
            codeDiv.classList.add(toCls);
          } 
        }
        return false;
      }
    }
    const hideAllCode = window.document.getElementById("quarto-hide-all-code");
    if (hideAllCode) {
      hideAllCode.addEventListener("click", toggleCodeHandler(false));
    }
    const showAllCode = window.document.getElementById("quarto-show-all-code");
    if (showAllCode) {
      showAllCode.addEventListener("click", toggleCodeHandler(true));
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../chapters/32-classification-methods.html" class="pagination-link" aria-label="Classification and Performance Metrics">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">33</span>&nbsp; <span class="chapter-title">Classification and Performance Metrics</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../chapters/34-svm.html" class="pagination-link" aria-label="Support Vector Machines">
        <span class="nav-page-text"><span class="chapter-number">35</span>&nbsp; <span class="chapter-title">Support Vector Machines</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb36" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a><span class="fu"># Decision Trees and Random Forests {#sec-trees-forests}</span></span>
<span id="cb36-2"><a href="#cb36-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-5"><a href="#cb36-5" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb36-6"><a href="#cb36-6" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: false</span></span>
<span id="cb36-7"><a href="#cb36-7" aria-hidden="true" tabindex="-1"></a><span class="co">#| message: false</span></span>
<span id="cb36-8"><a href="#cb36-8" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tidyverse)</span>
<span id="cb36-9"><a href="#cb36-9" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(rpart)</span>
<span id="cb36-10"><a href="#cb36-10" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(rpart.plot)</span>
<span id="cb36-11"><a href="#cb36-11" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(randomForest)</span>
<span id="cb36-12"><a href="#cb36-12" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(dslabs)</span>
<span id="cb36-13"><a href="#cb36-13" aria-hidden="true" tabindex="-1"></a><span class="fu">theme_set</span>(<span class="fu">theme_minimal</span>())</span>
<span id="cb36-14"><a href="#cb36-14" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb36-15"><a href="#cb36-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-16"><a href="#cb36-16" aria-hidden="true" tabindex="-1"></a><span class="fu">## Introduction to Tree-Based Methods</span></span>
<span id="cb36-17"><a href="#cb36-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-18"><a href="#cb36-18" aria-hidden="true" tabindex="-1"></a>**Decision trees** are a fundamentally different approach to prediction. Instead of fitting a smooth function or finding nearest neighbors, trees partition the feature space into rectangular regions and make a simple prediction within each region.</span>
<span id="cb36-19"><a href="#cb36-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-20"><a href="#cb36-20" aria-hidden="true" tabindex="-1"></a>Trees are popular for several compelling reasons. They are highly interpretable—you can draw a tree as a flow chart and explain it to non-statisticians who may never have taken a statistics course. They handle both numeric and categorical predictors naturally, without requiring dummy variables or other transformations. They capture non-linear relationships and interactions automatically through their recursive splitting structure. And they are robust to outliers and don't require feature scaling, since only the rank order of values matters for splitting.</span>
<span id="cb36-21"><a href="#cb36-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-22"><a href="#cb36-22" aria-hidden="true" tabindex="-1"></a>However, individual trees have high variance and are prone to overfitting. **Random forests** address this limitation by averaging many trees together, dramatically improving prediction accuracy at the cost of some interpretability.</span>
<span id="cb36-23"><a href="#cb36-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-24"><a href="#cb36-24" aria-hidden="true" tabindex="-1"></a>@fig-decision-tree-example shows a bioengineering application of decision trees: predicting musculoskeletal injury risk based on physiological factors. The tree structure makes the decision process transparent—you can trace the path from root to leaf and understand exactly why a particular prediction was made. Each internal node asks a yes/no question (Is BMI ≥ 30? Is exercise frequency ≥ 3x/week?), and each leaf node provides a prediction with an associated probability.</span>
<span id="cb36-25"><a href="#cb36-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-26"><a href="#cb36-26" aria-hidden="true" tabindex="-1"></a><span class="al">![A decision tree for predicting musculoskeletal injury risk using bioengineering variables. The tree partitions patients based on BMI, exercise frequency, and age, assigning each leaf node an injury risk probability. This interpretability—being able to trace exactly how a prediction is made—is a key advantage of tree-based methods.](../images/ch33/ch33_decision_tree_example.png)</span>{#fig-decision-tree-example fig-align="center" width="90%"}</span>
<span id="cb36-27"><a href="#cb36-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-28"><a href="#cb36-28" aria-hidden="true" tabindex="-1"></a><span class="fu">## Decision Trees (CART)</span></span>
<span id="cb36-29"><a href="#cb36-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-30"><a href="#cb36-30" aria-hidden="true" tabindex="-1"></a>**Classification and Regression Trees (CART)** make predictions by recursively partitioning the feature space. At each node, the algorithm asks a yes/no question about a single feature, splitting observations into two groups.</span>
<span id="cb36-31"><a href="#cb36-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-32"><a href="#cb36-32" aria-hidden="true" tabindex="-1"></a><span class="fu">### Motivating Example: Olive Oil Classification</span></span>
<span id="cb36-33"><a href="#cb36-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-34"><a href="#cb36-34" aria-hidden="true" tabindex="-1"></a>Consider a dataset of olive oil samples from three regions of Italy, with measurements of 8 fatty acids:</span>
<span id="cb36-35"><a href="#cb36-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-38"><a href="#cb36-38" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb36-39"><a href="#cb36-39" aria-hidden="true" tabindex="-1"></a><span class="fu">data</span>(<span class="st">"olive"</span>)</span>
<span id="cb36-40"><a href="#cb36-40" aria-hidden="true" tabindex="-1"></a><span class="fu">names</span>(olive)</span>
<span id="cb36-41"><a href="#cb36-41" aria-hidden="true" tabindex="-1"></a><span class="fu">table</span>(olive<span class="sc">$</span>region)</span>
<span id="cb36-42"><a href="#cb36-42" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb36-43"><a href="#cb36-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-44"><a href="#cb36-44" aria-hidden="true" tabindex="-1"></a>We want to predict the region of origin from fatty acid composition. Let's examine how some predictors separate regions:</span>
<span id="cb36-45"><a href="#cb36-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-48"><a href="#cb36-48" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb36-49"><a href="#cb36-49" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-olive-eda</span></span>
<span id="cb36-50"><a href="#cb36-50" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "Distribution of two fatty acids by region. These predictors clearly separate the regions."</span></span>
<span id="cb36-51"><a href="#cb36-51" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-width: 9</span></span>
<span id="cb36-52"><a href="#cb36-52" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-height: 4</span></span>
<span id="cb36-53"><a href="#cb36-53" aria-hidden="true" tabindex="-1"></a>olive <span class="ot">&lt;-</span> <span class="fu">select</span>(olive, <span class="sc">-</span>area)</span>
<span id="cb36-54"><a href="#cb36-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-55"><a href="#cb36-55" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">2</span>))</span>
<span id="cb36-56"><a href="#cb36-56" aria-hidden="true" tabindex="-1"></a><span class="fu">boxplot</span>(eicosenoic <span class="sc">~</span> region, <span class="at">data =</span> olive, <span class="at">col =</span> <span class="fu">c</span>(<span class="st">"coral"</span>, <span class="st">"lightgreen"</span>, <span class="st">"lightblue"</span>),</span>
<span id="cb36-57"><a href="#cb36-57" aria-hidden="true" tabindex="-1"></a>        <span class="at">main =</span> <span class="st">"Eicosenoic by Region"</span>)</span>
<span id="cb36-58"><a href="#cb36-58" aria-hidden="true" tabindex="-1"></a><span class="fu">boxplot</span>(linoleic <span class="sc">~</span> region, <span class="at">data =</span> olive, <span class="at">col =</span> <span class="fu">c</span>(<span class="st">"coral"</span>, <span class="st">"lightgreen"</span>, <span class="st">"lightblue"</span>),</span>
<span id="cb36-59"><a href="#cb36-59" aria-hidden="true" tabindex="-1"></a>        <span class="at">main =</span> <span class="st">"Linoleic by Region"</span>)</span>
<span id="cb36-60"><a href="#cb36-60" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb36-61"><a href="#cb36-61" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-62"><a href="#cb36-62" aria-hidden="true" tabindex="-1"></a>Notice that eicosenoic is only present in Southern Italy, and linoleic separates Northern Italy from Sardinia. We can visualize this separation:</span>
<span id="cb36-63"><a href="#cb36-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-66"><a href="#cb36-66" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb36-67"><a href="#cb36-67" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-olive-two-predictors</span></span>
<span id="cb36-68"><a href="#cb36-68" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "With just two predictors, simple rules can perfectly separate the regions"</span></span>
<span id="cb36-69"><a href="#cb36-69" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-width: 7</span></span>
<span id="cb36-70"><a href="#cb36-70" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-height: 5</span></span>
<span id="cb36-71"><a href="#cb36-71" aria-hidden="true" tabindex="-1"></a>olive <span class="sc">%&gt;%</span></span>
<span id="cb36-72"><a href="#cb36-72" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(eicosenoic, linoleic, <span class="at">color =</span> region)) <span class="sc">+</span></span>
<span id="cb36-73"><a href="#cb36-73" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>() <span class="sc">+</span></span>
<span id="cb36-74"><a href="#cb36-74" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_vline</span>(<span class="at">xintercept =</span> <span class="fl">0.065</span>, <span class="at">lty =</span> <span class="dv">2</span>) <span class="sc">+</span></span>
<span id="cb36-75"><a href="#cb36-75" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_segment</span>(<span class="at">x =</span> <span class="sc">-</span><span class="fl">0.2</span>, <span class="at">y =</span> <span class="fl">10.54</span>, <span class="at">xend =</span> <span class="fl">0.065</span>, <span class="at">yend =</span> <span class="fl">10.54</span>,</span>
<span id="cb36-76"><a href="#cb36-76" aria-hidden="true" tabindex="-1"></a>               <span class="at">color =</span> <span class="st">"black"</span>, <span class="at">lty =</span> <span class="dv">2</span>) <span class="sc">+</span></span>
<span id="cb36-77"><a href="#cb36-77" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">"Perfect Classification with Simple Rules"</span>)</span>
<span id="cb36-78"><a href="#cb36-78" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb36-79"><a href="#cb36-79" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-80"><a href="#cb36-80" aria-hidden="true" tabindex="-1"></a>By eye, we can construct a simple prediction rule: if eicosenoic is greater than 0.065, predict Southern Italy. If not, check linoleic—if it's greater than 10.535, predict Sardinia; otherwise, predict Northern Italy.</span>
<span id="cb36-81"><a href="#cb36-81" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-82"><a href="#cb36-82" aria-hidden="true" tabindex="-1"></a>This is exactly what a decision tree learns from data—a flow chart of yes/no questions that partition the feature space into regions, each assigned to a single class.</span>
<span id="cb36-83"><a href="#cb36-83" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-84"><a href="#cb36-84" aria-hidden="true" tabindex="-1"></a><span class="fu">### How Trees Work</span></span>
<span id="cb36-85"><a href="#cb36-85" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-86"><a href="#cb36-86" aria-hidden="true" tabindex="-1"></a>Trees create partitions recursively. We start with one partition (the entire dataset). After the first split we have two partitions, then four, and so on.</span>
<span id="cb36-87"><a href="#cb36-87" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-88"><a href="#cb36-88" aria-hidden="true" tabindex="-1"></a>For each split, we find a predictor $j$ and value $s$ that define two new partitions:</span>
<span id="cb36-89"><a href="#cb36-89" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-90"><a href="#cb36-90" aria-hidden="true" tabindex="-1"></a>$$R_1(j,s) = <span class="sc">\{</span>x : x_j &lt; s<span class="sc">\}</span> \quad \text{and} \quad R_2(j,s) = <span class="sc">\{</span>x : x_j \geq s<span class="sc">\}</span>$$</span>
<span id="cb36-91"><a href="#cb36-91" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-92"><a href="#cb36-92" aria-hidden="true" tabindex="-1"></a>**For regression trees**, we choose the split that minimizes the **residual sum of squares (RSS)**:</span>
<span id="cb36-93"><a href="#cb36-93" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-94"><a href="#cb36-94" aria-hidden="true" tabindex="-1"></a>$$\sum_{i: x_i \in R_1} (y_i - \hat{y}_{R_1})^2 + \sum_{i: x_i \in R_2} (y_i - \hat{y}_{R_2})^2$$</span>
<span id="cb36-95"><a href="#cb36-95" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-96"><a href="#cb36-96" aria-hidden="true" tabindex="-1"></a>**For classification trees**, we use **Gini impurity**:</span>
<span id="cb36-97"><a href="#cb36-97" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-98"><a href="#cb36-98" aria-hidden="true" tabindex="-1"></a>$$\text{Gini}(j) = \sum_{k=1}^K \hat{p}_{j,k}(1-\hat{p}_{j,k})$$</span>
<span id="cb36-99"><a href="#cb36-99" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-100"><a href="#cb36-100" aria-hidden="true" tabindex="-1"></a>where $\hat{p}_{j,k}$ is the proportion of observations in partition $j$ that are of class $k$. A pure node (all one class) has Gini = 0.</span>
<span id="cb36-101"><a href="#cb36-101" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-102"><a href="#cb36-102" aria-hidden="true" tabindex="-1"></a><span class="fu">### Building a Classification Tree</span></span>
<span id="cb36-103"><a href="#cb36-103" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-106"><a href="#cb36-106" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb36-107"><a href="#cb36-107" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-cart-classification</span></span>
<span id="cb36-108"><a href="#cb36-108" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "A CART decision tree for classifying iris species. Each node shows the predicted class and the splitting rule."</span></span>
<span id="cb36-109"><a href="#cb36-109" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-width: 8</span></span>
<span id="cb36-110"><a href="#cb36-110" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-height: 6</span></span>
<span id="cb36-111"><a href="#cb36-111" aria-hidden="true" tabindex="-1"></a><span class="co"># Build a classification tree</span></span>
<span id="cb36-112"><a href="#cb36-112" aria-hidden="true" tabindex="-1"></a><span class="fu">data</span>(iris)</span>
<span id="cb36-113"><a href="#cb36-113" aria-hidden="true" tabindex="-1"></a>tree_class <span class="ot">&lt;-</span> <span class="fu">rpart</span>(Species <span class="sc">~</span> ., <span class="at">data =</span> iris, <span class="at">method =</span> <span class="st">"class"</span>)</span>
<span id="cb36-114"><a href="#cb36-114" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-115"><a href="#cb36-115" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualize with rpart.plot</span></span>
<span id="cb36-116"><a href="#cb36-116" aria-hidden="true" tabindex="-1"></a><span class="fu">rpart.plot</span>(tree_class, <span class="at">extra =</span> <span class="dv">104</span>, <span class="at">box.palette =</span> <span class="st">"RdYlGn"</span>,</span>
<span id="cb36-117"><a href="#cb36-117" aria-hidden="true" tabindex="-1"></a>           <span class="at">main =</span> <span class="st">"Classification Tree for Iris Species"</span>)</span>
<span id="cb36-118"><a href="#cb36-118" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb36-119"><a href="#cb36-119" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-120"><a href="#cb36-120" aria-hidden="true" tabindex="-1"></a><span class="fu">### Interpreting Tree Output</span></span>
<span id="cb36-121"><a href="#cb36-121" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-122"><a href="#cb36-122" aria-hidden="true" tabindex="-1"></a>The tree visualization conveys several pieces of information at each node. The **node prediction** shows the class that would be assigned to observations reaching that node—this is simply the most common class in that region of the feature space. The **split rule** shows which feature and threshold value are used to partition observations into the left and right child nodes. The **proportions** display the distribution of classes among observations at that node, helping you see how "pure" each region is. Finally, the **sample size** indicates what percentage of the training data reaches each node, showing which branches contain most of the data.</span>
<span id="cb36-123"><a href="#cb36-123" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-126"><a href="#cb36-126" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb36-127"><a href="#cb36-127" aria-hidden="true" tabindex="-1"></a><span class="co"># Detailed tree summary</span></span>
<span id="cb36-128"><a href="#cb36-128" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(tree_class, <span class="at">cp =</span> <span class="fl">0.1</span>)</span>
<span id="cb36-129"><a href="#cb36-129" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb36-130"><a href="#cb36-130" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-131"><a href="#cb36-131" aria-hidden="true" tabindex="-1"></a><span class="fu">### Regression Trees</span></span>
<span id="cb36-132"><a href="#cb36-132" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-133"><a href="#cb36-133" aria-hidden="true" tabindex="-1"></a>When the outcome is continuous, we call the method a **regression tree**:</span>
<span id="cb36-134"><a href="#cb36-134" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-137"><a href="#cb36-137" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb36-138"><a href="#cb36-138" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-polls-tree</span></span>
<span id="cb36-139"><a href="#cb36-139" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "Regression tree for poll data. The tree partitions time into regions with constant predictions."</span></span>
<span id="cb36-140"><a href="#cb36-140" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-width: 8</span></span>
<span id="cb36-141"><a href="#cb36-141" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-height: 5</span></span>
<span id="cb36-142"><a href="#cb36-142" aria-hidden="true" tabindex="-1"></a><span class="fu">data</span>(<span class="st">"polls_2008"</span>)</span>
<span id="cb36-143"><a href="#cb36-143" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-144"><a href="#cb36-144" aria-hidden="true" tabindex="-1"></a>fit <span class="ot">&lt;-</span> <span class="fu">rpart</span>(margin <span class="sc">~</span> ., <span class="at">data =</span> polls_2008)</span>
<span id="cb36-145"><a href="#cb36-145" aria-hidden="true" tabindex="-1"></a>rafalib<span class="sc">::</span><span class="fu">mypar</span>()</span>
<span id="cb36-146"><a href="#cb36-146" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(fit, <span class="at">margin =</span> <span class="fl">0.1</span>)</span>
<span id="cb36-147"><a href="#cb36-147" aria-hidden="true" tabindex="-1"></a><span class="fu">text</span>(fit, <span class="at">cex =</span> <span class="fl">0.75</span>)</span>
<span id="cb36-148"><a href="#cb36-148" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb36-149"><a href="#cb36-149" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-150"><a href="#cb36-150" aria-hidden="true" tabindex="-1"></a>The tree creates a step function:</span>
<span id="cb36-151"><a href="#cb36-151" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-154"><a href="#cb36-154" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb36-155"><a href="#cb36-155" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-polls-tree-fit</span></span>
<span id="cb36-156"><a href="#cb36-156" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "Regression tree predictions form a step function"</span></span>
<span id="cb36-157"><a href="#cb36-157" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-width: 7</span></span>
<span id="cb36-158"><a href="#cb36-158" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-height: 5</span></span>
<span id="cb36-159"><a href="#cb36-159" aria-hidden="true" tabindex="-1"></a>polls_2008 <span class="sc">%&gt;%</span></span>
<span id="cb36-160"><a href="#cb36-160" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">y_hat =</span> <span class="fu">predict</span>(fit)) <span class="sc">%&gt;%</span></span>
<span id="cb36-161"><a href="#cb36-161" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>() <span class="sc">+</span></span>
<span id="cb36-162"><a href="#cb36-162" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="fu">aes</span>(day, margin)) <span class="sc">+</span></span>
<span id="cb36-163"><a href="#cb36-163" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_step</span>(<span class="fu">aes</span>(day, y_hat), <span class="at">col =</span> <span class="st">"red"</span>, <span class="at">linewidth =</span> <span class="dv">1</span>) <span class="sc">+</span></span>
<span id="cb36-164"><a href="#cb36-164" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">"Regression Tree Fit"</span>, <span class="at">x =</span> <span class="st">"Day"</span>, <span class="at">y =</span> <span class="st">"Poll margin"</span>)</span>
<span id="cb36-165"><a href="#cb36-165" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb36-166"><a href="#cb36-166" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-167"><a href="#cb36-167" aria-hidden="true" tabindex="-1"></a><span class="fu">### Decision Boundaries</span></span>
<span id="cb36-168"><a href="#cb36-168" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-169"><a href="#cb36-169" aria-hidden="true" tabindex="-1"></a>Trees partition the feature space into rectangular regions:</span>
<span id="cb36-170"><a href="#cb36-170" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-173"><a href="#cb36-173" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb36-174"><a href="#cb36-174" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-tree-boundary</span></span>
<span id="cb36-175"><a href="#cb36-175" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "Decision tree partition of feature space. Each rectangular region is assigned to a class."</span></span>
<span id="cb36-176"><a href="#cb36-176" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-width: 8</span></span>
<span id="cb36-177"><a href="#cb36-177" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-height: 6</span></span>
<span id="cb36-178"><a href="#cb36-178" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualize decision boundary for 2D case</span></span>
<span id="cb36-179"><a href="#cb36-179" aria-hidden="true" tabindex="-1"></a>tree_2d <span class="ot">&lt;-</span> <span class="fu">rpart</span>(Species <span class="sc">~</span> Petal.Length <span class="sc">+</span> Petal.Width, <span class="at">data =</span> iris, <span class="at">method =</span> <span class="st">"class"</span>)</span>
<span id="cb36-180"><a href="#cb36-180" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-181"><a href="#cb36-181" aria-hidden="true" tabindex="-1"></a><span class="co"># Create grid for prediction</span></span>
<span id="cb36-182"><a href="#cb36-182" aria-hidden="true" tabindex="-1"></a>petal_length_seq <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="dv">0</span>, <span class="dv">7</span>, <span class="at">length.out =</span> <span class="dv">200</span>)</span>
<span id="cb36-183"><a href="#cb36-183" aria-hidden="true" tabindex="-1"></a>petal_width_seq <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="dv">0</span>, <span class="dv">3</span>, <span class="at">length.out =</span> <span class="dv">200</span>)</span>
<span id="cb36-184"><a href="#cb36-184" aria-hidden="true" tabindex="-1"></a>grid <span class="ot">&lt;-</span> <span class="fu">expand.grid</span>(<span class="at">Petal.Length =</span> petal_length_seq,</span>
<span id="cb36-185"><a href="#cb36-185" aria-hidden="true" tabindex="-1"></a>                    <span class="at">Petal.Width =</span> petal_width_seq)</span>
<span id="cb36-186"><a href="#cb36-186" aria-hidden="true" tabindex="-1"></a>grid<span class="sc">$</span>pred <span class="ot">&lt;-</span> <span class="fu">predict</span>(tree_2d, grid, <span class="at">type =</span> <span class="st">"class"</span>)</span>
<span id="cb36-187"><a href="#cb36-187" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-188"><a href="#cb36-188" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot</span></span>
<span id="cb36-189"><a href="#cb36-189" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(grid<span class="sc">$</span>Petal.Length, grid<span class="sc">$</span>Petal.Width,</span>
<span id="cb36-190"><a href="#cb36-190" aria-hidden="true" tabindex="-1"></a>     <span class="at">col =</span> <span class="fu">c</span>(<span class="fu">rgb</span>(<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="fl">0.1</span>), <span class="fu">rgb</span>(<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="fl">0.1</span>), <span class="fu">rgb</span>(<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="fl">0.1</span>))[grid<span class="sc">$</span>pred],</span>
<span id="cb36-191"><a href="#cb36-191" aria-hidden="true" tabindex="-1"></a>     <span class="at">pch =</span> <span class="dv">15</span>, <span class="at">cex =</span> <span class="fl">0.3</span>,</span>
<span id="cb36-192"><a href="#cb36-192" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">"Petal Length"</span>, <span class="at">ylab =</span> <span class="st">"Petal Width"</span>,</span>
<span id="cb36-193"><a href="#cb36-193" aria-hidden="true" tabindex="-1"></a>     <span class="at">main =</span> <span class="st">"Decision Tree Boundaries"</span>)</span>
<span id="cb36-194"><a href="#cb36-194" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span>(iris<span class="sc">$</span>Petal.Length, iris<span class="sc">$</span>Petal.Width,</span>
<span id="cb36-195"><a href="#cb36-195" aria-hidden="true" tabindex="-1"></a>       <span class="at">col =</span> <span class="fu">c</span>(<span class="st">"red"</span>, <span class="st">"green"</span>, <span class="st">"blue"</span>)[iris<span class="sc">$</span>Species],</span>
<span id="cb36-196"><a href="#cb36-196" aria-hidden="true" tabindex="-1"></a>       <span class="at">pch =</span> <span class="dv">19</span>, <span class="at">cex =</span> <span class="fl">0.8</span>)</span>
<span id="cb36-197"><a href="#cb36-197" aria-hidden="true" tabindex="-1"></a><span class="fu">legend</span>(<span class="st">"topleft"</span>, <span class="fu">levels</span>(iris<span class="sc">$</span>Species),</span>
<span id="cb36-198"><a href="#cb36-198" aria-hidden="true" tabindex="-1"></a>       <span class="at">col =</span> <span class="fu">c</span>(<span class="st">"red"</span>, <span class="st">"green"</span>, <span class="st">"blue"</span>), <span class="at">pch =</span> <span class="dv">19</span>)</span>
<span id="cb36-199"><a href="#cb36-199" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb36-200"><a href="#cb36-200" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-201"><a href="#cb36-201" aria-hidden="true" tabindex="-1"></a><span class="fu">### Controlling Tree Complexity</span></span>
<span id="cb36-202"><a href="#cb36-202" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-203"><a href="#cb36-203" aria-hidden="true" tabindex="-1"></a>Trees easily overfit—without constraints, they can keep splitting until each leaf contains a single observation, perfectly memorizing the training data. Several parameters control complexity to prevent this.</span>
<span id="cb36-204"><a href="#cb36-204" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-205"><a href="#cb36-205" aria-hidden="true" tabindex="-1"></a>The **complexity parameter (cp)** requires that each split must improve the model's fit by at least this factor, measured as a proportion of the initial error. Larger values of cp produce simpler trees with fewer splits. The **minsplit** parameter sets the minimum number of observations required in a node before the algorithm will attempt to split it (the default is 20). Similarly, **minbucket** sets the minimum number of observations allowed in each terminal (leaf) node. Finally, **maxdepth** directly limits how deep the tree can grow, restricting the number of sequential splitting decisions.</span>
<span id="cb36-206"><a href="#cb36-206" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-209"><a href="#cb36-209" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb36-210"><a href="#cb36-210" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-tree-complexity</span></span>
<span id="cb36-211"><a href="#cb36-211" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "Effect of complexity parameter on tree structure: smaller cp allows more complexity"</span></span>
<span id="cb36-212"><a href="#cb36-212" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-width: 9</span></span>
<span id="cb36-213"><a href="#cb36-213" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-height: 4</span></span>
<span id="cb36-214"><a href="#cb36-214" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">3</span>))</span>
<span id="cb36-215"><a href="#cb36-215" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-216"><a href="#cb36-216" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (cp_val <span class="cf">in</span> <span class="fu">c</span>(<span class="fl">0.1</span>, <span class="fl">0.02</span>, <span class="fl">0.001</span>)) {</span>
<span id="cb36-217"><a href="#cb36-217" aria-hidden="true" tabindex="-1"></a>  tree <span class="ot">&lt;-</span> <span class="fu">rpart</span>(Species <span class="sc">~</span> ., <span class="at">data =</span> iris, <span class="at">cp =</span> cp_val)</span>
<span id="cb36-218"><a href="#cb36-218" aria-hidden="true" tabindex="-1"></a>  <span class="fu">rpart.plot</span>(tree, <span class="at">main =</span> <span class="fu">paste</span>(<span class="st">"cp ="</span>, cp_val))</span>
<span id="cb36-219"><a href="#cb36-219" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb36-220"><a href="#cb36-220" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb36-221"><a href="#cb36-221" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-222"><a href="#cb36-222" aria-hidden="true" tabindex="-1"></a><span class="fu">### Overfitting Example</span></span>
<span id="cb36-223"><a href="#cb36-223" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-224"><a href="#cb36-224" aria-hidden="true" tabindex="-1"></a>Setting cp = 0 and minsplit = 2 causes severe overfitting:</span>
<span id="cb36-225"><a href="#cb36-225" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-228"><a href="#cb36-228" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb36-229"><a href="#cb36-229" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-polls-overfit</span></span>
<span id="cb36-230"><a href="#cb36-230" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "With cp=0 and minsplit=2, the tree overfits by memorizing every point"</span></span>
<span id="cb36-231"><a href="#cb36-231" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-width: 7</span></span>
<span id="cb36-232"><a href="#cb36-232" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-height: 5</span></span>
<span id="cb36-233"><a href="#cb36-233" aria-hidden="true" tabindex="-1"></a>fit_overfit <span class="ot">&lt;-</span> <span class="fu">rpart</span>(margin <span class="sc">~</span> ., <span class="at">data =</span> polls_2008,</span>
<span id="cb36-234"><a href="#cb36-234" aria-hidden="true" tabindex="-1"></a>                      <span class="at">control =</span> <span class="fu">rpart.control</span>(<span class="at">cp =</span> <span class="dv">0</span>, <span class="at">minsplit =</span> <span class="dv">2</span>))</span>
<span id="cb36-235"><a href="#cb36-235" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-236"><a href="#cb36-236" aria-hidden="true" tabindex="-1"></a>polls_2008 <span class="sc">%&gt;%</span></span>
<span id="cb36-237"><a href="#cb36-237" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">y_hat =</span> <span class="fu">predict</span>(fit_overfit)) <span class="sc">%&gt;%</span></span>
<span id="cb36-238"><a href="#cb36-238" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>() <span class="sc">+</span></span>
<span id="cb36-239"><a href="#cb36-239" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="fu">aes</span>(day, margin)) <span class="sc">+</span></span>
<span id="cb36-240"><a href="#cb36-240" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_step</span>(<span class="fu">aes</span>(day, y_hat), <span class="at">col =</span> <span class="st">"red"</span>) <span class="sc">+</span></span>
<span id="cb36-241"><a href="#cb36-241" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">"Overfitting with cp = 0"</span>, <span class="at">x =</span> <span class="st">"Day"</span>, <span class="at">y =</span> <span class="st">"Poll margin"</span>)</span>
<span id="cb36-242"><a href="#cb36-242" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb36-243"><a href="#cb36-243" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-244"><a href="#cb36-244" aria-hidden="true" tabindex="-1"></a><span class="fu">### Pruning with Cross-Validation</span></span>
<span id="cb36-245"><a href="#cb36-245" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-246"><a href="#cb36-246" aria-hidden="true" tabindex="-1"></a>We use cross-validation to select optimal complexity:</span>
<span id="cb36-247"><a href="#cb36-247" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-250"><a href="#cb36-250" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb36-251"><a href="#cb36-251" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-tree-cv</span></span>
<span id="cb36-252"><a href="#cb36-252" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "Cross-validation error vs. tree complexity. The dashed line shows one SE above minimum."</span></span>
<span id="cb36-253"><a href="#cb36-253" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-width: 7</span></span>
<span id="cb36-254"><a href="#cb36-254" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-height: 5</span></span>
<span id="cb36-255"><a href="#cb36-255" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit full tree</span></span>
<span id="cb36-256"><a href="#cb36-256" aria-hidden="true" tabindex="-1"></a>full_tree <span class="ot">&lt;-</span> <span class="fu">rpart</span>(Species <span class="sc">~</span> ., <span class="at">data =</span> iris, <span class="at">cp =</span> <span class="fl">0.001</span>)</span>
<span id="cb36-257"><a href="#cb36-257" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-258"><a href="#cb36-258" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot CV error vs complexity</span></span>
<span id="cb36-259"><a href="#cb36-259" aria-hidden="true" tabindex="-1"></a><span class="fu">plotcp</span>(full_tree)</span>
<span id="cb36-260"><a href="#cb36-260" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-261"><a href="#cb36-261" aria-hidden="true" tabindex="-1"></a><span class="co"># Print CP table</span></span>
<span id="cb36-262"><a href="#cb36-262" aria-hidden="true" tabindex="-1"></a><span class="fu">printcp</span>(full_tree)</span>
<span id="cb36-263"><a href="#cb36-263" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-264"><a href="#cb36-264" aria-hidden="true" tabindex="-1"></a><span class="co"># Prune to optimal cp</span></span>
<span id="cb36-265"><a href="#cb36-265" aria-hidden="true" tabindex="-1"></a>best_cp <span class="ot">&lt;-</span> full_tree<span class="sc">$</span>cptable[<span class="fu">which.min</span>(full_tree<span class="sc">$</span>cptable[, <span class="st">"xerror"</span>]), <span class="st">"CP"</span>]</span>
<span id="cb36-266"><a href="#cb36-266" aria-hidden="true" tabindex="-1"></a>pruned_tree <span class="ot">&lt;-</span> <span class="fu">prune</span>(full_tree, <span class="at">cp =</span> best_cp)</span>
<span id="cb36-267"><a href="#cb36-267" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb36-268"><a href="#cb36-268" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-269"><a href="#cb36-269" aria-hidden="true" tabindex="-1"></a><span class="fu">## Random Forests</span></span>
<span id="cb36-270"><a href="#cb36-270" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-271"><a href="#cb36-271" aria-hidden="true" tabindex="-1"></a>**Random forests** <span class="co">[</span><span class="ot">@breiman2001random</span><span class="co">]</span> address trees' high variance by averaging many trees. The key insight: averaging many noisy but unbiased estimators reduces variance.</span>
<span id="cb36-272"><a href="#cb36-272" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-273"><a href="#cb36-273" aria-hidden="true" tabindex="-1"></a><span class="fu">### The Random Forest Algorithm</span></span>
<span id="cb36-274"><a href="#cb36-274" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-275"><a href="#cb36-275" aria-hidden="true" tabindex="-1"></a>The random forest algorithm builds $B$ decision trees using the training set, but with two sources of randomness that make the trees different from one another.</span>
<span id="cb36-276"><a href="#cb36-276" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-277"><a href="#cb36-277" aria-hidden="true" tabindex="-1"></a>First, each tree is trained on a **bootstrap sample**—a sample of $n$ observations drawn with replacement from the training data. This means each tree sees a slightly different dataset. Second, at each split, the algorithm considers only a random subset of $m$ features rather than all available predictors. This "random feature selection" prevents all trees from splitting on the same strong predictors and produces more diverse trees.</span>
<span id="cb36-278"><a href="#cb36-278" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-279"><a href="#cb36-279" aria-hidden="true" tabindex="-1"></a>For prediction, the forest combines the predictions of all $B$ trees. In classification, the final prediction is the **majority vote** across all trees—whichever class is predicted most often wins. In regression, the final prediction is simply the **average** of all individual tree predictions.</span>
<span id="cb36-280"><a href="#cb36-280" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-283"><a href="#cb36-283" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb36-284"><a href="#cb36-284" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-rf-smoothing</span></span>
<span id="cb36-285"><a href="#cb36-285" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "Random forest predictions are smoother than single trees because averaging step functions produces smooth curves"</span></span>
<span id="cb36-286"><a href="#cb36-286" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-width: 9</span></span>
<span id="cb36-287"><a href="#cb36-287" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-height: 4</span></span>
<span id="cb36-288"><a href="#cb36-288" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">2</span>))</span>
<span id="cb36-289"><a href="#cb36-289" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-290"><a href="#cb36-290" aria-hidden="true" tabindex="-1"></a><span class="co"># Single tree</span></span>
<span id="cb36-291"><a href="#cb36-291" aria-hidden="true" tabindex="-1"></a>fit_tree <span class="ot">&lt;-</span> <span class="fu">rpart</span>(margin <span class="sc">~</span> ., <span class="at">data =</span> polls_2008)</span>
<span id="cb36-292"><a href="#cb36-292" aria-hidden="true" tabindex="-1"></a>polls_2008 <span class="sc">%&gt;%</span></span>
<span id="cb36-293"><a href="#cb36-293" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">y_hat =</span> <span class="fu">predict</span>(fit_tree)) <span class="sc">%&gt;%</span></span>
<span id="cb36-294"><a href="#cb36-294" aria-hidden="true" tabindex="-1"></a>  <span class="fu">with</span>(<span class="fu">plot</span>(day, margin, <span class="at">pch =</span> <span class="dv">16</span>, <span class="at">cex =</span> <span class="fl">0.5</span>,</span>
<span id="cb36-295"><a href="#cb36-295" aria-hidden="true" tabindex="-1"></a>            <span class="at">main =</span> <span class="st">"Single Regression Tree"</span>))</span>
<span id="cb36-296"><a href="#cb36-296" aria-hidden="true" tabindex="-1"></a>polls_2008 <span class="sc">%&gt;%</span></span>
<span id="cb36-297"><a href="#cb36-297" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">y_hat =</span> <span class="fu">predict</span>(fit_tree)) <span class="sc">%&gt;%</span></span>
<span id="cb36-298"><a href="#cb36-298" aria-hidden="true" tabindex="-1"></a>  <span class="fu">with</span>(<span class="fu">lines</span>(day, y_hat, <span class="at">col =</span> <span class="st">"red"</span>, <span class="at">type =</span> <span class="st">"s"</span>, <span class="at">lwd =</span> <span class="dv">2</span>))</span>
<span id="cb36-299"><a href="#cb36-299" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-300"><a href="#cb36-300" aria-hidden="true" tabindex="-1"></a><span class="co"># Random forest</span></span>
<span id="cb36-301"><a href="#cb36-301" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">42</span>)</span>
<span id="cb36-302"><a href="#cb36-302" aria-hidden="true" tabindex="-1"></a>fit_rf <span class="ot">&lt;-</span> <span class="fu">randomForest</span>(margin <span class="sc">~</span> ., <span class="at">data =</span> polls_2008)</span>
<span id="cb36-303"><a href="#cb36-303" aria-hidden="true" tabindex="-1"></a>polls_2008 <span class="sc">%&gt;%</span></span>
<span id="cb36-304"><a href="#cb36-304" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">y_hat =</span> <span class="fu">predict</span>(fit_rf, <span class="at">newdata =</span> polls_2008)) <span class="sc">%&gt;%</span></span>
<span id="cb36-305"><a href="#cb36-305" aria-hidden="true" tabindex="-1"></a>  <span class="fu">with</span>(<span class="fu">plot</span>(day, margin, <span class="at">pch =</span> <span class="dv">16</span>, <span class="at">cex =</span> <span class="fl">0.5</span>,</span>
<span id="cb36-306"><a href="#cb36-306" aria-hidden="true" tabindex="-1"></a>            <span class="at">main =</span> <span class="st">"Random Forest"</span>))</span>
<span id="cb36-307"><a href="#cb36-307" aria-hidden="true" tabindex="-1"></a>polls_2008 <span class="sc">%&gt;%</span></span>
<span id="cb36-308"><a href="#cb36-308" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">y_hat =</span> <span class="fu">predict</span>(fit_rf, <span class="at">newdata =</span> polls_2008)) <span class="sc">%&gt;%</span></span>
<span id="cb36-309"><a href="#cb36-309" aria-hidden="true" tabindex="-1"></a>  <span class="fu">with</span>(<span class="fu">lines</span>(day, y_hat, <span class="at">col =</span> <span class="st">"blue"</span>, <span class="at">lwd =</span> <span class="dv">2</span>))</span>
<span id="cb36-310"><a href="#cb36-310" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb36-311"><a href="#cb36-311" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-312"><a href="#cb36-312" aria-hidden="true" tabindex="-1"></a><span class="fu">### Random Forests in R</span></span>
<span id="cb36-313"><a href="#cb36-313" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-316"><a href="#cb36-316" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb36-317"><a href="#cb36-317" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-rf-model</span></span>
<span id="cb36-318"><a href="#cb36-318" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "Random forest OOB error rate decreasing as more trees are added"</span></span>
<span id="cb36-319"><a href="#cb36-319" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-width: 7</span></span>
<span id="cb36-320"><a href="#cb36-320" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-height: 5</span></span>
<span id="cb36-321"><a href="#cb36-321" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">42</span>)</span>
<span id="cb36-322"><a href="#cb36-322" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-323"><a href="#cb36-323" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit random forest</span></span>
<span id="cb36-324"><a href="#cb36-324" aria-hidden="true" tabindex="-1"></a>rf_model <span class="ot">&lt;-</span> <span class="fu">randomForest</span>(Species <span class="sc">~</span> ., <span class="at">data =</span> iris,</span>
<span id="cb36-325"><a href="#cb36-325" aria-hidden="true" tabindex="-1"></a>                          <span class="at">ntree =</span> <span class="dv">500</span>,       <span class="co"># Number of trees</span></span>
<span id="cb36-326"><a href="#cb36-326" aria-hidden="true" tabindex="-1"></a>                          <span class="at">mtry =</span> <span class="dv">2</span>,          <span class="co"># Features tried at each split</span></span>
<span id="cb36-327"><a href="#cb36-327" aria-hidden="true" tabindex="-1"></a>                          <span class="at">importance =</span> <span class="cn">TRUE</span>)  <span class="co"># Calculate variable importance</span></span>
<span id="cb36-328"><a href="#cb36-328" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-329"><a href="#cb36-329" aria-hidden="true" tabindex="-1"></a><span class="co"># Model summary</span></span>
<span id="cb36-330"><a href="#cb36-330" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(rf_model)</span>
<span id="cb36-331"><a href="#cb36-331" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-332"><a href="#cb36-332" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot error vs number of trees</span></span>
<span id="cb36-333"><a href="#cb36-333" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(rf_model, <span class="at">main =</span> <span class="st">"Random Forest: Error vs. Number of Trees"</span>)</span>
<span id="cb36-334"><a href="#cb36-334" aria-hidden="true" tabindex="-1"></a><span class="fu">legend</span>(<span class="st">"topright"</span>, <span class="fu">colnames</span>(rf_model<span class="sc">$</span>err.rate), <span class="at">col =</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">4</span>, <span class="at">lty =</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">4</span>)</span>
<span id="cb36-335"><a href="#cb36-335" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb36-336"><a href="#cb36-336" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-337"><a href="#cb36-337" aria-hidden="true" tabindex="-1"></a><span class="fu">### Out-of-Bag (OOB) Error</span></span>
<span id="cb36-338"><a href="#cb36-338" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-339"><a href="#cb36-339" aria-hidden="true" tabindex="-1"></a>Each bootstrap sample uses about 63% of observations. The remaining 37% (**out-of-bag** samples) provide a built-in test set:</span>
<span id="cb36-340"><a href="#cb36-340" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-343"><a href="#cb36-343" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb36-344"><a href="#cb36-344" aria-hidden="true" tabindex="-1"></a><span class="co"># OOB confusion matrix</span></span>
<span id="cb36-345"><a href="#cb36-345" aria-hidden="true" tabindex="-1"></a>rf_model<span class="sc">$</span>confusion</span>
<span id="cb36-346"><a href="#cb36-346" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-347"><a href="#cb36-347" aria-hidden="true" tabindex="-1"></a><span class="co"># OOB error rate</span></span>
<span id="cb36-348"><a href="#cb36-348" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"OOB Error Rate:"</span>, <span class="fu">round</span>(<span class="dv">1</span> <span class="sc">-</span> <span class="fu">sum</span>(<span class="fu">diag</span>(rf_model<span class="sc">$</span>confusion[,<span class="dv">1</span><span class="sc">:</span><span class="dv">3</span>])) <span class="sc">/</span></span>
<span id="cb36-349"><a href="#cb36-349" aria-hidden="true" tabindex="-1"></a>                              <span class="fu">sum</span>(rf_model<span class="sc">$</span>confusion[,<span class="dv">1</span><span class="sc">:</span><span class="dv">3</span>]), <span class="dv">3</span>), <span class="st">"</span><span class="sc">\n</span><span class="st">"</span>)</span>
<span id="cb36-350"><a href="#cb36-350" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb36-351"><a href="#cb36-351" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-352"><a href="#cb36-352" aria-hidden="true" tabindex="-1"></a>OOB error is nearly as accurate as cross-validation but comes "for free" during training.</span>
<span id="cb36-353"><a href="#cb36-353" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-354"><a href="#cb36-354" aria-hidden="true" tabindex="-1"></a><span class="fu">### Variable Importance</span></span>
<span id="cb36-355"><a href="#cb36-355" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-356"><a href="#cb36-356" aria-hidden="true" tabindex="-1"></a>Random forests provide measures of predictor importance:</span>
<span id="cb36-357"><a href="#cb36-357" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-360"><a href="#cb36-360" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb36-361"><a href="#cb36-361" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-rf-importance</span></span>
<span id="cb36-362"><a href="#cb36-362" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "Variable importance: Mean Decrease Accuracy measures how much removing a variable hurts prediction; Mean Decrease Gini measures total reduction in node impurity"</span></span>
<span id="cb36-363"><a href="#cb36-363" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-width: 8</span></span>
<span id="cb36-364"><a href="#cb36-364" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-height: 5</span></span>
<span id="cb36-365"><a href="#cb36-365" aria-hidden="true" tabindex="-1"></a><span class="co"># Variable importance plot</span></span>
<span id="cb36-366"><a href="#cb36-366" aria-hidden="true" tabindex="-1"></a><span class="fu">varImpPlot</span>(rf_model, <span class="at">main =</span> <span class="st">"Variable Importance"</span>)</span>
<span id="cb36-367"><a href="#cb36-367" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-368"><a href="#cb36-368" aria-hidden="true" tabindex="-1"></a><span class="co"># Numeric importance values</span></span>
<span id="cb36-369"><a href="#cb36-369" aria-hidden="true" tabindex="-1"></a><span class="fu">importance</span>(rf_model)</span>
<span id="cb36-370"><a href="#cb36-370" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb36-371"><a href="#cb36-371" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-372"><a href="#cb36-372" aria-hidden="true" tabindex="-1"></a>**Mean Decrease Accuracy**: For each tree, predictions are made on OOB samples. Then variable $j$ is randomly permuted and predictions are made again. The decrease in accuracy measures importance.</span>
<span id="cb36-373"><a href="#cb36-373" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-374"><a href="#cb36-374" aria-hidden="true" tabindex="-1"></a>**Mean Decrease Gini**: Total decrease in Gini impurity from splits on variable $j$, averaged over all trees.</span>
<span id="cb36-375"><a href="#cb36-375" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-376"><a href="#cb36-376" aria-hidden="true" tabindex="-1"></a><span class="fu">### Tuning Random Forests</span></span>
<span id="cb36-377"><a href="#cb36-377" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-378"><a href="#cb36-378" aria-hidden="true" tabindex="-1"></a>Random forests have several key parameters that can be tuned to optimize performance.</span>
<span id="cb36-379"><a href="#cb36-379" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-380"><a href="#cb36-380" aria-hidden="true" tabindex="-1"></a>The **ntree** parameter specifies the number of trees in the forest. Generally, more trees are better—the error decreases and stabilizes as trees are added—but with diminishing returns beyond a certain point. The computational cost grows linearly with the number of trees, so practical considerations often limit this to a few hundred or thousand trees.</span>
<span id="cb36-381"><a href="#cb36-381" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-382"><a href="#cb36-382" aria-hidden="true" tabindex="-1"></a>The **mtry** parameter determines how many features are randomly considered at each split. Smaller values increase the diversity among trees (reducing correlation), while larger values allow each tree to use the strongest predictors more often. The **nodesize** parameter controls the minimum size of terminal nodes, affecting how deep trees can grow. Smaller values allow trees to grow deeper and capture more complex patterns, but may overfit.</span>
<span id="cb36-383"><a href="#cb36-383" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-386"><a href="#cb36-386" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb36-387"><a href="#cb36-387" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-rf-tuning</span></span>
<span id="cb36-388"><a href="#cb36-388" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "Random forest OOB error as a function of mtry"</span></span>
<span id="cb36-389"><a href="#cb36-389" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-width: 7</span></span>
<span id="cb36-390"><a href="#cb36-390" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-height: 5</span></span>
<span id="cb36-391"><a href="#cb36-391" aria-hidden="true" tabindex="-1"></a><span class="co"># Tune mtry</span></span>
<span id="cb36-392"><a href="#cb36-392" aria-hidden="true" tabindex="-1"></a>oob_error <span class="ot">&lt;-</span> <span class="fu">sapply</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">4</span>, <span class="cf">function</span>(m) {</span>
<span id="cb36-393"><a href="#cb36-393" aria-hidden="true" tabindex="-1"></a>  rf <span class="ot">&lt;-</span> <span class="fu">randomForest</span>(Species <span class="sc">~</span> ., <span class="at">data =</span> iris, <span class="at">mtry =</span> m, <span class="at">ntree =</span> <span class="dv">200</span>)</span>
<span id="cb36-394"><a href="#cb36-394" aria-hidden="true" tabindex="-1"></a>  rf<span class="sc">$</span>err.rate[<span class="dv">200</span>, <span class="st">"OOB"</span>]</span>
<span id="cb36-395"><a href="#cb36-395" aria-hidden="true" tabindex="-1"></a>})</span>
<span id="cb36-396"><a href="#cb36-396" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-397"><a href="#cb36-397" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">4</span>, oob_error, <span class="at">type =</span> <span class="st">"b"</span>, <span class="at">pch =</span> <span class="dv">19</span>,</span>
<span id="cb36-398"><a href="#cb36-398" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">"mtry (features at each split)"</span>,</span>
<span id="cb36-399"><a href="#cb36-399" aria-hidden="true" tabindex="-1"></a>     <span class="at">ylab =</span> <span class="st">"OOB Error Rate"</span>,</span>
<span id="cb36-400"><a href="#cb36-400" aria-hidden="true" tabindex="-1"></a>     <span class="at">main =</span> <span class="st">"Tuning mtry Parameter"</span>)</span>
<span id="cb36-401"><a href="#cb36-401" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb36-402"><a href="#cb36-402" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-403"><a href="#cb36-403" aria-hidden="true" tabindex="-1"></a>Default values for mtry are often reasonable starting points. For classification problems, the default is $m = \sqrt{p}$, the square root of the number of predictors. For regression problems, the default is $m = p/3$, one-third of the predictors. These defaults strike a balance between allowing strong predictors to contribute while maintaining diversity among trees.</span>
<span id="cb36-404"><a href="#cb36-404" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-405"><a href="#cb36-405" aria-hidden="true" tabindex="-1"></a><span class="fu">### Random Forest for Regression</span></span>
<span id="cb36-406"><a href="#cb36-406" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-409"><a href="#cb36-409" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb36-410"><a href="#cb36-410" aria-hidden="true" tabindex="-1"></a><span class="co"># Regression random forest</span></span>
<span id="cb36-411"><a href="#cb36-411" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">42</span>)</span>
<span id="cb36-412"><a href="#cb36-412" aria-hidden="true" tabindex="-1"></a>rf_reg <span class="ot">&lt;-</span> <span class="fu">randomForest</span>(mpg <span class="sc">~</span> ., <span class="at">data =</span> mtcars, <span class="at">ntree =</span> <span class="dv">500</span>, <span class="at">importance =</span> <span class="cn">TRUE</span>)</span>
<span id="cb36-413"><a href="#cb36-413" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-414"><a href="#cb36-414" aria-hidden="true" tabindex="-1"></a><span class="co"># Performance</span></span>
<span id="cb36-415"><a href="#cb36-415" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"Variance explained:"</span>, <span class="fu">round</span>(rf_reg<span class="sc">$</span>rsq[<span class="dv">500</span>] <span class="sc">*</span> <span class="dv">100</span>, <span class="dv">1</span>), <span class="st">"%</span><span class="sc">\n</span><span class="st">"</span>)</span>
<span id="cb36-416"><a href="#cb36-416" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"MSE:"</span>, <span class="fu">round</span>(rf_reg<span class="sc">$</span>mse[<span class="dv">500</span>], <span class="dv">2</span>), <span class="st">"</span><span class="sc">\n</span><span class="st">"</span>)</span>
<span id="cb36-417"><a href="#cb36-417" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-418"><a href="#cb36-418" aria-hidden="true" tabindex="-1"></a><span class="co"># Variable importance</span></span>
<span id="cb36-419"><a href="#cb36-419" aria-hidden="true" tabindex="-1"></a><span class="fu">varImpPlot</span>(rf_reg, <span class="at">main =</span> <span class="st">"Variable Importance for MPG Prediction"</span>)</span>
<span id="cb36-420"><a href="#cb36-420" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb36-421"><a href="#cb36-421" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-422"><a href="#cb36-422" aria-hidden="true" tabindex="-1"></a><span class="fu">## Comparing Trees and Forests</span></span>
<span id="cb36-423"><a href="#cb36-423" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-424"><a href="#cb36-424" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> Aspect <span class="pp">|</span> Decision Tree <span class="pp">|</span> Random Forest <span class="pp">|</span></span>
<span id="cb36-425"><a href="#cb36-425" aria-hidden="true" tabindex="-1"></a><span class="pp">|:-------|:--------------|:--------------|</span></span>
<span id="cb36-426"><a href="#cb36-426" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **Interpretability** <span class="pp">|</span> High <span class="pp">|</span> Medium <span class="pp">|</span></span>
<span id="cb36-427"><a href="#cb36-427" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **Variance** <span class="pp">|</span> High <span class="pp">|</span> Low <span class="pp">|</span></span>
<span id="cb36-428"><a href="#cb36-428" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **Bias** <span class="pp">|</span> Medium <span class="pp">|</span> Medium-Low <span class="pp">|</span></span>
<span id="cb36-429"><a href="#cb36-429" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **Overfitting** <span class="pp">|</span> Prone <span class="pp">|</span> Resistant <span class="pp">|</span></span>
<span id="cb36-430"><a href="#cb36-430" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **Speed** <span class="pp">|</span> Fast <span class="pp">|</span> Slower <span class="pp">|</span></span>
<span id="cb36-431"><a href="#cb36-431" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **Variable importance** <span class="pp">|</span> Limited <span class="pp">|</span> Built-in <span class="pp">|</span></span>
<span id="cb36-432"><a href="#cb36-432" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-433"><a href="#cb36-433" aria-hidden="true" tabindex="-1"></a><span class="fu">### When to Use Each Method</span></span>
<span id="cb36-434"><a href="#cb36-434" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-435"><a href="#cb36-435" aria-hidden="true" tabindex="-1"></a>The choice between decision trees and random forests depends on your priorities.</span>
<span id="cb36-436"><a href="#cb36-436" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-437"><a href="#cb36-437" aria-hidden="true" tabindex="-1"></a>**Decision trees** are the right choice when interpretability is paramount. If you need to explain the model to stakeholders—clinicians, regulators, or business managers—a single tree can be drawn as a simple flow chart that anyone can follow. Trees are also valuable during exploratory analysis, when you want to understand which variables matter and how they interact before committing to a more complex model.</span>
<span id="cb36-438"><a href="#cb36-438" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-439"><a href="#cb36-439" aria-hidden="true" tabindex="-1"></a>**Random forests** are preferred when prediction accuracy is the primary goal. They consistently outperform single trees across a wide range of problems. They work best when you have sufficient data to support averaging many trees, and when you want built-in variable importance measures to understand which predictors contribute most to predictions. The tradeoff is that random forests are less interpretable—you can no longer point to a single decision rule and explain exactly why a prediction was made.</span>
<span id="cb36-440"><a href="#cb36-440" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-441"><a href="#cb36-441" aria-hidden="true" tabindex="-1"></a><span class="fu">## Exercises</span></span>
<span id="cb36-442"><a href="#cb36-442" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-443"><a href="#cb36-443" aria-hidden="true" tabindex="-1"></a>::: {.callout-note}</span>
<span id="cb36-444"><a href="#cb36-444" aria-hidden="true" tabindex="-1"></a><span class="fu">### Exercise TF.1: Decision Trees</span></span>
<span id="cb36-445"><a href="#cb36-445" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-446"><a href="#cb36-446" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>Create a simple dataset:</span>
<span id="cb36-449"><a href="#cb36-449" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb36-450"><a href="#cb36-450" aria-hidden="true" tabindex="-1"></a><span class="co">#| eval: false</span></span>
<span id="cb36-451"><a href="#cb36-451" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">1000</span></span>
<span id="cb36-452"><a href="#cb36-452" aria-hidden="true" tabindex="-1"></a>sigma <span class="ot">&lt;-</span> <span class="fl">0.25</span></span>
<span id="cb36-453"><a href="#cb36-453" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(n, <span class="dv">0</span>, <span class="dv">1</span>)</span>
<span id="cb36-454"><a href="#cb36-454" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> <span class="fl">0.75</span> <span class="sc">*</span> x <span class="sc">+</span> <span class="fu">rnorm</span>(n, <span class="dv">0</span>, sigma)</span>
<span id="cb36-455"><a href="#cb36-455" aria-hidden="true" tabindex="-1"></a>dat <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">x =</span> x, <span class="at">y =</span> y)</span>
<span id="cb36-456"><a href="#cb36-456" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb36-457"><a href="#cb36-457" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-458"><a href="#cb36-458" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>Use <span class="in">`rpart`</span> to fit a regression tree. Plot the tree.</span>
<span id="cb36-459"><a href="#cb36-459" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-460"><a href="#cb36-460" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>Make a scatterplot of y vs x with the predicted values overlaid.</span>
<span id="cb36-461"><a href="#cb36-461" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-462"><a href="#cb36-462" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>Try different values of <span class="in">`cp`</span>. How does tree complexity change?</span>
<span id="cb36-463"><a href="#cb36-463" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb36-464"><a href="#cb36-464" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-465"><a href="#cb36-465" aria-hidden="true" tabindex="-1"></a>::: {.callout-note}</span>
<span id="cb36-466"><a href="#cb36-466" aria-hidden="true" tabindex="-1"></a><span class="fu">### Exercise TF.2: Random Forests</span></span>
<span id="cb36-467"><a href="#cb36-467" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-468"><a href="#cb36-468" aria-hidden="true" tabindex="-1"></a><span class="ss">5. </span>Using the same data, fit a random forest with <span class="in">`randomForest`</span>.</span>
<span id="cb36-469"><a href="#cb36-469" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-470"><a href="#cb36-470" aria-hidden="true" tabindex="-1"></a><span class="ss">6. </span>Plot the predictions. How do they differ from the single tree?</span>
<span id="cb36-471"><a href="#cb36-471" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-472"><a href="#cb36-472" aria-hidden="true" tabindex="-1"></a><span class="ss">7. </span>Use <span class="in">`plot(rf)`</span> to check if the forest has converged.</span>
<span id="cb36-473"><a href="#cb36-473" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-474"><a href="#cb36-474" aria-hidden="true" tabindex="-1"></a><span class="ss">8. </span>Experiment with <span class="in">`nodesize`</span> and <span class="in">`maxnodes`</span> to control smoothness.</span>
<span id="cb36-475"><a href="#cb36-475" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb36-476"><a href="#cb36-476" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-477"><a href="#cb36-477" aria-hidden="true" tabindex="-1"></a>::: {.callout-note}</span>
<span id="cb36-478"><a href="#cb36-478" aria-hidden="true" tabindex="-1"></a><span class="fu">### Exercise TF.3: Classification Trees</span></span>
<span id="cb36-479"><a href="#cb36-479" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-480"><a href="#cb36-480" aria-hidden="true" tabindex="-1"></a><span class="ss">9. </span>Use the <span class="in">`tissue_gene_expression`</span> dataset:</span>
<span id="cb36-483"><a href="#cb36-483" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb36-484"><a href="#cb36-484" aria-hidden="true" tabindex="-1"></a><span class="co">#| eval: false</span></span>
<span id="cb36-485"><a href="#cb36-485" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(dslabs)</span>
<span id="cb36-486"><a href="#cb36-486" aria-hidden="true" tabindex="-1"></a><span class="fu">data</span>(<span class="st">"tissue_gene_expression"</span>)</span>
<span id="cb36-487"><a href="#cb36-487" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb36-488"><a href="#cb36-488" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-489"><a href="#cb36-489" aria-hidden="true" tabindex="-1"></a><span class="ss">10. </span>Fit a classification tree using <span class="in">`caret::train`</span> with <span class="in">`method = "rpart"`</span>. Use cross-validation to select optimal <span class="in">`cp`</span>.</span>
<span id="cb36-490"><a href="#cb36-490" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-491"><a href="#cb36-491" aria-hidden="true" tabindex="-1"></a><span class="ss">11. </span>Examine the confusion matrix. Which tissues are most often confused?</span>
<span id="cb36-492"><a href="#cb36-492" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-493"><a href="#cb36-493" aria-hidden="true" tabindex="-1"></a><span class="ss">12. </span>Compare to a random forest. Does it improve accuracy?</span>
<span id="cb36-494"><a href="#cb36-494" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb36-495"><a href="#cb36-495" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-496"><a href="#cb36-496" aria-hidden="true" tabindex="-1"></a><span class="fu">## Summary</span></span>
<span id="cb36-497"><a href="#cb36-497" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-498"><a href="#cb36-498" aria-hidden="true" tabindex="-1"></a>This chapter introduced **decision trees** and **random forests**, two of the most widely used machine learning methods.</span>
<span id="cb36-499"><a href="#cb36-499" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-500"><a href="#cb36-500" aria-hidden="true" tabindex="-1"></a>**Decision trees (CART)** recursively partition the data using simple yes/no rules based on predictor values. Their greatest strength is interpretability—a tree can be drawn as a flow chart that anyone can understand. They handle non-linear relationships and interactions automatically through their splitting structure, without requiring you to specify these features in advance. However, individual trees are prone to overfitting without careful tuning. The complexity parameter (cp), minsplit, and maxdepth control how complex a tree can become, and cross-validation helps select the optimal level of complexity.</span>
<span id="cb36-501"><a href="#cb36-501" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-502"><a href="#cb36-502" aria-hidden="true" tabindex="-1"></a>**Random forests** combine many trees to achieve robust prediction. By training each tree on a bootstrap sample and considering only a random subset of features at each split, the algorithm produces diverse trees whose errors tend to cancel out when averaged. The out-of-bag (OOB) error provides a built-in estimate of generalization performance without needing a separate validation set. Variable importance measures identify which predictors contribute most to predictions.</span>
<span id="cb36-503"><a href="#cb36-503" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-504"><a href="#cb36-504" aria-hidden="true" tabindex="-1"></a>The fundamental tradeoff is between interpretability and accuracy. Individual trees have high variance—small changes in the training data can produce very different trees—but they are easy to interpret. Random forests reduce variance through averaging, achieving substantially better prediction accuracy, but sacrifice the simple interpretability of a single tree. For maximum interpretability, use a carefully pruned decision tree. For maximum prediction accuracy, use a random forest.</span>
<span id="cb36-505"><a href="#cb36-505" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-506"><a href="#cb36-506" aria-hidden="true" tabindex="-1"></a><span class="fu">## Additional Resources</span></span>
<span id="cb36-507"><a href="#cb36-507" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-508"><a href="#cb36-508" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>@james2023islr - Comprehensive treatment of tree-based methods</span>
<span id="cb36-509"><a href="#cb36-509" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>@hastie2009elements - Theoretical foundations</span>
<span id="cb36-510"><a href="#cb36-510" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>@breiman2001random - Original random forest paper</span>
<span id="cb36-511"><a href="#cb36-511" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="in">`rpart`</span> and <span class="in">`randomForest`</span> package documentation</span>
</code><button title="Copy to Clipboard" class="code-copy-button" data-in-quarto-modal=""><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p>Statistics for Biosciences and Bioengineering</p>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    </div>
    <div class="nav-footer-right">
<p>Built with <a href="https://quarto.org/">Quarto</a></p>
</div>
  </div>
</footer>




</body></html>