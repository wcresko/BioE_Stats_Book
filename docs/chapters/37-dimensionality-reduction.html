<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.30">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>38&nbsp; Dimensionality Reduction and Multivariate Methods – Statistics for the Biosciences and Bioengineering</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../chapters/38-tsne-umap.html" rel="next">
<link href="../chapters/36-clustering.html" rel="prev">
<script src="../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-de070a7b0ab54f8780927367ac907214.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap-485d01fc63b59abcd3ee1bf1e8e2748d.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar floating quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../chapters/29-intro-statistical-learning.html">Statistical Learning</a></li><li class="breadcrumb-item"><a href="../chapters/37-dimensionality-reduction.html"><span class="chapter-number">38</span>&nbsp; <span class="chapter-title">Dimensionality Reduction and Multivariate Methods</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../">Statistics for the Biosciences and Bioengineering</a> 
        <div class="sidebar-tools-main">
  <a href="" class="quarto-reader-toggle quarto-navigation-tool px-1" onclick="window.quartoToggleReader(); return false;" title="Toggle reader mode">
  <div class="quarto-reader-toggle-btn">
  <i class="bi"></i>
  </div>
</a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Why This Book?</span></span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">Core Data Science Tools</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/01-introduction.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Introduction</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/02-installing-tools.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Installing Core Tools</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/03-unix-command-line.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Unix and the Command Line</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/04-r-rstudio.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">R and RStudio</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/05-markdown-latex.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Markdown and LaTeX</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true">
 <span class="menu-text">Data Exploration</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/06-tidy-data.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Tidy Data</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/07-data-wrangling.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Data Wrangling with dplyr</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/08-exploratory-data-analysis.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Exploratory Data Analysis</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/09-data-visualization.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Data Visualization</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true">
 <span class="menu-text">Probability and Distributions</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/10-probability-foundations.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Foundations of Probability</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/11-discrete-distributions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Discrete Probability Distributions</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/12-continuous-distributions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Continuous Probability Distributions</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/13-sampling-estimation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Sampling and Parameter Estimation</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true">
 <span class="menu-text">Statistical Inference</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/14-experimental-design.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Experimental Design Principles</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/15-hypothesis-testing.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Hypothesis Testing</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/16-t-tests.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">T-Tests</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/17-nonparametric-tests.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Nonparametric Tests</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/18-bootstrapping.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">Resampling Methods</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/19-presenting-results.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">Presenting Statistical Results</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="true">
 <span class="menu-text">Linear Models</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/20-what-are-models.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">21</span>&nbsp; <span class="chapter-title">What are Models?</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/21-correlation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">22</span>&nbsp; <span class="chapter-title">Correlation</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/22-simple-linear-regression.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">23</span>&nbsp; <span class="chapter-title">Simple Linear Regression</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/23-residual-analysis.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">24</span>&nbsp; <span class="chapter-title">Residual Analysis</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/24-statistical-power.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">25</span>&nbsp; <span class="chapter-title">Statistical Power</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/25-multiple-regression.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">26</span>&nbsp; <span class="chapter-title">Multiple Regression</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/26-single-factor-anova.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">27</span>&nbsp; <span class="chapter-title">Single Factor ANOVA</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/27-multifactor-anova.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">28</span>&nbsp; <span class="chapter-title">Multi-Factor ANOVA</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/28-glm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">29</span>&nbsp; <span class="chapter-title">Generalized Linear Models</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="true">
 <span class="menu-text">Statistical Learning</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-6" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/29-intro-statistical-learning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">30</span>&nbsp; <span class="chapter-title">Introduction to Statistical Learning</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/30-model-validation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">31</span>&nbsp; <span class="chapter-title">Model Validation and the Bias-Variance Tradeoff</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/31-regularization.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">32</span>&nbsp; <span class="chapter-title">Regularization Methods</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/32-smoothing.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">33</span>&nbsp; <span class="chapter-title">Smoothing and Non-Parametric Regression</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/33-classification-methods.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">34</span>&nbsp; <span class="chapter-title">Classification and Performance Metrics</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/34-trees-forests.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">35</span>&nbsp; <span class="chapter-title">Decision Trees and Random Forests</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/35-svm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">36</span>&nbsp; <span class="chapter-title">Support Vector Machines</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/36-clustering.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">37</span>&nbsp; <span class="chapter-title">Clustering</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/37-dimensionality-reduction.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">38</span>&nbsp; <span class="chapter-title">Dimensionality Reduction and Multivariate Methods</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/38-tsne-umap.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">39</span>&nbsp; <span class="chapter-title">Non-Linear Dimensionality Reduction: t-SNE and UMAP</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/39-bayesian-statistics.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">40</span>&nbsp; <span class="chapter-title">Bayesian Statistics</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/40-deep-learning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">41</span>&nbsp; <span class="chapter-title">Introduction to Deep Learning</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="true">
 <span class="menu-text">Appendices</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-7" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/A1-eugenics-history.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">42</span>&nbsp; <span class="chapter-title">The Eugenics History of Statistics</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/A2-keyboard-shortcuts.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">43</span>&nbsp; <span class="chapter-title">Keyboard Shortcuts Reference</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/A3-unix-reference.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">44</span>&nbsp; <span class="chapter-title">Unix Command Reference</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/A4-r-reference.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">45</span>&nbsp; <span class="chapter-title">R Command Reference</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/A5-quarto-reference.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">46</span>&nbsp; <span class="chapter-title">Quarto Markdown Reference</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/A6-latex-reference.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">47</span>&nbsp; <span class="chapter-title">LaTeX Command Reference</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/A7-greek-letters.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">48</span>&nbsp; <span class="chapter-title">Greek Letters in Mathematics and Statistics</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/A8-probability-distributions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">49</span>&nbsp; <span class="chapter-title">Common Probability Distributions</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/A9-sampling-distributions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">50</span>&nbsp; <span class="chapter-title">Sampling Distributions in Hypothesis Testing</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/A10-matrix-algebra.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">51</span>&nbsp; <span class="chapter-title">Matrix Algebra Fundamentals</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/A11-high-performance-computing.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">52</span>&nbsp; <span class="chapter-title">High Performance Computing</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#the-challenge-of-high-dimensional-data" id="toc-the-challenge-of-high-dimensional-data" class="nav-link active" data-scroll-target="#the-challenge-of-high-dimensional-data"><span class="header-section-number">38.1</span> The Challenge of High-Dimensional Data</a></li>
  <li><a href="#principal-component-analysis-pca" id="toc-principal-component-analysis-pca" class="nav-link" data-scroll-target="#principal-component-analysis-pca"><span class="header-section-number">38.2</span> Principal Component Analysis (PCA)</a>
  <ul class="collapse">
  <li><a href="#intuition-preserving-distance" id="toc-intuition-preserving-distance" class="nav-link" data-scroll-target="#intuition-preserving-distance">Intuition: Preserving Distance</a></li>
  <li><a href="#linear-transformations" id="toc-linear-transformations" class="nav-link" data-scroll-target="#linear-transformations">Linear Transformations</a></li>
  <li><a href="#orthogonal-transformations" id="toc-orthogonal-transformations" class="nav-link" data-scroll-target="#orthogonal-transformations">Orthogonal Transformations</a></li>
  <li><a href="#the-eigenanalysis-foundation" id="toc-the-eigenanalysis-foundation" class="nav-link" data-scroll-target="#the-eigenanalysis-foundation">The Eigenanalysis Foundation</a></li>
  <li><a href="#pca-in-r" id="toc-pca-in-r" class="nav-link" data-scroll-target="#pca-in-r">PCA in R</a></li>
  <li><a href="#loadings-what-variables-drive-each-pc" id="toc-loadings-what-variables-drive-each-pc" class="nav-link" data-scroll-target="#loadings-what-variables-drive-each-pc">Loadings: What Variables Drive Each PC?</a></li>
  <li><a href="#interpreting-pca-results" id="toc-interpreting-pca-results" class="nav-link" data-scroll-target="#interpreting-pca-results">Interpreting PCA Results</a></li>
  <li><a href="#biplot-visualization" id="toc-biplot-visualization" class="nav-link" data-scroll-target="#biplot-visualization">Biplot Visualization</a></li>
  <li><a href="#mnist-example-high-dimensional-image-data" id="toc-mnist-example-high-dimensional-image-data" class="nav-link" data-scroll-target="#mnist-example-high-dimensional-image-data">MNIST Example: High-Dimensional Image Data</a></li>
  <li><a href="#applying-pca-to-improve-classification" id="toc-applying-pca-to-improve-classification" class="nav-link" data-scroll-target="#applying-pca-to-improve-classification">Applying PCA to Improve Classification</a></li>
  </ul></li>
  <li><a href="#principal-coordinate-analysis-pcoa" id="toc-principal-coordinate-analysis-pcoa" class="nav-link" data-scroll-target="#principal-coordinate-analysis-pcoa"><span class="header-section-number">38.3</span> Principal Coordinate Analysis (PCoA)</a>
  <ul class="collapse">
  <li><a href="#when-to-use-pcoa-vs.-pca" id="toc-when-to-use-pcoa-vs.-pca" class="nav-link" data-scroll-target="#when-to-use-pcoa-vs.-pca">When to Use PCoA vs.&nbsp;PCA</a></li>
  </ul></li>
  <li><a href="#non-metric-multidimensional-scaling-nmds" id="toc-non-metric-multidimensional-scaling-nmds" class="nav-link" data-scroll-target="#non-metric-multidimensional-scaling-nmds"><span class="header-section-number">38.4</span> Non-Metric Multidimensional Scaling (NMDS)</a></li>
  <li><a href="#manova-multivariate-analysis-of-variance" id="toc-manova-multivariate-analysis-of-variance" class="nav-link" data-scroll-target="#manova-multivariate-analysis-of-variance"><span class="header-section-number">38.5</span> MANOVA: Multivariate Analysis of Variance</a>
  <ul class="collapse">
  <li><a href="#why-not-multiple-anovas" id="toc-why-not-multiple-anovas" class="nav-link" data-scroll-target="#why-not-multiple-anovas">Why Not Multiple ANOVAs?</a></li>
  <li><a href="#the-manova-framework" id="toc-the-manova-framework" class="nav-link" data-scroll-target="#the-manova-framework">The MANOVA Framework</a></li>
  <li><a href="#test-statistics" id="toc-test-statistics" class="nav-link" data-scroll-target="#test-statistics">Test Statistics</a></li>
  <li><a href="#manova-in-r" id="toc-manova-in-r" class="nav-link" data-scroll-target="#manova-in-r">MANOVA in R</a></li>
  <li><a href="#follow-up-analyses" id="toc-follow-up-analyses" class="nav-link" data-scroll-target="#follow-up-analyses">Follow-Up Analyses</a></li>
  <li><a href="#manova-assumptions" id="toc-manova-assumptions" class="nav-link" data-scroll-target="#manova-assumptions">MANOVA Assumptions</a></li>
  </ul></li>
  <li><a href="#discriminant-function-analysis-dfa" id="toc-discriminant-function-analysis-dfa" class="nav-link" data-scroll-target="#discriminant-function-analysis-dfa"><span class="header-section-number">38.6</span> Discriminant Function Analysis (DFA)</a>
  <ul class="collapse">
  <li><a href="#the-goal-of-dfa" id="toc-the-goal-of-dfa" class="nav-link" data-scroll-target="#the-goal-of-dfa">The Goal of DFA</a></li>
  <li><a href="#dfa-in-r" id="toc-dfa-in-r" class="nav-link" data-scroll-target="#dfa-in-r">DFA in R</a></li>
  <li><a href="#interpreting-dfa-output" id="toc-interpreting-dfa-output" class="nav-link" data-scroll-target="#interpreting-dfa-output">Interpreting DFA Output</a></li>
  <li><a href="#using-dfa-for-prediction" id="toc-using-dfa-for-prediction" class="nav-link" data-scroll-target="#using-dfa-for-prediction">Using DFA for Prediction</a></li>
  <li><a href="#cross-validated-classification" id="toc-cross-validated-classification" class="nav-link" data-scroll-target="#cross-validated-classification">Cross-Validated Classification</a></li>
  <li><a href="#dfa-for-biomarker-discovery" id="toc-dfa-for-biomarker-discovery" class="nav-link" data-scroll-target="#dfa-for-biomarker-discovery">DFA for Biomarker Discovery</a></li>
  </ul></li>
  <li><a href="#comparing-methods" id="toc-comparing-methods" class="nav-link" data-scroll-target="#comparing-methods"><span class="header-section-number">38.7</span> Comparing Methods</a></li>
  <li><a href="#using-ordination-scores-in-further-analyses" id="toc-using-ordination-scores-in-further-analyses" class="nav-link" data-scroll-target="#using-ordination-scores-in-further-analyses"><span class="header-section-number">38.8</span> Using Ordination Scores in Further Analyses</a></li>
  <li><a href="#practical-workflow" id="toc-practical-workflow" class="nav-link" data-scroll-target="#practical-workflow"><span class="header-section-number">38.9</span> Practical Workflow</a></li>
  <li><a href="#exercises" id="toc-exercises" class="nav-link" data-scroll-target="#exercises"><span class="header-section-number">38.10</span> Exercises</a></li>
  <li><a href="#non-linear-methods" id="toc-non-linear-methods" class="nav-link" data-scroll-target="#non-linear-methods"><span class="header-section-number">38.11</span> Non-Linear Methods</a></li>
  <li><a href="#summary" id="toc-summary" class="nav-link" data-scroll-target="#summary"><span class="header-section-number">38.12</span> Summary</a></li>
  <li><a href="#additional-resources" id="toc-additional-resources" class="nav-link" data-scroll-target="#additional-resources"><span class="header-section-number">38.13</span> Additional Resources</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../chapters/29-intro-statistical-learning.html">Statistical Learning</a></li><li class="breadcrumb-item"><a href="../chapters/37-dimensionality-reduction.html"><span class="chapter-number">38</span>&nbsp; <span class="chapter-title">Dimensionality Reduction and Multivariate Methods</span></a></li></ol></nav>
<div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title"><span id="sec-dimensionality-reduction" class="quarto-section-identifier"><span class="chapter-number">38</span>&nbsp; <span class="chapter-title">Dimensionality Reduction and Multivariate Methods</span></span></h1><button type="button" class="btn code-tools-button dropdown-toggle" id="quarto-code-tools-menu" data-bs-toggle="dropdown" aria-expanded="false"><i class="bi"></i> Code</button><ul class="dropdown-menu dropdown-menu-end" aria-labelelledby="quarto-code-tools-menu"><li><a id="quarto-show-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Show All Code</a></li><li><a id="quarto-hide-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Hide All Code</a></li><li><hr class="dropdown-divider"></li><li><a id="quarto-view-source" class="dropdown-item" href="javascript:void(0)" role="button">View Source</a></li></ul></div></div>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<section id="the-challenge-of-high-dimensional-data" class="level2" data-number="38.1">
<h2 data-number="38.1" class="anchored" data-anchor-id="the-challenge-of-high-dimensional-data"><span class="header-section-number">38.1</span> The Challenge of High-Dimensional Data</h2>
<p>Modern biology generates datasets with many variables: gene expression across thousands of genes, metabolomic profiles with hundreds of compounds, morphological measurements on many traits. When datasets have many variables, visualization becomes challenging and statistical analysis becomes complicated.</p>
<p>A typical machine learning challenge might include hundreds or thousands of predictors. For example, to compare each of the 784 features in a digit recognition problem, we would need to create 306,936 scatterplots! Creating a single scatter-plot of all the data is impossible due to the high dimensionality.</p>
<p><strong>Dimensionality reduction</strong> creates a smaller set of new variables that capture most of the information in the original data. The general idea is to reduce the dimension of the dataset while preserving important characteristics, such as the distance between features or observations. These techniques help us:</p>
<ul>
<li>Visualize high-dimensional data in 2D or 3D</li>
<li>Identify patterns and clusters</li>
<li>Remove noise and redundancy</li>
<li>Reduce complexity of downstream models</li>
<li>Create composite variables for analysis</li>
</ul>
</section>
<section id="principal-component-analysis-pca" class="level2" data-number="38.2">
<h2 data-number="38.2" class="anchored" data-anchor-id="principal-component-analysis-pca"><span class="header-section-number">38.2</span> Principal Component Analysis (PCA)</h2>
<p><strong>Principal Component Analysis (PCA)</strong> <span class="citation" data-cites="pearson1901lines hotelling1933analysis">(<a href="../references.html#ref-pearson1901lines" role="doc-biblioref">Pearson 1901</a>; <a href="../references.html#ref-hotelling1933analysis" role="doc-biblioref">Hotelling 1933</a>)</span> is the most widely used dimensionality reduction technique. It finds new variables (principal components) that are linear combinations of the originals, chosen to capture maximum variance. The technique behind it—the singular value decomposition—is also useful in many other contexts.</p>
<section id="intuition-preserving-distance" class="level3">
<h3 class="anchored" data-anchor-id="intuition-preserving-distance">Intuition: Preserving Distance</h3>
<p>Before diving into the mathematics, let’s build intuition with a simple example. Consider twin heights data where we have height measurements for 100 pairs of twins—some adults, some children.</p>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1988</span>)</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">100</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>Sigma <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">c</span>(<span class="dv">9</span>, <span class="dv">9</span> <span class="sc">*</span> <span class="fl">0.9</span>, <span class="dv">9</span> <span class="sc">*</span> <span class="fl">0.92</span>, <span class="dv">9</span> <span class="sc">*</span> <span class="dv">1</span>), <span class="dv">2</span>, <span class="dv">2</span>)</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">rbind</span>(<span class="fu">mvrnorm</span>(n <span class="sc">/</span> <span class="dv">2</span>, <span class="fu">c</span>(<span class="dv">69</span>, <span class="dv">69</span>), Sigma),</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>           <span class="fu">mvrnorm</span>(n <span class="sc">/</span> <span class="dv">2</span>, <span class="fu">c</span>(<span class="dv">55</span>, <span class="dv">55</span>), Sigma))</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>lim <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">48</span>, <span class="dv">78</span>)</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>rafalib<span class="sc">::</span><span class="fu">mypar</span>()</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(x, <span class="at">xlim =</span> lim, <span class="at">ylim =</span> lim,</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">"Twin 1 Height"</span>, <span class="at">ylab =</span> <span class="st">"Twin 2 Height"</span>,</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>     <span class="at">main =</span> <span class="st">"Twin Heights: Adults and Children"</span>)</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(x[<span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">2</span>), ], <span class="at">col =</span> <span class="st">"blue"</span>, <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(x[<span class="fu">c</span>(<span class="dv">2</span>, <span class="dv">51</span>), ], <span class="at">col =</span> <span class="st">"red"</span>, <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span>(x[<span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">51</span>), ], <span class="at">pch =</span> <span class="dv">16</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div id="fig-twin-heights" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-twin-heights-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="37-dimensionality-reduction_files/figure-html/fig-twin-heights-1.png" class="img-fluid figure-img" width="576">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-twin-heights-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;38.1: Simulated twin heights showing two groups (adults and children) with high correlation between twins
</figcaption>
</figure>
</div>
</div>
</div>
<p>The scatterplot reveals high correlation and two clear groups: adults (upper right) and children (lower left). We can compute distances between observations:</p>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>d <span class="ot">&lt;-</span> <span class="fu">dist</span>(x)</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"Distance between observations 1 and 2:"</span>, <span class="fu">round</span>(<span class="fu">as.matrix</span>(d)[<span class="dv">1</span>, <span class="dv">2</span>], <span class="dv">2</span>), <span class="st">"</span><span class="sc">\n</span><span class="st">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Distance between observations 1 and 2: 1.98 </code></pre>
</div>
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"Distance between observations 2 and 51:"</span>, <span class="fu">round</span>(<span class="fu">as.matrix</span>(d)[<span class="dv">2</span>, <span class="dv">51</span>], <span class="dv">2</span>), <span class="st">"</span><span class="sc">\n</span><span class="st">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Distance between observations 2 and 51: 18.74 </code></pre>
</div>
</div>
<p>Now suppose we want to reduce from two dimensions to one while preserving these distances. A naive approach is to simply use one of the original variables:</p>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>z <span class="ot">&lt;-</span> x[, <span class="dv">1</span>]</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>rafalib<span class="sc">::</span><span class="fu">mypar</span>()</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(<span class="fu">dist</span>(x) <span class="sc">/</span> <span class="fu">sqrt</span>(<span class="dv">2</span>), <span class="fu">dist</span>(z),</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">"Original distance (scaled)"</span>, <span class="at">ylab =</span> <span class="st">"One-dimension distance"</span>,</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>     <span class="at">main =</span> <span class="st">"Distance Approximation Using One Variable"</span>)</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="at">col =</span> <span class="st">"red"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div id="fig-naive-reduction" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-naive-reduction-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="37-dimensionality-reduction_files/figure-html/fig-naive-reduction-1.png" class="img-fluid figure-img" width="576">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-naive-reduction-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;38.2: Using only one dimension (Twin 1 height) underestimates the true distances
</figcaption>
</figure>
</div>
</div>
</div>
<p>This works reasonably well, but we can do better. Notice that the variation in the data lies mostly along the diagonal. If we transform to the average and difference:</p>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>z <span class="ot">&lt;-</span> <span class="fu">cbind</span>((x[, <span class="dv">2</span>] <span class="sc">+</span> x[, <span class="dv">1</span>]) <span class="sc">/</span> <span class="dv">2</span>, x[, <span class="dv">2</span>] <span class="sc">-</span> x[, <span class="dv">1</span>])</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>rafalib<span class="sc">::</span><span class="fu">mypar</span>()</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(z, <span class="at">xlim =</span> lim, <span class="at">ylim =</span> lim <span class="sc">-</span> <span class="fu">mean</span>(lim),</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">"Average Height"</span>, <span class="at">ylab =</span> <span class="st">"Difference"</span>,</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>     <span class="at">main =</span> <span class="st">"Transformed Coordinates"</span>)</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(z[<span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">2</span>), ], <span class="at">col =</span> <span class="st">"blue"</span>, <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(z[<span class="fu">c</span>(<span class="dv">2</span>, <span class="dv">51</span>), ], <span class="at">col =</span> <span class="st">"red"</span>, <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span>(z[<span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">51</span>), ], <span class="at">pch =</span> <span class="dv">16</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div id="fig-better-reduction" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-better-reduction-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="37-dimensionality-reduction_files/figure-html/fig-better-reduction-1.png" class="img-fluid figure-img" width="576">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-better-reduction-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;38.3: After rotating to average and difference coordinates, distances are mostly explained by the first dimension
</figcaption>
</figure>
</div>
</div>
</div>
<p>Now the distances are mostly explained by the first dimension (the average). Using just this one dimension gives better distance approximation:</p>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"Typical error with original variable:"</span>,</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>    <span class="fu">round</span>(<span class="fu">sd</span>(<span class="fu">dist</span>(x) <span class="sc">-</span> <span class="fu">dist</span>(x[, <span class="dv">1</span>]) <span class="sc">*</span> <span class="fu">sqrt</span>(<span class="dv">2</span>)), <span class="dv">2</span>), <span class="st">"</span><span class="sc">\n</span><span class="st">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Typical error with original variable: 1.21 </code></pre>
</div>
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"Typical error with average:"</span>,</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>    <span class="fu">round</span>(<span class="fu">sd</span>(<span class="fu">dist</span>(x) <span class="sc">-</span> <span class="fu">dist</span>(z[, <span class="dv">1</span>]) <span class="sc">*</span> <span class="fu">sqrt</span>(<span class="dv">2</span>)), <span class="dv">2</span>), <span class="st">"</span><span class="sc">\n</span><span class="st">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Typical error with average: 0.32 </code></pre>
</div>
</div>
<p>The average of the twin heights is essentially the first principal component! PCA finds these optimal linear combinations automatically.</p>
</section>
<section id="linear-transformations" class="level3">
<h3 class="anchored" data-anchor-id="linear-transformations">Linear Transformations</h3>
<p>Each row of the original matrix <span class="math inline">\(X\)</span> was transformed using a linear transformation to create <span class="math inline">\(Z\)</span>:</p>
<p><span class="math display">\[Z_{i,1} = a_{1,1} X_{i,1} + a_{2,1} X_{i,2}\]</span></p>
<p>with <span class="math inline">\(a_{1,1} = 0.5\)</span> and <span class="math inline">\(a_{2,1} = 0.5\)</span> (the average).</p>
<p>In matrix notation: <span class="math display">\[
Z = X A
\mbox{ with }
A = \begin{pmatrix}
1/2 &amp; 1 \\
1/2 &amp; -1
\end{pmatrix}
\]</span></p>
<p><strong>Dimension reduction</strong> can be described as applying a transformation <span class="math inline">\(A\)</span> to a matrix <span class="math inline">\(X\)</span> that moves the information to the first few columns of <span class="math inline">\(Z = XA\)</span>, then keeping just these informative columns.</p>
</section>
<section id="orthogonal-transformations" class="level3">
<h3 class="anchored" data-anchor-id="orthogonal-transformations">Orthogonal Transformations</h3>
<p>To preserve distances exactly, we need an <strong>orthogonal transformation</strong>—one where the columns of <span class="math inline">\(A\)</span> have unit length and are perpendicular:</p>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Orthogonal transformation</span></span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>z[, <span class="dv">1</span>] <span class="ot">&lt;-</span> (x[, <span class="dv">1</span>] <span class="sc">+</span> x[, <span class="dv">2</span>]) <span class="sc">/</span> <span class="fu">sqrt</span>(<span class="dv">2</span>)</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>z[, <span class="dv">2</span>] <span class="ot">&lt;-</span> (x[, <span class="dv">2</span>] <span class="sc">-</span> x[, <span class="dv">1</span>]) <span class="sc">/</span> <span class="fu">sqrt</span>(<span class="dv">2</span>)</span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a><span class="co"># This preserves the original distances exactly</span></span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"Maximum distance difference:"</span>, <span class="fu">max</span>(<span class="fu">abs</span>(<span class="fu">dist</span>(z) <span class="sc">-</span> <span class="fu">dist</span>(x))), <span class="st">"</span><span class="sc">\n</span><span class="st">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Maximum distance difference: 3.241851e-14 </code></pre>
</div>
</div>
<p>An orthogonal rotation preserves all distances while reorganizing the variance. Now most variance is in the first dimension:</p>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="fu">qplot</span>(z[, <span class="dv">1</span>], <span class="at">bins =</span> <span class="dv">20</span>, <span class="at">color =</span> <span class="fu">I</span>(<span class="st">"black"</span>), <span class="at">fill =</span> <span class="fu">I</span>(<span class="st">"steelblue"</span>)) <span class="sc">+</span></span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">x =</span> <span class="st">"First Principal Component"</span>, <span class="at">y =</span> <span class="st">"Count"</span>,</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a>       <span class="at">title =</span> <span class="st">"PC1 Separates Adults from Children"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div id="fig-variance-concentration" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-variance-concentration-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="37-dimensionality-reduction_files/figure-html/fig-variance-concentration-1.png" class="img-fluid figure-img" width="576">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-variance-concentration-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;38.4: After orthogonal rotation, the first dimension clearly separates adults from children
</figcaption>
</figure>
</div>
</div>
</div>
<p>The first PC clearly shows the two groups. We’ve reduced two dimensions to one with minimal information loss because the original variables were highly correlated:</p>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"Correlation between twin heights:"</span>, <span class="fu">round</span>(<span class="fu">cor</span>(x[, <span class="dv">1</span>], x[, <span class="dv">2</span>]), <span class="dv">3</span>), <span class="st">"</span><span class="sc">\n</span><span class="st">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Correlation between twin heights: 0.988 </code></pre>
</div>
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"Correlation between PCs:"</span>, <span class="fu">round</span>(<span class="fu">cor</span>(z[, <span class="dv">1</span>], z[, <span class="dv">2</span>]), <span class="dv">3</span>), <span class="st">"</span><span class="sc">\n</span><span class="st">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Correlation between PCs: 0.088 </code></pre>
</div>
</div>
</section>
<section id="the-eigenanalysis-foundation" class="level3">
<h3 class="anchored" data-anchor-id="the-eigenanalysis-foundation">The Eigenanalysis Foundation</h3>
<p>The mathematical foundation involves <strong>eigenanalysis</strong>: decomposing the covariance (or correlation) matrix to find directions of maximum variation. For a detailed treatment of eigenvalues, eigenvectors, and their interpretation, see <a href="A10-matrix-algebra.html#sec-eigenanalysis" class="quarto-xref"><span>Section 52.9</span></a>.</p>
<p>Given a covariance matrix <span class="math inline">\(\Sigma\)</span>, eigenanalysis finds:</p>
<ul>
<li><strong>Eigenvectors</strong>: The directions of the principal components (loadings)</li>
<li><strong>Eigenvalues</strong>: The variance explained by each component</li>
</ul>
<p>The first principal component points in the direction of maximum variance. Each subsequent component is orthogonal (uncorrelated) and captures remaining variance in decreasing order.</p>
<p>The total variability can be defined as the sum of variances: <span class="math display">\[
v_1 + v_2 + \cdots + v_p
\]</span></p>
<p>An orthogonal transformation preserves total variability but redistributes it. PCA finds the transformation that concentrates variance in the first few components.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Interpreting Eigenvalues and Eigenvectors
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><strong>Eigenvalues</strong> tell you <em>how much</em> variance each PC captures—larger means more important</li>
<li><strong>Eigenvectors</strong> (loadings) tell you <em>how</em> original variables combine to form each PC</li>
<li>Variables with large absolute loadings contribute strongly to that PC</li>
<li>The sum of all eigenvalues equals the total variance in the data</li>
</ul>
<p>See <a href="A10-matrix-algebra.html#sec-eigen-pca" class="quarto-xref"><span>Section 52.9.5</span></a> for a complete explanation with examples.</p>
</div>
</div>
<div id="fig-pca-geometry" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-pca-geometry-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../images/ch36/ch36_pca_geometry.jpeg" class="img-fluid quarto-figure quarto-figure-center figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-pca-geometry-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;38.5: Geometric interpretation of principal components as directions of maximum variance
</figcaption>
</figure>
</div>
</section>
<section id="pca-in-r" class="level3">
<h3 class="anchored" data-anchor-id="pca-in-r">PCA in R</h3>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="co"># PCA on iris data</span></span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>iris_pca <span class="ot">&lt;-</span> <span class="fu">prcomp</span>(iris[, <span class="dv">1</span><span class="sc">:</span><span class="dv">4</span>], <span class="at">scale. =</span> <span class="cn">TRUE</span>)</span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Variance explained</span></span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(iris_pca)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Importance of components:
                          PC1    PC2     PC3     PC4
Standard deviation     1.7084 0.9560 0.38309 0.14393
Proportion of Variance 0.7296 0.2285 0.03669 0.00518
Cumulative Proportion  0.7296 0.9581 0.99482 1.00000</code></pre>
</div>
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Scree plot</span></span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">2</span>))</span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(iris_pca, <span class="at">type =</span> <span class="st">"l"</span>, <span class="at">main =</span> <span class="st">"Scree Plot"</span>)</span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a><span class="co"># PC scores colored by species</span></span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(iris_pca<span class="sc">$</span>x[, <span class="dv">1</span><span class="sc">:</span><span class="dv">2</span>],</span>
<span id="cb21-7"><a href="#cb21-7" aria-hidden="true" tabindex="-1"></a>     <span class="at">col =</span> <span class="fu">c</span>(<span class="st">"red"</span>, <span class="st">"green"</span>, <span class="st">"blue"</span>)[iris<span class="sc">$</span>Species],</span>
<span id="cb21-8"><a href="#cb21-8" aria-hidden="true" tabindex="-1"></a>     <span class="at">pch =</span> <span class="dv">19</span>, <span class="at">xlab =</span> <span class="st">"PC1"</span>, <span class="at">ylab =</span> <span class="st">"PC2"</span>,</span>
<span id="cb21-9"><a href="#cb21-9" aria-hidden="true" tabindex="-1"></a>     <span class="at">main =</span> <span class="st">"PCA of Iris Data"</span>)</span>
<span id="cb21-10"><a href="#cb21-10" aria-hidden="true" tabindex="-1"></a><span class="fu">legend</span>(<span class="st">"topright"</span>, <span class="fu">levels</span>(iris<span class="sc">$</span>Species),</span>
<span id="cb21-11"><a href="#cb21-11" aria-hidden="true" tabindex="-1"></a>       <span class="at">col =</span> <span class="fu">c</span>(<span class="st">"red"</span>, <span class="st">"green"</span>, <span class="st">"blue"</span>), <span class="at">pch =</span> <span class="dv">19</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div id="fig-pca-iris" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-pca-iris-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="37-dimensionality-reduction_files/figure-html/fig-pca-iris-1.png" class="img-fluid figure-img" width="768">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-pca-iris-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;38.6: PCA scree plot and principal component scores for iris data colored by species
</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="loadings-what-variables-drive-each-pc" class="level3">
<h3 class="anchored" data-anchor-id="loadings-what-variables-drive-each-pc">Loadings: What Variables Drive Each PC?</h3>
<p>Each principal component is defined by its <strong>loadings</strong>—the coefficients showing how much each original variable contributes:</p>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Loadings (rotation matrix)</span></span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a>iris_pca<span class="sc">$</span>rotation</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>                    PC1         PC2        PC3        PC4
Sepal.Length  0.5210659 -0.37741762  0.7195664  0.2612863
Sepal.Width  -0.2693474 -0.92329566 -0.2443818 -0.1235096
Petal.Length  0.5804131 -0.02449161 -0.1421264 -0.8014492
Petal.Width   0.5648565 -0.06694199 -0.6342727  0.5235971</code></pre>
</div>
</div>
<p>Large absolute loadings indicate that a variable strongly influences that component. The sign indicates the direction of the relationship.</p>
</section>
<section id="interpreting-pca-results" class="level3">
<h3 class="anchored" data-anchor-id="interpreting-pca-results">Interpreting PCA Results</h3>
<p>Key elements of PCA output:</p>
<ul>
<li><strong>Eigenvalues</strong>: Variance explained by each component (shown in scree plot)</li>
<li><strong>Proportion of variance</strong>: How much of total variance each PC captures</li>
<li><strong>Loadings</strong>: Coefficients relating original variables to PCs</li>
<li><strong>Scores</strong>: Values of the new variables for each observation</li>
</ul>
<p>The first few PCs often capture most of the meaningful variation, allowing you to reduce many variables to just 2-3 for visualization and analysis.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
How Many Components to Keep?
</div>
</div>
<div class="callout-body-container callout-body">
<p>Common approaches:</p>
<ul>
<li>Keep components with eigenvalues &gt; 1 (Kaiser criterion)</li>
<li>Keep enough to explain 80-90% of variance</li>
<li>Look for an “elbow” in the scree plot</li>
<li>Use cross-validation if using PCs for prediction</li>
</ul>
</div>
</div>
</section>
<section id="biplot-visualization" class="level3">
<h3 class="anchored" data-anchor-id="biplot-visualization">Biplot Visualization</h3>
<p>A <strong>biplot</strong> shows both observations (scores) and variables (loadings) on the same plot:</p>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="fu">biplot</span>(iris_pca, <span class="at">col =</span> <span class="fu">c</span>(<span class="st">"gray50"</span>, <span class="st">"red"</span>), <span class="at">cex =</span> <span class="fl">0.7</span>,</span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a>       <span class="at">main =</span> <span class="st">"PCA Biplot of Iris Data"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div id="fig-pca-biplot" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-pca-biplot-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="37-dimensionality-reduction_files/figure-html/fig-pca-biplot-1.png" class="img-fluid figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-pca-biplot-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;38.7: PCA biplot showing both observations and variable loadings simultaneously
</figcaption>
</figure>
</div>
</div>
</div>
<p>Arrows show variable loadings—their direction and length indicate how each variable relates to the principal components.</p>
</section>
<section id="mnist-example-high-dimensional-image-data" class="level3">
<h3 class="anchored" data-anchor-id="mnist-example-high-dimensional-image-data">MNIST Example: High-Dimensional Image Data</h3>
<p>The real power of PCA becomes apparent with truly high-dimensional data. The MNIST dataset of handwritten digits has 784 features (28×28 pixels). Can we reduce this dimensionality while preserving useful information?</p>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(dslabs)</span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> (<span class="sc">!</span><span class="fu">exists</span>(<span class="st">"mnist"</span>)) mnist <span class="ot">&lt;-</span> <span class="fu">read_mnist</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>Because pixels near each other on the image grid are correlated, we expect dimension reduction to work well:</p>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a>col_means <span class="ot">&lt;-</span> <span class="fu">colMeans</span>(mnist<span class="sc">$</span>test<span class="sc">$</span>images)</span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a>pca <span class="ot">&lt;-</span> <span class="fu">prcomp</span>(mnist<span class="sc">$</span>train<span class="sc">$</span>images)</span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-4"><a href="#cb26-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Scree plot</span></span>
<span id="cb26-5"><a href="#cb26-5" aria-hidden="true" tabindex="-1"></a>pc <span class="ot">&lt;-</span> <span class="dv">1</span><span class="sc">:</span><span class="fu">ncol</span>(mnist<span class="sc">$</span>test<span class="sc">$</span>images)</span>
<span id="cb26-6"><a href="#cb26-6" aria-hidden="true" tabindex="-1"></a><span class="fu">qplot</span>(pc[<span class="dv">1</span><span class="sc">:</span><span class="dv">50</span>], pca<span class="sc">$</span>sdev[<span class="dv">1</span><span class="sc">:</span><span class="dv">50</span>], <span class="at">geom =</span> <span class="st">"line"</span>) <span class="sc">+</span></span>
<span id="cb26-7"><a href="#cb26-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">x =</span> <span class="st">"Principal Component"</span>, <span class="at">y =</span> <span class="st">"Standard Deviation"</span>,</span>
<span id="cb26-8"><a href="#cb26-8" aria-hidden="true" tabindex="-1"></a>       <span class="at">title =</span> <span class="st">"MNIST Scree Plot (first 50 PCs)"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div id="fig-mnist-pca" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-mnist-pca-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="37-dimensionality-reduction_files/figure-html/fig-mnist-pca-1.png" class="img-fluid figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-mnist-pca-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;38.8: Variance explained by principal components of MNIST digit images. The first few PCs capture substantial variance.
</figcaption>
</figure>
</div>
</div>
</div>
<p>The first few PCs capture substantial variance:</p>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(pca)<span class="sc">$</span>importance[, <span class="dv">1</span><span class="sc">:</span><span class="dv">5</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>                             PC1       PC2       PC3       PC4       PC5
Standard deviation     576.82291 493.23822 459.89930 429.85624 408.56680
Proportion of Variance   0.09705   0.07096   0.06169   0.05389   0.04869
Cumulative Proportion    0.09705   0.16801   0.22970   0.28359   0.33228</code></pre>
</div>
</div>
<p>Even with just two dimensions, we can see structure related to digit class:</p>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a><span class="fu">data.frame</span>(<span class="at">PC1 =</span> pca<span class="sc">$</span>x[, <span class="dv">1</span>], <span class="at">PC2 =</span> pca<span class="sc">$</span>x[, <span class="dv">2</span>],</span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a>           <span class="at">label =</span> <span class="fu">factor</span>(mnist<span class="sc">$</span>train<span class="sc">$</span>label)) <span class="sc">%&gt;%</span></span>
<span id="cb29-3"><a href="#cb29-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">sample_n</span>(<span class="dv">2000</span>) <span class="sc">%&gt;%</span></span>
<span id="cb29-4"><a href="#cb29-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(PC1, PC2, <span class="at">fill =</span> label)) <span class="sc">+</span></span>
<span id="cb29-5"><a href="#cb29-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">cex =</span> <span class="dv">3</span>, <span class="at">pch =</span> <span class="dv">21</span>, <span class="at">alpha =</span> <span class="fl">0.7</span>) <span class="sc">+</span></span>
<span id="cb29-6"><a href="#cb29-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">"MNIST Digits in PC Space"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div id="fig-mnist-pca-scores" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-mnist-pca-scores-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="37-dimensionality-reduction_files/figure-html/fig-mnist-pca-scores-1.png" class="img-fluid figure-img" width="768">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-mnist-pca-scores-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;38.9: First two principal components of MNIST data colored by digit label. Different digits occupy different regions of PC space.
</figcaption>
</figure>
</div>
</div>
</div>
<p>We can visualize what each PC “looks for” by reshaping the loadings back to the 28×28 grid:</p>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb30"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(RColorBrewer)</span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a>tmp <span class="ot">&lt;-</span> <span class="fu">lapply</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">4</span>, <span class="cf">function</span>(i) {</span>
<span id="cb30-3"><a href="#cb30-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">expand.grid</span>(<span class="at">Row =</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">28</span>, <span class="at">Column =</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">28</span>) <span class="sc">%&gt;%</span></span>
<span id="cb30-4"><a href="#cb30-4" aria-hidden="true" tabindex="-1"></a>    <span class="fu">mutate</span>(<span class="at">id =</span> i, <span class="at">label =</span> <span class="fu">paste0</span>(<span class="st">"PC"</span>, i),</span>
<span id="cb30-5"><a href="#cb30-5" aria-hidden="true" tabindex="-1"></a>           <span class="at">value =</span> pca<span class="sc">$</span>rotation[, i])</span>
<span id="cb30-6"><a href="#cb30-6" aria-hidden="true" tabindex="-1"></a>})</span>
<span id="cb30-7"><a href="#cb30-7" aria-hidden="true" tabindex="-1"></a>tmp <span class="ot">&lt;-</span> <span class="fu">Reduce</span>(rbind, tmp)</span>
<span id="cb30-8"><a href="#cb30-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-9"><a href="#cb30-9" aria-hidden="true" tabindex="-1"></a>tmp <span class="sc">%&gt;%</span></span>
<span id="cb30-10"><a href="#cb30-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(Row, Column, <span class="at">fill =</span> value)) <span class="sc">+</span></span>
<span id="cb30-11"><a href="#cb30-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_raster</span>() <span class="sc">+</span></span>
<span id="cb30-12"><a href="#cb30-12" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_y_reverse</span>() <span class="sc">+</span></span>
<span id="cb30-13"><a href="#cb30-13" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_fill_gradientn</span>(<span class="at">colors =</span> <span class="fu">brewer.pal</span>(<span class="dv">9</span>, <span class="st">"RdBu"</span>)) <span class="sc">+</span></span>
<span id="cb30-14"><a href="#cb30-14" aria-hidden="true" tabindex="-1"></a>  <span class="fu">facet_wrap</span>(<span class="sc">~</span>label, <span class="at">nrow =</span> <span class="dv">1</span>) <span class="sc">+</span></span>
<span id="cb30-15"><a href="#cb30-15" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_minimal</span>() <span class="sc">+</span></span>
<span id="cb30-16"><a href="#cb30-16" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">"PCA Loadings as Images"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div id="fig-mnist-loadings" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-mnist-loadings-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="37-dimensionality-reduction_files/figure-html/fig-mnist-loadings-1.png" class="img-fluid figure-img" width="864">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-mnist-loadings-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;38.10: First four principal component loadings visualized as images. Each PC captures different aspects of digit structure.
</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="applying-pca-to-improve-classification" class="level3">
<h3 class="anchored" data-anchor-id="applying-pca-to-improve-classification">Applying PCA to Improve Classification</h3>
<p>PCA can reduce model complexity while maintaining predictive performance. Let’s use 36 PCs (explaining ~80% of variance) for kNN classification:</p>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(caret)</span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a>k <span class="ot">&lt;-</span> <span class="dv">36</span></span>
<span id="cb31-3"><a href="#cb31-3" aria-hidden="true" tabindex="-1"></a>x_train <span class="ot">&lt;-</span> pca<span class="sc">$</span>x[, <span class="dv">1</span><span class="sc">:</span>k]</span>
<span id="cb31-4"><a href="#cb31-4" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> <span class="fu">factor</span>(mnist<span class="sc">$</span>train<span class="sc">$</span>labels)</span>
<span id="cb31-5"><a href="#cb31-5" aria-hidden="true" tabindex="-1"></a>fit <span class="ot">&lt;-</span> <span class="fu">knn3</span>(x_train, y)</span>
<span id="cb31-6"><a href="#cb31-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-7"><a href="#cb31-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Transform test set using training PCA</span></span>
<span id="cb31-8"><a href="#cb31-8" aria-hidden="true" tabindex="-1"></a>x_test <span class="ot">&lt;-</span> <span class="fu">sweep</span>(mnist<span class="sc">$</span>test<span class="sc">$</span>images, <span class="dv">2</span>, col_means) <span class="sc">%*%</span> pca<span class="sc">$</span>rotation</span>
<span id="cb31-9"><a href="#cb31-9" aria-hidden="true" tabindex="-1"></a>x_test <span class="ot">&lt;-</span> x_test[, <span class="dv">1</span><span class="sc">:</span>k]</span>
<span id="cb31-10"><a href="#cb31-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-11"><a href="#cb31-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Predict and evaluate</span></span>
<span id="cb31-12"><a href="#cb31-12" aria-hidden="true" tabindex="-1"></a>y_hat <span class="ot">&lt;-</span> <span class="fu">predict</span>(fit, x_test, <span class="at">type =</span> <span class="st">"class"</span>)</span>
<span id="cb31-13"><a href="#cb31-13" aria-hidden="true" tabindex="-1"></a><span class="fu">confusionMatrix</span>(y_hat, <span class="fu">factor</span>(mnist<span class="sc">$</span>test<span class="sc">$</span>labels))<span class="sc">$</span>overall[<span class="st">"Accuracy"</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Accuracy 
  0.9751 </code></pre>
</div>
</div>
<p>With just 36 dimensions (instead of 784), we achieve excellent accuracy. This demonstrates the power of PCA for both visualization and model simplification.</p>
</section>
</section>
<section id="principal-coordinate-analysis-pcoa" class="level2" data-number="38.3">
<h2 data-number="38.3" class="anchored" data-anchor-id="principal-coordinate-analysis-pcoa"><span class="header-section-number">38.3</span> Principal Coordinate Analysis (PCoA)</h2>
<p>While PCA uses correlations among variables, <strong>Principal Coordinate Analysis (PCoA)</strong> (also called Metric Multidimensional Scaling) starts with a dissimilarity matrix among observations. This is valuable when:</p>
<ul>
<li>You have a meaningful distance metric (e.g., genetic distances)</li>
<li>Variables are mixed types or non-numeric</li>
<li>The data are counts (e.g., microbiome data)</li>
</ul>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb33"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a><span class="co"># PCoA example using Euclidean distances</span></span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a>dist_matrix <span class="ot">&lt;-</span> <span class="fu">dist</span>(iris[, <span class="dv">1</span><span class="sc">:</span><span class="dv">4</span>])</span>
<span id="cb33-3"><a href="#cb33-3" aria-hidden="true" tabindex="-1"></a>pcoa_result <span class="ot">&lt;-</span> <span class="fu">cmdscale</span>(dist_matrix, <span class="at">k =</span> <span class="dv">2</span>, <span class="at">eig =</span> <span class="cn">TRUE</span>)</span>
<span id="cb33-4"><a href="#cb33-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-5"><a href="#cb33-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Proportion of variance explained</span></span>
<span id="cb33-6"><a href="#cb33-6" aria-hidden="true" tabindex="-1"></a>eig_vals <span class="ot">&lt;-</span> pcoa_result<span class="sc">$</span>eig[pcoa_result<span class="sc">$</span>eig <span class="sc">&gt;</span> <span class="dv">0</span>]</span>
<span id="cb33-7"><a href="#cb33-7" aria-hidden="true" tabindex="-1"></a>var_explained <span class="ot">&lt;-</span> eig_vals <span class="sc">/</span> <span class="fu">sum</span>(eig_vals)</span>
<span id="cb33-8"><a href="#cb33-8" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"Variance explained by first two axes:"</span>,</span>
<span id="cb33-9"><a href="#cb33-9" aria-hidden="true" tabindex="-1"></a>    <span class="fu">round</span>(<span class="fu">sum</span>(var_explained[<span class="dv">1</span><span class="sc">:</span><span class="dv">2</span>]) <span class="sc">*</span> <span class="dv">100</span>, <span class="dv">1</span>), <span class="st">"%</span><span class="sc">\n</span><span class="st">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Variance explained by first two axes: 97.8 %</code></pre>
</div>
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb35"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot</span></span>
<span id="cb35-2"><a href="#cb35-2" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(pcoa_result<span class="sc">$</span>points,</span>
<span id="cb35-3"><a href="#cb35-3" aria-hidden="true" tabindex="-1"></a>     <span class="at">col =</span> <span class="fu">c</span>(<span class="st">"red"</span>, <span class="st">"green"</span>, <span class="st">"blue"</span>)[iris<span class="sc">$</span>Species],</span>
<span id="cb35-4"><a href="#cb35-4" aria-hidden="true" tabindex="-1"></a>     <span class="at">pch =</span> <span class="dv">19</span>,</span>
<span id="cb35-5"><a href="#cb35-5" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="fu">paste0</span>(<span class="st">"PCoA1 ("</span>, <span class="fu">round</span>(var_explained[<span class="dv">1</span>]<span class="sc">*</span><span class="dv">100</span>, <span class="dv">1</span>), <span class="st">"%)"</span>),</span>
<span id="cb35-6"><a href="#cb35-6" aria-hidden="true" tabindex="-1"></a>     <span class="at">ylab =</span> <span class="fu">paste0</span>(<span class="st">"PCoA2 ("</span>, <span class="fu">round</span>(var_explained[<span class="dv">2</span>]<span class="sc">*</span><span class="dv">100</span>, <span class="dv">1</span>), <span class="st">"%)"</span>),</span>
<span id="cb35-7"><a href="#cb35-7" aria-hidden="true" tabindex="-1"></a>     <span class="at">main =</span> <span class="st">"PCoA of Iris Data"</span>)</span>
<span id="cb35-8"><a href="#cb35-8" aria-hidden="true" tabindex="-1"></a><span class="fu">legend</span>(<span class="st">"topright"</span>, <span class="fu">levels</span>(iris<span class="sc">$</span>Species),</span>
<span id="cb35-9"><a href="#cb35-9" aria-hidden="true" tabindex="-1"></a>       <span class="at">col =</span> <span class="fu">c</span>(<span class="st">"red"</span>, <span class="st">"green"</span>, <span class="st">"blue"</span>), <span class="at">pch =</span> <span class="dv">19</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div id="fig-pcoa" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-pcoa-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="37-dimensionality-reduction_files/figure-html/fig-pcoa-1.png" class="img-fluid figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-pcoa-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;38.11: Principal Coordinate Analysis (PCoA) ordination of iris data using Euclidean distances
</figcaption>
</figure>
</div>
</div>
</div>
<section id="when-to-use-pcoa-vs.-pca" class="level3">
<h3 class="anchored" data-anchor-id="when-to-use-pcoa-vs.-pca">When to Use PCoA vs.&nbsp;PCA</h3>
<ul>
<li><strong>PCA</strong>: Variables are measured on a common scale; interested in variable contributions</li>
<li><strong>PCoA</strong>: Have a distance matrix; want to preserve distances among samples</li>
<li>For Euclidean distances, PCA and PCoA give equivalent results</li>
</ul>
</section>
</section>
<section id="non-metric-multidimensional-scaling-nmds" class="level2" data-number="38.4">
<h2 data-number="38.4" class="anchored" data-anchor-id="non-metric-multidimensional-scaling-nmds"><span class="header-section-number">38.4</span> Non-Metric Multidimensional Scaling (NMDS)</h2>
<p><strong>NMDS</strong> is an ordination technique that preserves rank-order of distances rather than exact distances. It’s widely used in ecology because it makes no assumptions about the data distribution.</p>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb36"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a><span class="co"># NMDS example</span></span>
<span id="cb36-2"><a href="#cb36-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(vegan)</span>
<span id="cb36-3"><a href="#cb36-3" aria-hidden="true" tabindex="-1"></a>nmds_result <span class="ot">&lt;-</span> <span class="fu">metaMDS</span>(iris[, <span class="dv">1</span><span class="sc">:</span><span class="dv">4</span>], <span class="at">k =</span> <span class="dv">2</span>, <span class="at">trymax =</span> <span class="dv">100</span>, <span class="at">trace =</span> <span class="cn">FALSE</span>)</span>
<span id="cb36-4"><a href="#cb36-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-5"><a href="#cb36-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Stress value indicates fit (&lt; 0.1 is good, &lt; 0.2 is acceptable)</span></span>
<span id="cb36-6"><a href="#cb36-6" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"Stress:"</span>, <span class="fu">round</span>(nmds_result<span class="sc">$</span>stress, <span class="dv">3</span>), <span class="st">"</span><span class="sc">\n</span><span class="st">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Stress: 0.038 </code></pre>
</div>
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb38"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(nmds_result<span class="sc">$</span>points,</span>
<span id="cb38-2"><a href="#cb38-2" aria-hidden="true" tabindex="-1"></a>     <span class="at">col =</span> <span class="fu">c</span>(<span class="st">"red"</span>, <span class="st">"green"</span>, <span class="st">"blue"</span>)[iris<span class="sc">$</span>Species],</span>
<span id="cb38-3"><a href="#cb38-3" aria-hidden="true" tabindex="-1"></a>     <span class="at">pch =</span> <span class="dv">19</span>, <span class="at">xlab =</span> <span class="st">"NMDS1"</span>, <span class="at">ylab =</span> <span class="st">"NMDS2"</span>,</span>
<span id="cb38-4"><a href="#cb38-4" aria-hidden="true" tabindex="-1"></a>     <span class="at">main =</span> <span class="st">"NMDS of Iris Data"</span>)</span>
<span id="cb38-5"><a href="#cb38-5" aria-hidden="true" tabindex="-1"></a><span class="fu">legend</span>(<span class="st">"topright"</span>, <span class="fu">levels</span>(iris<span class="sc">$</span>Species),</span>
<span id="cb38-6"><a href="#cb38-6" aria-hidden="true" tabindex="-1"></a>       <span class="at">col =</span> <span class="fu">c</span>(<span class="st">"red"</span>, <span class="st">"green"</span>, <span class="st">"blue"</span>), <span class="at">pch =</span> <span class="dv">19</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div id="fig-nmds" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-nmds-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="37-dimensionality-reduction_files/figure-html/fig-nmds-1.png" class="img-fluid figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-nmds-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;38.12: Non-metric multidimensional scaling (NMDS) ordination of iris data
</figcaption>
</figure>
</div>
</div>
</div>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Metric vs.&nbsp;Non-Metric Methods
</div>
</div>
<div class="callout-body-container callout-body">
<p>PCA and metric PCoA produce scores on a ratio scale—differences between scores are meaningful. These can be used directly in linear models.</p>
<p>Non-metric multidimensional scaling (NMDS) produces ordinal rankings only. NMDS scores should <strong>not</strong> be used in parametric analyses like ANOVA or regression.</p>
</div>
</div>
</section>
<section id="manova-multivariate-analysis-of-variance" class="level2" data-number="38.5">
<h2 data-number="38.5" class="anchored" data-anchor-id="manova-multivariate-analysis-of-variance"><span class="header-section-number">38.5</span> MANOVA: Multivariate Analysis of Variance</h2>
<p>When you have multiple response variables and want to test for group differences, <strong>MANOVA</strong> (Multivariate Analysis of Variance) is the appropriate technique. It extends ANOVA to multiple dependent variables simultaneously.</p>
<section id="why-not-multiple-anovas" class="level3">
<h3 class="anchored" data-anchor-id="why-not-multiple-anovas">Why Not Multiple ANOVAs?</h3>
<p>Running separate ANOVAs on each variable:</p>
<ul>
<li>Ignores correlations among response variables</li>
<li>Inflates Type I error rate with multiple tests</li>
<li>May miss differences only apparent when variables are considered together</li>
</ul>
<p>MANOVA tests whether group centroids differ in multivariate space.</p>
</section>
<section id="the-manova-framework" class="level3">
<h3 class="anchored" data-anchor-id="the-manova-framework">The MANOVA Framework</h3>
<p>MANOVA decomposes the total multivariate variation:</p>
<p><span class="math display">\[\mathbf{T} = \mathbf{H} + \mathbf{E}\]</span></p>
<p>where:</p>
<ul>
<li><strong>T</strong>: Total sum of squares and cross-products matrix</li>
<li><strong>H</strong>: Hypothesis (between-groups) matrix</li>
<li><strong>E</strong>: Error (within-groups) matrix</li>
</ul>
<p>These are matrices because we have multiple response variables.</p>
<div id="fig-manova-decomposition" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-manova-decomposition-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../images/ch36/ch36_manova_decomposition.jpeg" class="img-fluid quarto-figure quarto-figure-center figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-manova-decomposition-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;38.13: MANOVA decomposes multivariate variation into between-group and within-group components
</figcaption>
</figure>
</div>
</section>
<section id="test-statistics" class="level3">
<h3 class="anchored" data-anchor-id="test-statistics">Test Statistics</h3>
<p>Several test statistics exist for MANOVA, each a function of the eigenvalues of <span class="math inline">\(\mathbf{HE}^{-1}\)</span>:</p>
<table class="caption-top table">
<colgroup>
<col style="width: 45%">
<col style="width: 54%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Statistic</th>
<th style="text-align: left;">Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><strong>Wilks’ Lambda (Λ)</strong></td>
<td style="text-align: left;">Product of 1/(1+λᵢ); most commonly used</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Hotelling-Lawley Trace</strong></td>
<td style="text-align: left;">Sum of eigenvalues</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>Pillai’s Trace</strong></td>
<td style="text-align: left;">Sum of λᵢ/(1+λᵢ); most robust</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Roy’s Largest Root</strong></td>
<td style="text-align: left;">Maximum eigenvalue; most powerful but sensitive</td>
</tr>
</tbody>
</table>
<p><strong>Pillai’s Trace</strong> is generally recommended because it’s most robust to violations of assumptions.</p>
</section>
<section id="manova-in-r" class="level3">
<h3 class="anchored" data-anchor-id="manova-in-r">MANOVA in R</h3>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb39"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a><span class="co"># MANOVA on iris data</span></span>
<span id="cb39-2"><a href="#cb39-2" aria-hidden="true" tabindex="-1"></a>manova_model <span class="ot">&lt;-</span> <span class="fu">manova</span>(<span class="fu">cbind</span>(Sepal.Length, Sepal.Width,</span>
<span id="cb39-3"><a href="#cb39-3" aria-hidden="true" tabindex="-1"></a>                              Petal.Length, Petal.Width) <span class="sc">~</span> Species,</span>
<span id="cb39-4"><a href="#cb39-4" aria-hidden="true" tabindex="-1"></a>                       <span class="at">data =</span> iris)</span>
<span id="cb39-5"><a href="#cb39-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-6"><a href="#cb39-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Summary with different test statistics</span></span>
<span id="cb39-7"><a href="#cb39-7" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(manova_model, <span class="at">test =</span> <span class="st">"Pillai"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>           Df Pillai approx F num Df den Df    Pr(&gt;F)    
Species     2 1.1919   53.466      8    290 &lt; 2.2e-16 ***
Residuals 147                                            
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</code></pre>
</div>
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb41"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(manova_model, <span class="at">test =</span> <span class="st">"Wilks"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>           Df    Wilks approx F num Df den Df    Pr(&gt;F)    
Species     2 0.023439   199.15      8    288 &lt; 2.2e-16 ***
Residuals 147                                              
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</code></pre>
</div>
</div>
<p>The significant result tells us that species differ in their multivariate centroid—the combination of all four measurements.</p>
</section>
<section id="follow-up-analyses" class="level3">
<h3 class="anchored" data-anchor-id="follow-up-analyses">Follow-Up Analyses</h3>
<p>A significant MANOVA should be followed by:</p>
<ol type="1">
<li><strong>Univariate ANOVAs</strong> to see which variables differ</li>
<li><strong>Discriminant Function Analysis</strong> to understand how groups differ</li>
</ol>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb43"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Univariate follow-ups</span></span>
<span id="cb43-2"><a href="#cb43-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary.aov</span>(manova_model)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code> Response Sepal.Length :
             Df Sum Sq Mean Sq F value    Pr(&gt;F)    
Species       2 63.212  31.606  119.26 &lt; 2.2e-16 ***
Residuals   147 38.956   0.265                      
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

 Response Sepal.Width :
             Df Sum Sq Mean Sq F value    Pr(&gt;F)    
Species       2 11.345  5.6725   49.16 &lt; 2.2e-16 ***
Residuals   147 16.962  0.1154                      
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

 Response Petal.Length :
             Df Sum Sq Mean Sq F value    Pr(&gt;F)    
Species       2 437.10 218.551  1180.2 &lt; 2.2e-16 ***
Residuals   147  27.22   0.185                      
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

 Response Petal.Width :
             Df Sum Sq Mean Sq F value    Pr(&gt;F)    
Species       2 80.413  40.207  960.01 &lt; 2.2e-16 ***
Residuals   147  6.157   0.042                      
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</code></pre>
</div>
</div>
</section>
<section id="manova-assumptions" class="level3">
<h3 class="anchored" data-anchor-id="manova-assumptions">MANOVA Assumptions</h3>
<p>MANOVA assumes:</p>
<ol type="1">
<li><strong>Multivariate normality</strong> within groups</li>
<li><strong>Homogeneity of covariance matrices</strong> across groups</li>
<li><strong>Independence</strong> of observations</li>
<li><strong>No multicollinearity</strong> among response variables</li>
</ol>
<p>Test homogeneity of covariance matrices with Box’s M test (though it’s sensitive to non-normality):</p>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb45"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb45-1"><a href="#cb45-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Box's M test (requires biotools package)</span></span>
<span id="cb45-2"><a href="#cb45-2" aria-hidden="true" tabindex="-1"></a><span class="co"># library(biotools)</span></span>
<span id="cb45-3"><a href="#cb45-3" aria-hidden="true" tabindex="-1"></a><span class="co"># boxM(iris[, 1:4], iris$Species)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
</section>
<section id="discriminant-function-analysis-dfa" class="level2" data-number="38.6">
<h2 data-number="38.6" class="anchored" data-anchor-id="discriminant-function-analysis-dfa"><span class="header-section-number">38.6</span> Discriminant Function Analysis (DFA)</h2>
<p><strong>Discriminant Function Analysis</strong> (DFA, also called Linear Discriminant Analysis or LDA) finds linear combinations of variables that best separate groups. It complements MANOVA by showing <em>how</em> groups differ.</p>
<section id="the-goal-of-dfa" class="level3">
<h3 class="anchored" data-anchor-id="the-goal-of-dfa">The Goal of DFA</h3>
<p>DFA finds discriminant functions—weighted combinations of original variables—that maximize separation between groups while minimizing variation within groups.</p>
<p>The first discriminant function captures the most separation, the second captures remaining separation orthogonal to the first, and so on.</p>
<div id="fig-dfa-concept" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-dfa-concept-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../images/ch36/ch36_dfa_concept.jpeg" class="img-fluid quarto-figure quarto-figure-center figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-dfa-concept-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;38.14: Discriminant function analysis finds linear combinations that maximize group separation
</figcaption>
</figure>
</div>
</section>
<section id="dfa-in-r" class="level3">
<h3 class="anchored" data-anchor-id="dfa-in-r">DFA in R</h3>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb46"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb46-1"><a href="#cb46-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Linear Discriminant Analysis</span></span>
<span id="cb46-2"><a href="#cb46-2" aria-hidden="true" tabindex="-1"></a>lda_model <span class="ot">&lt;-</span> <span class="fu">lda</span>(Species <span class="sc">~</span> ., <span class="at">data =</span> iris)</span>
<span id="cb46-3"><a href="#cb46-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-4"><a href="#cb46-4" aria-hidden="true" tabindex="-1"></a><span class="co"># View the model</span></span>
<span id="cb46-5"><a href="#cb46-5" aria-hidden="true" tabindex="-1"></a>lda_model</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Call:
lda(Species ~ ., data = iris)

Prior probabilities of groups:
    setosa versicolor  virginica 
 0.3333333  0.3333333  0.3333333 

Group means:
           Sepal.Length Sepal.Width Petal.Length Petal.Width
setosa            5.006       3.428        1.462       0.246
versicolor        5.936       2.770        4.260       1.326
virginica         6.588       2.974        5.552       2.026

Coefficients of linear discriminants:
                    LD1         LD2
Sepal.Length  0.8293776 -0.02410215
Sepal.Width   1.5344731 -2.16452123
Petal.Length -2.2012117  0.93192121
Petal.Width  -2.8104603 -2.83918785

Proportion of trace:
   LD1    LD2 
0.9912 0.0088 </code></pre>
</div>
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb48"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb48-1"><a href="#cb48-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Discriminant scores</span></span>
<span id="cb48-2"><a href="#cb48-2" aria-hidden="true" tabindex="-1"></a>lda_scores <span class="ot">&lt;-</span> <span class="fu">predict</span>(lda_model)<span class="sc">$</span>x</span>
<span id="cb48-3"><a href="#cb48-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-4"><a href="#cb48-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot</span></span>
<span id="cb48-5"><a href="#cb48-5" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(lda_scores,</span>
<span id="cb48-6"><a href="#cb48-6" aria-hidden="true" tabindex="-1"></a>     <span class="at">col =</span> <span class="fu">c</span>(<span class="st">"red"</span>, <span class="st">"green"</span>, <span class="st">"blue"</span>)[iris<span class="sc">$</span>Species],</span>
<span id="cb48-7"><a href="#cb48-7" aria-hidden="true" tabindex="-1"></a>     <span class="at">pch =</span> <span class="dv">19</span>,</span>
<span id="cb48-8"><a href="#cb48-8" aria-hidden="true" tabindex="-1"></a>     <span class="at">main =</span> <span class="st">"Discriminant Function Scores"</span>,</span>
<span id="cb48-9"><a href="#cb48-9" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">"LD1"</span>, <span class="at">ylab =</span> <span class="st">"LD2"</span>)</span>
<span id="cb48-10"><a href="#cb48-10" aria-hidden="true" tabindex="-1"></a><span class="fu">legend</span>(<span class="st">"topright"</span>, <span class="fu">levels</span>(iris<span class="sc">$</span>Species),</span>
<span id="cb48-11"><a href="#cb48-11" aria-hidden="true" tabindex="-1"></a>       <span class="at">col =</span> <span class="fu">c</span>(<span class="st">"red"</span>, <span class="st">"green"</span>, <span class="st">"blue"</span>), <span class="at">pch =</span> <span class="dv">19</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div id="fig-lda-scores" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-lda-scores-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="37-dimensionality-reduction_files/figure-html/fig-lda-scores-1.png" class="img-fluid figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-lda-scores-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;38.15: Linear discriminant analysis showing group separation along discriminant functions
</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="interpreting-dfa-output" class="level3">
<h3 class="anchored" data-anchor-id="interpreting-dfa-output">Interpreting DFA Output</h3>
<p>Key components:</p>
<ul>
<li><strong>Coefficients of linear discriminants</strong>: Weights for creating discriminant scores</li>
<li><strong>Proportion of trace</strong>: Variance explained by each discriminant function</li>
<li><strong>Group means</strong>: Average score on each discriminant function for each group</li>
</ul>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb49"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb49-1"><a href="#cb49-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Coefficients (loadings)</span></span>
<span id="cb49-2"><a href="#cb49-2" aria-hidden="true" tabindex="-1"></a>lda_model<span class="sc">$</span>scaling</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>                    LD1         LD2
Sepal.Length  0.8293776 -0.02410215
Sepal.Width   1.5344731 -2.16452123
Petal.Length -2.2012117  0.93192121
Petal.Width  -2.8104603 -2.83918785</code></pre>
</div>
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb51"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb51-1"><a href="#cb51-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Proportion of separation explained</span></span>
<span id="cb51-2"><a href="#cb51-2" aria-hidden="true" tabindex="-1"></a>lda_model<span class="sc">$</span>svd<span class="sc">^</span><span class="dv">2</span> <span class="sc">/</span> <span class="fu">sum</span>(lda_model<span class="sc">$</span>svd<span class="sc">^</span><span class="dv">2</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.991212605 0.008787395</code></pre>
</div>
</div>
</section>
<section id="using-dfa-for-prediction" class="level3">
<h3 class="anchored" data-anchor-id="using-dfa-for-prediction">Using DFA for Prediction</h3>
<p>DFA can classify new observations into groups based on their discriminant scores:</p>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb53"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb53-1"><a href="#cb53-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Classification accuracy</span></span>
<span id="cb53-2"><a href="#cb53-2" aria-hidden="true" tabindex="-1"></a>predictions <span class="ot">&lt;-</span> <span class="fu">predict</span>(lda_model)<span class="sc">$</span>class</span>
<span id="cb53-3"><a href="#cb53-3" aria-hidden="true" tabindex="-1"></a><span class="fu">table</span>(<span class="at">Predicted =</span> predictions, <span class="at">Actual =</span> iris<span class="sc">$</span>Species)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>            Actual
Predicted    setosa versicolor virginica
  setosa         50          0         0
  versicolor      0         48         1
  virginica       0          2        49</code></pre>
</div>
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb55"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb55-1"><a href="#cb55-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Classification accuracy</span></span>
<span id="cb55-2"><a href="#cb55-2" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>(predictions <span class="sc">==</span> iris<span class="sc">$</span>Species)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.98</code></pre>
</div>
</div>
</section>
<section id="cross-validated-classification" class="level3">
<h3 class="anchored" data-anchor-id="cross-validated-classification">Cross-Validated Classification</h3>
<p>For honest estimates of classification accuracy, use leave-one-out cross-validation:</p>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb57"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb57-1"><a href="#cb57-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Cross-validated LDA</span></span>
<span id="cb57-2"><a href="#cb57-2" aria-hidden="true" tabindex="-1"></a>lda_cv <span class="ot">&lt;-</span> <span class="fu">lda</span>(Species <span class="sc">~</span> ., <span class="at">data =</span> iris, <span class="at">CV =</span> <span class="cn">TRUE</span>)</span>
<span id="cb57-3"><a href="#cb57-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-4"><a href="#cb57-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Cross-validated classification table</span></span>
<span id="cb57-5"><a href="#cb57-5" aria-hidden="true" tabindex="-1"></a><span class="fu">table</span>(<span class="at">Predicted =</span> lda_cv<span class="sc">$</span>class, <span class="at">Actual =</span> iris<span class="sc">$</span>Species)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>            Actual
Predicted    setosa versicolor virginica
  setosa         50          0         0
  versicolor      0         48         1
  virginica       0          2        49</code></pre>
</div>
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb59"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb59-1"><a href="#cb59-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Cross-validated accuracy</span></span>
<span id="cb59-2"><a href="#cb59-2" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>(lda_cv<span class="sc">$</span>class <span class="sc">==</span> iris<span class="sc">$</span>Species)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.98</code></pre>
</div>
</div>
</section>
<section id="dfa-for-biomarker-discovery" class="level3">
<h3 class="anchored" data-anchor-id="dfa-for-biomarker-discovery">DFA for Biomarker Discovery</h3>
<p>DFA is valuable for identifying which variables best distinguish groups—useful in biomarker discovery:</p>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb61"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb61-1"><a href="#cb61-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Which variables contribute most to separation?</span></span>
<span id="cb61-2"><a href="#cb61-2" aria-hidden="true" tabindex="-1"></a>scaling_df <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(</span>
<span id="cb61-3"><a href="#cb61-3" aria-hidden="true" tabindex="-1"></a>  <span class="at">Variable =</span> <span class="fu">rownames</span>(lda_model<span class="sc">$</span>scaling),</span>
<span id="cb61-4"><a href="#cb61-4" aria-hidden="true" tabindex="-1"></a>  <span class="at">LD1 =</span> <span class="fu">abs</span>(lda_model<span class="sc">$</span>scaling[, <span class="dv">1</span>]),</span>
<span id="cb61-5"><a href="#cb61-5" aria-hidden="true" tabindex="-1"></a>  <span class="at">LD2 =</span> <span class="fu">abs</span>(lda_model<span class="sc">$</span>scaling[, <span class="dv">2</span>])</span>
<span id="cb61-6"><a href="#cb61-6" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb61-7"><a href="#cb61-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-8"><a href="#cb61-8" aria-hidden="true" tabindex="-1"></a><span class="fu">barplot</span>(scaling_df<span class="sc">$</span>LD1, <span class="at">names.arg =</span> scaling_df<span class="sc">$</span>Variable,</span>
<span id="cb61-9"><a href="#cb61-9" aria-hidden="true" tabindex="-1"></a>        <span class="at">main =</span> <span class="st">"Variable Contributions to LD1"</span>,</span>
<span id="cb61-10"><a href="#cb61-10" aria-hidden="true" tabindex="-1"></a>        <span class="at">ylab =</span> <span class="st">"Absolute Coefficient"</span>,</span>
<span id="cb61-11"><a href="#cb61-11" aria-hidden="true" tabindex="-1"></a>        <span class="at">col =</span> <span class="st">"steelblue"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div id="fig-lda-loadings" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-lda-loadings-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="37-dimensionality-reduction_files/figure-html/fig-lda-loadings-1.png" class="img-fluid figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-lda-loadings-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;38.16: Variable contributions to the first linear discriminant function
</figcaption>
</figure>
</div>
</div>
</div>
</section>
</section>
<section id="comparing-methods" class="level2" data-number="38.7">
<h2 data-number="38.7" class="anchored" data-anchor-id="comparing-methods"><span class="header-section-number">38.7</span> Comparing Methods</h2>
<table class="caption-top table">
<colgroup>
<col style="width: 17%">
<col style="width: 15%">
<col style="width: 17%">
<col style="width: 28%">
<col style="width: 21%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Method</th>
<th style="text-align: left;">Input</th>
<th style="text-align: left;">Output</th>
<th style="text-align: left;">Supervision</th>
<th style="text-align: left;">Best For</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">PCA</td>
<td style="text-align: left;">Variables</td>
<td style="text-align: left;">Continuous scores</td>
<td style="text-align: left;">None</td>
<td style="text-align: left;">Reducing correlated variables</td>
</tr>
<tr class="even">
<td style="text-align: left;">PCoA</td>
<td style="text-align: left;">Distance matrix</td>
<td style="text-align: left;">Continuous scores</td>
<td style="text-align: left;">None</td>
<td style="text-align: left;">Preserving sample distances</td>
</tr>
<tr class="odd">
<td style="text-align: left;">NMDS</td>
<td style="text-align: left;">Distance matrix</td>
<td style="text-align: left;">Ordinal scores</td>
<td style="text-align: left;">None</td>
<td style="text-align: left;">Ecological community data</td>
</tr>
<tr class="even">
<td style="text-align: left;">MANOVA</td>
<td style="text-align: left;">Variables + groups</td>
<td style="text-align: left;">Test statistics</td>
<td style="text-align: left;">Groups known</td>
<td style="text-align: left;">Testing group differences</td>
</tr>
<tr class="odd">
<td style="text-align: left;">DFA</td>
<td style="text-align: left;">Variables + groups</td>
<td style="text-align: left;">Discriminant scores</td>
<td style="text-align: left;">Groups known</td>
<td style="text-align: left;">Classifying observations</td>
</tr>
</tbody>
</table>
<p>For clustering methods (hierarchical, k-means) that group observations based on similarity, see <a href="36-clustering.html" class="quarto-xref"><span>Chapter 37</span></a>.</p>
</section>
<section id="using-ordination-scores-in-further-analyses" class="level2" data-number="38.8">
<h2 data-number="38.8" class="anchored" data-anchor-id="using-ordination-scores-in-further-analyses"><span class="header-section-number">38.8</span> Using Ordination Scores in Further Analyses</h2>
<p>PC scores and discriminant scores are legitimate new variables that can be used in downstream analysis:</p>
<ul>
<li>Regression of scores on other continuous variables</li>
<li>ANOVA comparing groups on ordination scores</li>
<li>Correlation of scores with environmental gradients</li>
</ul>
<p>This is valuable when you have many correlated variables and want to reduce dimensionality before hypothesis testing.</p>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb62"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb62-1"><a href="#cb62-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Use PC scores in ANOVA</span></span>
<span id="cb62-2"><a href="#cb62-2" aria-hidden="true" tabindex="-1"></a>pc_scores <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(</span>
<span id="cb62-3"><a href="#cb62-3" aria-hidden="true" tabindex="-1"></a>  <span class="at">PC1 =</span> iris_pca<span class="sc">$</span>x[, <span class="dv">1</span>],</span>
<span id="cb62-4"><a href="#cb62-4" aria-hidden="true" tabindex="-1"></a>  <span class="at">PC2 =</span> iris_pca<span class="sc">$</span>x[, <span class="dv">2</span>],</span>
<span id="cb62-5"><a href="#cb62-5" aria-hidden="true" tabindex="-1"></a>  <span class="at">Species =</span> iris<span class="sc">$</span>Species</span>
<span id="cb62-6"><a href="#cb62-6" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb62-7"><a href="#cb62-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb62-8"><a href="#cb62-8" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(<span class="fu">aov</span>(PC1 <span class="sc">~</span> Species, <span class="at">data =</span> pc_scores))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>             Df Sum Sq Mean Sq F value Pr(&gt;F)    
Species       2  406.4  203.21    1051 &lt;2e-16 ***
Residuals   147   28.4    0.19                   
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</code></pre>
</div>
</div>
</section>
<section id="practical-workflow" class="level2" data-number="38.9">
<h2 data-number="38.9" class="anchored" data-anchor-id="practical-workflow"><span class="header-section-number">38.9</span> Practical Workflow</h2>
<ol type="1">
<li><p><strong>Explore data</strong>: Check for outliers, missing values, scaling issues</p></li>
<li><p><strong>Standardize if needed</strong>: Especially important when variables are on different scales</p></li>
<li><p><strong>Choose appropriate method</strong>: Based on your data type and question</p></li>
<li><p><strong>Examine output</strong>: Scree plots, loadings, clustering diagnostics</p></li>
<li><p><strong>Validate</strong>: Cross-validation for classification; permutation tests for significance</p></li>
<li><p><strong>Interpret biologically</strong>: What do the patterns mean in your system?</p></li>
</ol>
</section>
<section id="exercises" class="level2" data-number="38.10">
<h2 data-number="38.10" class="anchored" data-anchor-id="exercises"><span class="header-section-number">38.10</span> Exercises</h2>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Exercise DR.1: PCA Exploration
</div>
</div>
<div class="callout-body-container callout-body">
<ol type="1">
<li>We want to explore the <code>tissue_gene_expression</code> predictors by plotting them.</li>
</ol>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb64"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb64-1"><a href="#cb64-1" aria-hidden="true" tabindex="-1"></a><span class="fu">data</span>(<span class="st">"tissue_gene_expression"</span>)</span>
<span id="cb64-2"><a href="#cb64-2" aria-hidden="true" tabindex="-1"></a><span class="fu">dim</span>(tissue_gene_expression<span class="sc">$</span>x)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>We want to get an idea of which observations are close to each other, but the predictors are 500-dimensional so plotting is difficult. Plot the first two principal components with color representing tissue type.</p>
<ol start="2" type="1">
<li><p>The predictors for each observation are measured on the same device and experimental procedure. This introduces biases that can affect all the predictors from one observation. For each observation, compute the average across all predictors and then plot this against the first PC with color representing tissue. Report the correlation.</p></li>
<li><p>We see an association with the first PC and the observation averages. Redo the PCA but only after removing the center (row means).</p></li>
<li><p>For the first 10 PCs, make a boxplot showing the values for each tissue.</p></li>
<li><p>Plot the percent variance explained by PC number. Hint: use the <code>summary</code> function.</p></li>
</ol>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Exercise DR.2: Distance Matrices
</div>
</div>
<div class="callout-body-container callout-body">
<ol start="6" type="1">
<li>Load the following dataset and compute distances:</li>
</ol>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb65"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb65-1"><a href="#cb65-1" aria-hidden="true" tabindex="-1"></a><span class="fu">data</span>(<span class="st">"tissue_gene_expression"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>Compare the distance between the first two observations (both cerebellums), the 39th and 40th (both colons), and the 73rd and 74th (both endometriums). Are observations of the same tissue type closer to each other?</p>
<ol start="7" type="1">
<li>Make an image plot of all the distances using the <code>image</code> function. Hint: convert the distance object to a matrix first. Does the pattern suggest that observations of the same tissue type are generally closer?</li>
</ol>
</div>
</div>
</section>
<section id="non-linear-methods" class="level2" data-number="38.11">
<h2 data-number="38.11" class="anchored" data-anchor-id="non-linear-methods"><span class="header-section-number">38.11</span> Non-Linear Methods</h2>
<p>The methods covered in this chapter are primarily <strong>linear</strong> dimensionality reduction techniques. PCA finds linear combinations of variables, and PCoA preserves Euclidean distances. However, biological data often lies on complex, non-linear manifolds.</p>
<p>For visualization of complex, non-linear structure, see <a href="38-tsne-umap.html" class="quarto-xref"><span>Chapter 39</span></a> which covers:</p>
<ul>
<li><strong>t-SNE</strong> (t-distributed Stochastic Neighbor Embedding): Excellent for visualizing local cluster structure</li>
<li><strong>UMAP</strong> (Uniform Manifold Approximation and Projection): Faster than t-SNE and may better preserve global structure</li>
</ul>
<p>These non-linear methods are particularly valuable for single-cell RNA-seq data and other high-dimensional biological datasets with complex structure.</p>
</section>
<section id="summary" class="level2" data-number="38.12">
<h2 data-number="38.12" class="anchored" data-anchor-id="summary"><span class="header-section-number">38.12</span> Summary</h2>
<ul>
<li>Dimensionality reduction creates fewer variables that capture most information</li>
<li><strong>PCA</strong> finds linear combinations that maximize variance
<ul>
<li>Orthogonal transformations preserve distances while concentrating variance</li>
<li>Highly correlated variables can be effectively reduced to fewer dimensions</li>
<li>Loadings show which original variables contribute to each PC</li>
<li>The twin heights example shows how correlated variables compress to one dimension</li>
</ul></li>
<li><strong>PCoA</strong> works from distance matrices; useful for ecological and genetic data</li>
<li><strong>NMDS</strong> preserves rank-order of distances; robust for non-normal data</li>
<li><strong>MANOVA</strong> tests whether groups differ on multiple response variables simultaneously</li>
<li><strong>DFA/LDA</strong> finds combinations that best discriminate known groups</li>
<li>These methods can be combined: use PCA to reduce dimensions, then classify</li>
<li>PCA can substantially reduce model complexity while maintaining predictive performance</li>
<li>For non-linear structure, consider t-SNE or UMAP (see <a href="38-tsne-umap.html" class="quarto-xref"><span>Chapter 39</span></a>)</li>
</ul>
</section>
<section id="additional-resources" class="level2" data-number="38.13">
<h2 data-number="38.13" class="anchored" data-anchor-id="additional-resources"><span class="header-section-number">38.13</span> Additional Resources</h2>
<ul>
<li><a href="A10-matrix-algebra.html" class="quarto-xref"><span>Chapter 52</span></a> - Matrix algebra fundamentals and eigenanalysis for PCA</li>
<li><a href="38-tsne-umap.html" class="quarto-xref"><span>Chapter 39</span></a> - Non-linear dimensionality reduction with t-SNE and UMAP</li>
<li><span class="citation" data-cites="james2023islr">James et al. (<a href="../references.html#ref-james2023islr" role="doc-biblioref">2023</a>)</span> - Modern treatment of dimensionality reduction and clustering</li>
<li><span class="citation" data-cites="logan2010biostatistical">Logan (<a href="../references.html#ref-logan2010biostatistical" role="doc-biblioref">2010</a>)</span> - MANOVA and DFA in biological research contexts</li>
<li>Borcard, D., Gillet, F., &amp; Legendre, P. (2018). <em>Numerical Ecology with R</em> - Comprehensive ordination methods</li>
</ul>


<!-- -->

<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list" style="display: none">
<div id="ref-hotelling1933analysis" class="csl-entry" role="listitem">
Hotelling, Harold. 1933. <span>“Analysis of a Complex of Statistical Variables into Principal Components.”</span> <em>Journal of Educational Psychology</em> 24 (6): 417–41.
</div>
<div id="ref-james2023islr" class="csl-entry" role="listitem">
James, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2023. <em>An Introduction to Statistical Learning with Applications in r</em>. 2nd ed. Springer. <a href="https://www.statlearning.com">https://www.statlearning.com</a>.
</div>
<div id="ref-logan2010biostatistical" class="csl-entry" role="listitem">
Logan, Murray. 2010. <em>Biostatistical Design and Analysis Using r</em>. Wiley-Blackwell.
</div>
<div id="ref-pearson1901lines" class="csl-entry" role="listitem">
Pearson, Karl. 1901. <span>“On Lines and Planes of Closest Fit to Systems of Points in Space.”</span> <em>The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science</em> 2 (11): 559–72.
</div>
</div>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
    const viewSource = window.document.getElementById('quarto-view-source') ||
                       window.document.getElementById('quarto-code-tools-source');
    if (viewSource) {
      const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
      viewSource.addEventListener("click", function(e) {
        if (sourceUrl) {
          // rstudio viewer pane
          if (/\bcapabilities=\b/.test(window.location)) {
            window.open(sourceUrl);
          } else {
            window.location.href = sourceUrl;
          }
        } else {
          const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
          modal.show();
        }
        return false;
      });
    }
    function toggleCodeHandler(show) {
      return function(e) {
        const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
        for (let i=0; i<detailsSrc.length; i++) {
          const details = detailsSrc[i].parentElement;
          if (show) {
            details.open = true;
          } else {
            details.removeAttribute("open");
          }
        }
        const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
        const fromCls = show ? "hidden" : "unhidden";
        const toCls = show ? "unhidden" : "hidden";
        for (let i=0; i<cellCodeDivs.length; i++) {
          const codeDiv = cellCodeDivs[i];
          if (codeDiv.classList.contains(fromCls)) {
            codeDiv.classList.remove(fromCls);
            codeDiv.classList.add(toCls);
          } 
        }
        return false;
      }
    }
    const hideAllCode = window.document.getElementById("quarto-hide-all-code");
    if (hideAllCode) {
      hideAllCode.addEventListener("click", toggleCodeHandler(false));
    }
    const showAllCode = window.document.getElementById("quarto-show-all-code");
    if (showAllCode) {
      showAllCode.addEventListener("click", toggleCodeHandler(true));
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../chapters/36-clustering.html" class="pagination-link" aria-label="Clustering">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">37</span>&nbsp; <span class="chapter-title">Clustering</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../chapters/38-tsne-umap.html" class="pagination-link" aria-label="Non-Linear Dimensionality Reduction: t-SNE and UMAP">
        <span class="nav-page-text"><span class="chapter-number">39</span>&nbsp; <span class="chapter-title">Non-Linear Dimensionality Reduction: t-SNE and UMAP</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb66" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb66-1"><a href="#cb66-1" aria-hidden="true" tabindex="-1"></a><span class="fu"># Dimensionality Reduction and Multivariate Methods {#sec-dimensionality-reduction}</span></span>
<span id="cb66-2"><a href="#cb66-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-5"><a href="#cb66-5" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb66-6"><a href="#cb66-6" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: false</span></span>
<span id="cb66-7"><a href="#cb66-7" aria-hidden="true" tabindex="-1"></a><span class="co">#| message: false</span></span>
<span id="cb66-8"><a href="#cb66-8" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tidyverse)</span>
<span id="cb66-9"><a href="#cb66-9" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(MASS)</span>
<span id="cb66-10"><a href="#cb66-10" aria-hidden="true" tabindex="-1"></a><span class="fu">theme_set</span>(<span class="fu">theme_minimal</span>())</span>
<span id="cb66-11"><a href="#cb66-11" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb66-12"><a href="#cb66-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-13"><a href="#cb66-13" aria-hidden="true" tabindex="-1"></a><span class="fu">## The Challenge of High-Dimensional Data</span></span>
<span id="cb66-14"><a href="#cb66-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-15"><a href="#cb66-15" aria-hidden="true" tabindex="-1"></a>Modern biology generates datasets with many variables: gene expression across thousands of genes, metabolomic profiles with hundreds of compounds, morphological measurements on many traits. When datasets have many variables, visualization becomes challenging and statistical analysis becomes complicated.</span>
<span id="cb66-16"><a href="#cb66-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-17"><a href="#cb66-17" aria-hidden="true" tabindex="-1"></a>A typical machine learning challenge might include hundreds or thousands of predictors. For example, to compare each of the 784 features in a digit recognition problem, we would need to create 306,936 scatterplots! Creating a single scatter-plot of all the data is impossible due to the high dimensionality.</span>
<span id="cb66-18"><a href="#cb66-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-19"><a href="#cb66-19" aria-hidden="true" tabindex="-1"></a>**Dimensionality reduction** creates a smaller set of new variables that capture most of the information in the original data. The general idea is to reduce the dimension of the dataset while preserving important characteristics, such as the distance between features or observations. These techniques help us:</span>
<span id="cb66-20"><a href="#cb66-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-21"><a href="#cb66-21" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Visualize high-dimensional data in 2D or 3D</span>
<span id="cb66-22"><a href="#cb66-22" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Identify patterns and clusters</span>
<span id="cb66-23"><a href="#cb66-23" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Remove noise and redundancy</span>
<span id="cb66-24"><a href="#cb66-24" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Reduce complexity of downstream models</span>
<span id="cb66-25"><a href="#cb66-25" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Create composite variables for analysis</span>
<span id="cb66-26"><a href="#cb66-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-27"><a href="#cb66-27" aria-hidden="true" tabindex="-1"></a><span class="fu">## Principal Component Analysis (PCA)</span></span>
<span id="cb66-28"><a href="#cb66-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-29"><a href="#cb66-29" aria-hidden="true" tabindex="-1"></a>**Principal Component Analysis (PCA)** <span class="co">[</span><span class="ot">@pearson1901lines; @hotelling1933analysis</span><span class="co">]</span> is the most widely used dimensionality reduction technique. It finds new variables (principal components) that are linear combinations of the originals, chosen to capture maximum variance. The technique behind it—the singular value decomposition—is also useful in many other contexts.</span>
<span id="cb66-30"><a href="#cb66-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-31"><a href="#cb66-31" aria-hidden="true" tabindex="-1"></a><span class="fu">### Intuition: Preserving Distance</span></span>
<span id="cb66-32"><a href="#cb66-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-33"><a href="#cb66-33" aria-hidden="true" tabindex="-1"></a>Before diving into the mathematics, let's build intuition with a simple example. Consider twin heights data where we have height measurements for 100 pairs of twins—some adults, some children.</span>
<span id="cb66-34"><a href="#cb66-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-37"><a href="#cb66-37" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb66-38"><a href="#cb66-38" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-twin-heights</span></span>
<span id="cb66-39"><a href="#cb66-39" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "Simulated twin heights showing two groups (adults and children) with high correlation between twins"</span></span>
<span id="cb66-40"><a href="#cb66-40" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-width: 6</span></span>
<span id="cb66-41"><a href="#cb66-41" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-height: 6</span></span>
<span id="cb66-42"><a href="#cb66-42" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1988</span>)</span>
<span id="cb66-43"><a href="#cb66-43" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">100</span></span>
<span id="cb66-44"><a href="#cb66-44" aria-hidden="true" tabindex="-1"></a>Sigma <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">c</span>(<span class="dv">9</span>, <span class="dv">9</span> <span class="sc">*</span> <span class="fl">0.9</span>, <span class="dv">9</span> <span class="sc">*</span> <span class="fl">0.92</span>, <span class="dv">9</span> <span class="sc">*</span> <span class="dv">1</span>), <span class="dv">2</span>, <span class="dv">2</span>)</span>
<span id="cb66-45"><a href="#cb66-45" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">rbind</span>(<span class="fu">mvrnorm</span>(n <span class="sc">/</span> <span class="dv">2</span>, <span class="fu">c</span>(<span class="dv">69</span>, <span class="dv">69</span>), Sigma),</span>
<span id="cb66-46"><a href="#cb66-46" aria-hidden="true" tabindex="-1"></a>           <span class="fu">mvrnorm</span>(n <span class="sc">/</span> <span class="dv">2</span>, <span class="fu">c</span>(<span class="dv">55</span>, <span class="dv">55</span>), Sigma))</span>
<span id="cb66-47"><a href="#cb66-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-48"><a href="#cb66-48" aria-hidden="true" tabindex="-1"></a>lim <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">48</span>, <span class="dv">78</span>)</span>
<span id="cb66-49"><a href="#cb66-49" aria-hidden="true" tabindex="-1"></a>rafalib<span class="sc">::</span><span class="fu">mypar</span>()</span>
<span id="cb66-50"><a href="#cb66-50" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(x, <span class="at">xlim =</span> lim, <span class="at">ylim =</span> lim,</span>
<span id="cb66-51"><a href="#cb66-51" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">"Twin 1 Height"</span>, <span class="at">ylab =</span> <span class="st">"Twin 2 Height"</span>,</span>
<span id="cb66-52"><a href="#cb66-52" aria-hidden="true" tabindex="-1"></a>     <span class="at">main =</span> <span class="st">"Twin Heights: Adults and Children"</span>)</span>
<span id="cb66-53"><a href="#cb66-53" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(x[<span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">2</span>), ], <span class="at">col =</span> <span class="st">"blue"</span>, <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb66-54"><a href="#cb66-54" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(x[<span class="fu">c</span>(<span class="dv">2</span>, <span class="dv">51</span>), ], <span class="at">col =</span> <span class="st">"red"</span>, <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb66-55"><a href="#cb66-55" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span>(x[<span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">51</span>), ], <span class="at">pch =</span> <span class="dv">16</span>)</span>
<span id="cb66-56"><a href="#cb66-56" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb66-57"><a href="#cb66-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-58"><a href="#cb66-58" aria-hidden="true" tabindex="-1"></a>The scatterplot reveals high correlation and two clear groups: adults (upper right) and children (lower left). We can compute distances between observations:</span>
<span id="cb66-59"><a href="#cb66-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-62"><a href="#cb66-62" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb66-63"><a href="#cb66-63" aria-hidden="true" tabindex="-1"></a>d <span class="ot">&lt;-</span> <span class="fu">dist</span>(x)</span>
<span id="cb66-64"><a href="#cb66-64" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"Distance between observations 1 and 2:"</span>, <span class="fu">round</span>(<span class="fu">as.matrix</span>(d)[<span class="dv">1</span>, <span class="dv">2</span>], <span class="dv">2</span>), <span class="st">"</span><span class="sc">\n</span><span class="st">"</span>)</span>
<span id="cb66-65"><a href="#cb66-65" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"Distance between observations 2 and 51:"</span>, <span class="fu">round</span>(<span class="fu">as.matrix</span>(d)[<span class="dv">2</span>, <span class="dv">51</span>], <span class="dv">2</span>), <span class="st">"</span><span class="sc">\n</span><span class="st">"</span>)</span>
<span id="cb66-66"><a href="#cb66-66" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb66-67"><a href="#cb66-67" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-68"><a href="#cb66-68" aria-hidden="true" tabindex="-1"></a>Now suppose we want to reduce from two dimensions to one while preserving these distances. A naive approach is to simply use one of the original variables:</span>
<span id="cb66-69"><a href="#cb66-69" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-72"><a href="#cb66-72" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb66-73"><a href="#cb66-73" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-naive-reduction</span></span>
<span id="cb66-74"><a href="#cb66-74" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "Using only one dimension (Twin 1 height) underestimates the true distances"</span></span>
<span id="cb66-75"><a href="#cb66-75" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-width: 6</span></span>
<span id="cb66-76"><a href="#cb66-76" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-height: 5</span></span>
<span id="cb66-77"><a href="#cb66-77" aria-hidden="true" tabindex="-1"></a>z <span class="ot">&lt;-</span> x[, <span class="dv">1</span>]</span>
<span id="cb66-78"><a href="#cb66-78" aria-hidden="true" tabindex="-1"></a>rafalib<span class="sc">::</span><span class="fu">mypar</span>()</span>
<span id="cb66-79"><a href="#cb66-79" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(<span class="fu">dist</span>(x) <span class="sc">/</span> <span class="fu">sqrt</span>(<span class="dv">2</span>), <span class="fu">dist</span>(z),</span>
<span id="cb66-80"><a href="#cb66-80" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">"Original distance (scaled)"</span>, <span class="at">ylab =</span> <span class="st">"One-dimension distance"</span>,</span>
<span id="cb66-81"><a href="#cb66-81" aria-hidden="true" tabindex="-1"></a>     <span class="at">main =</span> <span class="st">"Distance Approximation Using One Variable"</span>)</span>
<span id="cb66-82"><a href="#cb66-82" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="at">col =</span> <span class="st">"red"</span>)</span>
<span id="cb66-83"><a href="#cb66-83" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb66-84"><a href="#cb66-84" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-85"><a href="#cb66-85" aria-hidden="true" tabindex="-1"></a>This works reasonably well, but we can do better. Notice that the variation in the data lies mostly along the diagonal. If we transform to the average and difference:</span>
<span id="cb66-86"><a href="#cb66-86" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-89"><a href="#cb66-89" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb66-90"><a href="#cb66-90" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-better-reduction</span></span>
<span id="cb66-91"><a href="#cb66-91" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "After rotating to average and difference coordinates, distances are mostly explained by the first dimension"</span></span>
<span id="cb66-92"><a href="#cb66-92" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-width: 6</span></span>
<span id="cb66-93"><a href="#cb66-93" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-height: 6</span></span>
<span id="cb66-94"><a href="#cb66-94" aria-hidden="true" tabindex="-1"></a>z <span class="ot">&lt;-</span> <span class="fu">cbind</span>((x[, <span class="dv">2</span>] <span class="sc">+</span> x[, <span class="dv">1</span>]) <span class="sc">/</span> <span class="dv">2</span>, x[, <span class="dv">2</span>] <span class="sc">-</span> x[, <span class="dv">1</span>])</span>
<span id="cb66-95"><a href="#cb66-95" aria-hidden="true" tabindex="-1"></a>rafalib<span class="sc">::</span><span class="fu">mypar</span>()</span>
<span id="cb66-96"><a href="#cb66-96" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(z, <span class="at">xlim =</span> lim, <span class="at">ylim =</span> lim <span class="sc">-</span> <span class="fu">mean</span>(lim),</span>
<span id="cb66-97"><a href="#cb66-97" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">"Average Height"</span>, <span class="at">ylab =</span> <span class="st">"Difference"</span>,</span>
<span id="cb66-98"><a href="#cb66-98" aria-hidden="true" tabindex="-1"></a>     <span class="at">main =</span> <span class="st">"Transformed Coordinates"</span>)</span>
<span id="cb66-99"><a href="#cb66-99" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(z[<span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">2</span>), ], <span class="at">col =</span> <span class="st">"blue"</span>, <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb66-100"><a href="#cb66-100" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(z[<span class="fu">c</span>(<span class="dv">2</span>, <span class="dv">51</span>), ], <span class="at">col =</span> <span class="st">"red"</span>, <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb66-101"><a href="#cb66-101" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span>(z[<span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">51</span>), ], <span class="at">pch =</span> <span class="dv">16</span>)</span>
<span id="cb66-102"><a href="#cb66-102" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb66-103"><a href="#cb66-103" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-104"><a href="#cb66-104" aria-hidden="true" tabindex="-1"></a>Now the distances are mostly explained by the first dimension (the average). Using just this one dimension gives better distance approximation:</span>
<span id="cb66-105"><a href="#cb66-105" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-108"><a href="#cb66-108" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb66-109"><a href="#cb66-109" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"Typical error with original variable:"</span>,</span>
<span id="cb66-110"><a href="#cb66-110" aria-hidden="true" tabindex="-1"></a>    <span class="fu">round</span>(<span class="fu">sd</span>(<span class="fu">dist</span>(x) <span class="sc">-</span> <span class="fu">dist</span>(x[, <span class="dv">1</span>]) <span class="sc">*</span> <span class="fu">sqrt</span>(<span class="dv">2</span>)), <span class="dv">2</span>), <span class="st">"</span><span class="sc">\n</span><span class="st">"</span>)</span>
<span id="cb66-111"><a href="#cb66-111" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"Typical error with average:"</span>,</span>
<span id="cb66-112"><a href="#cb66-112" aria-hidden="true" tabindex="-1"></a>    <span class="fu">round</span>(<span class="fu">sd</span>(<span class="fu">dist</span>(x) <span class="sc">-</span> <span class="fu">dist</span>(z[, <span class="dv">1</span>]) <span class="sc">*</span> <span class="fu">sqrt</span>(<span class="dv">2</span>)), <span class="dv">2</span>), <span class="st">"</span><span class="sc">\n</span><span class="st">"</span>)</span>
<span id="cb66-113"><a href="#cb66-113" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb66-114"><a href="#cb66-114" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-115"><a href="#cb66-115" aria-hidden="true" tabindex="-1"></a>The average of the twin heights is essentially the first principal component! PCA finds these optimal linear combinations automatically.</span>
<span id="cb66-116"><a href="#cb66-116" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-117"><a href="#cb66-117" aria-hidden="true" tabindex="-1"></a><span class="fu">### Linear Transformations</span></span>
<span id="cb66-118"><a href="#cb66-118" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-119"><a href="#cb66-119" aria-hidden="true" tabindex="-1"></a>Each row of the original matrix $X$ was transformed using a linear transformation to create $Z$:</span>
<span id="cb66-120"><a href="#cb66-120" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-121"><a href="#cb66-121" aria-hidden="true" tabindex="-1"></a>$$Z_{i,1} = a_{1,1} X_{i,1} + a_{2,1} X_{i,2}$$</span>
<span id="cb66-122"><a href="#cb66-122" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-123"><a href="#cb66-123" aria-hidden="true" tabindex="-1"></a>with $a_{1,1} = 0.5$ and $a_{2,1} = 0.5$ (the average).</span>
<span id="cb66-124"><a href="#cb66-124" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-125"><a href="#cb66-125" aria-hidden="true" tabindex="-1"></a>In matrix notation:</span>
<span id="cb66-126"><a href="#cb66-126" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb66-127"><a href="#cb66-127" aria-hidden="true" tabindex="-1"></a>Z = X A</span>
<span id="cb66-128"><a href="#cb66-128" aria-hidden="true" tabindex="-1"></a>\mbox{ with }</span>
<span id="cb66-129"><a href="#cb66-129" aria-hidden="true" tabindex="-1"></a>A = \begin{pmatrix}</span>
<span id="cb66-130"><a href="#cb66-130" aria-hidden="true" tabindex="-1"></a>1/2 &amp; 1 <span class="sc">\\</span></span>
<span id="cb66-131"><a href="#cb66-131" aria-hidden="true" tabindex="-1"></a>1/2 &amp; -1</span>
<span id="cb66-132"><a href="#cb66-132" aria-hidden="true" tabindex="-1"></a>\end{pmatrix}</span>
<span id="cb66-133"><a href="#cb66-133" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb66-134"><a href="#cb66-134" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-135"><a href="#cb66-135" aria-hidden="true" tabindex="-1"></a>**Dimension reduction** can be described as applying a transformation $A$ to a matrix $X$ that moves the information to the first few columns of $Z = XA$, then keeping just these informative columns.</span>
<span id="cb66-136"><a href="#cb66-136" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-137"><a href="#cb66-137" aria-hidden="true" tabindex="-1"></a><span class="fu">### Orthogonal Transformations</span></span>
<span id="cb66-138"><a href="#cb66-138" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-139"><a href="#cb66-139" aria-hidden="true" tabindex="-1"></a>To preserve distances exactly, we need an **orthogonal transformation**—one where the columns of $A$ have unit length and are perpendicular:</span>
<span id="cb66-140"><a href="#cb66-140" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-143"><a href="#cb66-143" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb66-144"><a href="#cb66-144" aria-hidden="true" tabindex="-1"></a><span class="co"># Orthogonal transformation</span></span>
<span id="cb66-145"><a href="#cb66-145" aria-hidden="true" tabindex="-1"></a>z[, <span class="dv">1</span>] <span class="ot">&lt;-</span> (x[, <span class="dv">1</span>] <span class="sc">+</span> x[, <span class="dv">2</span>]) <span class="sc">/</span> <span class="fu">sqrt</span>(<span class="dv">2</span>)</span>
<span id="cb66-146"><a href="#cb66-146" aria-hidden="true" tabindex="-1"></a>z[, <span class="dv">2</span>] <span class="ot">&lt;-</span> (x[, <span class="dv">2</span>] <span class="sc">-</span> x[, <span class="dv">1</span>]) <span class="sc">/</span> <span class="fu">sqrt</span>(<span class="dv">2</span>)</span>
<span id="cb66-147"><a href="#cb66-147" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-148"><a href="#cb66-148" aria-hidden="true" tabindex="-1"></a><span class="co"># This preserves the original distances exactly</span></span>
<span id="cb66-149"><a href="#cb66-149" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"Maximum distance difference:"</span>, <span class="fu">max</span>(<span class="fu">abs</span>(<span class="fu">dist</span>(z) <span class="sc">-</span> <span class="fu">dist</span>(x))), <span class="st">"</span><span class="sc">\n</span><span class="st">"</span>)</span>
<span id="cb66-150"><a href="#cb66-150" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb66-151"><a href="#cb66-151" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-152"><a href="#cb66-152" aria-hidden="true" tabindex="-1"></a>An orthogonal rotation preserves all distances while reorganizing the variance. Now most variance is in the first dimension:</span>
<span id="cb66-153"><a href="#cb66-153" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-156"><a href="#cb66-156" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb66-157"><a href="#cb66-157" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-variance-concentration</span></span>
<span id="cb66-158"><a href="#cb66-158" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "After orthogonal rotation, the first dimension clearly separates adults from children"</span></span>
<span id="cb66-159"><a href="#cb66-159" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-width: 6</span></span>
<span id="cb66-160"><a href="#cb66-160" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-height: 4</span></span>
<span id="cb66-161"><a href="#cb66-161" aria-hidden="true" tabindex="-1"></a><span class="fu">qplot</span>(z[, <span class="dv">1</span>], <span class="at">bins =</span> <span class="dv">20</span>, <span class="at">color =</span> <span class="fu">I</span>(<span class="st">"black"</span>), <span class="at">fill =</span> <span class="fu">I</span>(<span class="st">"steelblue"</span>)) <span class="sc">+</span></span>
<span id="cb66-162"><a href="#cb66-162" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">x =</span> <span class="st">"First Principal Component"</span>, <span class="at">y =</span> <span class="st">"Count"</span>,</span>
<span id="cb66-163"><a href="#cb66-163" aria-hidden="true" tabindex="-1"></a>       <span class="at">title =</span> <span class="st">"PC1 Separates Adults from Children"</span>)</span>
<span id="cb66-164"><a href="#cb66-164" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb66-165"><a href="#cb66-165" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-166"><a href="#cb66-166" aria-hidden="true" tabindex="-1"></a>The first PC clearly shows the two groups. We've reduced two dimensions to one with minimal information loss because the original variables were highly correlated:</span>
<span id="cb66-167"><a href="#cb66-167" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-170"><a href="#cb66-170" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb66-171"><a href="#cb66-171" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"Correlation between twin heights:"</span>, <span class="fu">round</span>(<span class="fu">cor</span>(x[, <span class="dv">1</span>], x[, <span class="dv">2</span>]), <span class="dv">3</span>), <span class="st">"</span><span class="sc">\n</span><span class="st">"</span>)</span>
<span id="cb66-172"><a href="#cb66-172" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"Correlation between PCs:"</span>, <span class="fu">round</span>(<span class="fu">cor</span>(z[, <span class="dv">1</span>], z[, <span class="dv">2</span>]), <span class="dv">3</span>), <span class="st">"</span><span class="sc">\n</span><span class="st">"</span>)</span>
<span id="cb66-173"><a href="#cb66-173" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb66-174"><a href="#cb66-174" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-175"><a href="#cb66-175" aria-hidden="true" tabindex="-1"></a><span class="fu">### The Eigenanalysis Foundation</span></span>
<span id="cb66-176"><a href="#cb66-176" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-177"><a href="#cb66-177" aria-hidden="true" tabindex="-1"></a>The mathematical foundation involves **eigenanalysis**: decomposing the covariance (or correlation) matrix to find directions of maximum variation. For a detailed treatment of eigenvalues, eigenvectors, and their interpretation, see @sec-eigenanalysis.</span>
<span id="cb66-178"><a href="#cb66-178" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-179"><a href="#cb66-179" aria-hidden="true" tabindex="-1"></a>Given a covariance matrix $\Sigma$, eigenanalysis finds:</span>
<span id="cb66-180"><a href="#cb66-180" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-181"><a href="#cb66-181" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Eigenvectors**: The directions of the principal components (loadings)</span>
<span id="cb66-182"><a href="#cb66-182" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Eigenvalues**: The variance explained by each component</span>
<span id="cb66-183"><a href="#cb66-183" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-184"><a href="#cb66-184" aria-hidden="true" tabindex="-1"></a>The first principal component points in the direction of maximum variance. Each subsequent component is orthogonal (uncorrelated) and captures remaining variance in decreasing order.</span>
<span id="cb66-185"><a href="#cb66-185" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-186"><a href="#cb66-186" aria-hidden="true" tabindex="-1"></a>The total variability can be defined as the sum of variances:</span>
<span id="cb66-187"><a href="#cb66-187" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb66-188"><a href="#cb66-188" aria-hidden="true" tabindex="-1"></a>v_1 + v_2 + \cdots + v_p</span>
<span id="cb66-189"><a href="#cb66-189" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb66-190"><a href="#cb66-190" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-191"><a href="#cb66-191" aria-hidden="true" tabindex="-1"></a>An orthogonal transformation preserves total variability but redistributes it. PCA finds the transformation that concentrates variance in the first few components.</span>
<span id="cb66-192"><a href="#cb66-192" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-193"><a href="#cb66-193" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip}</span>
<span id="cb66-194"><a href="#cb66-194" aria-hidden="true" tabindex="-1"></a><span class="fu">## Interpreting Eigenvalues and Eigenvectors</span></span>
<span id="cb66-195"><a href="#cb66-195" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-196"><a href="#cb66-196" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Eigenvalues** tell you *how much* variance each PC captures—larger means more important</span>
<span id="cb66-197"><a href="#cb66-197" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Eigenvectors** (loadings) tell you *how* original variables combine to form each PC</span>
<span id="cb66-198"><a href="#cb66-198" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Variables with large absolute loadings contribute strongly to that PC</span>
<span id="cb66-199"><a href="#cb66-199" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>The sum of all eigenvalues equals the total variance in the data</span>
<span id="cb66-200"><a href="#cb66-200" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-201"><a href="#cb66-201" aria-hidden="true" tabindex="-1"></a>See @sec-eigen-pca for a complete explanation with examples.</span>
<span id="cb66-202"><a href="#cb66-202" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb66-203"><a href="#cb66-203" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-204"><a href="#cb66-204" aria-hidden="true" tabindex="-1"></a><span class="al">![Geometric interpretation of principal components as directions of maximum variance](../images/ch36/ch36_pca_geometry.jpeg)</span>{#fig-pca-geometry fig-align="center"}</span>
<span id="cb66-205"><a href="#cb66-205" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-206"><a href="#cb66-206" aria-hidden="true" tabindex="-1"></a><span class="fu">### PCA in R</span></span>
<span id="cb66-207"><a href="#cb66-207" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-210"><a href="#cb66-210" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb66-211"><a href="#cb66-211" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-pca-iris</span></span>
<span id="cb66-212"><a href="#cb66-212" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "PCA scree plot and principal component scores for iris data colored by species"</span></span>
<span id="cb66-213"><a href="#cb66-213" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-width: 8</span></span>
<span id="cb66-214"><a href="#cb66-214" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-height: 6</span></span>
<span id="cb66-215"><a href="#cb66-215" aria-hidden="true" tabindex="-1"></a><span class="co"># PCA on iris data</span></span>
<span id="cb66-216"><a href="#cb66-216" aria-hidden="true" tabindex="-1"></a>iris_pca <span class="ot">&lt;-</span> <span class="fu">prcomp</span>(iris[, <span class="dv">1</span><span class="sc">:</span><span class="dv">4</span>], <span class="at">scale. =</span> <span class="cn">TRUE</span>)</span>
<span id="cb66-217"><a href="#cb66-217" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-218"><a href="#cb66-218" aria-hidden="true" tabindex="-1"></a><span class="co"># Variance explained</span></span>
<span id="cb66-219"><a href="#cb66-219" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(iris_pca)</span>
<span id="cb66-220"><a href="#cb66-220" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-221"><a href="#cb66-221" aria-hidden="true" tabindex="-1"></a><span class="co"># Scree plot</span></span>
<span id="cb66-222"><a href="#cb66-222" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">2</span>))</span>
<span id="cb66-223"><a href="#cb66-223" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(iris_pca, <span class="at">type =</span> <span class="st">"l"</span>, <span class="at">main =</span> <span class="st">"Scree Plot"</span>)</span>
<span id="cb66-224"><a href="#cb66-224" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-225"><a href="#cb66-225" aria-hidden="true" tabindex="-1"></a><span class="co"># PC scores colored by species</span></span>
<span id="cb66-226"><a href="#cb66-226" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(iris_pca<span class="sc">$</span>x[, <span class="dv">1</span><span class="sc">:</span><span class="dv">2</span>],</span>
<span id="cb66-227"><a href="#cb66-227" aria-hidden="true" tabindex="-1"></a>     <span class="at">col =</span> <span class="fu">c</span>(<span class="st">"red"</span>, <span class="st">"green"</span>, <span class="st">"blue"</span>)[iris<span class="sc">$</span>Species],</span>
<span id="cb66-228"><a href="#cb66-228" aria-hidden="true" tabindex="-1"></a>     <span class="at">pch =</span> <span class="dv">19</span>, <span class="at">xlab =</span> <span class="st">"PC1"</span>, <span class="at">ylab =</span> <span class="st">"PC2"</span>,</span>
<span id="cb66-229"><a href="#cb66-229" aria-hidden="true" tabindex="-1"></a>     <span class="at">main =</span> <span class="st">"PCA of Iris Data"</span>)</span>
<span id="cb66-230"><a href="#cb66-230" aria-hidden="true" tabindex="-1"></a><span class="fu">legend</span>(<span class="st">"topright"</span>, <span class="fu">levels</span>(iris<span class="sc">$</span>Species),</span>
<span id="cb66-231"><a href="#cb66-231" aria-hidden="true" tabindex="-1"></a>       <span class="at">col =</span> <span class="fu">c</span>(<span class="st">"red"</span>, <span class="st">"green"</span>, <span class="st">"blue"</span>), <span class="at">pch =</span> <span class="dv">19</span>)</span>
<span id="cb66-232"><a href="#cb66-232" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb66-233"><a href="#cb66-233" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-234"><a href="#cb66-234" aria-hidden="true" tabindex="-1"></a><span class="fu">### Loadings: What Variables Drive Each PC?</span></span>
<span id="cb66-235"><a href="#cb66-235" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-236"><a href="#cb66-236" aria-hidden="true" tabindex="-1"></a>Each principal component is defined by its **loadings**—the coefficients showing how much each original variable contributes:</span>
<span id="cb66-237"><a href="#cb66-237" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-240"><a href="#cb66-240" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb66-241"><a href="#cb66-241" aria-hidden="true" tabindex="-1"></a><span class="co"># Loadings (rotation matrix)</span></span>
<span id="cb66-242"><a href="#cb66-242" aria-hidden="true" tabindex="-1"></a>iris_pca<span class="sc">$</span>rotation</span>
<span id="cb66-243"><a href="#cb66-243" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb66-244"><a href="#cb66-244" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-245"><a href="#cb66-245" aria-hidden="true" tabindex="-1"></a>Large absolute loadings indicate that a variable strongly influences that component. The sign indicates the direction of the relationship.</span>
<span id="cb66-246"><a href="#cb66-246" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-247"><a href="#cb66-247" aria-hidden="true" tabindex="-1"></a><span class="fu">### Interpreting PCA Results</span></span>
<span id="cb66-248"><a href="#cb66-248" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-249"><a href="#cb66-249" aria-hidden="true" tabindex="-1"></a>Key elements of PCA output:</span>
<span id="cb66-250"><a href="#cb66-250" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-251"><a href="#cb66-251" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Eigenvalues**: Variance explained by each component (shown in scree plot)</span>
<span id="cb66-252"><a href="#cb66-252" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Proportion of variance**: How much of total variance each PC captures</span>
<span id="cb66-253"><a href="#cb66-253" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Loadings**: Coefficients relating original variables to PCs</span>
<span id="cb66-254"><a href="#cb66-254" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Scores**: Values of the new variables for each observation</span>
<span id="cb66-255"><a href="#cb66-255" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-256"><a href="#cb66-256" aria-hidden="true" tabindex="-1"></a>The first few PCs often capture most of the meaningful variation, allowing you to reduce many variables to just 2-3 for visualization and analysis.</span>
<span id="cb66-257"><a href="#cb66-257" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-258"><a href="#cb66-258" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip}</span>
<span id="cb66-259"><a href="#cb66-259" aria-hidden="true" tabindex="-1"></a><span class="fu">## How Many Components to Keep?</span></span>
<span id="cb66-260"><a href="#cb66-260" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-261"><a href="#cb66-261" aria-hidden="true" tabindex="-1"></a>Common approaches:</span>
<span id="cb66-262"><a href="#cb66-262" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-263"><a href="#cb66-263" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Keep components with eigenvalues &gt; 1 (Kaiser criterion)</span>
<span id="cb66-264"><a href="#cb66-264" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Keep enough to explain 80-90% of variance</span>
<span id="cb66-265"><a href="#cb66-265" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Look for an "elbow" in the scree plot</span>
<span id="cb66-266"><a href="#cb66-266" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Use cross-validation if using PCs for prediction</span>
<span id="cb66-267"><a href="#cb66-267" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb66-268"><a href="#cb66-268" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-269"><a href="#cb66-269" aria-hidden="true" tabindex="-1"></a><span class="fu">### Biplot Visualization</span></span>
<span id="cb66-270"><a href="#cb66-270" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-271"><a href="#cb66-271" aria-hidden="true" tabindex="-1"></a>A **biplot** shows both observations (scores) and variables (loadings) on the same plot:</span>
<span id="cb66-272"><a href="#cb66-272" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-275"><a href="#cb66-275" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb66-276"><a href="#cb66-276" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-pca-biplot</span></span>
<span id="cb66-277"><a href="#cb66-277" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "PCA biplot showing both observations and variable loadings simultaneously"</span></span>
<span id="cb66-278"><a href="#cb66-278" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-width: 7</span></span>
<span id="cb66-279"><a href="#cb66-279" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-height: 6</span></span>
<span id="cb66-280"><a href="#cb66-280" aria-hidden="true" tabindex="-1"></a><span class="fu">biplot</span>(iris_pca, <span class="at">col =</span> <span class="fu">c</span>(<span class="st">"gray50"</span>, <span class="st">"red"</span>), <span class="at">cex =</span> <span class="fl">0.7</span>,</span>
<span id="cb66-281"><a href="#cb66-281" aria-hidden="true" tabindex="-1"></a>       <span class="at">main =</span> <span class="st">"PCA Biplot of Iris Data"</span>)</span>
<span id="cb66-282"><a href="#cb66-282" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb66-283"><a href="#cb66-283" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-284"><a href="#cb66-284" aria-hidden="true" tabindex="-1"></a>Arrows show variable loadings—their direction and length indicate how each variable relates to the principal components.</span>
<span id="cb66-285"><a href="#cb66-285" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-286"><a href="#cb66-286" aria-hidden="true" tabindex="-1"></a><span class="fu">### MNIST Example: High-Dimensional Image Data</span></span>
<span id="cb66-287"><a href="#cb66-287" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-288"><a href="#cb66-288" aria-hidden="true" tabindex="-1"></a>The real power of PCA becomes apparent with truly high-dimensional data. The MNIST dataset of handwritten digits has 784 features (28×28 pixels). Can we reduce this dimensionality while preserving useful information?</span>
<span id="cb66-289"><a href="#cb66-289" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-292"><a href="#cb66-292" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb66-293"><a href="#cb66-293" aria-hidden="true" tabindex="-1"></a><span class="co">#| message: false</span></span>
<span id="cb66-294"><a href="#cb66-294" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(dslabs)</span>
<span id="cb66-295"><a href="#cb66-295" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> (<span class="sc">!</span><span class="fu">exists</span>(<span class="st">"mnist"</span>)) mnist <span class="ot">&lt;-</span> <span class="fu">read_mnist</span>()</span>
<span id="cb66-296"><a href="#cb66-296" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb66-297"><a href="#cb66-297" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-298"><a href="#cb66-298" aria-hidden="true" tabindex="-1"></a>Because pixels near each other on the image grid are correlated, we expect dimension reduction to work well:</span>
<span id="cb66-299"><a href="#cb66-299" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-302"><a href="#cb66-302" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb66-303"><a href="#cb66-303" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-mnist-pca</span></span>
<span id="cb66-304"><a href="#cb66-304" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "Variance explained by principal components of MNIST digit images. The first few PCs capture substantial variance."</span></span>
<span id="cb66-305"><a href="#cb66-305" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-width: 7</span></span>
<span id="cb66-306"><a href="#cb66-306" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-height: 5</span></span>
<span id="cb66-307"><a href="#cb66-307" aria-hidden="true" tabindex="-1"></a>col_means <span class="ot">&lt;-</span> <span class="fu">colMeans</span>(mnist<span class="sc">$</span>test<span class="sc">$</span>images)</span>
<span id="cb66-308"><a href="#cb66-308" aria-hidden="true" tabindex="-1"></a>pca <span class="ot">&lt;-</span> <span class="fu">prcomp</span>(mnist<span class="sc">$</span>train<span class="sc">$</span>images)</span>
<span id="cb66-309"><a href="#cb66-309" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-310"><a href="#cb66-310" aria-hidden="true" tabindex="-1"></a><span class="co"># Scree plot</span></span>
<span id="cb66-311"><a href="#cb66-311" aria-hidden="true" tabindex="-1"></a>pc <span class="ot">&lt;-</span> <span class="dv">1</span><span class="sc">:</span><span class="fu">ncol</span>(mnist<span class="sc">$</span>test<span class="sc">$</span>images)</span>
<span id="cb66-312"><a href="#cb66-312" aria-hidden="true" tabindex="-1"></a><span class="fu">qplot</span>(pc[<span class="dv">1</span><span class="sc">:</span><span class="dv">50</span>], pca<span class="sc">$</span>sdev[<span class="dv">1</span><span class="sc">:</span><span class="dv">50</span>], <span class="at">geom =</span> <span class="st">"line"</span>) <span class="sc">+</span></span>
<span id="cb66-313"><a href="#cb66-313" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">x =</span> <span class="st">"Principal Component"</span>, <span class="at">y =</span> <span class="st">"Standard Deviation"</span>,</span>
<span id="cb66-314"><a href="#cb66-314" aria-hidden="true" tabindex="-1"></a>       <span class="at">title =</span> <span class="st">"MNIST Scree Plot (first 50 PCs)"</span>)</span>
<span id="cb66-315"><a href="#cb66-315" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb66-316"><a href="#cb66-316" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-317"><a href="#cb66-317" aria-hidden="true" tabindex="-1"></a>The first few PCs capture substantial variance:</span>
<span id="cb66-318"><a href="#cb66-318" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-321"><a href="#cb66-321" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb66-322"><a href="#cb66-322" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(pca)<span class="sc">$</span>importance[, <span class="dv">1</span><span class="sc">:</span><span class="dv">5</span>]</span>
<span id="cb66-323"><a href="#cb66-323" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb66-324"><a href="#cb66-324" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-325"><a href="#cb66-325" aria-hidden="true" tabindex="-1"></a>Even with just two dimensions, we can see structure related to digit class:</span>
<span id="cb66-326"><a href="#cb66-326" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-329"><a href="#cb66-329" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb66-330"><a href="#cb66-330" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-mnist-pca-scores</span></span>
<span id="cb66-331"><a href="#cb66-331" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "First two principal components of MNIST data colored by digit label. Different digits occupy different regions of PC space."</span></span>
<span id="cb66-332"><a href="#cb66-332" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-width: 8</span></span>
<span id="cb66-333"><a href="#cb66-333" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-height: 6</span></span>
<span id="cb66-334"><a href="#cb66-334" aria-hidden="true" tabindex="-1"></a><span class="fu">data.frame</span>(<span class="at">PC1 =</span> pca<span class="sc">$</span>x[, <span class="dv">1</span>], <span class="at">PC2 =</span> pca<span class="sc">$</span>x[, <span class="dv">2</span>],</span>
<span id="cb66-335"><a href="#cb66-335" aria-hidden="true" tabindex="-1"></a>           <span class="at">label =</span> <span class="fu">factor</span>(mnist<span class="sc">$</span>train<span class="sc">$</span>label)) <span class="sc">%&gt;%</span></span>
<span id="cb66-336"><a href="#cb66-336" aria-hidden="true" tabindex="-1"></a>  <span class="fu">sample_n</span>(<span class="dv">2000</span>) <span class="sc">%&gt;%</span></span>
<span id="cb66-337"><a href="#cb66-337" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(PC1, PC2, <span class="at">fill =</span> label)) <span class="sc">+</span></span>
<span id="cb66-338"><a href="#cb66-338" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">cex =</span> <span class="dv">3</span>, <span class="at">pch =</span> <span class="dv">21</span>, <span class="at">alpha =</span> <span class="fl">0.7</span>) <span class="sc">+</span></span>
<span id="cb66-339"><a href="#cb66-339" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">"MNIST Digits in PC Space"</span>)</span>
<span id="cb66-340"><a href="#cb66-340" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb66-341"><a href="#cb66-341" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-342"><a href="#cb66-342" aria-hidden="true" tabindex="-1"></a>We can visualize what each PC "looks for" by reshaping the loadings back to the 28×28 grid:</span>
<span id="cb66-343"><a href="#cb66-343" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-346"><a href="#cb66-346" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb66-347"><a href="#cb66-347" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-mnist-loadings</span></span>
<span id="cb66-348"><a href="#cb66-348" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "First four principal component loadings visualized as images. Each PC captures different aspects of digit structure."</span></span>
<span id="cb66-349"><a href="#cb66-349" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-width: 9</span></span>
<span id="cb66-350"><a href="#cb66-350" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-height: 3</span></span>
<span id="cb66-351"><a href="#cb66-351" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(RColorBrewer)</span>
<span id="cb66-352"><a href="#cb66-352" aria-hidden="true" tabindex="-1"></a>tmp <span class="ot">&lt;-</span> <span class="fu">lapply</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">4</span>, <span class="cf">function</span>(i) {</span>
<span id="cb66-353"><a href="#cb66-353" aria-hidden="true" tabindex="-1"></a>  <span class="fu">expand.grid</span>(<span class="at">Row =</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">28</span>, <span class="at">Column =</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">28</span>) <span class="sc">%&gt;%</span></span>
<span id="cb66-354"><a href="#cb66-354" aria-hidden="true" tabindex="-1"></a>    <span class="fu">mutate</span>(<span class="at">id =</span> i, <span class="at">label =</span> <span class="fu">paste0</span>(<span class="st">"PC"</span>, i),</span>
<span id="cb66-355"><a href="#cb66-355" aria-hidden="true" tabindex="-1"></a>           <span class="at">value =</span> pca<span class="sc">$</span>rotation[, i])</span>
<span id="cb66-356"><a href="#cb66-356" aria-hidden="true" tabindex="-1"></a>})</span>
<span id="cb66-357"><a href="#cb66-357" aria-hidden="true" tabindex="-1"></a>tmp <span class="ot">&lt;-</span> <span class="fu">Reduce</span>(rbind, tmp)</span>
<span id="cb66-358"><a href="#cb66-358" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-359"><a href="#cb66-359" aria-hidden="true" tabindex="-1"></a>tmp <span class="sc">%&gt;%</span></span>
<span id="cb66-360"><a href="#cb66-360" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(Row, Column, <span class="at">fill =</span> value)) <span class="sc">+</span></span>
<span id="cb66-361"><a href="#cb66-361" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_raster</span>() <span class="sc">+</span></span>
<span id="cb66-362"><a href="#cb66-362" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_y_reverse</span>() <span class="sc">+</span></span>
<span id="cb66-363"><a href="#cb66-363" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_fill_gradientn</span>(<span class="at">colors =</span> <span class="fu">brewer.pal</span>(<span class="dv">9</span>, <span class="st">"RdBu"</span>)) <span class="sc">+</span></span>
<span id="cb66-364"><a href="#cb66-364" aria-hidden="true" tabindex="-1"></a>  <span class="fu">facet_wrap</span>(<span class="sc">~</span>label, <span class="at">nrow =</span> <span class="dv">1</span>) <span class="sc">+</span></span>
<span id="cb66-365"><a href="#cb66-365" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_minimal</span>() <span class="sc">+</span></span>
<span id="cb66-366"><a href="#cb66-366" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">"PCA Loadings as Images"</span>)</span>
<span id="cb66-367"><a href="#cb66-367" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb66-368"><a href="#cb66-368" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-369"><a href="#cb66-369" aria-hidden="true" tabindex="-1"></a><span class="fu">### Applying PCA to Improve Classification</span></span>
<span id="cb66-370"><a href="#cb66-370" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-371"><a href="#cb66-371" aria-hidden="true" tabindex="-1"></a>PCA can reduce model complexity while maintaining predictive performance. Let's use 36 PCs (explaining ~80% of variance) for kNN classification:</span>
<span id="cb66-372"><a href="#cb66-372" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-375"><a href="#cb66-375" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb66-376"><a href="#cb66-376" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(caret)</span>
<span id="cb66-377"><a href="#cb66-377" aria-hidden="true" tabindex="-1"></a>k <span class="ot">&lt;-</span> <span class="dv">36</span></span>
<span id="cb66-378"><a href="#cb66-378" aria-hidden="true" tabindex="-1"></a>x_train <span class="ot">&lt;-</span> pca<span class="sc">$</span>x[, <span class="dv">1</span><span class="sc">:</span>k]</span>
<span id="cb66-379"><a href="#cb66-379" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> <span class="fu">factor</span>(mnist<span class="sc">$</span>train<span class="sc">$</span>labels)</span>
<span id="cb66-380"><a href="#cb66-380" aria-hidden="true" tabindex="-1"></a>fit <span class="ot">&lt;-</span> <span class="fu">knn3</span>(x_train, y)</span>
<span id="cb66-381"><a href="#cb66-381" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-382"><a href="#cb66-382" aria-hidden="true" tabindex="-1"></a><span class="co"># Transform test set using training PCA</span></span>
<span id="cb66-383"><a href="#cb66-383" aria-hidden="true" tabindex="-1"></a>x_test <span class="ot">&lt;-</span> <span class="fu">sweep</span>(mnist<span class="sc">$</span>test<span class="sc">$</span>images, <span class="dv">2</span>, col_means) <span class="sc">%*%</span> pca<span class="sc">$</span>rotation</span>
<span id="cb66-384"><a href="#cb66-384" aria-hidden="true" tabindex="-1"></a>x_test <span class="ot">&lt;-</span> x_test[, <span class="dv">1</span><span class="sc">:</span>k]</span>
<span id="cb66-385"><a href="#cb66-385" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-386"><a href="#cb66-386" aria-hidden="true" tabindex="-1"></a><span class="co"># Predict and evaluate</span></span>
<span id="cb66-387"><a href="#cb66-387" aria-hidden="true" tabindex="-1"></a>y_hat <span class="ot">&lt;-</span> <span class="fu">predict</span>(fit, x_test, <span class="at">type =</span> <span class="st">"class"</span>)</span>
<span id="cb66-388"><a href="#cb66-388" aria-hidden="true" tabindex="-1"></a><span class="fu">confusionMatrix</span>(y_hat, <span class="fu">factor</span>(mnist<span class="sc">$</span>test<span class="sc">$</span>labels))<span class="sc">$</span>overall[<span class="st">"Accuracy"</span>]</span>
<span id="cb66-389"><a href="#cb66-389" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb66-390"><a href="#cb66-390" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-391"><a href="#cb66-391" aria-hidden="true" tabindex="-1"></a>With just 36 dimensions (instead of 784), we achieve excellent accuracy. This demonstrates the power of PCA for both visualization and model simplification.</span>
<span id="cb66-392"><a href="#cb66-392" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-393"><a href="#cb66-393" aria-hidden="true" tabindex="-1"></a><span class="fu">## Principal Coordinate Analysis (PCoA)</span></span>
<span id="cb66-394"><a href="#cb66-394" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-395"><a href="#cb66-395" aria-hidden="true" tabindex="-1"></a>While PCA uses correlations among variables, **Principal Coordinate Analysis (PCoA)** (also called Metric Multidimensional Scaling) starts with a dissimilarity matrix among observations. This is valuable when:</span>
<span id="cb66-396"><a href="#cb66-396" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-397"><a href="#cb66-397" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>You have a meaningful distance metric (e.g., genetic distances)</span>
<span id="cb66-398"><a href="#cb66-398" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Variables are mixed types or non-numeric</span>
<span id="cb66-399"><a href="#cb66-399" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>The data are counts (e.g., microbiome data)</span>
<span id="cb66-400"><a href="#cb66-400" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-403"><a href="#cb66-403" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb66-404"><a href="#cb66-404" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-pcoa</span></span>
<span id="cb66-405"><a href="#cb66-405" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "Principal Coordinate Analysis (PCoA) ordination of iris data using Euclidean distances"</span></span>
<span id="cb66-406"><a href="#cb66-406" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-width: 7</span></span>
<span id="cb66-407"><a href="#cb66-407" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-height: 5</span></span>
<span id="cb66-408"><a href="#cb66-408" aria-hidden="true" tabindex="-1"></a><span class="co"># PCoA example using Euclidean distances</span></span>
<span id="cb66-409"><a href="#cb66-409" aria-hidden="true" tabindex="-1"></a>dist_matrix <span class="ot">&lt;-</span> <span class="fu">dist</span>(iris[, <span class="dv">1</span><span class="sc">:</span><span class="dv">4</span>])</span>
<span id="cb66-410"><a href="#cb66-410" aria-hidden="true" tabindex="-1"></a>pcoa_result <span class="ot">&lt;-</span> <span class="fu">cmdscale</span>(dist_matrix, <span class="at">k =</span> <span class="dv">2</span>, <span class="at">eig =</span> <span class="cn">TRUE</span>)</span>
<span id="cb66-411"><a href="#cb66-411" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-412"><a href="#cb66-412" aria-hidden="true" tabindex="-1"></a><span class="co"># Proportion of variance explained</span></span>
<span id="cb66-413"><a href="#cb66-413" aria-hidden="true" tabindex="-1"></a>eig_vals <span class="ot">&lt;-</span> pcoa_result<span class="sc">$</span>eig[pcoa_result<span class="sc">$</span>eig <span class="sc">&gt;</span> <span class="dv">0</span>]</span>
<span id="cb66-414"><a href="#cb66-414" aria-hidden="true" tabindex="-1"></a>var_explained <span class="ot">&lt;-</span> eig_vals <span class="sc">/</span> <span class="fu">sum</span>(eig_vals)</span>
<span id="cb66-415"><a href="#cb66-415" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"Variance explained by first two axes:"</span>,</span>
<span id="cb66-416"><a href="#cb66-416" aria-hidden="true" tabindex="-1"></a>    <span class="fu">round</span>(<span class="fu">sum</span>(var_explained[<span class="dv">1</span><span class="sc">:</span><span class="dv">2</span>]) <span class="sc">*</span> <span class="dv">100</span>, <span class="dv">1</span>), <span class="st">"%</span><span class="sc">\n</span><span class="st">"</span>)</span>
<span id="cb66-417"><a href="#cb66-417" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-418"><a href="#cb66-418" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot</span></span>
<span id="cb66-419"><a href="#cb66-419" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(pcoa_result<span class="sc">$</span>points,</span>
<span id="cb66-420"><a href="#cb66-420" aria-hidden="true" tabindex="-1"></a>     <span class="at">col =</span> <span class="fu">c</span>(<span class="st">"red"</span>, <span class="st">"green"</span>, <span class="st">"blue"</span>)[iris<span class="sc">$</span>Species],</span>
<span id="cb66-421"><a href="#cb66-421" aria-hidden="true" tabindex="-1"></a>     <span class="at">pch =</span> <span class="dv">19</span>,</span>
<span id="cb66-422"><a href="#cb66-422" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="fu">paste0</span>(<span class="st">"PCoA1 ("</span>, <span class="fu">round</span>(var_explained[<span class="dv">1</span>]<span class="sc">*</span><span class="dv">100</span>, <span class="dv">1</span>), <span class="st">"%)"</span>),</span>
<span id="cb66-423"><a href="#cb66-423" aria-hidden="true" tabindex="-1"></a>     <span class="at">ylab =</span> <span class="fu">paste0</span>(<span class="st">"PCoA2 ("</span>, <span class="fu">round</span>(var_explained[<span class="dv">2</span>]<span class="sc">*</span><span class="dv">100</span>, <span class="dv">1</span>), <span class="st">"%)"</span>),</span>
<span id="cb66-424"><a href="#cb66-424" aria-hidden="true" tabindex="-1"></a>     <span class="at">main =</span> <span class="st">"PCoA of Iris Data"</span>)</span>
<span id="cb66-425"><a href="#cb66-425" aria-hidden="true" tabindex="-1"></a><span class="fu">legend</span>(<span class="st">"topright"</span>, <span class="fu">levels</span>(iris<span class="sc">$</span>Species),</span>
<span id="cb66-426"><a href="#cb66-426" aria-hidden="true" tabindex="-1"></a>       <span class="at">col =</span> <span class="fu">c</span>(<span class="st">"red"</span>, <span class="st">"green"</span>, <span class="st">"blue"</span>), <span class="at">pch =</span> <span class="dv">19</span>)</span>
<span id="cb66-427"><a href="#cb66-427" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb66-428"><a href="#cb66-428" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-429"><a href="#cb66-429" aria-hidden="true" tabindex="-1"></a><span class="fu">### When to Use PCoA vs. PCA</span></span>
<span id="cb66-430"><a href="#cb66-430" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-431"><a href="#cb66-431" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**PCA**: Variables are measured on a common scale; interested in variable contributions</span>
<span id="cb66-432"><a href="#cb66-432" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**PCoA**: Have a distance matrix; want to preserve distances among samples</span>
<span id="cb66-433"><a href="#cb66-433" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>For Euclidean distances, PCA and PCoA give equivalent results</span>
<span id="cb66-434"><a href="#cb66-434" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-435"><a href="#cb66-435" aria-hidden="true" tabindex="-1"></a><span class="fu">## Non-Metric Multidimensional Scaling (NMDS)</span></span>
<span id="cb66-436"><a href="#cb66-436" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-437"><a href="#cb66-437" aria-hidden="true" tabindex="-1"></a>**NMDS** is an ordination technique that preserves rank-order of distances rather than exact distances. It's widely used in ecology because it makes no assumptions about the data distribution.</span>
<span id="cb66-438"><a href="#cb66-438" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-441"><a href="#cb66-441" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb66-442"><a href="#cb66-442" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-nmds</span></span>
<span id="cb66-443"><a href="#cb66-443" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "Non-metric multidimensional scaling (NMDS) ordination of iris data"</span></span>
<span id="cb66-444"><a href="#cb66-444" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-width: 7</span></span>
<span id="cb66-445"><a href="#cb66-445" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-height: 5</span></span>
<span id="cb66-446"><a href="#cb66-446" aria-hidden="true" tabindex="-1"></a><span class="co">#| message: false</span></span>
<span id="cb66-447"><a href="#cb66-447" aria-hidden="true" tabindex="-1"></a><span class="co"># NMDS example</span></span>
<span id="cb66-448"><a href="#cb66-448" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(vegan)</span>
<span id="cb66-449"><a href="#cb66-449" aria-hidden="true" tabindex="-1"></a>nmds_result <span class="ot">&lt;-</span> <span class="fu">metaMDS</span>(iris[, <span class="dv">1</span><span class="sc">:</span><span class="dv">4</span>], <span class="at">k =</span> <span class="dv">2</span>, <span class="at">trymax =</span> <span class="dv">100</span>, <span class="at">trace =</span> <span class="cn">FALSE</span>)</span>
<span id="cb66-450"><a href="#cb66-450" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-451"><a href="#cb66-451" aria-hidden="true" tabindex="-1"></a><span class="co"># Stress value indicates fit (&lt; 0.1 is good, &lt; 0.2 is acceptable)</span></span>
<span id="cb66-452"><a href="#cb66-452" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"Stress:"</span>, <span class="fu">round</span>(nmds_result<span class="sc">$</span>stress, <span class="dv">3</span>), <span class="st">"</span><span class="sc">\n</span><span class="st">"</span>)</span>
<span id="cb66-453"><a href="#cb66-453" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-454"><a href="#cb66-454" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(nmds_result<span class="sc">$</span>points,</span>
<span id="cb66-455"><a href="#cb66-455" aria-hidden="true" tabindex="-1"></a>     <span class="at">col =</span> <span class="fu">c</span>(<span class="st">"red"</span>, <span class="st">"green"</span>, <span class="st">"blue"</span>)[iris<span class="sc">$</span>Species],</span>
<span id="cb66-456"><a href="#cb66-456" aria-hidden="true" tabindex="-1"></a>     <span class="at">pch =</span> <span class="dv">19</span>, <span class="at">xlab =</span> <span class="st">"NMDS1"</span>, <span class="at">ylab =</span> <span class="st">"NMDS2"</span>,</span>
<span id="cb66-457"><a href="#cb66-457" aria-hidden="true" tabindex="-1"></a>     <span class="at">main =</span> <span class="st">"NMDS of Iris Data"</span>)</span>
<span id="cb66-458"><a href="#cb66-458" aria-hidden="true" tabindex="-1"></a><span class="fu">legend</span>(<span class="st">"topright"</span>, <span class="fu">levels</span>(iris<span class="sc">$</span>Species),</span>
<span id="cb66-459"><a href="#cb66-459" aria-hidden="true" tabindex="-1"></a>       <span class="at">col =</span> <span class="fu">c</span>(<span class="st">"red"</span>, <span class="st">"green"</span>, <span class="st">"blue"</span>), <span class="at">pch =</span> <span class="dv">19</span>)</span>
<span id="cb66-460"><a href="#cb66-460" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb66-461"><a href="#cb66-461" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-462"><a href="#cb66-462" aria-hidden="true" tabindex="-1"></a>::: {.callout-warning}</span>
<span id="cb66-463"><a href="#cb66-463" aria-hidden="true" tabindex="-1"></a><span class="fu">## Metric vs. Non-Metric Methods</span></span>
<span id="cb66-464"><a href="#cb66-464" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-465"><a href="#cb66-465" aria-hidden="true" tabindex="-1"></a>PCA and metric PCoA produce scores on a ratio scale—differences between scores are meaningful. These can be used directly in linear models.</span>
<span id="cb66-466"><a href="#cb66-466" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-467"><a href="#cb66-467" aria-hidden="true" tabindex="-1"></a>Non-metric multidimensional scaling (NMDS) produces ordinal rankings only. NMDS scores should **not** be used in parametric analyses like ANOVA or regression.</span>
<span id="cb66-468"><a href="#cb66-468" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb66-469"><a href="#cb66-469" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-470"><a href="#cb66-470" aria-hidden="true" tabindex="-1"></a><span class="fu">## MANOVA: Multivariate Analysis of Variance</span></span>
<span id="cb66-471"><a href="#cb66-471" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-472"><a href="#cb66-472" aria-hidden="true" tabindex="-1"></a>When you have multiple response variables and want to test for group differences, **MANOVA** (Multivariate Analysis of Variance) is the appropriate technique. It extends ANOVA to multiple dependent variables simultaneously.</span>
<span id="cb66-473"><a href="#cb66-473" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-474"><a href="#cb66-474" aria-hidden="true" tabindex="-1"></a><span class="fu">### Why Not Multiple ANOVAs?</span></span>
<span id="cb66-475"><a href="#cb66-475" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-476"><a href="#cb66-476" aria-hidden="true" tabindex="-1"></a>Running separate ANOVAs on each variable:</span>
<span id="cb66-477"><a href="#cb66-477" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-478"><a href="#cb66-478" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Ignores correlations among response variables</span>
<span id="cb66-479"><a href="#cb66-479" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Inflates Type I error rate with multiple tests</span>
<span id="cb66-480"><a href="#cb66-480" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>May miss differences only apparent when variables are considered together</span>
<span id="cb66-481"><a href="#cb66-481" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-482"><a href="#cb66-482" aria-hidden="true" tabindex="-1"></a>MANOVA tests whether group centroids differ in multivariate space.</span>
<span id="cb66-483"><a href="#cb66-483" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-484"><a href="#cb66-484" aria-hidden="true" tabindex="-1"></a><span class="fu">### The MANOVA Framework</span></span>
<span id="cb66-485"><a href="#cb66-485" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-486"><a href="#cb66-486" aria-hidden="true" tabindex="-1"></a>MANOVA decomposes the total multivariate variation:</span>
<span id="cb66-487"><a href="#cb66-487" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-488"><a href="#cb66-488" aria-hidden="true" tabindex="-1"></a>$$\mathbf{T} = \mathbf{H} + \mathbf{E}$$</span>
<span id="cb66-489"><a href="#cb66-489" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-490"><a href="#cb66-490" aria-hidden="true" tabindex="-1"></a>where:</span>
<span id="cb66-491"><a href="#cb66-491" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-492"><a href="#cb66-492" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**T**: Total sum of squares and cross-products matrix</span>
<span id="cb66-493"><a href="#cb66-493" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**H**: Hypothesis (between-groups) matrix</span>
<span id="cb66-494"><a href="#cb66-494" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**E**: Error (within-groups) matrix</span>
<span id="cb66-495"><a href="#cb66-495" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-496"><a href="#cb66-496" aria-hidden="true" tabindex="-1"></a>These are matrices because we have multiple response variables.</span>
<span id="cb66-497"><a href="#cb66-497" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-498"><a href="#cb66-498" aria-hidden="true" tabindex="-1"></a><span class="al">![MANOVA decomposes multivariate variation into between-group and within-group components](../images/ch36/ch36_manova_decomposition.jpeg)</span>{#fig-manova-decomposition fig-align="center"}</span>
<span id="cb66-499"><a href="#cb66-499" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-500"><a href="#cb66-500" aria-hidden="true" tabindex="-1"></a><span class="fu">### Test Statistics</span></span>
<span id="cb66-501"><a href="#cb66-501" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-502"><a href="#cb66-502" aria-hidden="true" tabindex="-1"></a>Several test statistics exist for MANOVA, each a function of the eigenvalues of $\mathbf{HE}^{-1}$:</span>
<span id="cb66-503"><a href="#cb66-503" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-504"><a href="#cb66-504" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> Statistic <span class="pp">|</span> Description <span class="pp">|</span></span>
<span id="cb66-505"><a href="#cb66-505" aria-hidden="true" tabindex="-1"></a><span class="pp">|:----------|:------------|</span></span>
<span id="cb66-506"><a href="#cb66-506" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **Wilks' Lambda (Λ)** <span class="pp">|</span> Product of 1/(1+λᵢ); most commonly used <span class="pp">|</span></span>
<span id="cb66-507"><a href="#cb66-507" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **Hotelling-Lawley Trace** <span class="pp">|</span> Sum of eigenvalues <span class="pp">|</span></span>
<span id="cb66-508"><a href="#cb66-508" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **Pillai's Trace** <span class="pp">|</span> Sum of λᵢ/(1+λᵢ); most robust <span class="pp">|</span></span>
<span id="cb66-509"><a href="#cb66-509" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **Roy's Largest Root** <span class="pp">|</span> Maximum eigenvalue; most powerful but sensitive <span class="pp">|</span></span>
<span id="cb66-510"><a href="#cb66-510" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-511"><a href="#cb66-511" aria-hidden="true" tabindex="-1"></a>**Pillai's Trace** is generally recommended because it's most robust to violations of assumptions.</span>
<span id="cb66-512"><a href="#cb66-512" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-513"><a href="#cb66-513" aria-hidden="true" tabindex="-1"></a><span class="fu">### MANOVA in R</span></span>
<span id="cb66-514"><a href="#cb66-514" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-517"><a href="#cb66-517" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb66-518"><a href="#cb66-518" aria-hidden="true" tabindex="-1"></a><span class="co"># MANOVA on iris data</span></span>
<span id="cb66-519"><a href="#cb66-519" aria-hidden="true" tabindex="-1"></a>manova_model <span class="ot">&lt;-</span> <span class="fu">manova</span>(<span class="fu">cbind</span>(Sepal.Length, Sepal.Width,</span>
<span id="cb66-520"><a href="#cb66-520" aria-hidden="true" tabindex="-1"></a>                              Petal.Length, Petal.Width) <span class="sc">~</span> Species,</span>
<span id="cb66-521"><a href="#cb66-521" aria-hidden="true" tabindex="-1"></a>                       <span class="at">data =</span> iris)</span>
<span id="cb66-522"><a href="#cb66-522" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-523"><a href="#cb66-523" aria-hidden="true" tabindex="-1"></a><span class="co"># Summary with different test statistics</span></span>
<span id="cb66-524"><a href="#cb66-524" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(manova_model, <span class="at">test =</span> <span class="st">"Pillai"</span>)</span>
<span id="cb66-525"><a href="#cb66-525" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(manova_model, <span class="at">test =</span> <span class="st">"Wilks"</span>)</span>
<span id="cb66-526"><a href="#cb66-526" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb66-527"><a href="#cb66-527" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-528"><a href="#cb66-528" aria-hidden="true" tabindex="-1"></a>The significant result tells us that species differ in their multivariate centroid—the combination of all four measurements.</span>
<span id="cb66-529"><a href="#cb66-529" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-530"><a href="#cb66-530" aria-hidden="true" tabindex="-1"></a><span class="fu">### Follow-Up Analyses</span></span>
<span id="cb66-531"><a href="#cb66-531" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-532"><a href="#cb66-532" aria-hidden="true" tabindex="-1"></a>A significant MANOVA should be followed by:</span>
<span id="cb66-533"><a href="#cb66-533" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-534"><a href="#cb66-534" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**Univariate ANOVAs** to see which variables differ</span>
<span id="cb66-535"><a href="#cb66-535" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**Discriminant Function Analysis** to understand how groups differ</span>
<span id="cb66-536"><a href="#cb66-536" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-539"><a href="#cb66-539" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb66-540"><a href="#cb66-540" aria-hidden="true" tabindex="-1"></a><span class="co"># Univariate follow-ups</span></span>
<span id="cb66-541"><a href="#cb66-541" aria-hidden="true" tabindex="-1"></a><span class="fu">summary.aov</span>(manova_model)</span>
<span id="cb66-542"><a href="#cb66-542" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb66-543"><a href="#cb66-543" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-544"><a href="#cb66-544" aria-hidden="true" tabindex="-1"></a><span class="fu">### MANOVA Assumptions</span></span>
<span id="cb66-545"><a href="#cb66-545" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-546"><a href="#cb66-546" aria-hidden="true" tabindex="-1"></a>MANOVA assumes:</span>
<span id="cb66-547"><a href="#cb66-547" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-548"><a href="#cb66-548" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**Multivariate normality** within groups</span>
<span id="cb66-549"><a href="#cb66-549" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**Homogeneity of covariance matrices** across groups</span>
<span id="cb66-550"><a href="#cb66-550" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**Independence** of observations</span>
<span id="cb66-551"><a href="#cb66-551" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>**No multicollinearity** among response variables</span>
<span id="cb66-552"><a href="#cb66-552" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-553"><a href="#cb66-553" aria-hidden="true" tabindex="-1"></a>Test homogeneity of covariance matrices with Box's M test (though it's sensitive to non-normality):</span>
<span id="cb66-554"><a href="#cb66-554" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-557"><a href="#cb66-557" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb66-558"><a href="#cb66-558" aria-hidden="true" tabindex="-1"></a><span class="co">#| eval: false</span></span>
<span id="cb66-559"><a href="#cb66-559" aria-hidden="true" tabindex="-1"></a><span class="co"># Box's M test (requires biotools package)</span></span>
<span id="cb66-560"><a href="#cb66-560" aria-hidden="true" tabindex="-1"></a><span class="co"># library(biotools)</span></span>
<span id="cb66-561"><a href="#cb66-561" aria-hidden="true" tabindex="-1"></a><span class="co"># boxM(iris[, 1:4], iris$Species)</span></span>
<span id="cb66-562"><a href="#cb66-562" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb66-563"><a href="#cb66-563" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-564"><a href="#cb66-564" aria-hidden="true" tabindex="-1"></a><span class="fu">## Discriminant Function Analysis (DFA)</span></span>
<span id="cb66-565"><a href="#cb66-565" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-566"><a href="#cb66-566" aria-hidden="true" tabindex="-1"></a>**Discriminant Function Analysis** (DFA, also called Linear Discriminant Analysis or LDA) finds linear combinations of variables that best separate groups. It complements MANOVA by showing *how* groups differ.</span>
<span id="cb66-567"><a href="#cb66-567" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-568"><a href="#cb66-568" aria-hidden="true" tabindex="-1"></a><span class="fu">### The Goal of DFA</span></span>
<span id="cb66-569"><a href="#cb66-569" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-570"><a href="#cb66-570" aria-hidden="true" tabindex="-1"></a>DFA finds discriminant functions—weighted combinations of original variables—that maximize separation between groups while minimizing variation within groups.</span>
<span id="cb66-571"><a href="#cb66-571" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-572"><a href="#cb66-572" aria-hidden="true" tabindex="-1"></a>The first discriminant function captures the most separation, the second captures remaining separation orthogonal to the first, and so on.</span>
<span id="cb66-573"><a href="#cb66-573" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-574"><a href="#cb66-574" aria-hidden="true" tabindex="-1"></a><span class="al">![Discriminant function analysis finds linear combinations that maximize group separation](../images/ch36/ch36_dfa_concept.jpeg)</span>{#fig-dfa-concept fig-align="center"}</span>
<span id="cb66-575"><a href="#cb66-575" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-576"><a href="#cb66-576" aria-hidden="true" tabindex="-1"></a><span class="fu">### DFA in R</span></span>
<span id="cb66-577"><a href="#cb66-577" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-580"><a href="#cb66-580" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb66-581"><a href="#cb66-581" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-lda-scores</span></span>
<span id="cb66-582"><a href="#cb66-582" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "Linear discriminant analysis showing group separation along discriminant functions"</span></span>
<span id="cb66-583"><a href="#cb66-583" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-width: 7</span></span>
<span id="cb66-584"><a href="#cb66-584" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-height: 5</span></span>
<span id="cb66-585"><a href="#cb66-585" aria-hidden="true" tabindex="-1"></a><span class="co"># Linear Discriminant Analysis</span></span>
<span id="cb66-586"><a href="#cb66-586" aria-hidden="true" tabindex="-1"></a>lda_model <span class="ot">&lt;-</span> <span class="fu">lda</span>(Species <span class="sc">~</span> ., <span class="at">data =</span> iris)</span>
<span id="cb66-587"><a href="#cb66-587" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-588"><a href="#cb66-588" aria-hidden="true" tabindex="-1"></a><span class="co"># View the model</span></span>
<span id="cb66-589"><a href="#cb66-589" aria-hidden="true" tabindex="-1"></a>lda_model</span>
<span id="cb66-590"><a href="#cb66-590" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-591"><a href="#cb66-591" aria-hidden="true" tabindex="-1"></a><span class="co"># Discriminant scores</span></span>
<span id="cb66-592"><a href="#cb66-592" aria-hidden="true" tabindex="-1"></a>lda_scores <span class="ot">&lt;-</span> <span class="fu">predict</span>(lda_model)<span class="sc">$</span>x</span>
<span id="cb66-593"><a href="#cb66-593" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-594"><a href="#cb66-594" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot</span></span>
<span id="cb66-595"><a href="#cb66-595" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(lda_scores,</span>
<span id="cb66-596"><a href="#cb66-596" aria-hidden="true" tabindex="-1"></a>     <span class="at">col =</span> <span class="fu">c</span>(<span class="st">"red"</span>, <span class="st">"green"</span>, <span class="st">"blue"</span>)[iris<span class="sc">$</span>Species],</span>
<span id="cb66-597"><a href="#cb66-597" aria-hidden="true" tabindex="-1"></a>     <span class="at">pch =</span> <span class="dv">19</span>,</span>
<span id="cb66-598"><a href="#cb66-598" aria-hidden="true" tabindex="-1"></a>     <span class="at">main =</span> <span class="st">"Discriminant Function Scores"</span>,</span>
<span id="cb66-599"><a href="#cb66-599" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">"LD1"</span>, <span class="at">ylab =</span> <span class="st">"LD2"</span>)</span>
<span id="cb66-600"><a href="#cb66-600" aria-hidden="true" tabindex="-1"></a><span class="fu">legend</span>(<span class="st">"topright"</span>, <span class="fu">levels</span>(iris<span class="sc">$</span>Species),</span>
<span id="cb66-601"><a href="#cb66-601" aria-hidden="true" tabindex="-1"></a>       <span class="at">col =</span> <span class="fu">c</span>(<span class="st">"red"</span>, <span class="st">"green"</span>, <span class="st">"blue"</span>), <span class="at">pch =</span> <span class="dv">19</span>)</span>
<span id="cb66-602"><a href="#cb66-602" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb66-603"><a href="#cb66-603" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-604"><a href="#cb66-604" aria-hidden="true" tabindex="-1"></a><span class="fu">### Interpreting DFA Output</span></span>
<span id="cb66-605"><a href="#cb66-605" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-606"><a href="#cb66-606" aria-hidden="true" tabindex="-1"></a>Key components:</span>
<span id="cb66-607"><a href="#cb66-607" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-608"><a href="#cb66-608" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Coefficients of linear discriminants**: Weights for creating discriminant scores</span>
<span id="cb66-609"><a href="#cb66-609" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Proportion of trace**: Variance explained by each discriminant function</span>
<span id="cb66-610"><a href="#cb66-610" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Group means**: Average score on each discriminant function for each group</span>
<span id="cb66-611"><a href="#cb66-611" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-614"><a href="#cb66-614" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb66-615"><a href="#cb66-615" aria-hidden="true" tabindex="-1"></a><span class="co"># Coefficients (loadings)</span></span>
<span id="cb66-616"><a href="#cb66-616" aria-hidden="true" tabindex="-1"></a>lda_model<span class="sc">$</span>scaling</span>
<span id="cb66-617"><a href="#cb66-617" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-618"><a href="#cb66-618" aria-hidden="true" tabindex="-1"></a><span class="co"># Proportion of separation explained</span></span>
<span id="cb66-619"><a href="#cb66-619" aria-hidden="true" tabindex="-1"></a>lda_model<span class="sc">$</span>svd<span class="sc">^</span><span class="dv">2</span> <span class="sc">/</span> <span class="fu">sum</span>(lda_model<span class="sc">$</span>svd<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb66-620"><a href="#cb66-620" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb66-621"><a href="#cb66-621" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-622"><a href="#cb66-622" aria-hidden="true" tabindex="-1"></a><span class="fu">### Using DFA for Prediction</span></span>
<span id="cb66-623"><a href="#cb66-623" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-624"><a href="#cb66-624" aria-hidden="true" tabindex="-1"></a>DFA can classify new observations into groups based on their discriminant scores:</span>
<span id="cb66-625"><a href="#cb66-625" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-628"><a href="#cb66-628" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb66-629"><a href="#cb66-629" aria-hidden="true" tabindex="-1"></a><span class="co"># Classification accuracy</span></span>
<span id="cb66-630"><a href="#cb66-630" aria-hidden="true" tabindex="-1"></a>predictions <span class="ot">&lt;-</span> <span class="fu">predict</span>(lda_model)<span class="sc">$</span>class</span>
<span id="cb66-631"><a href="#cb66-631" aria-hidden="true" tabindex="-1"></a><span class="fu">table</span>(<span class="at">Predicted =</span> predictions, <span class="at">Actual =</span> iris<span class="sc">$</span>Species)</span>
<span id="cb66-632"><a href="#cb66-632" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-633"><a href="#cb66-633" aria-hidden="true" tabindex="-1"></a><span class="co"># Classification accuracy</span></span>
<span id="cb66-634"><a href="#cb66-634" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>(predictions <span class="sc">==</span> iris<span class="sc">$</span>Species)</span>
<span id="cb66-635"><a href="#cb66-635" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb66-636"><a href="#cb66-636" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-637"><a href="#cb66-637" aria-hidden="true" tabindex="-1"></a><span class="fu">### Cross-Validated Classification</span></span>
<span id="cb66-638"><a href="#cb66-638" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-639"><a href="#cb66-639" aria-hidden="true" tabindex="-1"></a>For honest estimates of classification accuracy, use leave-one-out cross-validation:</span>
<span id="cb66-640"><a href="#cb66-640" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-643"><a href="#cb66-643" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb66-644"><a href="#cb66-644" aria-hidden="true" tabindex="-1"></a><span class="co"># Cross-validated LDA</span></span>
<span id="cb66-645"><a href="#cb66-645" aria-hidden="true" tabindex="-1"></a>lda_cv <span class="ot">&lt;-</span> <span class="fu">lda</span>(Species <span class="sc">~</span> ., <span class="at">data =</span> iris, <span class="at">CV =</span> <span class="cn">TRUE</span>)</span>
<span id="cb66-646"><a href="#cb66-646" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-647"><a href="#cb66-647" aria-hidden="true" tabindex="-1"></a><span class="co"># Cross-validated classification table</span></span>
<span id="cb66-648"><a href="#cb66-648" aria-hidden="true" tabindex="-1"></a><span class="fu">table</span>(<span class="at">Predicted =</span> lda_cv<span class="sc">$</span>class, <span class="at">Actual =</span> iris<span class="sc">$</span>Species)</span>
<span id="cb66-649"><a href="#cb66-649" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-650"><a href="#cb66-650" aria-hidden="true" tabindex="-1"></a><span class="co"># Cross-validated accuracy</span></span>
<span id="cb66-651"><a href="#cb66-651" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>(lda_cv<span class="sc">$</span>class <span class="sc">==</span> iris<span class="sc">$</span>Species)</span>
<span id="cb66-652"><a href="#cb66-652" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb66-653"><a href="#cb66-653" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-654"><a href="#cb66-654" aria-hidden="true" tabindex="-1"></a><span class="fu">### DFA for Biomarker Discovery</span></span>
<span id="cb66-655"><a href="#cb66-655" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-656"><a href="#cb66-656" aria-hidden="true" tabindex="-1"></a>DFA is valuable for identifying which variables best distinguish groups—useful in biomarker discovery:</span>
<span id="cb66-657"><a href="#cb66-657" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-660"><a href="#cb66-660" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb66-661"><a href="#cb66-661" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-lda-loadings</span></span>
<span id="cb66-662"><a href="#cb66-662" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "Variable contributions to the first linear discriminant function"</span></span>
<span id="cb66-663"><a href="#cb66-663" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-width: 7</span></span>
<span id="cb66-664"><a href="#cb66-664" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-height: 5</span></span>
<span id="cb66-665"><a href="#cb66-665" aria-hidden="true" tabindex="-1"></a><span class="co"># Which variables contribute most to separation?</span></span>
<span id="cb66-666"><a href="#cb66-666" aria-hidden="true" tabindex="-1"></a>scaling_df <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(</span>
<span id="cb66-667"><a href="#cb66-667" aria-hidden="true" tabindex="-1"></a>  <span class="at">Variable =</span> <span class="fu">rownames</span>(lda_model<span class="sc">$</span>scaling),</span>
<span id="cb66-668"><a href="#cb66-668" aria-hidden="true" tabindex="-1"></a>  <span class="at">LD1 =</span> <span class="fu">abs</span>(lda_model<span class="sc">$</span>scaling[, <span class="dv">1</span>]),</span>
<span id="cb66-669"><a href="#cb66-669" aria-hidden="true" tabindex="-1"></a>  <span class="at">LD2 =</span> <span class="fu">abs</span>(lda_model<span class="sc">$</span>scaling[, <span class="dv">2</span>])</span>
<span id="cb66-670"><a href="#cb66-670" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb66-671"><a href="#cb66-671" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-672"><a href="#cb66-672" aria-hidden="true" tabindex="-1"></a><span class="fu">barplot</span>(scaling_df<span class="sc">$</span>LD1, <span class="at">names.arg =</span> scaling_df<span class="sc">$</span>Variable,</span>
<span id="cb66-673"><a href="#cb66-673" aria-hidden="true" tabindex="-1"></a>        <span class="at">main =</span> <span class="st">"Variable Contributions to LD1"</span>,</span>
<span id="cb66-674"><a href="#cb66-674" aria-hidden="true" tabindex="-1"></a>        <span class="at">ylab =</span> <span class="st">"Absolute Coefficient"</span>,</span>
<span id="cb66-675"><a href="#cb66-675" aria-hidden="true" tabindex="-1"></a>        <span class="at">col =</span> <span class="st">"steelblue"</span>)</span>
<span id="cb66-676"><a href="#cb66-676" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb66-677"><a href="#cb66-677" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-678"><a href="#cb66-678" aria-hidden="true" tabindex="-1"></a><span class="fu">## Comparing Methods</span></span>
<span id="cb66-679"><a href="#cb66-679" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-680"><a href="#cb66-680" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> Method <span class="pp">|</span> Input <span class="pp">|</span> Output <span class="pp">|</span> Supervision <span class="pp">|</span> Best For <span class="pp">|</span></span>
<span id="cb66-681"><a href="#cb66-681" aria-hidden="true" tabindex="-1"></a><span class="pp">|:-------|:------|:-------|:------------|:---------|</span></span>
<span id="cb66-682"><a href="#cb66-682" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> PCA <span class="pp">|</span> Variables <span class="pp">|</span> Continuous scores <span class="pp">|</span> None <span class="pp">|</span> Reducing correlated variables <span class="pp">|</span></span>
<span id="cb66-683"><a href="#cb66-683" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> PCoA <span class="pp">|</span> Distance matrix <span class="pp">|</span> Continuous scores <span class="pp">|</span> None <span class="pp">|</span> Preserving sample distances <span class="pp">|</span></span>
<span id="cb66-684"><a href="#cb66-684" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> NMDS <span class="pp">|</span> Distance matrix <span class="pp">|</span> Ordinal scores <span class="pp">|</span> None <span class="pp">|</span> Ecological community data <span class="pp">|</span></span>
<span id="cb66-685"><a href="#cb66-685" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> MANOVA <span class="pp">|</span> Variables + groups <span class="pp">|</span> Test statistics <span class="pp">|</span> Groups known <span class="pp">|</span> Testing group differences <span class="pp">|</span></span>
<span id="cb66-686"><a href="#cb66-686" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> DFA <span class="pp">|</span> Variables + groups <span class="pp">|</span> Discriminant scores <span class="pp">|</span> Groups known <span class="pp">|</span> Classifying observations <span class="pp">|</span></span>
<span id="cb66-687"><a href="#cb66-687" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-688"><a href="#cb66-688" aria-hidden="true" tabindex="-1"></a>For clustering methods (hierarchical, k-means) that group observations based on similarity, see @sec-clustering.</span>
<span id="cb66-689"><a href="#cb66-689" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-690"><a href="#cb66-690" aria-hidden="true" tabindex="-1"></a><span class="fu">## Using Ordination Scores in Further Analyses</span></span>
<span id="cb66-691"><a href="#cb66-691" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-692"><a href="#cb66-692" aria-hidden="true" tabindex="-1"></a>PC scores and discriminant scores are legitimate new variables that can be used in downstream analysis:</span>
<span id="cb66-693"><a href="#cb66-693" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-694"><a href="#cb66-694" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Regression of scores on other continuous variables</span>
<span id="cb66-695"><a href="#cb66-695" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>ANOVA comparing groups on ordination scores</span>
<span id="cb66-696"><a href="#cb66-696" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Correlation of scores with environmental gradients</span>
<span id="cb66-697"><a href="#cb66-697" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-698"><a href="#cb66-698" aria-hidden="true" tabindex="-1"></a>This is valuable when you have many correlated variables and want to reduce dimensionality before hypothesis testing.</span>
<span id="cb66-699"><a href="#cb66-699" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-702"><a href="#cb66-702" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb66-703"><a href="#cb66-703" aria-hidden="true" tabindex="-1"></a><span class="co"># Use PC scores in ANOVA</span></span>
<span id="cb66-704"><a href="#cb66-704" aria-hidden="true" tabindex="-1"></a>pc_scores <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(</span>
<span id="cb66-705"><a href="#cb66-705" aria-hidden="true" tabindex="-1"></a>  <span class="at">PC1 =</span> iris_pca<span class="sc">$</span>x[, <span class="dv">1</span>],</span>
<span id="cb66-706"><a href="#cb66-706" aria-hidden="true" tabindex="-1"></a>  <span class="at">PC2 =</span> iris_pca<span class="sc">$</span>x[, <span class="dv">2</span>],</span>
<span id="cb66-707"><a href="#cb66-707" aria-hidden="true" tabindex="-1"></a>  <span class="at">Species =</span> iris<span class="sc">$</span>Species</span>
<span id="cb66-708"><a href="#cb66-708" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb66-709"><a href="#cb66-709" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-710"><a href="#cb66-710" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(<span class="fu">aov</span>(PC1 <span class="sc">~</span> Species, <span class="at">data =</span> pc_scores))</span>
<span id="cb66-711"><a href="#cb66-711" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb66-712"><a href="#cb66-712" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-713"><a href="#cb66-713" aria-hidden="true" tabindex="-1"></a><span class="fu">## Practical Workflow</span></span>
<span id="cb66-714"><a href="#cb66-714" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-715"><a href="#cb66-715" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**Explore data**: Check for outliers, missing values, scaling issues</span>
<span id="cb66-716"><a href="#cb66-716" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-717"><a href="#cb66-717" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**Standardize if needed**: Especially important when variables are on different scales</span>
<span id="cb66-718"><a href="#cb66-718" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-719"><a href="#cb66-719" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**Choose appropriate method**: Based on your data type and question</span>
<span id="cb66-720"><a href="#cb66-720" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-721"><a href="#cb66-721" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>**Examine output**: Scree plots, loadings, clustering diagnostics</span>
<span id="cb66-722"><a href="#cb66-722" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-723"><a href="#cb66-723" aria-hidden="true" tabindex="-1"></a><span class="ss">5. </span>**Validate**: Cross-validation for classification; permutation tests for significance</span>
<span id="cb66-724"><a href="#cb66-724" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-725"><a href="#cb66-725" aria-hidden="true" tabindex="-1"></a><span class="ss">6. </span>**Interpret biologically**: What do the patterns mean in your system?</span>
<span id="cb66-726"><a href="#cb66-726" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-727"><a href="#cb66-727" aria-hidden="true" tabindex="-1"></a><span class="fu">## Exercises</span></span>
<span id="cb66-728"><a href="#cb66-728" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-729"><a href="#cb66-729" aria-hidden="true" tabindex="-1"></a>::: {.callout-note}</span>
<span id="cb66-730"><a href="#cb66-730" aria-hidden="true" tabindex="-1"></a><span class="fu">### Exercise DR.1: PCA Exploration</span></span>
<span id="cb66-731"><a href="#cb66-731" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-732"><a href="#cb66-732" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>We want to explore the <span class="in">`tissue_gene_expression`</span> predictors by plotting them.</span>
<span id="cb66-733"><a href="#cb66-733" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-736"><a href="#cb66-736" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb66-737"><a href="#cb66-737" aria-hidden="true" tabindex="-1"></a><span class="co">#| eval: false</span></span>
<span id="cb66-738"><a href="#cb66-738" aria-hidden="true" tabindex="-1"></a><span class="fu">data</span>(<span class="st">"tissue_gene_expression"</span>)</span>
<span id="cb66-739"><a href="#cb66-739" aria-hidden="true" tabindex="-1"></a><span class="fu">dim</span>(tissue_gene_expression<span class="sc">$</span>x)</span>
<span id="cb66-740"><a href="#cb66-740" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb66-741"><a href="#cb66-741" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-742"><a href="#cb66-742" aria-hidden="true" tabindex="-1"></a>We want to get an idea of which observations are close to each other, but the predictors are 500-dimensional so plotting is difficult. Plot the first two principal components with color representing tissue type.</span>
<span id="cb66-743"><a href="#cb66-743" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-744"><a href="#cb66-744" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>The predictors for each observation are measured on the same device and experimental procedure. This introduces biases that can affect all the predictors from one observation. For each observation, compute the average across all predictors and then plot this against the first PC with color representing tissue. Report the correlation.</span>
<span id="cb66-745"><a href="#cb66-745" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-746"><a href="#cb66-746" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>We see an association with the first PC and the observation averages. Redo the PCA but only after removing the center (row means).</span>
<span id="cb66-747"><a href="#cb66-747" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-748"><a href="#cb66-748" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>For the first 10 PCs, make a boxplot showing the values for each tissue.</span>
<span id="cb66-749"><a href="#cb66-749" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-750"><a href="#cb66-750" aria-hidden="true" tabindex="-1"></a><span class="ss">5. </span>Plot the percent variance explained by PC number. Hint: use the <span class="in">`summary`</span> function.</span>
<span id="cb66-751"><a href="#cb66-751" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb66-752"><a href="#cb66-752" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-753"><a href="#cb66-753" aria-hidden="true" tabindex="-1"></a>::: {.callout-note}</span>
<span id="cb66-754"><a href="#cb66-754" aria-hidden="true" tabindex="-1"></a><span class="fu">### Exercise DR.2: Distance Matrices</span></span>
<span id="cb66-755"><a href="#cb66-755" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-756"><a href="#cb66-756" aria-hidden="true" tabindex="-1"></a><span class="ss">6. </span>Load the following dataset and compute distances:</span>
<span id="cb66-757"><a href="#cb66-757" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-760"><a href="#cb66-760" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb66-761"><a href="#cb66-761" aria-hidden="true" tabindex="-1"></a><span class="co">#| eval: false</span></span>
<span id="cb66-762"><a href="#cb66-762" aria-hidden="true" tabindex="-1"></a><span class="fu">data</span>(<span class="st">"tissue_gene_expression"</span>)</span>
<span id="cb66-763"><a href="#cb66-763" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb66-764"><a href="#cb66-764" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-765"><a href="#cb66-765" aria-hidden="true" tabindex="-1"></a>Compare the distance between the first two observations (both cerebellums), the 39th and 40th (both colons), and the 73rd and 74th (both endometriums). Are observations of the same tissue type closer to each other?</span>
<span id="cb66-766"><a href="#cb66-766" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-767"><a href="#cb66-767" aria-hidden="true" tabindex="-1"></a><span class="ss">7. </span>Make an image plot of all the distances using the <span class="in">`image`</span> function. Hint: convert the distance object to a matrix first. Does the pattern suggest that observations of the same tissue type are generally closer?</span>
<span id="cb66-768"><a href="#cb66-768" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb66-769"><a href="#cb66-769" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-770"><a href="#cb66-770" aria-hidden="true" tabindex="-1"></a><span class="fu">## Non-Linear Methods</span></span>
<span id="cb66-771"><a href="#cb66-771" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-772"><a href="#cb66-772" aria-hidden="true" tabindex="-1"></a>The methods covered in this chapter are primarily **linear** dimensionality reduction techniques. PCA finds linear combinations of variables, and PCoA preserves Euclidean distances. However, biological data often lies on complex, non-linear manifolds.</span>
<span id="cb66-773"><a href="#cb66-773" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-774"><a href="#cb66-774" aria-hidden="true" tabindex="-1"></a>For visualization of complex, non-linear structure, see @sec-tsne-umap which covers:</span>
<span id="cb66-775"><a href="#cb66-775" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-776"><a href="#cb66-776" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**t-SNE** (t-distributed Stochastic Neighbor Embedding): Excellent for visualizing local cluster structure</span>
<span id="cb66-777"><a href="#cb66-777" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**UMAP** (Uniform Manifold Approximation and Projection): Faster than t-SNE and may better preserve global structure</span>
<span id="cb66-778"><a href="#cb66-778" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-779"><a href="#cb66-779" aria-hidden="true" tabindex="-1"></a>These non-linear methods are particularly valuable for single-cell RNA-seq data and other high-dimensional biological datasets with complex structure.</span>
<span id="cb66-780"><a href="#cb66-780" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-781"><a href="#cb66-781" aria-hidden="true" tabindex="-1"></a><span class="fu">## Summary</span></span>
<span id="cb66-782"><a href="#cb66-782" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-783"><a href="#cb66-783" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Dimensionality reduction creates fewer variables that capture most information</span>
<span id="cb66-784"><a href="#cb66-784" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**PCA** finds linear combinations that maximize variance</span>
<span id="cb66-785"><a href="#cb66-785" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>Orthogonal transformations preserve distances while concentrating variance</span>
<span id="cb66-786"><a href="#cb66-786" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>Highly correlated variables can be effectively reduced to fewer dimensions</span>
<span id="cb66-787"><a href="#cb66-787" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>Loadings show which original variables contribute to each PC</span>
<span id="cb66-788"><a href="#cb66-788" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>The twin heights example shows how correlated variables compress to one dimension</span>
<span id="cb66-789"><a href="#cb66-789" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**PCoA** works from distance matrices; useful for ecological and genetic data</span>
<span id="cb66-790"><a href="#cb66-790" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**NMDS** preserves rank-order of distances; robust for non-normal data</span>
<span id="cb66-791"><a href="#cb66-791" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**MANOVA** tests whether groups differ on multiple response variables simultaneously</span>
<span id="cb66-792"><a href="#cb66-792" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**DFA/LDA** finds combinations that best discriminate known groups</span>
<span id="cb66-793"><a href="#cb66-793" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>These methods can be combined: use PCA to reduce dimensions, then classify</span>
<span id="cb66-794"><a href="#cb66-794" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>PCA can substantially reduce model complexity while maintaining predictive performance</span>
<span id="cb66-795"><a href="#cb66-795" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>For non-linear structure, consider t-SNE or UMAP (see @sec-tsne-umap)</span>
<span id="cb66-796"><a href="#cb66-796" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-797"><a href="#cb66-797" aria-hidden="true" tabindex="-1"></a><span class="fu">## Additional Resources</span></span>
<span id="cb66-798"><a href="#cb66-798" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-799"><a href="#cb66-799" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>@sec-matrix-algebra - Matrix algebra fundamentals and eigenanalysis for PCA</span>
<span id="cb66-800"><a href="#cb66-800" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>@sec-tsne-umap - Non-linear dimensionality reduction with t-SNE and UMAP</span>
<span id="cb66-801"><a href="#cb66-801" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>@james2023islr - Modern treatment of dimensionality reduction and clustering</span>
<span id="cb66-802"><a href="#cb66-802" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>@logan2010biostatistical - MANOVA and DFA in biological research contexts</span>
<span id="cb66-803"><a href="#cb66-803" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Borcard, D., Gillet, F., &amp; Legendre, P. (2018). *Numerical Ecology with R* - Comprehensive ordination methods</span>
</code><button title="Copy to Clipboard" class="code-copy-button" data-in-quarto-modal=""><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p>Statistics for Biosciences and Bioengineering</p>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    </div>
    <div class="nav-footer-right">
<p>Built with <a href="https://quarto.org/">Quarto</a></p>
</div>
  </div>
</footer>




</body></html>