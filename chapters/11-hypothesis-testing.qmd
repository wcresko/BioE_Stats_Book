# Hypothesis Testing {#sec-hypothesis-testing}

```{r}
#| echo: false
#| message: false
library(tidyverse)
theme_set(theme_minimal())
```

## What is a Hypothesis?

A hypothesis is a statement of belief about the world—a claim that can be evaluated with data. In statistics, we formalize hypothesis testing as a framework for using data to decide between competing claims.

The **null hypothesis** ($H_0$) represents a default position, typically stating that there is no effect, no difference, or no relationship. The **alternative hypothesis** ($H_A$) represents what we would conclude if we reject the null—typically that there is an effect, difference, or relationship.

For example, consider testing whether an amino acid substitution changes the catalytic rate of an enzyme:

- $H_0$: The substitution does not change the catalytic rate
- $H_A$: The substitution does change the catalytic rate

The alternative hypothesis might be directional (the substitution increases the rate) or non-directional (the substitution changes the rate, in either direction). This distinction affects how we calculate p-values.

## The Logic of Hypothesis Testing

Hypothesis testing follows a specific logic. We assume the null hypothesis is true and ask: how likely would we be to observe data as extreme as what we actually observed? If this probability is very small, we conclude that the null hypothesis is unlikely to be true and reject it in favor of the alternative.

Key questions in hypothesis testing include:

- What is the probability of rejecting a true null hypothesis?
- What is the probability of failing to reject a false null hypothesis?
- How do we decide when to reject the null hypothesis?
- What can we conclude if we fail to reject?

## Type I and Type II Errors

Two types of mistakes are possible in hypothesis testing.

![](../images/week_3.007.jpeg){fig-align="center"}

A **Type I error** occurs when we reject a true null hypothesis—concluding there is an effect when there is not. The probability of a Type I error is denoted $\alpha$ and is called the significance level. By convention, $\alpha$ is often set to 0.05, meaning we accept a 5% chance of falsely rejecting a true null.

A **Type II error** occurs when we fail to reject a false null hypothesis—concluding there is no effect when there actually is one. The probability of a Type II error is denoted $\beta$.

**Power** is the probability of correctly rejecting a false null hypothesis: Power = $1 - \beta$. Power depends on the effect size (how big the true effect is), sample size, significance level, and variability in the data.

| | $H_0$ True | $H_0$ False |
|:--|:--|:--|
| Reject $H_0$ | Type I Error ($\alpha$) | Correct Decision (Power) |
| Fail to Reject $H_0$ | Correct Decision | Type II Error ($\beta$) |

### Understanding Statistical Power

Power analysis is essential for designing experiments that can actually detect effects of interest. A study with low power is unlikely to find real effects even when they exist—wasting resources and potentially leading to incorrect conclusions.

The key factors affecting power are:

1. **Effect size**: Larger effects are easier to detect
2. **Sample size**: More data provides more information
3. **Significance level ($\alpha$)**: Higher $\alpha$ increases power but also increases Type I error rate
4. **Variability**: Less noise makes signals easier to detect

For a two-sample t-test, the relationship between these factors can be expressed approximately as:

$$\text{Power} \approx \Phi\left(\frac{|\mu_1 - \mu_2|}{\sigma}\sqrt{\frac{n}{2}} - z_{1-\alpha/2}\right)$$

where $\Phi$ is the standard normal CDF, and $z_{1-\alpha/2}$ is the critical value.

```{r}
#| fig-width: 8
#| fig-height: 5
# Visualize how power depends on effect size and sample size
library(pwr)

# Power curves for different effect sizes
effect_sizes <- c(0.2, 0.5, 0.8)  # Cohen's d: small, medium, large
sample_sizes <- seq(5, 100, by = 5)

par(mfrow = c(1, 1))
colors <- c("blue", "darkgreen", "red")

plot(NULL, xlim = c(5, 100), ylim = c(0, 1),
     xlab = "Sample Size (per group)", ylab = "Power",
     main = "Power Curves for Two-Sample t-Test")
abline(h = 0.8, lty = 2, col = "gray")

for (i in 1:3) {
  powers <- sapply(sample_sizes, function(n) {
    pwr.t.test(n = n, d = effect_sizes[i], sig.level = 0.05, type = "two.sample")$power
  })
  lines(sample_sizes, powers, col = colors[i], lwd = 2)
}

legend("bottomright",
       legend = c("Small (d=0.2)", "Medium (d=0.5)", "Large (d=0.8)"),
       col = colors, lwd = 2)
```

### A Priori Power Analysis

Before conducting an experiment, power analysis helps determine the necessary sample size. The question is: "How many observations do I need to have an 80% chance of detecting an effect of a given size?"

```{r}
# Sample size calculation for 80% power
# Detecting a medium effect (d = 0.5) with alpha = 0.05
power_result <- pwr.t.test(d = 0.5, sig.level = 0.05, power = 0.8, type = "two.sample")
cat("Required sample size per group:", ceiling(power_result$n), "\n")

# For a small effect (d = 0.2)
power_small <- pwr.t.test(d = 0.2, sig.level = 0.05, power = 0.8, type = "two.sample")
cat("For small effect (d=0.2):", ceiling(power_small$n), "per group\n")
```

Notice how detecting small effects requires substantially larger samples. This is why pilot studies and literature-based effect size estimates are valuable for planning.

## P-Values

The **p-value** is the probability of observing a test statistic as extreme or more extreme than the one calculated from the data, assuming the null hypothesis is true.

A small p-value indicates that the observed data would be unlikely if the null hypothesis were true, providing evidence against the null. A large p-value indicates that the data are consistent with the null hypothesis.

The p-value is NOT the probability that the null hypothesis is true. It is the probability of the data (or more extreme data) given the null hypothesis, not the probability of the null hypothesis given the data.

## Significance Level and Decision Rules

The **significance level** $\alpha$ is the threshold below which we reject the null hypothesis. If $p < \alpha$, we reject $H_0$. If $p \geq \alpha$, we fail to reject $H_0$.

![](../images/week_3.008.jpeg){fig-align="center"}

The conventional choice of $\alpha = 0.05$ is arbitrary but widely used. In contexts where Type I errors are particularly costly (e.g., approving an ineffective drug), smaller $\alpha$ values may be appropriate. In exploratory research, larger $\alpha$ values might be acceptable.

Important: "fail to reject" is not the same as "accept." Failing to reject the null hypothesis means the data did not provide sufficient evidence against it, not that the null hypothesis is true.

## Test Statistics and Statistical Distributions

A **test statistic** summarizes the data in a way that allows comparison to a known distribution under the null hypothesis. Different tests use different statistics: the t-statistic for t-tests, the F-statistic for ANOVA, the chi-squared statistic for contingency tables.

Just like raw data, test statistics are random variables with their own sampling distributions. Under the null hypothesis, we know what distribution the test statistic should follow. We can then calculate how unusual our observed statistic is under this distribution.

![](../images/week_3.001.jpeg){fig-align="center"}

## One-Tailed vs. Two-Tailed Tests

A **two-tailed test** considers extreme values in both directions. The alternative hypothesis is non-directional: $H_A: \mu \neq \mu_0$. Extreme values in either tail of the distribution count as evidence against the null.

A **one-tailed test** considers extreme values in only one direction. The alternative hypothesis is directional: $H_A: \mu > \mu_0$ or $H_A: \mu < \mu_0$. Only extreme values in the specified direction count as evidence against the null.

![](../images/week_3.002.jpeg){fig-align="center"}

Two-tailed tests are more conservative and are appropriate when you do not have a strong prior expectation about the direction of an effect. One-tailed tests have more power to detect effects in the specified direction but will miss effects in the opposite direction.

## Multiple Testing

When you perform many hypothesis tests, the probability of at least one Type I error increases. If you test 20 independent hypotheses at $\alpha = 0.05$, you expect about one false positive even when all null hypotheses are true.

Several approaches address multiple testing:

**Bonferroni correction** divides $\alpha$ by the number of tests. For 20 tests, use $\alpha = 0.05/20 = 0.0025$. This is conservative and may miss true effects.

**False Discovery Rate (FDR)** control allows some false positives but controls their proportion among rejected hypotheses. This is less conservative than Bonferroni and widely used in genomics and other high-throughput applications.

## Practical vs. Statistical Significance

Statistical significance does not imply practical importance. With a large enough sample, even trivially small effects become statistically significant. Conversely, practically important effects may not reach statistical significance with small samples.

Always consider effect sizes alongside p-values. Report confidence intervals, which convey both the magnitude of an effect and the uncertainty about it. A 95% confidence interval that excludes zero is equivalent to statistical significance at $\alpha = 0.05$, but also shows the range of plausible effect sizes.

### The Connection Between P-Values and Confidence Intervals

P-values and confidence intervals are mathematically linked. Understanding this connection deepens your grasp of both concepts.

For testing whether a parameter equals some null value $\theta_0$ (e.g., testing if $\mu = 0$ or $\mu_1 - \mu_2 = 0$):

::: {.callout-tip}
## The P-Value / CI Duality

- If the $(1-\alpha)$ confidence interval **excludes** $\theta_0$, then $p < \alpha$
- If the $(1-\alpha)$ confidence interval **includes** $\theta_0$, then $p \geq \alpha$

A 95% CI that doesn't contain zero corresponds to p < 0.05 for a two-tailed test.
:::

This relationship makes sense when you think about it: the confidence interval represents the range of parameter values that are "compatible" with the data. If the null value falls outside this range, the data provide evidence against it (small p-value). If the null value falls inside, the data are consistent with it (large p-value).

```{r}
#| fig-width: 8
#| fig-height: 5
# Demonstrate the CI-pvalue relationship
set.seed(42)

# Three scenarios
scenarios <- list(
  list(true_mean = 5, label = "CI excludes 0"),
  list(true_mean = 2, label = "CI barely excludes 0"),
  list(true_mean = 0.5, label = "CI includes 0")
)

par(mfrow = c(1, 3))

for (scenario in scenarios) {
  sample_data <- rnorm(30, mean = scenario$true_mean, sd = 5)
  result <- t.test(sample_data)

  # Plot CI
  ci <- result$conf.int
  mean_est <- result$estimate

  plot(1, mean_est, xlim = c(0.5, 1.5), ylim = c(-4, 12),
       pch = 19, cex = 1.5, xaxt = "n", xlab = "",
       ylab = "Estimated Mean",
       main = paste0(scenario$label, "\np = ", round(result$p.value, 4)))

  arrows(1, ci[1], 1, ci[2], angle = 90, code = 3, length = 0.1, lwd = 2)
  abline(h = 0, col = "red", lty = 2, lwd = 2)
}
```

The beauty of confidence intervals is that they provide *more* information than p-values alone:

1. **Direction**: You see whether the effect is positive or negative
2. **Magnitude**: You see the estimated size of the effect
3. **Precision**: The width shows your uncertainty
4. **Significance**: Whether zero is included tells you if p < 0.05

This is why many statisticians advocate for reporting confidence intervals as the primary summary, with p-values as secondary.

### Standardized Effect Sizes

Effect sizes quantify the magnitude of an effect in a standardized way, allowing comparison across studies.

**Cohen's d** measures the difference between two means in standard deviation units:

$$d = \frac{\bar{x}_1 - \bar{x}_2}{s_{\text{pooled}}}$$

where $s_{\text{pooled}} = \sqrt{\frac{(n_1-1)s_1^2 + (n_2-1)s_2^2}{n_1 + n_2 - 2}}$

Cohen's guidelines suggest $d = 0.2$ is small, $d = 0.5$ is medium, and $d = 0.8$ is large, though these benchmarks should be interpreted in context [@cohen1988statistical].

```{r}
# Calculate Cohen's d
group1 <- c(23, 25, 28, 31, 27, 29)
group2 <- c(18, 20, 22, 19, 21, 23)

# Pooled standard deviation
n1 <- length(group1)
n2 <- length(group2)
s_pooled <- sqrt(((n1-1)*var(group1) + (n2-1)*var(group2)) / (n1 + n2 - 2))

# Cohen's d
cohens_d <- (mean(group1) - mean(group2)) / s_pooled
cat("Cohen's d:", round(cohens_d, 3), "\n")
```

Other effect sizes include:
- **Pearson's r**: Correlation coefficient (-1 to 1)
- **$\eta^2$ (eta-squared)**: Proportion of variance explained in ANOVA
- **Odds ratio**: Effect size for binary outcomes

## Critiques of NHST

The Null Hypothesis Significance Testing (NHST) framework, while widely used, has important limitations that researchers should understand.

::: {.callout-warning}
## Limitations of P-Values

1. **P-values do not measure effect size**: A tiny, meaningless effect can have p < 0.001 with enough data
2. **P-values do not measure probability that $H_0$ is true**: This is a common misinterpretation
3. **The 0.05 threshold is arbitrary**: There is nothing magical about $\alpha = 0.05$
4. **Dichotomous thinking**: Treating p = 0.049 and p = 0.051 as fundamentally different is misleading
5. **Publication bias**: Studies with p < 0.05 are more likely to be published, distorting the literature
:::

### Alternatives and Complements to NHST

**Confidence intervals** provide more information than p-values alone, showing both the estimated effect and uncertainty.

**Effect sizes** communicate the practical magnitude of results.

**Bayesian methods** provide probability statements about hypotheses themselves (not just the data given the hypothesis).

**Equivalence testing** can demonstrate that an effect is negligibly small, not just "not significantly different from zero."

The American Statistical Association's 2016 statement on p-values emphasizes that p-values should not be used in isolation and that scientific conclusions should not be based solely on whether a p-value crosses a threshold.

## Example: Null Distribution via Randomization

We can create empirical null distributions through randomization, providing an alternative to parametric assumptions.

```{r}
#| fig-width: 7
#| fig-height: 5
# Two groups to compare
set.seed(56)
pop_1 <- rnorm(n = 50, mean = 20.1, sd = 2)
pop_2 <- rnorm(n = 50, mean = 19.3, sd = 2)

# Observed t-statistic
t_obs <- t.test(x = pop_1, y = pop_2, alternative = "greater")$statistic

# Create null distribution by randomization
pops_comb <- c(pop_1, pop_2)

t_rand <- replicate(1000, {
  pops_shuf <- sample(pops_comb)
  t.test(x = pops_shuf[1:50], y = pops_shuf[51:100], alternative = "greater")$statistic
})

# Plot null distribution
hist(t_rand, breaks = 30, main = "Randomization Null Distribution",
     xlab = "t-statistic", col = "lightblue")
abline(v = t_obs, col = "red", lwd = 2)
```

```{r}
# Calculate p-value
p_value <- sum(t_rand >= t_obs) / 1000
cat("Observed t:", round(t_obs, 3), "\n")
cat("P-value:", p_value, "\n")
```

![](../images/week_3.009.jpeg){fig-align="center"}

## Summary

Hypothesis testing provides a framework for using data to evaluate claims about populations. Key concepts include:

- Null and alternative hypotheses formalize competing claims
- Type I errors (false positives) and Type II errors (false negatives) represent the two ways we can be wrong
- P-values quantify evidence against the null hypothesis
- Significance levels set thresholds for decision-making
- Multiple testing requires adjustment to control error rates
- Statistical significance does not imply practical importance

In the following chapters, we apply this framework to specific tests: t-tests for comparing means, chi-squared tests for categorical data, and nonparametric alternatives when assumptions are violated.

## Additional Resources

- @cohen1988statistical - The classic reference on statistical power analysis
- @logan2010biostatistical - Comprehensive treatment of hypothesis testing in biological contexts
