# Core Concepts in Statistical Learning {#sec-statistical-learning}

```{r}
#| echo: false
#| message: false
library(tidyverse)
theme_set(theme_minimal())
```

## From Inference to Prediction

Traditional statistics emphasizes inference—understanding relationships, testing hypotheses, and quantifying uncertainty. Statistical learning (or machine learning) shifts focus toward prediction—building models that accurately predict outcomes for new data.

Both approaches use similar mathematical tools, but the goals differ. In inference, we want to understand the true relationship between variables. In prediction, we want accurate predictions, even if the model does not perfectly capture the underlying mechanism.

## The Overfitting Problem

Models are built to fit training data as closely as possible. A linear regression minimizes squared errors; a logistic regression maximizes likelihood. But models that fit training data too well often predict poorly on new data.

**Overfitting** occurs when a model captures noise specific to the training data rather than the true underlying pattern. Complex models with many parameters are especially susceptible.

The solution is to evaluate models on data they have not seen—held-out test data or through cross-validation.

## Cross-Validation

Cross-validation estimates how well a model will generalize to new data.

**K-fold cross-validation**:
1. Split data into k roughly equal parts (folds)
2. For each fold: train on k-1 folds, test on the held-out fold
3. Average performance across all folds

```{r}
#| label: fig-cross-validation
#| fig-cap: "Cross-validation example demonstrating prediction error estimation"
#| fig-width: 7
#| fig-height: 5
# Simple CV example with linear regression
library(boot)

# Generate data
set.seed(42)
x <- rnorm(100)
y <- 2 + 3*x + rnorm(100)
data <- data.frame(x, y)

# Fit model and perform CV
model <- glm(y ~ x, data = data)

# 10-fold cross-validation
cv_result <- cv.glm(data, model, K = 10)
cat("CV estimate of prediction error:", round(cv_result$delta[1], 3), "\n")
```

**Leave-one-out cross-validation (LOOCV)** is k-fold with k = n: each observation is held out once. More computationally expensive but lower variance.

## Bias-Variance Tradeoff

Prediction error has two components:

**Bias**: Error from approximating a complex reality with a simpler model. Simple models have high bias—they may miss important patterns.

**Variance**: Error from sensitivity to training data. Complex models have high variance—they change substantially with different training samples.

The best predictions come from models that balance bias and variance. As model complexity increases, bias decreases but variance increases. The optimal complexity minimizes total prediction error.

## Regularization: Controlling Model Complexity

**Regularization** addresses overfitting by adding a penalty term that discourages complex models. This is particularly important when you have many predictors relative to observations, or when predictors are correlated.

### The Regularization Idea

Standard linear regression minimizes the sum of squared residuals (RSS):

$$\text{RSS} = \sum_{i=1}^n (y_i - \beta_0 - \sum_{j=1}^p \beta_j x_{ij})^2$$

Regularized regression adds a penalty term $\lambda P(\beta)$ that shrinks coefficients toward zero:

$$\text{Minimize: } \text{RSS} + \lambda P(\beta)$$

The **regularization parameter** $\lambda$ controls the strength of the penalty:
- $\lambda = 0$: No penalty, equivalent to ordinary least squares
- $\lambda \to \infty$: Very strong penalty, coefficients shrink toward zero

### Ridge Regression (L2 Penalty)

**Ridge regression** uses the sum of squared coefficients as the penalty:

$$P(\beta) = \sum_{j=1}^p \beta_j^2$$

This shrinks all coefficients toward zero but never exactly to zero. Ridge is particularly effective when predictors are correlated (multicollinearity).

```{r}
#| label: fig-ridge-path
#| fig-cap: "Ridge regression coefficient paths: as lambda increases, coefficients shrink toward zero but never reach exactly zero"
#| fig-width: 8
#| fig-height: 5
library(glmnet)

# Generate sample data with correlated predictors
set.seed(42)
n <- 100
p <- 10
X <- matrix(rnorm(n * p), n, p)
# Create correlated predictors
X[, 2] <- X[, 1] + rnorm(n, sd = 0.5)
X[, 3] <- X[, 1] + rnorm(n, sd = 0.5)
true_beta <- c(3, -2, 1.5, rep(0, p - 3))
y <- X %*% true_beta + rnorm(n)

# Fit ridge regression across lambda values
ridge_fit <- glmnet(X, y, alpha = 0)  # alpha = 0 for ridge

# Plot coefficient paths
plot(ridge_fit, xvar = "lambda", main = "Ridge Regression Coefficients")
```

### Lasso Regression (L1 Penalty)

**Lasso** (Least Absolute Shrinkage and Selection Operator) uses the sum of absolute values as the penalty:

$$P(\beta) = \sum_{j=1}^p |\beta_j|$$

Unlike ridge, lasso can shrink coefficients exactly to zero, effectively performing **variable selection**. This produces sparse models that are easier to interpret.

```{r}
#| label: fig-lasso-path
#| fig-cap: "Lasso regression coefficient paths: as lambda increases, coefficients shrink and some become exactly zero (variable selection)"
#| fig-width: 8
#| fig-height: 5
# Fit lasso regression
lasso_fit <- glmnet(X, y, alpha = 1)  # alpha = 1 for lasso

plot(lasso_fit, xvar = "lambda", main = "Lasso Regression Coefficients")
```

### Elastic Net: Combining Ridge and Lasso

**Elastic net** combines both penalties:

$$P(\beta) = \alpha \sum_{j=1}^p |\beta_j| + (1-\alpha) \sum_{j=1}^p \beta_j^2$$

The mixing parameter $\alpha$ controls the balance:
- $\alpha = 0$: Pure ridge
- $\alpha = 1$: Pure lasso
- $0 < \alpha < 1$: Combination

Elastic net is often preferred when predictors are correlated—it tends to select groups of correlated variables together.

### Choosing Lambda with Cross-Validation

The regularization parameter $\lambda$ is typically chosen by cross-validation:

```{r}
#| label: fig-cv-lambda
#| fig-cap: "Cross-validation to select optimal lambda: the left dashed line marks the minimum error, the right marks the most regularized model within one standard error"
#| fig-width: 8
#| fig-height: 5
# Cross-validation for lasso
set.seed(123)
cv_lasso <- cv.glmnet(X, y, alpha = 1)

# Plot cross-validation results
plot(cv_lasso)

# Optimal lambda values
cat("Lambda with minimum CV error:", round(cv_lasso$lambda.min, 4), "\n")
cat("Lambda within 1 SE of minimum:", round(cv_lasso$lambda.1se, 4), "\n")
```

The `lambda.1se` (one standard error rule) often provides a more parsimonious model with nearly as good performance as the minimum.

### Comparing Regularization Methods

```{r}
# Fit models with optimal lambda
ridge_cv <- cv.glmnet(X, y, alpha = 0)
lasso_cv <- cv.glmnet(X, y, alpha = 1)

# Extract coefficients
coef_ols <- coef(lm(y ~ X))
coef_ridge <- coef(ridge_cv, s = "lambda.1se")
coef_lasso <- coef(lasso_cv, s = "lambda.1se")

# Compare (excluding intercept)
comparison <- data.frame(
  True = c(NA, true_beta),
  OLS = as.vector(coef_ols),
  Ridge = as.vector(coef_ridge),
  Lasso = as.vector(coef_lasso)
)
rownames(comparison) <- c("Intercept", paste0("X", 1:p))
round(comparison, 3)
```

Notice that lasso correctly identifies the zero coefficients (variables 4-10), while ridge shrinks them but doesn't eliminate them.

::: {.callout-tip}
## When to Use Each Method

- **Ridge**: When you believe all predictors are relevant and want to handle multicollinearity
- **Lasso**: When you want automatic variable selection and a sparse model
- **Elastic Net**: When predictors are correlated and you want both selection and grouping

**Important**: Always standardize predictors before applying regularization, as the penalty treats all coefficients equally. The `glmnet` function does this automatically by default.
:::

## Splines: Flexible Curve Fitting

While LOESS provides local smoothing, **splines** offer a more structured approach to fitting flexible curves. A spline is a piecewise polynomial function that joins smoothly at points called **knots**.

### Why Splines?

Linear regression assumes a straight-line relationship, which is often too restrictive. We could fit polynomial regression (e.g., $y = \beta_0 + \beta_1 x + \beta_2 x^2 + \beta_3 x^3$), but polynomials can behave erratically, especially at the edges of the data.

Splines provide flexibility while maintaining smooth, well-behaved curves.

### Regression Splines

**Regression splines** fit piecewise polynomials at fixed knot locations. The `splines` package provides basis functions for incorporating splines into linear models:

```{r}
#| label: fig-spline-comparison
#| fig-cap: "Comparison of linear, polynomial, and spline fits for non-linear data"
#| fig-width: 9
#| fig-height: 4
library(splines)

# Generate non-linear data
set.seed(42)
x <- seq(0, 10, length.out = 100)
y <- sin(x) + 0.5 * cos(2*x) + rnorm(100, sd = 0.3)
data <- data.frame(x, y)

# Fit different models
fit_linear <- lm(y ~ x, data = data)
fit_poly <- lm(y ~ poly(x, 5), data = data)
fit_spline <- lm(y ~ bs(x, df = 6), data = data)  # B-spline with 6 df

# Predictions
data$pred_linear <- predict(fit_linear)
data$pred_poly <- predict(fit_poly)
data$pred_spline <- predict(fit_spline)

par(mfrow = c(1, 3))

plot(x, y, pch = 16, col = "gray60", main = "Linear")
lines(x, data$pred_linear, col = "red", lwd = 2)

plot(x, y, pch = 16, col = "gray60", main = "Polynomial (degree 5)")
lines(x, data$pred_poly, col = "blue", lwd = 2)

plot(x, y, pch = 16, col = "gray60", main = "Spline (6 df)")
lines(x, data$pred_spline, col = "darkgreen", lwd = 2)
```

### Natural Splines

**Natural splines** add the constraint that the function is linear beyond the boundary knots. This prevents the wild behavior that polynomials often exhibit at the edges:

```{r}
#| label: fig-natural-spline
#| fig-cap: "Natural splines constrain the fit to be linear beyond the data boundaries, reducing edge effects"
#| fig-width: 7
#| fig-height: 5
# Compare B-spline and natural spline
fit_bs <- lm(y ~ bs(x, df = 6), data = data)
fit_ns <- lm(y ~ ns(x, df = 6), data = data)

# Extend prediction range to see edge behavior
x_ext <- seq(-2, 12, length.out = 200)
pred_bs <- predict(fit_bs, newdata = data.frame(x = x_ext))
pred_ns <- predict(fit_ns, newdata = data.frame(x = x_ext))

plot(x, y, pch = 16, col = "gray60", xlim = c(-2, 12), ylim = c(-3, 3),
     main = "B-spline vs Natural Spline at Boundaries")
lines(x_ext, pred_bs, col = "blue", lwd = 2)
lines(x_ext, pred_ns, col = "darkgreen", lwd = 2)
abline(v = range(x), lty = 2, col = "gray")
legend("topright", c("B-spline", "Natural spline", "Data range"),
       col = c("blue", "darkgreen", "gray"), lty = c(1, 1, 2), lwd = c(2, 2, 1))
```

### Smoothing Splines

**Smoothing splines** take a different approach: instead of pre-specifying knots, they place a knot at every data point and control smoothness through a penalty on the second derivative:

$$\text{Minimize: } \sum_{i=1}^n (y_i - f(x_i))^2 + \lambda \int f''(x)^2 dx$$

The smoothing parameter $\lambda$ is typically chosen by cross-validation:

```{r}
#| label: fig-smoothing-spline
#| fig-cap: "Smoothing spline with automatic cross-validation selection of the smoothing parameter"
#| fig-width: 7
#| fig-height: 5
# Fit smoothing spline with cross-validation
smooth_fit <- smooth.spline(x, y, cv = TRUE)

plot(x, y, pch = 16, col = "gray60", main = "Smoothing Spline")
lines(smooth_fit, col = "purple", lwd = 2)
cat("Optimal degrees of freedom:", round(smooth_fit$df, 2), "\n")
```

::: {.callout-note}
## Choosing the Right Approach

- **Regression splines (bs, ns)**: When you want to include splines in a regression model with other predictors
- **Natural splines**: When extrapolation behavior matters
- **Smoothing splines**: For exploratory smoothing with automatic tuning
- **LOESS**: For local, non-parametric smoothing (especially useful for visualization)
:::

## LOESS: Flexible Non-Parametric Smoothing

**LOESS** (Locally Estimated Scatterplot Smoothing) fits local regressions to subsets of data, weighted by distance from each point.

```{r}
#| label: fig-loess-comparison
#| fig-cap: "Comparison of linear regression and LOESS smoothing for non-linear data"
#| fig-width: 7
#| fig-height: 5
# Compare linear regression and LOESS
set.seed(123)
x <- seq(0, 4*pi, length.out = 100)
y <- sin(x) + rnorm(100, sd = 0.3)

plot(x, y, pch = 16, col = "gray60", main = "Linear vs LOESS")
abline(lm(y ~ x), col = "red", lwd = 2)
lines(x, predict(loess(y ~ x, span = 0.3)), col = "blue", lwd = 2)
legend("topright", c("Linear", "LOESS"), col = c("red", "blue"), lwd = 2)
```

The **span** parameter controls smoothness: smaller values fit more locally (more flexible), larger values average more broadly (smoother).

## Classification

When the response is categorical, we have a classification problem rather than regression. The goal is to predict which category an observation belongs to.

**Logistic regression** produces probabilities that can be converted to class predictions.

**Decision trees** recursively partition the feature space based on simple rules.

**Random forests** combine many decision trees for more robust predictions.

## K-Nearest Neighbors

**K-nearest neighbors (kNN)** is one of the simplest and most intuitive classification algorithms. To classify a new observation, kNN finds the k closest observations in the training data and assigns the most common class among those neighbors.

```{r}
#| label: fig-knn-concept
#| fig-cap: "K-nearest neighbors classification: the new point (star) is classified based on its nearest neighbors"
#| fig-width: 8
#| fig-height: 5
# Simulate two-class data
set.seed(42)
n <- 100
class1 <- data.frame(
  x1 = rnorm(n/2, mean = 2, sd = 1),
  x2 = rnorm(n/2, mean = 2, sd = 1),
  class = "A"
)
class2 <- data.frame(
  x1 = rnorm(n/2, mean = 4, sd = 1),
  x2 = rnorm(n/2, mean = 4, sd = 1),
  class = "B"
)
train_data <- rbind(class1, class2)

# New point to classify
new_point <- data.frame(x1 = 3.2, x2 = 3.5)

# Plot
plot(train_data$x1, train_data$x2,
     col = ifelse(train_data$class == "A", "blue", "red"),
     pch = 16, xlab = "Feature 1", ylab = "Feature 2",
     main = "K-Nearest Neighbors (k=5)")
points(new_point$x1, new_point$x2, pch = 8, cex = 2, lwd = 2)

# Find 5 nearest neighbors
distances <- sqrt((train_data$x1 - new_point$x1)^2 +
                  (train_data$x2 - new_point$x2)^2)
nearest <- order(distances)[1:5]

# Draw circles around nearest neighbors
points(train_data$x1[nearest], train_data$x2[nearest],
       cex = 2, col = ifelse(train_data$class[nearest] == "A", "blue", "red"))
legend("topleft", c("Class A", "Class B", "New point"),
       col = c("blue", "red", "black"), pch = c(16, 16, 8))
```

The choice of **k** is critical and illustrates the bias-variance tradeoff:

- **Small k** (e.g., k=1): Very flexible, low bias but high variance. The decision boundary is jagged and sensitive to individual training points—prone to overfitting.
- **Large k**: Smoother decision boundary, higher bias but lower variance. May miss local patterns—prone to underfitting.

```{r}
#| label: fig-knn-k-comparison
#| fig-cap: "Effect of k on kNN classification: small k creates complex boundaries (potential overfitting), large k creates smooth boundaries (potential underfitting)"
#| fig-width: 9
#| fig-height: 4
library(class)

# Create a grid for visualization
x1_grid <- seq(0, 6, length.out = 100)
x2_grid <- seq(0, 6, length.out = 100)
grid <- expand.grid(x1 = x1_grid, x2 = x2_grid)

par(mfrow = c(1, 3))

for (k_val in c(1, 15, 50)) {
  # Predict on grid
  pred <- knn(train = train_data[, 1:2],
              test = grid,
              cl = train_data$class,
              k = k_val)

  # Plot decision regions
  plot(grid$x1, grid$x2, col = ifelse(pred == "A",
       rgb(0, 0, 1, 0.1), rgb(1, 0, 0, 0.1)),
       pch = 15, cex = 0.5, xlab = "Feature 1", ylab = "Feature 2",
       main = paste("k =", k_val))
  points(train_data$x1, train_data$x2,
         col = ifelse(train_data$class == "A", "blue", "red"), pch = 16)
}
```

### Selecting k with Cross-Validation

We choose k by evaluating classification accuracy across different values using cross-validation:

```{r}
#| label: fig-knn-cv
#| fig-cap: "Cross-validation accuracy for different values of k: accuracy on training data decreases with k, but test accuracy peaks at intermediate values"
#| fig-width: 7
#| fig-height: 5
# Evaluate different k values
set.seed(123)
k_values <- seq(1, 50, by = 2)

# Simple holdout validation
test_idx <- sample(1:nrow(train_data), 30)
train_subset <- train_data[-test_idx, ]
test_subset <- train_data[test_idx, ]

accuracy <- sapply(k_values, function(k) {
  pred <- knn(train = train_subset[, 1:2],
              test = test_subset[, 1:2],
              cl = train_subset$class,
              k = k)
  mean(pred == test_subset$class)
})

train_accuracy <- sapply(k_values, function(k) {
  pred <- knn(train = train_subset[, 1:2],
              test = train_subset[, 1:2],
              cl = train_subset$class,
              k = k)
  mean(pred == train_subset$class)
})

plot(k_values, train_accuracy, type = "l", col = "blue", lwd = 2,
     xlab = "k (number of neighbors)", ylab = "Accuracy",
     main = "Training vs Test Accuracy", ylim = c(0.5, 1))
lines(k_values, accuracy, col = "red", lwd = 2)
legend("bottomright", c("Training", "Test"), col = c("blue", "red"), lwd = 2)
```

Notice that training accuracy is perfect (1.0) when k=1—each point is its own nearest neighbor. But test accuracy tells the true story of generalization performance.

## Confusion Matrices

Classification performance is evaluated with a **confusion matrix**:

|  | Predicted Positive | Predicted Negative |
|:--|:--:|:--:|
| Actual Positive | True Positive (TP) | False Negative (FN) |
| Actual Negative | False Positive (FP) | True Negative (TN) |

Key metrics:
- **Accuracy**: (TP + TN) / Total
- **Sensitivity** (Recall): TP / (TP + FN) — how many positives were caught
- **Specificity**: TN / (TN + FP) — how many negatives were correctly identified
- **Precision**: TP / (TP + FP) — among positive predictions, how many were correct

### The Problem with Accuracy

Accuracy can be misleading with **imbalanced classes**. If 95% of emails are legitimate, a classifier that labels everything as "not spam" achieves 95% accuracy while being completely useless for its intended purpose.

```{r}
# Imbalanced class example
set.seed(42)
# 95% negative, 5% positive (e.g., rare disease screening)
n <- 1000
actual <- factor(c(rep("Negative", 950), rep("Positive", 50)))

# Naive classifier: always predict negative
naive_pred <- factor(rep("Negative", n), levels = c("Negative", "Positive"))

# Calculate metrics
TP <- sum(naive_pred == "Positive" & actual == "Positive")
TN <- sum(naive_pred == "Negative" & actual == "Negative")
FP <- sum(naive_pred == "Positive" & actual == "Negative")
FN <- sum(naive_pred == "Negative" & actual == "Positive")

cat("Accuracy:", (TP + TN) / n, "\n")
cat("Sensitivity (Recall):", TP / (TP + FN), "\n")
cat("The classifier catches 0% of positive cases!\n")
```

### F1 Score and Balanced Accuracy

For imbalanced data, better metrics include:

**F1 Score**: The harmonic mean of precision and recall, balancing both concerns:

$$F_1 = 2 \cdot \frac{\text{Precision} \cdot \text{Recall}}{\text{Precision} + \text{Recall}} = \frac{2 \cdot TP}{2 \cdot TP + FP + FN}$$

**Balanced Accuracy**: The average of sensitivity and specificity:

$$\text{Balanced Accuracy} = \frac{\text{Sensitivity} + \text{Specificity}}{2}$$

```{r}
# Better classifier for the imbalanced data
set.seed(123)
# Suppose we have a model that catches 80% of positives but has some false positives
better_pred <- actual  # Start with actual
# Correctly identify 80% of positives
pos_idx <- which(actual == "Positive")
neg_idx <- which(actual == "Negative")
better_pred[sample(pos_idx, 10)] <- "Negative"  # Miss 10 of 50 positives (20%)
better_pred[sample(neg_idx, 50)] <- "Positive"  # 50 false positives

# Confusion matrix
TP <- sum(better_pred == "Positive" & actual == "Positive")
TN <- sum(better_pred == "Negative" & actual == "Negative")
FP <- sum(better_pred == "Positive" & actual == "Negative")
FN <- sum(better_pred == "Negative" & actual == "Positive")

precision <- TP / (TP + FP)
recall <- TP / (TP + FN)  # Sensitivity
specificity <- TN / (TN + FP)

# Calculate metrics
accuracy <- (TP + TN) / n
f1 <- 2 * precision * recall / (precision + recall)
balanced_acc <- (recall + specificity) / 2

cat("Accuracy:", round(accuracy, 3), "\n")
cat("Precision:", round(precision, 3), "\n")
cat("Recall (Sensitivity):", round(recall, 3), "\n")
cat("F1 Score:", round(f1, 3), "\n")
cat("Balanced Accuracy:", round(balanced_acc, 3), "\n")
```

::: {.callout-tip}
## Which Metric to Use?

- **Accuracy**: Only when classes are balanced
- **F1 Score**: When you care about both precision and recall equally
- **Sensitivity/Recall**: When missing positives is costly (disease screening)
- **Precision**: When false positives are costly (spam filtering)
- **Balanced Accuracy**: Quick summary for imbalanced data
:::

## ROC Curves and AUC

Many classifiers output probabilities rather than hard class labels. By varying the **threshold** for classifying as positive, we trade off sensitivity against specificity.

The **Receiver Operating Characteristic (ROC) curve** plots sensitivity (true positive rate) against 1 - specificity (false positive rate) at all possible thresholds.

```{r}
#| label: fig-roc-curve
#| fig-cap: "ROC curve showing the tradeoff between sensitivity and false positive rate; the dashed diagonal represents random guessing"
#| fig-width: 7
#| fig-height: 6
# Simulate a classifier with probabilities
set.seed(42)
n <- 500
actual <- factor(c(rep(1, 100), rep(0, 400)))  # 20% positive

# Generate predicted probabilities (imperfect classifier)
probs <- c(rbeta(100, 3, 2),   # Positives: higher probs
           rbeta(400, 2, 3))   # Negatives: lower probs

# Calculate ROC curve manually
thresholds <- seq(0, 1, by = 0.01)
roc_data <- data.frame(
  threshold = thresholds,
  TPR = sapply(thresholds, function(t) {
    pred <- ifelse(probs >= t, 1, 0)
    sum(pred == 1 & actual == 1) / sum(actual == 1)
  }),
  FPR = sapply(thresholds, function(t) {
    pred <- ifelse(probs >= t, 1, 0)
    sum(pred == 1 & actual == 0) / sum(actual == 0)
  })
)

# Plot ROC curve
plot(roc_data$FPR, roc_data$TPR, type = "l", lwd = 2, col = "blue",
     xlab = "False Positive Rate (1 - Specificity)",
     ylab = "True Positive Rate (Sensitivity)",
     main = "ROC Curve")
abline(0, 1, lty = 2, col = "gray")  # Random classifier line

# Add points for specific thresholds
highlight <- c(0.3, 0.5, 0.7)
for (t in highlight) {
  idx <- which.min(abs(roc_data$threshold - t))
  points(roc_data$FPR[idx], roc_data$TPR[idx], pch = 19, cex = 1.5)
  text(roc_data$FPR[idx] + 0.05, roc_data$TPR[idx],
       paste("t =", t), cex = 0.8)
}
legend("bottomright", c("ROC Curve", "Random Classifier"),
       col = c("blue", "gray"), lty = c(1, 2), lwd = c(2, 1))
```

### Area Under the Curve (AUC)

The **AUC** (Area Under the ROC Curve) summarizes classifier performance in a single number:

- **AUC = 0.5**: No better than random guessing
- **AUC = 1.0**: Perfect classification
- **AUC > 0.8**: Generally considered good

```{r}
# Calculate AUC using trapezoidal rule
auc <- sum(diff(roc_data$FPR[order(roc_data$FPR)]) *
           (head(roc_data$TPR[order(roc_data$FPR)], -1) +
            tail(roc_data$TPR[order(roc_data$FPR)], -1)) / 2)
cat("AUC:", round(abs(auc), 3), "\n")
```

### Comparing Classifiers with ROC

ROC curves allow direct comparison of classifiers:

```{r}
#| label: fig-roc-comparison
#| fig-cap: "Comparing classifiers using ROC curves: higher curves (larger AUC) indicate better performance"
#| fig-width: 7
#| fig-height: 6
# Simulate three classifiers of varying quality
set.seed(42)

# Good classifier
probs_good <- c(rbeta(100, 4, 1.5), rbeta(400, 1.5, 4))

# Medium classifier (our original)
probs_medium <- probs

# Poor classifier
probs_poor <- c(rbeta(100, 2, 2), rbeta(400, 2, 2))

# Function to calculate ROC data
calc_roc <- function(probs, actual) {
  thresholds <- seq(0, 1, by = 0.01)
  data.frame(
    TPR = sapply(thresholds, function(t) {
      pred <- ifelse(probs >= t, 1, 0)
      sum(pred == 1 & actual == 1) / sum(actual == 1)
    }),
    FPR = sapply(thresholds, function(t) {
      pred <- ifelse(probs >= t, 1, 0)
      sum(pred == 1 & actual == 0) / sum(actual == 0)
    })
  )
}

roc_good <- calc_roc(probs_good, actual)
roc_medium <- calc_roc(probs_medium, actual)
roc_poor <- calc_roc(probs_poor, actual)

# Plot comparison
plot(roc_good$FPR, roc_good$TPR, type = "l", lwd = 2, col = "darkgreen",
     xlab = "False Positive Rate", ylab = "True Positive Rate",
     main = "ROC Curve Comparison")
lines(roc_medium$FPR, roc_medium$TPR, lwd = 2, col = "blue")
lines(roc_poor$FPR, roc_poor$TPR, lwd = 2, col = "red")
abline(0, 1, lty = 2, col = "gray")

legend("bottomright",
       c("Good (AUC ≈ 0.90)", "Medium (AUC ≈ 0.75)", "Poor (AUC ≈ 0.50)"),
       col = c("darkgreen", "blue", "red"), lwd = 2)
```

### Precision-Recall Curves

For highly imbalanced data, **precision-recall curves** can be more informative than ROC curves because they focus on the minority (positive) class:

```{r}
#| label: fig-pr-curve
#| fig-cap: "Precision-recall curve for imbalanced classification; the horizontal dashed line shows baseline precision (proportion of positives)"
#| fig-width: 7
#| fig-height: 5
# Calculate precision-recall curve
pr_data <- data.frame(
  threshold = thresholds,
  precision = sapply(thresholds, function(t) {
    pred <- ifelse(probs >= t, 1, 0)
    tp <- sum(pred == 1 & actual == 1)
    fp <- sum(pred == 1 & actual == 0)
    if (tp + fp == 0) return(NA)
    tp / (tp + fp)
  }),
  recall = sapply(thresholds, function(t) {
    pred <- ifelse(probs >= t, 1, 0)
    sum(pred == 1 & actual == 1) / sum(actual == 1)
  })
)

# Remove NA values
pr_data <- pr_data[!is.na(pr_data$precision), ]

plot(pr_data$recall, pr_data$precision, type = "l", lwd = 2, col = "purple",
     xlab = "Recall (Sensitivity)", ylab = "Precision",
     main = "Precision-Recall Curve", ylim = c(0, 1))
abline(h = mean(actual == 1), lty = 2, col = "gray")  # Baseline
legend("topright", c("PR Curve", "Baseline (random)"),
       col = c("purple", "gray"), lty = c(1, 2), lwd = c(2, 1))
```

## Decision Trees

Decision trees make predictions by asking a series of yes/no questions about the features:

```{r}
#| label: fig-decision-tree
#| fig-cap: "Decision tree for classifying iris species based on sepal measurements"
#| fig-width: 7
#| fig-height: 5
library(rpart)
library(rpart.plot)

# Build a simple decision tree
data(iris)
tree_model <- rpart(Species ~ Sepal.Length + Sepal.Width, data = iris)
rpart.plot(tree_model)
```

Trees are interpretable but prone to overfitting. Pruning (removing branches) or using ensembles helps.

## Random Forests

Random forests improve on single trees by:
1. Building many trees on bootstrap samples (bagging)
2. Using a random subset of features at each split
3. Averaging predictions across all trees

```{r}
library(randomForest)

rf_model <- randomForest(Species ~ ., data = iris, ntree = 100)
rf_model

# Variable importance
varImpPlot(rf_model)
```

## Practical Workflow

A typical statistical learning workflow:

1. **Split data** into training and test sets
2. **Explore** the training data
3. **Build candidate models** with different algorithms or parameters
4. **Evaluate** using cross-validation on training data
5. **Select** the best model
6. **Final evaluation** on held-out test data
7. **Report** honest estimates of performance

Never use test data for model building or selection—that defeats the purpose of holding it out.

## When to Use Statistical Learning

Statistical learning excels when:
- Prediction is the primary goal
- Relationships are complex or non-linear
- You have substantial data
- Interpretability is less critical

Traditional statistical methods may be preferable when:
- Understanding relationships matters more than prediction
- Sample sizes are small
- You need confidence intervals and hypothesis tests
- Interpretability is essential

## Connection to Dimensionality Reduction

High-dimensional data often benefit from dimensionality reduction before applying statistical learning methods. Techniques like PCA, clustering, and discriminant analysis are covered in detail in @sec-dimensionality-reduction.

## Summary

Statistical learning provides powerful tools for prediction and pattern discovery:

- Overfitting is the central challenge—models that fit training data too well predict poorly
- Cross-validation provides honest estimates of predictive performance
- The bias-variance tradeoff governs model complexity choices
- **Regularization** (ridge, lasso, elastic net) controls overfitting by penalizing model complexity
  - Ridge shrinks coefficients but keeps all predictors
  - Lasso performs variable selection by shrinking some coefficients to zero
  - Cross-validation selects the optimal regularization strength
- **Splines** provide flexible curve fitting with controlled smoothness
  - Regression splines use piecewise polynomials at fixed knots
  - Natural splines add boundary constraints for better extrapolation
  - Smoothing splines use penalties to control flexibility
- LOESS offers flexible non-parametric smoothing
- K-nearest neighbors illustrates how hyperparameters control model complexity
- Classification methods (kNN, decision trees, random forests) handle categorical outcomes
- Confusion matrices summarize classification performance with metrics like accuracy, sensitivity, and precision
- F1 score and balanced accuracy are better metrics for imbalanced data
- ROC curves and AUC allow comparison of classifiers across all thresholds
- Precision-recall curves are preferred for highly imbalanced problems
- The choice between traditional statistics and machine learning depends on goals

## Additional Resources

- @james2023islr - The standard introduction to statistical learning
- @thulin2025msr - Modern perspectives on statistics with R
- @crawley2007r - Practical statistical methods in R
