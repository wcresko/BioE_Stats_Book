# Statistical Learning {#sec-statistical-learning}

```{r}
#| echo: false
#| message: false
library(tidyverse)
theme_set(theme_minimal())
```

## From Inference to Prediction

Traditional statistics emphasizes inference—understanding relationships, testing hypotheses, and quantifying uncertainty. Statistical learning (or machine learning) shifts focus toward prediction—building models that accurately predict outcomes for new data.

Both approaches use similar mathematical tools, but the goals differ. In inference, we want to understand the true relationship between variables. In prediction, we want accurate predictions, even if the model does not perfectly capture the underlying mechanism.

## The Overfitting Problem

Models are built to fit training data as closely as possible. A linear regression minimizes squared errors; a logistic regression maximizes likelihood. But models that fit training data too well often predict poorly on new data.

**Overfitting** occurs when a model captures noise specific to the training data rather than the true underlying pattern. Complex models with many parameters are especially susceptible.

The solution is to evaluate models on data they have not seen—held-out test data or through cross-validation.

## Cross-Validation

Cross-validation estimates how well a model will generalize to new data.

**K-fold cross-validation**:
1. Split data into k roughly equal parts (folds)
2. For each fold: train on k-1 folds, test on the held-out fold
3. Average performance across all folds

```{r}
#| fig-width: 7
#| fig-height: 5
# Simple CV example with linear regression
library(boot)

# Generate data
set.seed(42)
x <- rnorm(100)
y <- 2 + 3*x + rnorm(100)
data <- data.frame(x, y)

# Fit model and perform CV
model <- glm(y ~ x, data = data)

# 10-fold cross-validation
cv_result <- cv.glm(data, model, K = 10)
cat("CV estimate of prediction error:", round(cv_result$delta[1], 3), "\n")
```

**Leave-one-out cross-validation (LOOCV)** is k-fold with k = n: each observation is held out once. More computationally expensive but lower variance.

## Bias-Variance Tradeoff

Prediction error has two components:

**Bias**: Error from approximating a complex reality with a simpler model. Simple models have high bias—they may miss important patterns.

**Variance**: Error from sensitivity to training data. Complex models have high variance—they change substantially with different training samples.

The best predictions come from models that balance bias and variance. As model complexity increases, bias decreases but variance increases. The optimal complexity minimizes total prediction error.

## LOESS: Flexible Non-Parametric Smoothing

**LOESS** (Locally Estimated Scatterplot Smoothing) fits local regressions to subsets of data, weighted by distance from each point.

```{r}
#| fig-width: 7
#| fig-height: 5
# Compare linear regression and LOESS
set.seed(123)
x <- seq(0, 4*pi, length.out = 100)
y <- sin(x) + rnorm(100, sd = 0.3)

plot(x, y, pch = 16, col = "gray60", main = "Linear vs LOESS")
abline(lm(y ~ x), col = "red", lwd = 2)
lines(x, predict(loess(y ~ x, span = 0.3)), col = "blue", lwd = 2)
legend("topright", c("Linear", "LOESS"), col = c("red", "blue"), lwd = 2)
```

The **span** parameter controls smoothness: smaller values fit more locally (more flexible), larger values average more broadly (smoother).

## Classification

When the response is categorical, we have a classification problem rather than regression. The goal is to predict which category an observation belongs to.

**Logistic regression** produces probabilities that can be converted to class predictions.

**Decision trees** recursively partition the feature space based on simple rules.

**Random forests** combine many decision trees for more robust predictions.

## Confusion Matrices

Classification performance is evaluated with a **confusion matrix**:

|  | Predicted Positive | Predicted Negative |
|:--|:--:|:--:|
| Actual Positive | True Positive (TP) | False Negative (FN) |
| Actual Negative | False Positive (FP) | True Negative (TN) |

Key metrics:
- **Accuracy**: (TP + TN) / Total
- **Sensitivity** (Recall): TP / (TP + FN) — how many positives were caught
- **Specificity**: TN / (TN + FP) — how many negatives were correctly identified
- **Precision**: TP / (TP + FP) — among positive predictions, how many were correct

## Decision Trees

Decision trees make predictions by asking a series of yes/no questions about the features:

```{r}
#| fig-width: 7
#| fig-height: 5
library(rpart)
library(rpart.plot)

# Build a simple decision tree
data(iris)
tree_model <- rpart(Species ~ Sepal.Length + Sepal.Width, data = iris)
rpart.plot(tree_model)
```

Trees are interpretable but prone to overfitting. Pruning (removing branches) or using ensembles helps.

## Random Forests

Random forests improve on single trees by:
1. Building many trees on bootstrap samples (bagging)
2. Using a random subset of features at each split
3. Averaging predictions across all trees

```{r}
library(randomForest)

rf_model <- randomForest(Species ~ ., data = iris, ntree = 100)
rf_model

# Variable importance
varImpPlot(rf_model)
```

## Practical Workflow

A typical statistical learning workflow:

1. **Split data** into training and test sets
2. **Explore** the training data
3. **Build candidate models** with different algorithms or parameters
4. **Evaluate** using cross-validation on training data
5. **Select** the best model
6. **Final evaluation** on held-out test data
7. **Report** honest estimates of performance

Never use test data for model building or selection—that defeats the purpose of holding it out.

## When to Use Statistical Learning

Statistical learning excels when:
- Prediction is the primary goal
- Relationships are complex or non-linear
- You have substantial data
- Interpretability is less critical

Traditional statistical methods may be preferable when:
- Understanding relationships matters more than prediction
- Sample sizes are small
- You need confidence intervals and hypothesis tests
- Interpretability is essential

## Dimensionality Reduction

When datasets have many variables, visualization and analysis become difficult. **Dimensionality reduction** creates a smaller set of new variables that capture most of the information in the original data.

### Principal Component Analysis (PCA)

**Principal Component Analysis (PCA)** is the most widely used dimensionality reduction technique. It finds new variables (principal components) that are linear combinations of the originals, chosen to capture maximum variance.

The key insight involves **eigenanalysis**: decomposing the covariance (or correlation) matrix of variables to find directions of maximum variation.

```{r}
#| fig-width: 8
#| fig-height: 6
# PCA on iris data
iris_pca <- prcomp(iris[, 1:4], scale. = TRUE)

# Variance explained
summary(iris_pca)

# Scree plot
par(mfrow = c(1, 2))
plot(iris_pca, type = "l", main = "Scree Plot")

# PC scores colored by species
plot(iris_pca$x[, 1:2],
     col = c("red", "green", "blue")[iris$Species],
     pch = 19, xlab = "PC1", ylab = "PC2",
     main = "PCA of Iris Data")
legend("topright", levels(iris$Species),
       col = c("red", "green", "blue"), pch = 19)
```

Each principal component is defined by its **loadings**—the coefficients showing how much each original variable contributes:

```{r}
# Loadings (rotation matrix)
iris_pca$rotation
```

### Interpreting PCA Results

Key elements of PCA output:

- **Eigenvalues**: Variance explained by each component (shown in scree plot)
- **Proportion of variance**: How much of total variance each PC captures
- **Loadings**: Coefficients relating original variables to PCs
- **Scores**: Values of the new variables for each observation

The first few PCs often capture most of the meaningful variation, allowing you to reduce many variables to just 2-3 for visualization and analysis.

::: {.callout-tip}
## How Many Components to Keep?

Common approaches:
- Keep components with eigenvalues > 1 (Kaiser criterion)
- Keep enough to explain 80-90% of variance
- Look for an "elbow" in the scree plot
- Use cross-validation if using PCs for prediction
:::

### Principal Coordinate Analysis (PCoA)

While PCA uses correlations among variables, **Principal Coordinate Analysis (PCoA)** starts with a dissimilarity matrix among observations. This is valuable when:

- You have a meaningful distance metric (e.g., genetic distances)
- Variables are mixed types or non-numeric
- The data are counts (e.g., microbiome data)

```{r}
#| fig-width: 7
#| fig-height: 5
# PCoA example using Euclidean distances
dist_matrix <- dist(iris[, 1:4])
pcoa_result <- cmdscale(dist_matrix, k = 2, eig = TRUE)

# Plot
plot(pcoa_result$points,
     col = c("red", "green", "blue")[iris$Species],
     pch = 19, xlab = "PCoA1", ylab = "PCoA2",
     main = "PCoA of Iris Data")
```

### Cluster Analysis

**Cluster analysis** groups observations based on their similarity. Unlike PCA, which creates new continuous variables, clustering assigns observations to discrete groups.

**Hierarchical clustering** builds a tree (dendrogram) of nested clusters:

```{r}
#| fig-width: 8
#| fig-height: 5
# Hierarchical clustering
iris_scaled <- scale(iris[, 1:4])
hc <- hclust(dist(iris_scaled), method = "complete")
plot(hc, labels = FALSE, main = "Hierarchical Clustering of Iris")
rect.hclust(hc, k = 3, border = "red")
```

**K-means clustering** partitions data into k groups by minimizing within-cluster variance:

```{r}
set.seed(42)
km <- kmeans(iris_scaled, centers = 3, nstart = 20)

# Compare to true species
table(km$cluster, iris$Species)
```

### Using PCA Scores in Further Analyses

PC scores are legitimate new variables that can be used in any downstream analysis:

- Regression of PCs on other continuous variables
- ANOVA comparing groups on PC scores
- Correlation of PCs with environmental gradients

This is valuable when you have many correlated variables and want to reduce dimensionality before hypothesis testing.

::: {.callout-warning}
## Metric vs. Non-Metric Methods

PCA and metric PCoA produce scores on a ratio scale—differences between scores are meaningful. These can be used directly in linear models.

Non-metric multidimensional scaling (NMDS) produces ordinal rankings only. NMDS scores should **not** be used in parametric analyses like ANOVA or regression.
:::
