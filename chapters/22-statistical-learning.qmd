# Core Concepts in Statistical Learning {#sec-statistical-learning}

```{r}
#| echo: false
#| message: false
library(tidyverse)
theme_set(theme_minimal())
```

## From Inference to Prediction

Traditional statistics emphasizes inference—understanding relationships, testing hypotheses, and quantifying uncertainty. Statistical learning (or machine learning) shifts focus toward prediction—building models that accurately predict outcomes for new data.

Both approaches use similar mathematical tools, but the goals differ. In inference, we want to understand the true relationship between variables. In prediction, we want accurate predictions, even if the model does not perfectly capture the underlying mechanism.

## The Overfitting Problem

Models are built to fit training data as closely as possible. A linear regression minimizes squared errors; a logistic regression maximizes likelihood. But models that fit training data too well often predict poorly on new data.

**Overfitting** occurs when a model captures noise specific to the training data rather than the true underlying pattern. Complex models with many parameters are especially susceptible.

The solution is to evaluate models on data they have not seen—held-out test data or through cross-validation.

## Cross-Validation

Cross-validation estimates how well a model will generalize to new data.

**K-fold cross-validation**:
1. Split data into k roughly equal parts (folds)
2. For each fold: train on k-1 folds, test on the held-out fold
3. Average performance across all folds

```{r}
#| label: fig-cross-validation
#| fig-cap: "Cross-validation example demonstrating prediction error estimation"
#| fig-width: 7
#| fig-height: 5
# Simple CV example with linear regression
library(boot)

# Generate data
set.seed(42)
x <- rnorm(100)
y <- 2 + 3*x + rnorm(100)
data <- data.frame(x, y)

# Fit model and perform CV
model <- glm(y ~ x, data = data)

# 10-fold cross-validation
cv_result <- cv.glm(data, model, K = 10)
cat("CV estimate of prediction error:", round(cv_result$delta[1], 3), "\n")
```

**Leave-one-out cross-validation (LOOCV)** is k-fold with k = n: each observation is held out once. More computationally expensive but lower variance.

## Bias-Variance Tradeoff

Prediction error has two components:

**Bias**: Error from approximating a complex reality with a simpler model. Simple models have high bias—they may miss important patterns.

**Variance**: Error from sensitivity to training data. Complex models have high variance—they change substantially with different training samples.

The best predictions come from models that balance bias and variance. As model complexity increases, bias decreases but variance increases. The optimal complexity minimizes total prediction error.

## Regularization: Controlling Model Complexity

**Regularization** addresses overfitting by adding a penalty term that discourages complex models. This is particularly important when you have many predictors relative to observations, or when predictors are correlated.

### The Regularization Idea

Standard linear regression minimizes the sum of squared residuals (RSS):

$$\text{RSS} = \sum_{i=1}^n (y_i - \beta_0 - \sum_{j=1}^p \beta_j x_{ij})^2$$

Regularized regression adds a penalty term $\lambda P(\beta)$ that shrinks coefficients toward zero:

$$\text{Minimize: } \text{RSS} + \lambda P(\beta)$$

The **regularization parameter** $\lambda$ controls the strength of the penalty:
- $\lambda = 0$: No penalty, equivalent to ordinary least squares
- $\lambda \to \infty$: Very strong penalty, coefficients shrink toward zero

### Ridge Regression (L2 Penalty)

**Ridge regression** uses the sum of squared coefficients as the penalty:

$$P(\beta) = \sum_{j=1}^p \beta_j^2$$

This shrinks all coefficients toward zero but never exactly to zero. Ridge is particularly effective when predictors are correlated (multicollinearity).

```{r}
#| label: fig-ridge-path
#| fig-cap: "Ridge regression coefficient paths: as lambda increases, coefficients shrink toward zero but never reach exactly zero"
#| fig-width: 8
#| fig-height: 5
library(glmnet)

# Generate sample data with correlated predictors
set.seed(42)
n <- 100
p <- 10
X <- matrix(rnorm(n * p), n, p)
# Create correlated predictors
X[, 2] <- X[, 1] + rnorm(n, sd = 0.5)
X[, 3] <- X[, 1] + rnorm(n, sd = 0.5)
true_beta <- c(3, -2, 1.5, rep(0, p - 3))
y <- X %*% true_beta + rnorm(n)

# Fit ridge regression across lambda values
ridge_fit <- glmnet(X, y, alpha = 0)  # alpha = 0 for ridge

# Plot coefficient paths
plot(ridge_fit, xvar = "lambda", main = "Ridge Regression Coefficients")
```

### Lasso Regression (L1 Penalty)

**Lasso** (Least Absolute Shrinkage and Selection Operator) uses the sum of absolute values as the penalty:

$$P(\beta) = \sum_{j=1}^p |\beta_j|$$

Unlike ridge, lasso can shrink coefficients exactly to zero, effectively performing **variable selection**. This produces sparse models that are easier to interpret.

```{r}
#| label: fig-lasso-path
#| fig-cap: "Lasso regression coefficient paths: as lambda increases, coefficients shrink and some become exactly zero (variable selection)"
#| fig-width: 8
#| fig-height: 5
# Fit lasso regression
lasso_fit <- glmnet(X, y, alpha = 1)  # alpha = 1 for lasso

plot(lasso_fit, xvar = "lambda", main = "Lasso Regression Coefficients")
```

### Elastic Net: Combining Ridge and Lasso

**Elastic net** combines both penalties:

$$P(\beta) = \alpha \sum_{j=1}^p |\beta_j| + (1-\alpha) \sum_{j=1}^p \beta_j^2$$

The mixing parameter $\alpha$ controls the balance:
- $\alpha = 0$: Pure ridge
- $\alpha = 1$: Pure lasso
- $0 < \alpha < 1$: Combination

Elastic net is often preferred when predictors are correlated—it tends to select groups of correlated variables together.

### Choosing Lambda with Cross-Validation

The regularization parameter $\lambda$ is typically chosen by cross-validation:

```{r}
#| label: fig-cv-lambda
#| fig-cap: "Cross-validation to select optimal lambda: the left dashed line marks the minimum error, the right marks the most regularized model within one standard error"
#| fig-width: 8
#| fig-height: 5
# Cross-validation for lasso
set.seed(123)
cv_lasso <- cv.glmnet(X, y, alpha = 1)

# Plot cross-validation results
plot(cv_lasso)

# Optimal lambda values
cat("Lambda with minimum CV error:", round(cv_lasso$lambda.min, 4), "\n")
cat("Lambda within 1 SE of minimum:", round(cv_lasso$lambda.1se, 4), "\n")
```

The `lambda.1se` (one standard error rule) often provides a more parsimonious model with nearly as good performance as the minimum.

### Comparing Regularization Methods

```{r}
# Fit models with optimal lambda
ridge_cv <- cv.glmnet(X, y, alpha = 0)
lasso_cv <- cv.glmnet(X, y, alpha = 1)

# Extract coefficients
coef_ols <- coef(lm(y ~ X))
coef_ridge <- coef(ridge_cv, s = "lambda.1se")
coef_lasso <- coef(lasso_cv, s = "lambda.1se")

# Compare (excluding intercept)
comparison <- data.frame(
  True = c(NA, true_beta),
  OLS = as.vector(coef_ols),
  Ridge = as.vector(coef_ridge),
  Lasso = as.vector(coef_lasso)
)
rownames(comparison) <- c("Intercept", paste0("X", 1:p))
round(comparison, 3)
```

Notice that lasso correctly identifies the zero coefficients (variables 4-10), while ridge shrinks them but doesn't eliminate them.

::: {.callout-tip}
## When to Use Each Method

- **Ridge**: When you believe all predictors are relevant and want to handle multicollinearity
- **Lasso**: When you want automatic variable selection and a sparse model
- **Elastic Net**: When predictors are correlated and you want both selection and grouping

**Important**: Always standardize predictors before applying regularization, as the penalty treats all coefficients equally. The `glmnet` function does this automatically by default.
:::

## Splines: Flexible Curve Fitting

While LOESS provides local smoothing, **splines** offer a more structured approach to fitting flexible curves. A spline is a piecewise polynomial function that joins smoothly at points called **knots**.

### Why Splines?

Linear regression assumes a straight-line relationship, which is often too restrictive. We could fit polynomial regression (e.g., $y = \beta_0 + \beta_1 x + \beta_2 x^2 + \beta_3 x^3$), but polynomials can behave erratically, especially at the edges of the data.

Splines provide flexibility while maintaining smooth, well-behaved curves.

### Regression Splines

**Regression splines** fit piecewise polynomials at fixed knot locations. The `splines` package provides basis functions for incorporating splines into linear models:

```{r}
#| label: fig-spline-comparison
#| fig-cap: "Comparison of linear, polynomial, and spline fits for non-linear data"
#| fig-width: 9
#| fig-height: 4
library(splines)

# Generate non-linear data
set.seed(42)
x <- seq(0, 10, length.out = 100)
y <- sin(x) + 0.5 * cos(2*x) + rnorm(100, sd = 0.3)
data <- data.frame(x, y)

# Fit different models
fit_linear <- lm(y ~ x, data = data)
fit_poly <- lm(y ~ poly(x, 5), data = data)
fit_spline <- lm(y ~ bs(x, df = 6), data = data)  # B-spline with 6 df

# Predictions
data$pred_linear <- predict(fit_linear)
data$pred_poly <- predict(fit_poly)
data$pred_spline <- predict(fit_spline)

par(mfrow = c(1, 3))

plot(x, y, pch = 16, col = "gray60", main = "Linear")
lines(x, data$pred_linear, col = "red", lwd = 2)

plot(x, y, pch = 16, col = "gray60", main = "Polynomial (degree 5)")
lines(x, data$pred_poly, col = "blue", lwd = 2)

plot(x, y, pch = 16, col = "gray60", main = "Spline (6 df)")
lines(x, data$pred_spline, col = "darkgreen", lwd = 2)
```

### Natural Splines

**Natural splines** add the constraint that the function is linear beyond the boundary knots. This prevents the wild behavior that polynomials often exhibit at the edges:

```{r}
#| label: fig-natural-spline
#| fig-cap: "Natural splines constrain the fit to be linear beyond the data boundaries, reducing edge effects"
#| fig-width: 7
#| fig-height: 5
# Compare B-spline and natural spline
fit_bs <- lm(y ~ bs(x, df = 6), data = data)
fit_ns <- lm(y ~ ns(x, df = 6), data = data)

# Extend prediction range to see edge behavior
x_ext <- seq(-2, 12, length.out = 200)
pred_bs <- predict(fit_bs, newdata = data.frame(x = x_ext))
pred_ns <- predict(fit_ns, newdata = data.frame(x = x_ext))

plot(x, y, pch = 16, col = "gray60", xlim = c(-2, 12), ylim = c(-3, 3),
     main = "B-spline vs Natural Spline at Boundaries")
lines(x_ext, pred_bs, col = "blue", lwd = 2)
lines(x_ext, pred_ns, col = "darkgreen", lwd = 2)
abline(v = range(x), lty = 2, col = "gray")
legend("topright", c("B-spline", "Natural spline", "Data range"),
       col = c("blue", "darkgreen", "gray"), lty = c(1, 1, 2), lwd = c(2, 2, 1))
```

### Smoothing Splines

**Smoothing splines** take a different approach: instead of pre-specifying knots, they place a knot at every data point and control smoothness through a penalty on the second derivative:

$$\text{Minimize: } \sum_{i=1}^n (y_i - f(x_i))^2 + \lambda \int f''(x)^2 dx$$

The smoothing parameter $\lambda$ is typically chosen by cross-validation:

```{r}
#| label: fig-smoothing-spline
#| fig-cap: "Smoothing spline with automatic cross-validation selection of the smoothing parameter"
#| fig-width: 7
#| fig-height: 5
# Fit smoothing spline with cross-validation
smooth_fit <- smooth.spline(x, y, cv = TRUE)

plot(x, y, pch = 16, col = "gray60", main = "Smoothing Spline")
lines(smooth_fit, col = "purple", lwd = 2)
cat("Optimal degrees of freedom:", round(smooth_fit$df, 2), "\n")
```

::: {.callout-note}
## Choosing the Right Approach

- **Regression splines (bs, ns)**: When you want to include splines in a regression model with other predictors
- **Natural splines**: When extrapolation behavior matters
- **Smoothing splines**: For exploratory smoothing with automatic tuning
- **LOESS**: For local, non-parametric smoothing (especially useful for visualization)
:::

## LOESS: Flexible Non-Parametric Smoothing

**LOESS** (Locally Estimated Scatterplot Smoothing) fits local regressions to subsets of data, weighted by distance from each point.

```{r}
#| label: fig-loess-comparison
#| fig-cap: "Comparison of linear regression and LOESS smoothing for non-linear data"
#| fig-width: 7
#| fig-height: 5
# Compare linear regression and LOESS
set.seed(123)
x <- seq(0, 4*pi, length.out = 100)
y <- sin(x) + rnorm(100, sd = 0.3)

plot(x, y, pch = 16, col = "gray60", main = "Linear vs LOESS")
abline(lm(y ~ x), col = "red", lwd = 2)
lines(x, predict(loess(y ~ x, span = 0.3)), col = "blue", lwd = 2)
legend("topright", c("Linear", "LOESS"), col = c("red", "blue"), lwd = 2)
```

The **span** parameter controls smoothness: smaller values fit more locally (more flexible), larger values average more broadly (smoother).

## Classification

When the response is categorical, we have a classification problem rather than regression. The goal is to predict which category an observation belongs to.

**Logistic regression** produces probabilities that can be converted to class predictions.

**Decision trees** recursively partition the feature space based on simple rules.

**Random forests** combine many decision trees for more robust predictions.

## K-Nearest Neighbors

**K-nearest neighbors (kNN)** is one of the simplest and most intuitive classification algorithms. To classify a new observation, kNN finds the k closest observations in the training data and assigns the most common class among those neighbors.

```{r}
#| label: fig-knn-concept
#| fig-cap: "K-nearest neighbors classification: the new point (star) is classified based on its nearest neighbors"
#| fig-width: 8
#| fig-height: 5
# Simulate two-class data
set.seed(42)
n <- 100
class1 <- data.frame(
  x1 = rnorm(n/2, mean = 2, sd = 1),
  x2 = rnorm(n/2, mean = 2, sd = 1),
  class = "A"
)
class2 <- data.frame(
  x1 = rnorm(n/2, mean = 4, sd = 1),
  x2 = rnorm(n/2, mean = 4, sd = 1),
  class = "B"
)
train_data <- rbind(class1, class2)

# New point to classify
new_point <- data.frame(x1 = 3.2, x2 = 3.5)

# Plot
plot(train_data$x1, train_data$x2,
     col = ifelse(train_data$class == "A", "blue", "red"),
     pch = 16, xlab = "Feature 1", ylab = "Feature 2",
     main = "K-Nearest Neighbors (k=5)")
points(new_point$x1, new_point$x2, pch = 8, cex = 2, lwd = 2)

# Find 5 nearest neighbors
distances <- sqrt((train_data$x1 - new_point$x1)^2 +
                  (train_data$x2 - new_point$x2)^2)
nearest <- order(distances)[1:5]

# Draw circles around nearest neighbors
points(train_data$x1[nearest], train_data$x2[nearest],
       cex = 2, col = ifelse(train_data$class[nearest] == "A", "blue", "red"))
legend("topleft", c("Class A", "Class B", "New point"),
       col = c("blue", "red", "black"), pch = c(16, 16, 8))
```

The choice of **k** is critical and illustrates the bias-variance tradeoff:

- **Small k** (e.g., k=1): Very flexible, low bias but high variance. The decision boundary is jagged and sensitive to individual training points—prone to overfitting.
- **Large k**: Smoother decision boundary, higher bias but lower variance. May miss local patterns—prone to underfitting.

```{r}
#| label: fig-knn-k-comparison
#| fig-cap: "Effect of k on kNN classification: small k creates complex boundaries (potential overfitting), large k creates smooth boundaries (potential underfitting)"
#| fig-width: 9
#| fig-height: 4
library(class)

# Create a grid for visualization
x1_grid <- seq(0, 6, length.out = 100)
x2_grid <- seq(0, 6, length.out = 100)
grid <- expand.grid(x1 = x1_grid, x2 = x2_grid)

par(mfrow = c(1, 3))

for (k_val in c(1, 15, 50)) {
  # Predict on grid
  pred <- knn(train = train_data[, 1:2],
              test = grid,
              cl = train_data$class,
              k = k_val)

  # Plot decision regions
  plot(grid$x1, grid$x2, col = ifelse(pred == "A",
       rgb(0, 0, 1, 0.1), rgb(1, 0, 0, 0.1)),
       pch = 15, cex = 0.5, xlab = "Feature 1", ylab = "Feature 2",
       main = paste("k =", k_val))
  points(train_data$x1, train_data$x2,
         col = ifelse(train_data$class == "A", "blue", "red"), pch = 16)
}
```

### Selecting k with Cross-Validation

We choose k by evaluating classification accuracy across different values using cross-validation:

```{r}
#| label: fig-knn-cv
#| fig-cap: "Cross-validation accuracy for different values of k: accuracy on training data decreases with k, but test accuracy peaks at intermediate values"
#| fig-width: 7
#| fig-height: 5
# Evaluate different k values
set.seed(123)
k_values <- seq(1, 50, by = 2)

# Simple holdout validation
test_idx <- sample(1:nrow(train_data), 30)
train_subset <- train_data[-test_idx, ]
test_subset <- train_data[test_idx, ]

accuracy <- sapply(k_values, function(k) {
  pred <- knn(train = train_subset[, 1:2],
              test = test_subset[, 1:2],
              cl = train_subset$class,
              k = k)
  mean(pred == test_subset$class)
})

train_accuracy <- sapply(k_values, function(k) {
  pred <- knn(train = train_subset[, 1:2],
              test = train_subset[, 1:2],
              cl = train_subset$class,
              k = k)
  mean(pred == train_subset$class)
})

plot(k_values, train_accuracy, type = "l", col = "blue", lwd = 2,
     xlab = "k (number of neighbors)", ylab = "Accuracy",
     main = "Training vs Test Accuracy", ylim = c(0.5, 1))
lines(k_values, accuracy, col = "red", lwd = 2)
legend("bottomright", c("Training", "Test"), col = c("blue", "red"), lwd = 2)
```

Notice that training accuracy is perfect (1.0) when k=1—each point is its own nearest neighbor. But test accuracy tells the true story of generalization performance.

## Confusion Matrices

Classification performance is evaluated with a **confusion matrix**:

|  | Predicted Positive | Predicted Negative |
|:--|:--:|:--:|
| Actual Positive | True Positive (TP) | False Negative (FN) |
| Actual Negative | False Positive (FP) | True Negative (TN) |

Key metrics:
- **Accuracy**: (TP + TN) / Total
- **Sensitivity** (Recall): TP / (TP + FN) — how many positives were caught
- **Specificity**: TN / (TN + FP) — how many negatives were correctly identified
- **Precision**: TP / (TP + FP) — among positive predictions, how many were correct

### The Problem with Accuracy

Accuracy can be misleading with **imbalanced classes**. If 95% of emails are legitimate, a classifier that labels everything as "not spam" achieves 95% accuracy while being completely useless for its intended purpose.

```{r}
# Imbalanced class example
set.seed(42)
# 95% negative, 5% positive (e.g., rare disease screening)
n <- 1000
actual <- factor(c(rep("Negative", 950), rep("Positive", 50)))

# Naive classifier: always predict negative
naive_pred <- factor(rep("Negative", n), levels = c("Negative", "Positive"))

# Calculate metrics
TP <- sum(naive_pred == "Positive" & actual == "Positive")
TN <- sum(naive_pred == "Negative" & actual == "Negative")
FP <- sum(naive_pred == "Positive" & actual == "Negative")
FN <- sum(naive_pred == "Negative" & actual == "Positive")

cat("Accuracy:", (TP + TN) / n, "\n")
cat("Sensitivity (Recall):", TP / (TP + FN), "\n")
cat("The classifier catches 0% of positive cases!\n")
```

### F1 Score and Balanced Accuracy

For imbalanced data, better metrics include:

**F1 Score**: The harmonic mean of precision and recall, balancing both concerns:

$$F_1 = 2 \cdot \frac{\text{Precision} \cdot \text{Recall}}{\text{Precision} + \text{Recall}} = \frac{2 \cdot TP}{2 \cdot TP + FP + FN}$$

**Balanced Accuracy**: The average of sensitivity and specificity:

$$\text{Balanced Accuracy} = \frac{\text{Sensitivity} + \text{Specificity}}{2}$$

```{r}
# Better classifier for the imbalanced data
set.seed(123)
# Suppose we have a model that catches 80% of positives but has some false positives
better_pred <- actual  # Start with actual
# Correctly identify 80% of positives
pos_idx <- which(actual == "Positive")
neg_idx <- which(actual == "Negative")
better_pred[sample(pos_idx, 10)] <- "Negative"  # Miss 10 of 50 positives (20%)
better_pred[sample(neg_idx, 50)] <- "Positive"  # 50 false positives

# Confusion matrix
TP <- sum(better_pred == "Positive" & actual == "Positive")
TN <- sum(better_pred == "Negative" & actual == "Negative")
FP <- sum(better_pred == "Positive" & actual == "Negative")
FN <- sum(better_pred == "Negative" & actual == "Positive")

precision <- TP / (TP + FP)
recall <- TP / (TP + FN)  # Sensitivity
specificity <- TN / (TN + FP)

# Calculate metrics
accuracy <- (TP + TN) / n
f1 <- 2 * precision * recall / (precision + recall)
balanced_acc <- (recall + specificity) / 2

cat("Accuracy:", round(accuracy, 3), "\n")
cat("Precision:", round(precision, 3), "\n")
cat("Recall (Sensitivity):", round(recall, 3), "\n")
cat("F1 Score:", round(f1, 3), "\n")
cat("Balanced Accuracy:", round(balanced_acc, 3), "\n")
```

::: {.callout-tip}
## Which Metric to Use?

- **Accuracy**: Only when classes are balanced
- **F1 Score**: When you care about both precision and recall equally
- **Sensitivity/Recall**: When missing positives is costly (disease screening)
- **Precision**: When false positives are costly (spam filtering)
- **Balanced Accuracy**: Quick summary for imbalanced data
:::

## ROC Curves and AUC

Many classifiers output probabilities rather than hard class labels. By varying the **threshold** for classifying as positive, we trade off sensitivity against specificity.

The **Receiver Operating Characteristic (ROC) curve** plots sensitivity (true positive rate) against 1 - specificity (false positive rate) at all possible thresholds.

```{r}
#| label: fig-roc-curve
#| fig-cap: "ROC curve showing the tradeoff between sensitivity and false positive rate; the dashed diagonal represents random guessing"
#| fig-width: 7
#| fig-height: 6
# Simulate a classifier with probabilities
set.seed(42)
n <- 500
actual <- factor(c(rep(1, 100), rep(0, 400)))  # 20% positive

# Generate predicted probabilities (imperfect classifier)
probs <- c(rbeta(100, 3, 2),   # Positives: higher probs
           rbeta(400, 2, 3))   # Negatives: lower probs

# Calculate ROC curve manually
thresholds <- seq(0, 1, by = 0.01)
roc_data <- data.frame(
  threshold = thresholds,
  TPR = sapply(thresholds, function(t) {
    pred <- ifelse(probs >= t, 1, 0)
    sum(pred == 1 & actual == 1) / sum(actual == 1)
  }),
  FPR = sapply(thresholds, function(t) {
    pred <- ifelse(probs >= t, 1, 0)
    sum(pred == 1 & actual == 0) / sum(actual == 0)
  })
)

# Plot ROC curve
plot(roc_data$FPR, roc_data$TPR, type = "l", lwd = 2, col = "blue",
     xlab = "False Positive Rate (1 - Specificity)",
     ylab = "True Positive Rate (Sensitivity)",
     main = "ROC Curve")
abline(0, 1, lty = 2, col = "gray")  # Random classifier line

# Add points for specific thresholds
highlight <- c(0.3, 0.5, 0.7)
for (t in highlight) {
  idx <- which.min(abs(roc_data$threshold - t))
  points(roc_data$FPR[idx], roc_data$TPR[idx], pch = 19, cex = 1.5)
  text(roc_data$FPR[idx] + 0.05, roc_data$TPR[idx],
       paste("t =", t), cex = 0.8)
}
legend("bottomright", c("ROC Curve", "Random Classifier"),
       col = c("blue", "gray"), lty = c(1, 2), lwd = c(2, 1))
```

### Area Under the Curve (AUC)

The **AUC** (Area Under the ROC Curve) summarizes classifier performance in a single number:

- **AUC = 0.5**: No better than random guessing
- **AUC = 1.0**: Perfect classification
- **AUC > 0.8**: Generally considered good

```{r}
# Calculate AUC using trapezoidal rule
auc <- sum(diff(roc_data$FPR[order(roc_data$FPR)]) *
           (head(roc_data$TPR[order(roc_data$FPR)], -1) +
            tail(roc_data$TPR[order(roc_data$FPR)], -1)) / 2)
cat("AUC:", round(abs(auc), 3), "\n")
```

### Comparing Classifiers with ROC

ROC curves allow direct comparison of classifiers:

```{r}
#| label: fig-roc-comparison
#| fig-cap: "Comparing classifiers using ROC curves: higher curves (larger AUC) indicate better performance"
#| fig-width: 7
#| fig-height: 6
# Simulate three classifiers of varying quality
set.seed(42)

# Good classifier
probs_good <- c(rbeta(100, 4, 1.5), rbeta(400, 1.5, 4))

# Medium classifier (our original)
probs_medium <- probs

# Poor classifier
probs_poor <- c(rbeta(100, 2, 2), rbeta(400, 2, 2))

# Function to calculate ROC data
calc_roc <- function(probs, actual) {
  thresholds <- seq(0, 1, by = 0.01)
  data.frame(
    TPR = sapply(thresholds, function(t) {
      pred <- ifelse(probs >= t, 1, 0)
      sum(pred == 1 & actual == 1) / sum(actual == 1)
    }),
    FPR = sapply(thresholds, function(t) {
      pred <- ifelse(probs >= t, 1, 0)
      sum(pred == 1 & actual == 0) / sum(actual == 0)
    })
  )
}

roc_good <- calc_roc(probs_good, actual)
roc_medium <- calc_roc(probs_medium, actual)
roc_poor <- calc_roc(probs_poor, actual)

# Plot comparison
plot(roc_good$FPR, roc_good$TPR, type = "l", lwd = 2, col = "darkgreen",
     xlab = "False Positive Rate", ylab = "True Positive Rate",
     main = "ROC Curve Comparison")
lines(roc_medium$FPR, roc_medium$TPR, lwd = 2, col = "blue")
lines(roc_poor$FPR, roc_poor$TPR, lwd = 2, col = "red")
abline(0, 1, lty = 2, col = "gray")

legend("bottomright",
       c("Good (AUC ≈ 0.90)", "Medium (AUC ≈ 0.75)", "Poor (AUC ≈ 0.50)"),
       col = c("darkgreen", "blue", "red"), lwd = 2)
```

### Precision-Recall Curves

For highly imbalanced data, **precision-recall curves** can be more informative than ROC curves because they focus on the minority (positive) class:

```{r}
#| label: fig-pr-curve
#| fig-cap: "Precision-recall curve for imbalanced classification; the horizontal dashed line shows baseline precision (proportion of positives)"
#| fig-width: 7
#| fig-height: 5
# Calculate precision-recall curve
pr_data <- data.frame(
  threshold = thresholds,
  precision = sapply(thresholds, function(t) {
    pred <- ifelse(probs >= t, 1, 0)
    tp <- sum(pred == 1 & actual == 1)
    fp <- sum(pred == 1 & actual == 0)
    if (tp + fp == 0) return(NA)
    tp / (tp + fp)
  }),
  recall = sapply(thresholds, function(t) {
    pred <- ifelse(probs >= t, 1, 0)
    sum(pred == 1 & actual == 1) / sum(actual == 1)
  })
)

# Remove NA values
pr_data <- pr_data[!is.na(pr_data$precision), ]

plot(pr_data$recall, pr_data$precision, type = "l", lwd = 2, col = "purple",
     xlab = "Recall (Sensitivity)", ylab = "Precision",
     main = "Precision-Recall Curve", ylim = c(0, 1))
abline(h = mean(actual == 1), lty = 2, col = "gray")  # Baseline
legend("topright", c("PR Curve", "Baseline (random)"),
       col = c("purple", "gray"), lty = c(1, 2), lwd = c(2, 1))
```

## Decision Trees (CART)

**Classification and Regression Trees (CART)** make predictions by recursively partitioning the feature space into regions. At each node, the algorithm asks a yes/no question about a single feature, splitting observations into two groups. The process continues until a stopping criterion is met.

### How Trees Work

The key idea: find the split that best separates the data at each step.

For **classification**, CART typically minimizes the **Gini impurity**:

$$
G = \sum_{k=1}^{K} p_k (1 - p_k)
$$

where $p_k$ is the proportion of observations in class $k$. A pure node (all one class) has $G = 0$.

For **regression**, CART minimizes the **residual sum of squares (RSS)**:

$$
\text{RSS} = \sum_{i \in R_1} (y_i - \bar{y}_{R_1})^2 + \sum_{i \in R_2} (y_i - \bar{y}_{R_2})^2
$$

where $R_1$ and $R_2$ are the two regions created by the split.

```{r}
#| label: fig-cart-classification
#| fig-cap: "A CART decision tree for classifying iris species. Each node shows the predicted class, proportion of observations, and the splitting rule."
#| fig-width: 8
#| fig-height: 6
library(rpart)
library(rpart.plot)

# Build a classification tree
data(iris)
tree_class <- rpart(Species ~ ., data = iris, method = "class")

# Visualize with rpart.plot
rpart.plot(tree_class, extra = 104, box.palette = "RdYlGn",
           main = "Classification Tree for Iris Species")
```

### Interpreting Tree Output

The tree visualization shows:

- **Node prediction**: The predicted class (or value for regression)
- **Split rule**: The feature and threshold used to split
- **Proportions**: Distribution of classes at each node
- **Sample size**: Number of observations reaching each node

```{r}
# Detailed tree summary
summary(tree_class, cp = 0.1)
```

### Regression Trees

Trees can also predict continuous outcomes:

```{r}
#| label: fig-cart-regression
#| fig-cap: "A regression tree predicting car fuel efficiency (mpg) from weight and horsepower"
#| fig-width: 8
#| fig-height: 5
# Build a regression tree
tree_reg <- rpart(mpg ~ wt + hp + cyl, data = mtcars, method = "anova")

rpart.plot(tree_reg, extra = 101, box.palette = "Blues",
           main = "Regression Tree for MPG")
```

### The Decision Boundary

Trees partition the feature space into rectangular regions:

```{r}
#| label: fig-tree-boundary
#| fig-cap: "Decision tree partition of the feature space. Each rectangular region is assigned to a class based on the majority vote of training points in that region."
#| fig-width: 8
#| fig-height: 6
# Visualize decision boundary for 2D case
tree_2d <- rpart(Species ~ Petal.Length + Petal.Width, data = iris, method = "class")

# Create grid for prediction
petal_length_seq <- seq(0, 7, length.out = 200)
petal_width_seq <- seq(0, 3, length.out = 200)
grid <- expand.grid(Petal.Length = petal_length_seq,
                    Petal.Width = petal_width_seq)
grid$pred <- predict(tree_2d, grid, type = "class")

# Plot
plot(grid$Petal.Length, grid$Petal.Width,
     col = c(rgb(1,0,0,0.1), rgb(0,1,0,0.1), rgb(0,0,1,0.1))[grid$pred],
     pch = 15, cex = 0.3,
     xlab = "Petal Length", ylab = "Petal Width",
     main = "Decision Tree Boundaries")
points(iris$Petal.Length, iris$Petal.Width,
       col = c("red", "green", "blue")[iris$Species],
       pch = 19, cex = 0.8)
legend("topleft", levels(iris$Species),
       col = c("red", "green", "blue"), pch = 19)
```

### Controlling Tree Complexity

Trees easily overfit—they can keep splitting until each leaf contains a single observation. Several parameters control complexity:

- **cp (complexity parameter)**: Minimum improvement required for a split
- **minsplit**: Minimum observations required to attempt a split
- **maxdepth**: Maximum depth of the tree

```{r}
#| label: fig-tree-complexity
#| fig-cap: "Effect of the complexity parameter on tree structure: smaller cp allows more splits and greater complexity"
#| fig-width: 9
#| fig-height: 4
par(mfrow = c(1, 3))

# Simple tree (high cp)
tree_simple <- rpart(Species ~ ., data = iris, cp = 0.1)
rpart.plot(tree_simple, main = "cp = 0.1 (simple)")

# Medium tree
tree_medium <- rpart(Species ~ ., data = iris, cp = 0.02)
rpart.plot(tree_medium, main = "cp = 0.02 (medium)")

# Complex tree (low cp)
tree_complex <- rpart(Species ~ ., data = iris, cp = 0.001)
rpart.plot(tree_complex, main = "cp = 0.001 (complex)")
```

### Pruning with Cross-Validation

The optimal complexity is typically chosen by cross-validation:

```{r}
#| label: fig-tree-cv
#| fig-cap: "Cross-validation error as a function of tree complexity. The dashed line shows one standard error above the minimum, often used to select a simpler tree."
#| fig-width: 7
#| fig-height: 5
# Fit full tree
full_tree <- rpart(Species ~ ., data = iris, cp = 0.001)

# Plot CV error vs complexity
plotcp(full_tree)

# Print CP table
printcp(full_tree)

# Prune to optimal cp
best_cp <- full_tree$cptable[which.min(full_tree$cptable[, "xerror"]), "CP"]
pruned_tree <- prune(full_tree, cp = best_cp)
```

::: {.callout-tip}
## Advantages and Disadvantages of Trees

**Advantages:**
- Highly interpretable—easy to explain to non-statisticians
- Handle both numeric and categorical predictors
- Capture non-linear relationships and interactions automatically
- Robust to outliers and don't require feature scaling

**Disadvantages:**
- High variance—small changes in data can produce very different trees
- Prone to overfitting without careful tuning
- Axis-aligned splits can't capture diagonal relationships efficiently
- Generally lower predictive accuracy than ensemble methods
:::

## Random Forests

**Random forests** address the high variance of individual trees by building many trees and averaging their predictions. The key innovations are **bagging** (bootstrap aggregating) and **random feature selection**.

### The Random Forest Algorithm

1. Draw $B$ bootstrap samples from the training data
2. For each sample, grow a tree with a twist: at each split, consider only a random subset of $m$ features (typically $m = \sqrt{p}$ for classification, $m = p/3$ for regression)
3. For prediction:
   - **Classification**: Vote across all trees (majority wins)
   - **Regression**: Average predictions across all trees

The randomness serves two purposes:
- **Bagging** reduces variance by averaging many noisy but unbiased trees
- **Random feature selection** decorrelates the trees, making the average more effective

### Random Forests in R

```{r}
#| label: fig-rf-model
#| fig-cap: "Random forest OOB error rate decreasing as more trees are added"
#| fig-width: 7
#| fig-height: 5
library(randomForest)
set.seed(42)

# Fit random forest
rf_model <- randomForest(Species ~ ., data = iris,
                          ntree = 500,       # Number of trees
                          mtry = 2,          # Features tried at each split
                          importance = TRUE)  # Calculate variable importance

# Model summary
print(rf_model)

# Plot error vs number of trees
plot(rf_model, main = "Random Forest: Error vs. Number of Trees")
legend("topright", colnames(rf_model$err.rate), col = 1:4, lty = 1:4)
```

### Out-of-Bag (OOB) Error

Each bootstrap sample uses about 63% of observations. The remaining 37% (out-of-bag samples) provide a built-in test set:

```{r}
# OOB confusion matrix
rf_model$confusion

# OOB error rate
cat("OOB Error Rate:", round(1 - sum(diag(rf_model$confusion[,1:3])) /
                              sum(rf_model$confusion[,1:3]), 3), "\n")
```

OOB error is nearly as accurate as cross-validation but comes "for free" during training.

### Variable Importance

Random forests provide measures of how important each predictor is:

```{r}
#| label: fig-rf-importance
#| fig-cap: "Variable importance from random forest: Mean Decrease Accuracy measures how much removing a variable hurts prediction; Mean Decrease Gini measures the total reduction in node impurity"
#| fig-width: 8
#| fig-height: 5
# Variable importance plot
varImpPlot(rf_model, main = "Variable Importance")

# Numeric importance values
importance(rf_model)
```

**Mean Decrease Accuracy**: For each tree, predictions are made on OOB samples. Then the values of variable $j$ are randomly permuted, and predictions are made again. The decrease in accuracy from permutation measures importance.

**Mean Decrease Gini**: Total decrease in Gini impurity from splits on variable $j$, averaged over all trees.

### Tuning Random Forests

Key parameters to tune:

- **ntree**: Number of trees (more is generally better, but with diminishing returns)
- **mtry**: Number of features considered at each split
- **nodesize**: Minimum size of terminal nodes

```{r}
#| label: fig-rf-tuning
#| fig-cap: "Random forest OOB error as a function of mtry (number of features considered at each split)"
#| fig-width: 7
#| fig-height: 5
# Tune mtry
oob_error <- sapply(1:4, function(m) {
  rf <- randomForest(Species ~ ., data = iris, mtry = m, ntree = 200)
  rf$err.rate[200, "OOB"]
})

plot(1:4, oob_error, type = "b", pch = 19,
     xlab = "mtry (features at each split)",
     ylab = "OOB Error Rate",
     main = "Tuning mtry Parameter")
```

### Random Forest for Regression

```{r}
# Regression random forest
set.seed(42)
rf_reg <- randomForest(mpg ~ ., data = mtcars, ntree = 500, importance = TRUE)

# Performance
cat("Variance explained:", round(rf_reg$rsq[500] * 100, 1), "%\n")
cat("MSE:", round(rf_reg$mse[500], 2), "\n")

# Variable importance for regression
varImpPlot(rf_reg, main = "Variable Importance for MPG Prediction")
```

## Support Vector Machines (SVM)

**Support Vector Machines** find the hyperplane that best separates classes by maximizing the margin—the distance between the boundary and the nearest points from each class.

### The Maximum Margin Classifier

For linearly separable data, infinitely many lines could separate the classes. SVM chooses the line with the largest margin:

```{r}
#| label: fig-svm-concept
#| fig-cap: "Support Vector Machine concept: the decision boundary (solid line) maximizes the margin (distance to nearest points). Support vectors are the points on the margin boundaries."
#| fig-width: 7
#| fig-height: 6
library(e1071)

# Create simple 2D data
set.seed(42)
n <- 40
x1 <- c(rnorm(n/2, mean = 0), rnorm(n/2, mean = 3))
x2 <- c(rnorm(n/2, mean = 0), rnorm(n/2, mean = 3))
y <- factor(c(rep(-1, n/2), rep(1, n/2)))
svm_data <- data.frame(x1, x2, y)

# Fit linear SVM
svm_linear <- svm(y ~ x1 + x2, data = svm_data, kernel = "linear",
                   cost = 10, scale = FALSE)

# Plot
plot(svm_linear, svm_data, x1 ~ x2,
     col = c("lightblue", "lightpink"),
     symbolPalette = c("blue", "red"),
     svSymbol = "x", dataSymbol = "o")
```

### Soft Margin and the Cost Parameter

Real data is rarely perfectly separable. **Soft margin** SVM allows some points to violate the margin, controlled by the cost parameter $C$:

- **High C**: Small margin, few violations (may overfit)
- **Low C**: Large margin, more violations (may underfit)

```{r}
#| label: fig-svm-cost
#| fig-cap: "Effect of cost parameter on SVM decision boundary: low cost allows more margin violations (smoother boundary), high cost enforces stricter separation"
#| fig-width: 9
#| fig-height: 4
par(mfrow = c(1, 3))

for (cost_val in c(0.1, 1, 100)) {
  svm_fit <- svm(y ~ x1 + x2, data = svm_data, kernel = "linear",
                  cost = cost_val, scale = FALSE)
  plot(svm_fit, svm_data, x1 ~ x2,
       col = c("lightblue", "lightpink"),
       main = paste("Cost =", cost_val))
}
```

### The Kernel Trick

For non-linear boundaries, SVM uses **kernels** to implicitly map data to higher dimensions where classes become linearly separable:

**Common kernels:**

- **Linear**: $K(x, x') = x \cdot x'$ (no transformation)
- **Polynomial**: $K(x, x') = (1 + x \cdot x')^d$
- **Radial Basis Function (RBF)**: $K(x, x') = \exp(-\gamma \|x - x'\|^2)$

```{r}
#| label: fig-svm-kernels
#| fig-cap: "Different SVM kernels produce different decision boundaries: linear kernels give straight lines, polynomial and RBF kernels can capture non-linear patterns"
#| fig-width: 9
#| fig-height: 4
# Create non-linear data
set.seed(123)
n <- 200
r1 <- runif(n/2, 0, 2)
theta1 <- runif(n/2, 0, 2*pi)
r2 <- runif(n/2, 3, 5)
theta2 <- runif(n/2, 0, 2*pi)

x1 <- c(r1 * cos(theta1), r2 * cos(theta2))
x2 <- c(r1 * sin(theta1), r2 * sin(theta2))
y <- factor(c(rep("inner", n/2), rep("outer", n/2)))
circle_data <- data.frame(x1, x2, y)

par(mfrow = c(1, 3))

# Linear (fails)
svm_lin <- svm(y ~ ., data = circle_data, kernel = "linear")
plot(svm_lin, circle_data, col = c("lightblue", "lightpink"),
     main = "Linear Kernel")

# Polynomial
svm_poly <- svm(y ~ ., data = circle_data, kernel = "polynomial", degree = 2)
plot(svm_poly, circle_data, col = c("lightblue", "lightpink"),
     main = "Polynomial Kernel (d=2)")

# RBF
svm_rbf <- svm(y ~ ., data = circle_data, kernel = "radial", gamma = 0.5)
plot(svm_rbf, circle_data, col = c("lightblue", "lightpink"),
     main = "RBF Kernel")
```

### Tuning SVM with Cross-Validation

The key parameters to tune are:
- **cost**: Penalty for margin violations
- **gamma**: For RBF kernel, controls the "reach" of each training example

```{r}
# Tune SVM using cross-validation
set.seed(42)
tune_result <- tune(svm, Species ~ ., data = iris,
                     kernel = "radial",
                     ranges = list(
                       cost = c(0.1, 1, 10, 100),
                       gamma = c(0.01, 0.1, 0.5, 1)
                     ))

# Best parameters
cat("Best parameters:\n")
print(tune_result$best.parameters)

# Best model performance
cat("\nBest model error:", round(tune_result$best.performance, 3), "\n")

# Use best model
best_svm <- tune_result$best.model
table(Predicted = predict(best_svm), Actual = iris$Species)
```

### Multiclass SVM

SVM is inherently binary, but extends to multiple classes via:

- **One-vs-One**: Fit $\binom{K}{2}$ classifiers for all pairs of classes; classify by voting
- **One-vs-All**: Fit $K$ classifiers (each class vs. rest); classify to highest-scoring class

R's `svm()` uses one-vs-one by default.

::: {.callout-note}
## SVM vs. Other Methods

**Advantages of SVM:**
- Effective in high-dimensional spaces (even when dimensions > samples)
- Memory efficient (uses only support vectors)
- Versatile through different kernels

**Disadvantages:**
- Doesn't provide probability estimates directly (though they can be computed)
- Sensitive to feature scaling—always standardize!
- Can be slow on very large datasets
- Kernel and parameter selection can be tricky
:::

## Comparing Classification Methods

Different methods have different strengths:

| Method | Interpretability | Handles Non-linearity | Speed | Best For |
|:-------|:-----------------|:----------------------|:------|:---------|
| kNN | Low | Yes (inherently) | Slow for large data | Simple problems, few features |
| Decision Tree | High | Yes | Fast | Interpretability needed |
| Random Forest | Medium | Yes | Moderate | General purpose, variable importance |
| SVM | Low | Yes (with kernels) | Moderate | High-dimensional data |
| Logistic Regression | High | No (needs feature engineering) | Fast | Probability estimates, inference |

## Practical Workflow

A typical statistical learning workflow:

1. **Split data** into training and test sets
2. **Explore** the training data
3. **Build candidate models** with different algorithms or parameters
4. **Evaluate** using cross-validation on training data
5. **Select** the best model
6. **Final evaluation** on held-out test data
7. **Report** honest estimates of performance

Never use test data for model building or selection—that defeats the purpose of holding it out.

## When to Use Statistical Learning

Statistical learning excels when:
- Prediction is the primary goal
- Relationships are complex or non-linear
- You have substantial data
- Interpretability is less critical

Traditional statistical methods may be preferable when:
- Understanding relationships matters more than prediction
- Sample sizes are small
- You need confidence intervals and hypothesis tests
- Interpretability is essential

## Connection to Dimensionality Reduction

High-dimensional data often benefit from dimensionality reduction before applying statistical learning methods. Techniques like PCA, clustering, and discriminant analysis are covered in detail in @sec-dimensionality-reduction.

## Summary

Statistical learning provides powerful tools for prediction and pattern discovery:

- **Overfitting** is the central challenge—models that fit training data too well predict poorly
- **Cross-validation** provides honest estimates of predictive performance
- The **bias-variance tradeoff** governs model complexity choices
- **Regularization** (ridge, lasso, elastic net) controls overfitting by penalizing model complexity
  - Ridge shrinks coefficients but keeps all predictors
  - Lasso performs variable selection by shrinking some coefficients to zero
  - Cross-validation selects the optimal regularization strength
- **Splines** provide flexible curve fitting with controlled smoothness
  - Regression splines use piecewise polynomials at fixed knots
  - Natural splines add boundary constraints for better extrapolation
  - Smoothing splines use penalties to control flexibility
- **LOESS** offers flexible non-parametric smoothing
- **K-nearest neighbors** illustrates how hyperparameters control model complexity
- **Decision trees (CART)** recursively partition data using simple rules
  - Highly interpretable but prone to overfitting
  - Controlled via complexity parameters and pruning
- **Random forests** combine many trees for robust predictions
  - Bagging and random feature selection reduce variance
  - Out-of-bag error provides built-in validation
  - Variable importance measures identify key predictors
- **Support vector machines** find maximum-margin decision boundaries
  - Kernel trick enables non-linear classification
  - Effective in high-dimensional spaces
- **Confusion matrices** summarize classification performance with metrics like accuracy, sensitivity, and precision
- **F1 score and balanced accuracy** are better metrics for imbalanced data
- **ROC curves and AUC** allow comparison of classifiers across all thresholds
- **Precision-recall curves** are preferred for highly imbalanced problems
- The choice between traditional statistics and machine learning depends on goals

## Additional Resources

- @james2023islr - The standard introduction to statistical learning
- @thulin2025msr - Modern perspectives on statistics with R
- @crawley2007r - Practical statistical methods in R
