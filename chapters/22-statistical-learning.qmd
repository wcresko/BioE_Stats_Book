# Statistical Learning {#sec-statistical-learning}

```{r}
#| echo: false
#| message: false
library(tidyverse)
theme_set(theme_minimal())
```

## From Inference to Prediction

Traditional statistics emphasizes inference—understanding relationships, testing hypotheses, and quantifying uncertainty. Statistical learning (or machine learning) shifts focus toward prediction—building models that accurately predict outcomes for new data.

Both approaches use similar mathematical tools, but the goals differ. In inference, we want to understand the true relationship between variables. In prediction, we want accurate predictions, even if the model does not perfectly capture the underlying mechanism.

## The Overfitting Problem

Models are built to fit training data as closely as possible. A linear regression minimizes squared errors; a logistic regression maximizes likelihood. But models that fit training data too well often predict poorly on new data.

**Overfitting** occurs when a model captures noise specific to the training data rather than the true underlying pattern. Complex models with many parameters are especially susceptible.

The solution is to evaluate models on data they have not seen—held-out test data or through cross-validation.

## Cross-Validation

Cross-validation estimates how well a model will generalize to new data.

**K-fold cross-validation**:
1. Split data into k roughly equal parts (folds)
2. For each fold: train on k-1 folds, test on the held-out fold
3. Average performance across all folds

```{r}
#| fig-width: 7
#| fig-height: 5
# Simple CV example with linear regression
library(boot)

# Generate data
set.seed(42)
x <- rnorm(100)
y <- 2 + 3*x + rnorm(100)
data <- data.frame(x, y)

# Fit model and perform CV
model <- glm(y ~ x, data = data)

# 10-fold cross-validation
cv_result <- cv.glm(data, model, K = 10)
cat("CV estimate of prediction error:", round(cv_result$delta[1], 3), "\n")
```

**Leave-one-out cross-validation (LOOCV)** is k-fold with k = n: each observation is held out once. More computationally expensive but lower variance.

## Bias-Variance Tradeoff

Prediction error has two components:

**Bias**: Error from approximating a complex reality with a simpler model. Simple models have high bias—they may miss important patterns.

**Variance**: Error from sensitivity to training data. Complex models have high variance—they change substantially with different training samples.

The best predictions come from models that balance bias and variance. As model complexity increases, bias decreases but variance increases. The optimal complexity minimizes total prediction error.

## LOESS: Flexible Non-Parametric Smoothing

**LOESS** (Locally Estimated Scatterplot Smoothing) fits local regressions to subsets of data, weighted by distance from each point.

```{r}
#| fig-width: 7
#| fig-height: 5
# Compare linear regression and LOESS
set.seed(123)
x <- seq(0, 4*pi, length.out = 100)
y <- sin(x) + rnorm(100, sd = 0.3)

plot(x, y, pch = 16, col = "gray60", main = "Linear vs LOESS")
abline(lm(y ~ x), col = "red", lwd = 2)
lines(x, predict(loess(y ~ x, span = 0.3)), col = "blue", lwd = 2)
legend("topright", c("Linear", "LOESS"), col = c("red", "blue"), lwd = 2)
```

The **span** parameter controls smoothness: smaller values fit more locally (more flexible), larger values average more broadly (smoother).

## Classification

When the response is categorical, we have a classification problem rather than regression. The goal is to predict which category an observation belongs to.

**Logistic regression** produces probabilities that can be converted to class predictions.

**Decision trees** recursively partition the feature space based on simple rules.

**Random forests** combine many decision trees for more robust predictions.

## Confusion Matrices

Classification performance is evaluated with a **confusion matrix**:

|  | Predicted Positive | Predicted Negative |
|:--|:--:|:--:|
| Actual Positive | True Positive (TP) | False Negative (FN) |
| Actual Negative | False Positive (FP) | True Negative (TN) |

Key metrics:
- **Accuracy**: (TP + TN) / Total
- **Sensitivity** (Recall): TP / (TP + FN) — how many positives were caught
- **Specificity**: TN / (TN + FP) — how many negatives were correctly identified
- **Precision**: TP / (TP + FP) — among positive predictions, how many were correct

## Decision Trees

Decision trees make predictions by asking a series of yes/no questions about the features:

```{r}
#| fig-width: 7
#| fig-height: 5
library(rpart)
library(rpart.plot)

# Build a simple decision tree
data(iris)
tree_model <- rpart(Species ~ Sepal.Length + Sepal.Width, data = iris)
rpart.plot(tree_model)
```

Trees are interpretable but prone to overfitting. Pruning (removing branches) or using ensembles helps.

## Random Forests

Random forests improve on single trees by:
1. Building many trees on bootstrap samples (bagging)
2. Using a random subset of features at each split
3. Averaging predictions across all trees

```{r}
library(randomForest)

rf_model <- randomForest(Species ~ ., data = iris, ntree = 100)
rf_model

# Variable importance
varImpPlot(rf_model)
```

## Practical Workflow

A typical statistical learning workflow:

1. **Split data** into training and test sets
2. **Explore** the training data
3. **Build candidate models** with different algorithms or parameters
4. **Evaluate** using cross-validation on training data
5. **Select** the best model
6. **Final evaluation** on held-out test data
7. **Report** honest estimates of performance

Never use test data for model building or selection—that defeats the purpose of holding it out.

## When to Use Statistical Learning

Statistical learning excels when:
- Prediction is the primary goal
- Relationships are complex or non-linear
- You have substantial data
- Interpretability is less critical

Traditional statistical methods may be preferable when:
- Understanding relationships matters more than prediction
- Sample sizes are small
- You need confidence intervals and hypothesis tests
- Interpretability is essential

## Connection to Dimensionality Reduction

High-dimensional data often benefit from dimensionality reduction before applying statistical learning methods. Techniques like PCA, clustering, and discriminant analysis are covered in detail in @sec-dimensionality-reduction.

## Summary

Statistical learning provides powerful tools for prediction and pattern discovery:

- Overfitting is the central challenge—models that fit training data too well predict poorly
- Cross-validation provides honest estimates of predictive performance
- The bias-variance tradeoff governs model complexity choices
- LOESS offers flexible non-parametric smoothing
- Classification methods (decision trees, random forests) handle categorical outcomes
- Confusion matrices summarize classification performance
- The choice between traditional statistics and machine learning depends on goals

## Additional Resources

- @james2023islr - The standard introduction to statistical learning
- @thulin2025msr - Modern perspectives on statistics with R
- @crawley2007r - Practical statistical methods in R
