# Bootstrapping {#sec-bootstrapping}

```{r}
#| echo: false
#| message: false
library(tidyverse)
theme_set(theme_minimal())
```

## The Bootstrap Idea

For the sample mean, we have elegant formulas for standard errors and confidence intervals derived from probability theory. But what about other statistics—the median, a correlation coefficient, the ratio of two means? For many estimators, no convenient formula exists.

The bootstrap, invented by Bradley Efron in 1979, provides a general solution. The key insight is that we can learn about the sampling distribution of a statistic by resampling from our data. If our sample is representative of the population, then samples drawn from our sample (with replacement) mimic what we would get from repeated sampling from the population.

![Bootstrap resampling procedure showing how multiple samples are drawn with replacement from the original data](../images/week_2.030.jpeg){#fig-bootstrap-resampling fig-align="center"}

## Why the Bootstrap Works

The bootstrap treats the observed sample as if it were the population. By drawing many samples with replacement from this "population," we create a distribution of the statistic of interest. This bootstrap distribution approximates the true sampling distribution.

The bootstrap standard error is the standard deviation of the bootstrap distribution. Bootstrap confidence intervals can be constructed from the percentiles of the bootstrap distribution—the 2.5th and 97.5th percentiles give an approximate 95% confidence interval.

## Bootstrap Procedure

The basic algorithm is straightforward:

1. Draw a random sample of size n from your data with replacement (the bootstrap sample)
2. Calculate the statistic of interest from this bootstrap sample
3. Repeat steps 1 and 2 many times (1000 or more)
4. Use the distribution of bootstrap statistics to estimate standard error or confidence intervals

```{r}
#| label: fig-bootstrap-dist
#| fig-cap: "Bootstrap distribution of the median from 1000 resamples, showing the estimated sampling distribution with 95% confidence intervals"
#| fig-width: 7
#| fig-height: 5
# Bootstrap example: estimating standard error of median
set.seed(42)
original_data <- rexp(50, rate = 0.1)  # Skewed distribution

# Observed median
observed_median <- median(original_data)

# Bootstrap
n_boot <- 1000
boot_medians <- replicate(n_boot, {
  boot_sample <- sample(original_data, replace = TRUE)
  median(boot_sample)
})

# Bootstrap standard error
boot_se <- sd(boot_medians)

# Bootstrap confidence interval (percentile method)
boot_ci <- quantile(boot_medians, c(0.025, 0.975))

cat("Observed median:", round(observed_median, 2), "\n")
cat("Bootstrap SE:", round(boot_se, 2), "\n")
cat("95% CI:", round(boot_ci, 2), "\n")

hist(boot_medians, breaks = 30, main = "Bootstrap Distribution of Median",
     xlab = "Median", col = "lightblue")
abline(v = observed_median, col = "red", lwd = 2)
abline(v = boot_ci, col = "blue", lwd = 2, lty = 2)
```

## Advantages of the Bootstrap

The bootstrap is remarkably versatile. It can be applied to almost any statistic—means, medians, correlations, regression coefficients, eigenvalues, and more. It works when no formula for standard errors exists. It is nonparametric, making no assumptions about the underlying distribution. It handles complex sampling designs and calculations that would be intractable analytically.

The bootstrap is widely used for assessing confidence in phylogenetic trees, where the complexity of tree-building algorithms makes analytical approaches impractical. In machine learning, bootstrap aggregating (bagging) improves prediction accuracy by combining models trained on bootstrap samples.

## When the Bootstrap Fails

The bootstrap is not a magic solution to all problems. It requires that the original sample be representative of the population—a biased sample produces biased bootstrap estimates. It can struggle with very small samples where the original data may not adequately represent the population.

Certain statistics, like the maximum of a sample, are poorly estimated by the bootstrap because the bootstrap distribution is bounded by the observed data. The bootstrap also assumes that observations are independent; for dependent data (like time series), specialized bootstrap methods are needed.

## Bootstrap Confidence Intervals

Several methods exist for constructing bootstrap confidence intervals. The **percentile method** uses the quantiles of the bootstrap distribution directly. The **basic bootstrap** method reflects the bootstrap distribution around the observed estimate. The **BCa (bias-corrected and accelerated)** method adjusts for bias and skewness in the bootstrap distribution.

```{r}
# Different bootstrap CI methods
library(boot)

# Define statistic function
median_fun <- function(data, indices) {
  median(data[indices])
}

# Run bootstrap
boot_result <- boot(original_data, median_fun, R = 1000)

# Different CI methods
boot.ci(boot_result, type = c("perc", "basic", "bca"))
```

The BCa method is generally preferred when computationally feasible, as it provides better coverage in many situations.

## Practical Recommendations

For most applications, 1000 bootstrap replications provide adequate precision for standard errors. For confidence intervals, especially when using the BCa method, 10,000 replications may be preferable. Always set a random seed for reproducibility.

Remember that the bootstrap estimates sampling variability—it cannot fix problems with biased samples or invalid measurements. Use it as a tool for understanding uncertainty, not as a cure for poor data quality.
