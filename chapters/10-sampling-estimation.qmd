# Sampling and Parameter Estimation {#sec-sampling-estimation}

```{r}
#| echo: false
#| message: false
library(tidyverse)
theme_set(theme_minimal())
```

## The Problem of Inference

Science often works by measuring samples to learn about populations. We cannot measure every protein in a cell, every patient with a disease, or every fish in the ocean. Instead, we take samples and use statistical inference to draw conclusions about the larger populations from which they came.

This creates a fundamental challenge: sample statistics vary from sample to sample, even when samples come from the same population. If you take two different random samples from a population and calculate their means, you will almost certainly get two different values. How, then, can we say anything reliable about the population?

The answer lies in understanding the sampling distribution—the distribution of a statistic across all possible samples of a given size.

## Parameters and Statistics

A **parameter** is a numerical characteristic of a population—the true population mean $\mu$, the true population standard deviation $\sigma$, the true proportion $p$. Parameters are typically fixed but unknown.

A **statistic** is a numerical characteristic of a sample—the sample mean $\bar{x}$, the sample standard deviation $s$, the sample proportion $\hat{p}$. Statistics are calculated from data and vary from sample to sample.

We use statistics to estimate parameters. The sample mean $\bar{x}$ estimates the population mean $\mu$. The sample standard deviation $s$ estimates the population standard deviation $\sigma$. These estimates will rarely equal the true parameter values exactly, but we can quantify how close they are likely to be.

## Point Estimates

A **point estimate** is a single number used as our best guess for a parameter. The sample mean is a natural point estimate for the population mean:

$$\bar{x} = \frac{1}{n} \sum_{i=1}^{n} x_i$$

What makes a good estimator? Ideally, an estimator should be:

**Unbiased**: On average, across many samples, the estimator equals the true parameter. The sample mean is an unbiased estimator of the population mean.

**Efficient**: Among unbiased estimators, it has the smallest variance. The sample mean is the most efficient estimator of a normal mean.

**Consistent**: As sample size increases, the estimator converges to the true parameter value.

## The Sampling Distribution of the Mean

Imagine drawing all possible samples of size $n$ from a population and calculating the mean of each. The distribution of these means is the sampling distribution of the mean.

The sampling distribution has remarkable properties:

1. Its mean equals the population mean: $E[\bar{X}] = \mu$
2. Its standard deviation (the **standard error**) equals: $SE = \frac{\sigma}{\sqrt{n}}$
3. For large samples, it is approximately normal (Central Limit Theorem)

```{r}
#| fig-width: 8
#| fig-height: 6
# Demonstrate sampling distribution
set.seed(32)

# Create a population
true_pop <- rpois(n = 10000, lambda = 3)
pop_mean <- mean(true_pop)
pop_sd <- sd(true_pop)

# Take many samples and compute their means
sample_sizes <- c(5, 20, 50, 200)
par(mfrow = c(2, 2))

for (n in sample_sizes) {
  sample_means <- replicate(1000, mean(sample(true_pop, n)))
  hist(sample_means, breaks = 30, main = paste("n =", n),
       xlab = "Sample Mean", col = "steelblue",
       xlim = c(1, 5))
  abline(v = pop_mean, col = "red", lwd = 2)
}
```

As sample size increases, the sampling distribution becomes narrower (smaller standard error) and more normal in shape. This is why larger samples give more precise estimates.

## Standard Error

The **standard error** (SE) measures the variability of a statistic across samples. For the sample mean:

$$SE_{\bar{x}} = \frac{\sigma}{\sqrt{n}}$$

Since we usually do not know $\sigma$, we estimate the standard error using the sample standard deviation:

$$\widehat{SE}_{\bar{x}} = \frac{s}{\sqrt{n}}$$

The standard error shrinks as sample size increases, but following a square root relationship. To halve the standard error, you need to quadruple the sample size.

```{r}
# Demonstrate how SE changes with sample size
set.seed(32)
true_pop <- rpois(n = 1000, lambda = 5)

# Sample size of 5
samps_5 <- replicate(n = 50, sample(true_pop, size = 5))
means_5 <- apply(samps_5, 2, mean)
se_5 <- sd(means_5)

# Sample size of 50
samps_50 <- replicate(n = 50, sample(true_pop, size = 50))
means_50 <- apply(samps_50, 2, mean)
se_50 <- sd(means_50)

cat("Standard error with n=5:", round(se_5, 3), "\n")
cat("Standard error with n=50:", round(se_50, 3), "\n")
cat("Ratio:", round(se_5/se_50, 2), "(theoretical: √10 =", round(sqrt(10), 2), ")\n")
```

![](../images/week_2.026.jpeg){fig-align="center"}

## Confidence Intervals

A point estimate tells us our best guess, but not how uncertain we are. A **confidence interval** provides a range of plausible values for the parameter along with a measure of confidence.

A 95% confidence interval for the population mean, when the population is normally distributed or the sample is large, is:

$$\bar{x} \pm t_{\alpha/2} \times \frac{s}{\sqrt{n}}$$

where $t_{\alpha/2}$ is the critical value from the t-distribution with $n-1$ degrees of freedom.

![](../images/week_2.027.jpeg){fig-align="center"}

The interpretation requires care: a 95% confidence interval means that if we repeated this procedure many times, 95% of the resulting intervals would contain the true parameter. Any particular interval either does or does not contain the true value—we just don't know which.

```{r}
# Calculate a confidence interval
set.seed(42)
sample_data <- rnorm(30, mean = 100, sd = 15)

sample_mean <- mean(sample_data)
sample_se <- sd(sample_data) / sqrt(length(sample_data))
t_crit <- qt(0.975, df = length(sample_data) - 1)

lower <- sample_mean - t_crit * sample_se
upper <- sample_mean + t_crit * sample_se

cat("Sample mean:", round(sample_mean, 2), "\n")
cat("95% CI: [", round(lower, 2), ",", round(upper, 2), "]\n")

# Or use t.test directly
t.test(sample_data)$conf.int
```

![](../images/week_2.028.jpeg){fig-align="center"}

## Coefficient of Variation

When comparing variability across groups with different means, the standard deviation alone can be misleading. The **coefficient of variation** (CV) standardizes variability relative to the mean:

$$CV = \frac{s}{\bar{x}} \times 100\%$$

A CV of 10% means the standard deviation is 10% of the mean. This allows meaningful comparisons between groups or measurements on different scales.

## Percentiles and Quantiles

**Percentiles** describe the relative position of values within a distribution. The $p$th percentile is the value below which $p$% of the data falls. The 50th percentile is the median, the 25th percentile is the first quartile, and the 75th percentile is the third quartile.

**Quantiles** divide data into equal parts. Quartiles divide into four parts, deciles into ten parts, percentiles into one hundred parts.

```{r}
# Calculate percentiles
data <- c(12, 15, 18, 22, 25, 28, 32, 35, 40, 45)

quantile(data, probs = c(0.25, 0.5, 0.75))
summary(data)
```

Quantiles form the basis for many statistical procedures, including constructing confidence intervals and calculating p-values.

## Bias and Variability

Two distinct types of error affect estimates:

**Bias** is systematic error—the tendency for an estimator to consistently over- or underestimate the true parameter. An unbiased estimator has zero bias: its average value across all possible samples equals the true parameter.

**Variability** is random error—the spread of estimates around their average value. Low variability means estimates cluster tightly together.

The ideal estimator has both low bias and low variability. Sometimes there is a tradeoff: a slightly biased estimator might have much lower variability, resulting in estimates that are closer to the truth on average.

The **mean squared error** (MSE) combines both sources of error:

$$MSE = Bias^2 + Variance$$

## The Bootstrap: Resampling for Estimation

The bootstrap, introduced by Bradley Efron in 1979, is a powerful resampling method for estimating standard errors and constructing confidence intervals when analytical formulas are unavailable or assumptions are questionable [@efron1979bootstrap].

The key insight is elegant: we can estimate the sampling distribution of a statistic by repeatedly resampling from our observed data. If our sample is representative of the population, then samples drawn from our sample (with replacement) should behave like samples drawn from the population.

### The Bootstrap Algorithm

1. Take a random sample with replacement from your data (same size as original)
2. Calculate the statistic of interest on this resampled data
3. Repeat steps 1-2 many times (typically 1000-10000)
4. The distribution of bootstrap statistics approximates the sampling distribution

```{r}
#| fig-width: 8
#| fig-height: 5
# Bootstrap estimation of the sampling distribution of the mean
set.seed(123)

# Our observed sample
original_sample <- c(23, 31, 28, 35, 42, 29, 33, 27, 38, 31)
n <- length(original_sample)

# Bootstrap: resample with replacement
n_bootstrap <- 5000
bootstrap_means <- numeric(n_bootstrap)

for (i in 1:n_bootstrap) {
  boot_sample <- sample(original_sample, size = n, replace = TRUE)
  bootstrap_means[i] <- mean(boot_sample)
}

# Compare bootstrap distribution to observed statistics
hist(bootstrap_means, breaks = 40, col = "steelblue",
     main = "Bootstrap Distribution of the Mean",
     xlab = "Sample Mean")
abline(v = mean(original_sample), col = "red", lwd = 2)
abline(v = quantile(bootstrap_means, c(0.025, 0.975)), col = "darkgreen", lwd = 2, lty = 2)

cat("Original sample mean:", mean(original_sample), "\n")
cat("Bootstrap SE:", sd(bootstrap_means), "\n")
cat("95% Bootstrap CI:", quantile(bootstrap_means, c(0.025, 0.975)), "\n")
```

### Bootstrap Confidence Intervals

The bootstrap provides several methods for constructing confidence intervals:

**Percentile method**: Use the 2.5th and 97.5th percentiles of the bootstrap distribution as the 95% CI bounds. This is the simplest approach shown above.

**Basic (reverse percentile) method**: Reflects the percentiles around the original estimate to correct for certain types of bias.

**BCa (bias-corrected and accelerated)**: A more sophisticated method that adjusts for both bias and skewness in the bootstrap distribution. This is often preferred when the sampling distribution is not symmetric.

```{r}
# Using the boot package for more sophisticated bootstrap CI
library(boot)

# Define statistic function
mean_stat <- function(data, indices) {
  mean(data[indices])
}

# Run bootstrap
boot_result <- boot(original_sample, mean_stat, R = 5000)

# Different CI methods
boot.ci(boot_result, type = c("perc", "basic", "bca"))
```

### When to Use the Bootstrap

The bootstrap is particularly valuable when:

- The sampling distribution of your statistic is unknown or complex
- Sample sizes are small and normality assumptions are questionable
- You are estimating something other than a mean (e.g., median, correlation, regression coefficients)
- Analytical formulas for standard errors do not exist

However, the bootstrap has limitations. It assumes your sample is representative of the population and works poorly with very small samples (n < 10-15) or when estimating extreme quantiles.

## Maximum Likelihood Estimation

**Maximum likelihood estimation (MLE)** provides a principled framework for parameter estimation. The likelihood function measures how probable the observed data would be for different parameter values. MLE finds the parameter values that make the observed data most probable.

For a sample $x_1, x_2, \ldots, x_n$ from a distribution with parameter $\theta$, the likelihood function is:

$$L(\theta | x_1, \ldots, x_n) = \prod_{i=1}^{n} f(x_i | \theta)$$

where $f(x_i | \theta)$ is the probability density (or mass) function. We typically work with the log-likelihood for computational convenience:

$$\ell(\theta) = \sum_{i=1}^{n} \log f(x_i | \theta)$$

The MLE $\hat{\theta}$ is the value that maximizes $\ell(\theta)$.

```{r}
# MLE example: estimating the rate parameter of an exponential distribution
set.seed(42)
exp_data <- rexp(100, rate = 0.5)  # True rate is 0.5

# Log-likelihood function for exponential
log_likelihood <- function(rate, data) {
  sum(dexp(data, rate = rate, log = TRUE))
}

# Find MLE
rates <- seq(0.1, 1, by = 0.01)
ll_values <- sapply(rates, log_likelihood, data = exp_data)

# Plot likelihood surface
plot(rates, ll_values, type = "l", lwd = 2,
     xlab = "Rate parameter", ylab = "Log-likelihood",
     main = "Log-likelihood for Exponential Rate")
abline(v = 1/mean(exp_data), col = "red", lwd = 2)  # MLE = 1/mean for exponential

cat("True rate: 0.5\n")
cat("MLE estimate:", round(1/mean(exp_data), 3), "\n")
```

MLEs have desirable properties: they are consistent (converge to true values as n increases) and asymptotically efficient (achieve the smallest possible variance for large samples).

## Simulation-Based Understanding of Estimation

Simulation provides powerful intuition for statistical concepts. By repeatedly sampling from known populations, we can directly observe sampling distributions.

```{r}
#| fig-width: 9
#| fig-height: 6
# Exploring properties of estimators through simulation
set.seed(456)

# True population parameters
pop_mean <- 50
pop_sd <- 10

# Simulate many samples and compute estimates
n_samples <- 2000
sample_sizes <- c(5, 15, 50)

par(mfrow = c(2, 3))

for (n in sample_sizes) {
  # Collect sample means
  sample_means <- replicate(n_samples, {
    samp <- rnorm(n, mean = pop_mean, sd = pop_sd)
    mean(samp)
  })

  # Plot distribution of sample means
  hist(sample_means, breaks = 40, col = "steelblue",
       main = paste("Sample Means (n =", n, ")"),
       xlab = "Sample Mean", xlim = c(35, 65))
  abline(v = pop_mean, col = "red", lwd = 2)

  # Calculate actual SE vs theoretical
  actual_se <- sd(sample_means)
  theoretical_se <- pop_sd / sqrt(n)

  legend("topright", bty = "n", cex = 0.8,
         legend = c(paste("Actual SE:", round(actual_se, 2)),
                    paste("Theoretical:", round(theoretical_se, 2))))
}

# Now do the same for sample standard deviations
for (n in sample_sizes) {
  # Collect sample SDs
  sample_sds <- replicate(n_samples, {
    samp <- rnorm(n, mean = pop_mean, sd = pop_sd)
    sd(samp)
  })

  hist(sample_sds, breaks = 40, col = "coral",
       main = paste("Sample SDs (n =", n, ")"),
       xlab = "Sample SD")
  abline(v = pop_sd, col = "red", lwd = 2)
}
```

This simulation reveals several important facts:
1. Sample means are unbiased (centered on the population mean)
2. The spread of sample means decreases as $\sqrt{n}$
3. Sample standard deviations are slightly biased for small samples but become unbiased as n increases

## Principles of Experimental Design

Good statistical analysis cannot rescue a poorly designed study. The way you collect data fundamentally determines what conclusions you can draw. Understanding key design principles is essential for planning experiments that yield valid, interpretable results.

### Randomization

**Randomization** assigns subjects to treatment groups by chance, ensuring that treatment groups are comparable. Without randomization, systematic differences between groups (confounders) can bias results.

```{r}
# Randomly assign 20 subjects to treatment or control
set.seed(42)
subjects <- 1:20
treatment_group <- sample(subjects, size = 10)
control_group <- setdiff(subjects, treatment_group)

cat("Treatment group:", treatment_group, "\n")
cat("Control group:", control_group, "\n")
```

Randomization provides the foundation for causal inference. Without it, we can only establish associations, not causation.

### Controls

Every experiment needs **controls**—groups that differ from treatment groups only in the variable of interest:

- **Negative control**: Receives no treatment; establishes baseline
- **Positive control**: Receives a treatment known to work; confirms the experimental system is functioning
- **Procedural control**: Receives all procedures except the active ingredient (e.g., sham surgery, vehicle-only injection)

Without proper controls, you cannot determine whether observed effects are due to your treatment or some other factor.

### Blinding

**Blinding** prevents knowledge of group assignment from influencing results:

- **Single-blind**: Subjects do not know which treatment they receive
- **Double-blind**: Neither subjects nor experimenters know group assignments
- **Triple-blind**: Subjects, experimenters, and data analysts are all blinded

Blinding prevents both placebo effects (subjects' expectations influencing outcomes) and experimenter bias (conscious or unconscious influence on measurements).

### Replication

**Replication** means having multiple independent observations in each treatment group. Replication is essential because it:

- Provides estimates of variability
- Enables statistical inference
- Increases precision of estimates
- Allows detection of real effects

The **unit of replication** must match the **unit of treatment**. If you treat tanks with different water temperatures and measure multiple fish per tank, your replicates are tanks, not fish.

::: {.callout-warning}
## Technical vs. Biological Replicates

- **Technical replicates**: Repeated measurements of the same sample (e.g., running the same sample through a machine twice)
- **Biological replicates**: Independent samples from different individuals or experimental units

Technical replicates measure precision of the measurement process. Biological replicates measure biological variability and enable inference to the population. Do not confuse them!
:::

### Blocking

**Blocking** groups experimental units that are similar to each other, then applies all treatments within each block. This reduces variability by accounting for known sources of heterogeneity.

Common blocking factors:
- Time (experimental batches, days)
- Location (different incubators, growth chambers)
- Individual (paired designs, repeated measures)

```{r}
# Randomized complete block design
# 4 treatments applied within each of 3 blocks
set.seed(123)
blocks <- 1:3
treatments <- c("A", "B", "C", "D")

design <- expand.grid(Block = blocks, Treatment = treatments)
design$Order <- NA

for (b in blocks) {
  block_rows <- design$Block == b
  design$Order[block_rows] <- sample(1:4)
}

design[order(design$Block, design$Order), ]
```

Blocking is particularly valuable when blocks correspond to major sources of variation (e.g., different labs, experimental days, genetic backgrounds).

### Sample Size Considerations

Determining appropriate sample size before collecting data is crucial. Too few samples waste resources by producing inconclusive results; too many waste resources by studying effects that were detectable with smaller samples.

Sample size depends on:
- **Effect size**: The minimum meaningful difference you want to detect
- **Variability**: The expected noise in your measurements
- **Significance level ($\alpha$)**: Usually 0.05
- **Power**: Usually 0.80 (80% chance of detecting a real effect)

Power analysis (covered in detail in a later chapter) formalizes these considerations.

## Key Takeaways

Understanding sampling distributions and estimation is fundamental to statistical inference. Key points to remember:

1. Statistics vary from sample to sample; this variability is quantified by the standard error
2. Larger samples give more precise estimates (smaller standard errors)
3. Confidence intervals quantify uncertainty about parameter estimates
4. The Central Limit Theorem explains why the normal distribution appears so frequently
5. Both bias and variability affect the quality of estimates
6. The bootstrap provides a flexible, computer-intensive approach to estimation when analytical methods are limited
7. Maximum likelihood provides a principled framework for parameter estimation
8. Good experimental design—randomization, controls, blinding, proper replication—is essential for valid inference

These concepts provide the foundation for hypothesis testing and the statistical inference methods we develop in subsequent chapters.

## Additional Resources

- @efron1979bootstrap - The original bootstrap paper, a landmark in modern statistics
- @irizarry2019introduction - Excellent chapters on sampling and estimation with R examples
