# Analysis of Variance {#sec-anova}

```{r}
#| echo: false
#| message: false
library(tidyverse)
theme_set(theme_minimal())
```

## Beyond Two Groups

The t-test compares two groups, but many experiments involve more than two. We might compare three drug treatments, five temperature conditions, or four genetic strains. Running multiple t-tests creates problems: with many comparisons, false positives become likely even when no true differences exist.

Analysis of Variance (ANOVA) provides a solution. It tests whether any of the group means differ from the others in a single test, controlling the overall Type I error rate.

## The ANOVA Framework

ANOVA partitions the total variation in the data into components: variation between groups (due to treatment effects) and variation within groups (due to random error).

![](../images/images_5b.016.jpeg){fig-align="center"}

The key insight is that if groups have equal means, the between-group variation should be similar to the within-group variation. If the between-group variation is much larger, the group means probably differ.

![](../images/images_5b.015.jpeg){fig-align="center"}

## The F-Test

ANOVA uses the F-statistic:

$$F = \frac{MS_{between}}{MS_{within}} = \frac{\text{Variance between groups}}{\text{Variance within groups}}$$

Under the null hypothesis (all group means equal), F follows an F-distribution. Large F values indicate that group means differ more than expected by chance.

## One-Way ANOVA in R

```{r}
# Example using iris data
iris_aov <- aov(Sepal.Length ~ Species, data = iris)
summary(iris_aov)
```

The significant p-value tells us that sepal length differs among species, but not which species differ from which.

## ANOVA Assumptions

Like the t-test, ANOVA assumes:

1. **Normality**: Observations within each group are normally distributed
2. **Homogeneity of variance**: Groups have equal variances
3. **Independence**: Observations are independent

ANOVA is robust to mild violations of normality, especially with balanced designs and large samples. Serious violations of homogeneity of variance are more problematic but can be addressed with Welch's ANOVA or transformations.

## Post-Hoc Comparisons

A significant ANOVA tells us groups differ but not how. **Post-hoc tests** compare specific pairs of groups while controlling for multiple comparisons.

**Tukey's HSD** (Honestly Significant Difference) compares all pairs:

```{r}
TukeyHSD(iris_aov)
```

Each pairwise comparison includes the difference in means, confidence interval, and adjusted p-value.

## Planned Contrasts

If you have specific hypotheses about which groups should differ (decided before seeing the data), planned contrasts are more powerful than post-hoc tests. They focus statistical power on the comparisons you care about.

```{r}
# Example: Compare setosa to the average of the other two species
contrasts(iris$Species) <- cbind(
  setosa_vs_others = c(2, -1, -1)
)
summary.lm(aov(Sepal.Length ~ Species, data = iris))
```

## Fixed vs. Random Effects

**Fixed effects** are specific treatments of interest that would be the same if the study were replicated—drug A, drug B, drug C. Conclusions apply only to these specific treatments.

**Random effects** are levels sampled from a larger population—particular subjects, batches, or locations. The goal is to generalize to the population of possible levels, not just those observed.

The distinction matters because it affects how F-ratios are calculated and what conclusions can be drawn.

## Two-Way ANOVA

When experiments have two factors, two-way ANOVA examines main effects of each factor and their interaction.

```{r}
#| fig-width: 7
#| fig-height: 5
# Simulated factorial design
set.seed(42)
n <- 20
data_factorial <- data.frame(
  response = c(rnorm(n, 10, 2), rnorm(n, 12, 2), 
               rnorm(n, 11, 2), rnorm(n, 18, 2)),
  factor_A = rep(c("A1", "A1", "A2", "A2"), each = n),
  factor_B = rep(c("B1", "B2", "B1", "B2"), each = n)
)

two_way <- aov(response ~ factor_A * factor_B, data = data_factorial)
summary(two_way)
```

An **interaction** means the effect of one factor depends on the level of the other. Significant interactions often require examining simple effects rather than main effects.

## Interaction Plots

```{r}
#| fig-width: 7
#| fig-height: 5
# Visualize interaction
interaction.plot(data_factorial$factor_A, data_factorial$factor_B, 
                 data_factorial$response,
                 col = c("blue", "red"), lwd = 2,
                 xlab = "Factor A", ylab = "Response",
                 trace.label = "Factor B")
```

Non-parallel lines suggest an interaction. Parallel lines suggest additive (non-interacting) effects.

![](../images/images_7a.022.jpeg){fig-align="center"}

## Nested Designs

In nested designs, levels of one factor exist only within levels of another. For example, technicians might be nested within labs—each technician works in only one lab.

Nested designs have no interaction term because not all combinations of factor levels exist. They are common when sampling is hierarchical.

## Practical Considerations

Report effect sizes (like $\eta^2$) alongside p-values. A significant ANOVA with tiny effect size may not be practically meaningful.

Check assumptions with residual plots. Consider transformations or nonparametric alternatives (Kruskal-Wallis) when assumptions are violated.

Plan your sample size using power analysis before collecting data, specifying the minimum effect size you want to detect.
