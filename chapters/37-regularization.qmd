# Regularization Methods {#sec-regularization}

```{r}
#| echo: false
#| message: false
library(tidyverse)
library(glmnet)
theme_set(theme_minimal())
```

## The Need for Regularization

Standard linear regression estimates coefficients by minimizing the residual sum of squares (RSS):

$$\text{RSS} = \sum_{i=1}^n \left(y_i - \beta_0 - \sum_{j=1}^p \beta_j x_{ij}\right)^2$$

This works well when you have many more observations than predictors ($n \gg p$) and the predictors are not highly correlated. But problems arise when:

- **Many predictors** ($p$ large relative to $n$): Coefficients become unstable, and variance is high
- **Correlated predictors** (multicollinearity): Small changes in data cause large changes in coefficients
- **Overfitting**: The model fits training data well but predicts poorly

**Regularization** addresses these issues by adding a penalty term that shrinks coefficients toward zero, trading a small increase in bias for a substantial reduction in variance.

## The Regularization Idea

Regularized regression minimizes a modified objective:

$$\text{Minimize: } \text{RSS} + \lambda \cdot P(\beta)$$

where:
- $P(\beta)$ is a **penalty function** that penalizes large coefficients
- $\lambda \geq 0$ is the **regularization parameter** controlling penalty strength

The penalty shrinks coefficients toward zero:
- $\lambda = 0$: No penalty, equivalent to ordinary least squares
- $\lambda \to \infty$: Very strong penalty, coefficients shrink to zero

Different penalty functions lead to different regularization methods.

## Ridge Regression (L2 Penalty)

**Ridge regression** [@hoerl1970ridge] uses the sum of squared coefficients as the penalty:

$$P(\beta) = \sum_{j=1}^p \beta_j^2$$

The full optimization problem is:

$$\hat{\beta}^{\text{ridge}} = \arg\min_\beta \left\{ \sum_{i=1}^n \left(y_i - \beta_0 - \sum_{j=1}^p \beta_j x_{ij}\right)^2 + \lambda \sum_{j=1}^p \beta_j^2 \right\}$$

### Properties of Ridge Regression

Ridge regression shrinks all coefficients toward zero but **never exactly to zero**. This means:

- All predictors remain in the model
- Useful when you believe all predictors are relevant
- Particularly effective for correlated predictors (handles multicollinearity)

```{r}
#| label: fig-ridge-path
#| fig-cap: "Ridge regression coefficient paths: as lambda increases, coefficients shrink toward zero but never reach exactly zero"
#| fig-width: 8
#| fig-height: 5
# Generate sample data with correlated predictors
set.seed(42)
n <- 100
p <- 10
X <- matrix(rnorm(n * p), n, p)
# Create correlated predictors
X[, 2] <- X[, 1] + rnorm(n, sd = 0.5)
X[, 3] <- X[, 1] + rnorm(n, sd = 0.5)
true_beta <- c(3, -2, 1.5, rep(0, p - 3))
y <- X %*% true_beta + rnorm(n)

# Fit ridge regression across lambda values
ridge_fit <- glmnet(X, y, alpha = 0)  # alpha = 0 for ridge

# Plot coefficient paths
plot(ridge_fit, xvar = "lambda", main = "Ridge Regression Coefficients")
```

### Geometric Interpretation

Ridge regression constrains coefficients to lie within a sphere (in 2D, a circle) centered at the origin:

$$\sum_{j=1}^p \beta_j^2 \leq t$$

where $t$ is inversely related to $\lambda$. The OLS solution may lie outside this constraint region, so ridge finds the point where the RSS contours first touch the constraint boundary.

```{r}
#| label: fig-ridge-geometry
#| fig-cap: "Geometric interpretation of ridge regression: the constraint region is circular, so the solution rarely lies exactly on an axis (coefficient rarely zero)"
#| fig-width: 6
#| fig-height: 6
# Geometric illustration
theta <- seq(0, 2*pi, length.out = 100)

plot(cos(theta), sin(theta), type = "l", lwd = 2, col = "blue",
     asp = 1, xlim = c(-1.5, 1.5), ylim = c(-1.5, 1.5),
     xlab = expression(beta[1]), ylab = expression(beta[2]),
     main = "Ridge Constraint Region")

# Add contours representing RSS
for (r in seq(0.3, 1.5, 0.3)) {
  lines(r * cos(theta) + 0.8, r * sin(theta) + 0.6,
        col = "gray", lty = 2)
}

# OLS estimate
points(0.8, 0.6, pch = 19, cex = 2, col = "red")
text(0.8, 0.75, "OLS", col = "red")

# Ridge estimate (on boundary)
points(0.5, 0.37, pch = 19, cex = 2, col = "blue")
text(0.5, 0.5, "Ridge", col = "blue")

legend("bottomleft", c("Constraint region", "RSS contours"),
       col = c("blue", "gray"), lty = c(1, 2), lwd = c(2, 1))
```

## Lasso Regression (L1 Penalty)

**Lasso** (Least Absolute Shrinkage and Selection Operator) [@tibshirani1996regression] uses the sum of absolute values as the penalty:

$$P(\beta) = \sum_{j=1}^p |\beta_j|$$

The optimization problem is:

$$\hat{\beta}^{\text{lasso}} = \arg\min_\beta \left\{ \sum_{i=1}^n \left(y_i - \beta_0 - \sum_{j=1}^p \beta_j x_{ij}\right)^2 + \lambda \sum_{j=1}^p |\beta_j| \right\}$$

### Properties of Lasso Regression

Unlike ridge, lasso can shrink coefficients **exactly to zero**, effectively performing **variable selection**:

- Produces **sparse models** with fewer predictors
- Easier to interpret than ridge (identifies "important" variables)
- May struggle when predictors are highly correlated (tends to select one arbitrarily)

```{r}
#| label: fig-lasso-path
#| fig-cap: "Lasso regression coefficient paths: as lambda increases, coefficients shrink and some become exactly zero (variable selection)"
#| fig-width: 8
#| fig-height: 5
# Fit lasso regression
lasso_fit <- glmnet(X, y, alpha = 1)  # alpha = 1 for lasso

plot(lasso_fit, xvar = "lambda", main = "Lasso Regression Coefficients")
```

### Geometric Interpretation

Lasso constrains coefficients to lie within a diamond (in 2D, an L1 ball):

$$\sum_{j=1}^p |\beta_j| \leq t$$

The diamond has corners on the axes, so the RSS contours often touch the constraint at a corner, forcing some coefficients to exactly zero.

```{r}
#| label: fig-lasso-geometry
#| fig-cap: "Geometric interpretation of lasso: the constraint region has corners on the axes, so the solution often lies at a corner where one or more coefficients are exactly zero"
#| fig-width: 6
#| fig-height: 6
# Diamond constraint
diamond_x <- c(0, 1, 0, -1, 0)
diamond_y <- c(1, 0, -1, 0, 1)

plot(diamond_x, diamond_y, type = "l", lwd = 2, col = "blue",
     asp = 1, xlim = c(-1.5, 1.5), ylim = c(-1.5, 1.5),
     xlab = expression(beta[1]), ylab = expression(beta[2]),
     main = "Lasso Constraint Region")

# Add contours
for (r in seq(0.3, 1.5, 0.3)) {
  lines(r * cos(theta) + 0.8, r * sin(theta) + 0.6,
        col = "gray", lty = 2)
}

# OLS estimate
points(0.8, 0.6, pch = 19, cex = 2, col = "red")
text(0.8, 0.75, "OLS", col = "red")

# Lasso estimate (at corner)
points(0.7, 0, pch = 19, cex = 2, col = "blue")
text(0.7, 0.15, "Lasso", col = "blue")

legend("bottomleft", c("Constraint region", "RSS contours"),
       col = c("blue", "gray"), lty = c(1, 2), lwd = c(2, 1))
```

## Elastic Net: Combining Ridge and Lasso

**Elastic net** combines both penalties:

$$P(\beta) = \alpha \sum_{j=1}^p |\beta_j| + (1-\alpha) \sum_{j=1}^p \beta_j^2$$

The mixing parameter $\alpha$ controls the balance:
- $\alpha = 0$: Pure ridge
- $\alpha = 1$: Pure lasso
- $0 < \alpha < 1$: Combination

Elastic net is often preferred when predictors are correlated—it tends to select groups of correlated variables together rather than arbitrarily choosing one.

```{r}
#| label: fig-elastic-net
#| fig-cap: "Elastic net coefficient paths with alpha = 0.5 (equal mix of L1 and L2 penalties)"
#| fig-width: 8
#| fig-height: 5
# Fit elastic net
enet_fit <- glmnet(X, y, alpha = 0.5)

plot(enet_fit, xvar = "lambda", main = "Elastic Net Coefficients (α = 0.5)")
```

```{r}
#| label: fig-enet-geometry
#| fig-cap: "Elastic net constraint region: intermediate between ridge (circle) and lasso (diamond)"
#| fig-width: 8
#| fig-height: 4
par(mfrow = c(1, 3))

# Ridge
plot(cos(theta), sin(theta), type = "l", lwd = 2, col = "blue",
     asp = 1, xlim = c(-1.2, 1.2), ylim = c(-1.2, 1.2),
     xlab = expression(beta[1]), ylab = expression(beta[2]),
     main = "Ridge (α = 0)")

# Elastic net
alpha_en <- 0.5
en_x <- en_y <- numeric(100)
for (i in 1:100) {
  # Approximate elastic net constraint boundary
  ang <- theta[i]
  r <- 1 / (alpha_en * (abs(cos(ang)) + abs(sin(ang))) + (1 - alpha_en))
  en_x[i] <- r * cos(ang)
  en_y[i] <- r * sin(ang)
}
plot(en_x, en_y, type = "l", lwd = 2, col = "purple",
     asp = 1, xlim = c(-1.2, 1.2), ylim = c(-1.2, 1.2),
     xlab = expression(beta[1]), ylab = expression(beta[2]),
     main = "Elastic Net (α = 0.5)")

# Lasso
plot(diamond_x, diamond_y, type = "l", lwd = 2, col = "red",
     asp = 1, xlim = c(-1.2, 1.2), ylim = c(-1.2, 1.2),
     xlab = expression(beta[1]), ylab = expression(beta[2]),
     main = "Lasso (α = 1)")
```

## Choosing Lambda with Cross-Validation

The regularization parameter $\lambda$ is typically chosen by cross-validation:

```{r}
#| label: fig-cv-lambda
#| fig-cap: "Cross-validation to select optimal lambda: the left dashed line marks the minimum error, the right marks the most regularized model within one standard error"
#| fig-width: 8
#| fig-height: 5
# Cross-validation for lasso
set.seed(123)
cv_lasso <- cv.glmnet(X, y, alpha = 1)

# Plot cross-validation results
plot(cv_lasso)

# Optimal lambda values
cat("Lambda with minimum CV error:", round(cv_lasso$lambda.min, 4), "\n")
cat("Lambda within 1 SE of minimum:", round(cv_lasso$lambda.1se, 4), "\n")
```

The `lambda.1se` (one standard error rule) often provides a more parsimonious model with nearly as good performance as the minimum.

### Extracting Coefficients

```{r}
# Coefficients at different lambda values
cat("\nCoefficients at lambda.min:\n")
coef(cv_lasso, s = "lambda.min")

cat("\nCoefficients at lambda.1se:\n")
coef(cv_lasso, s = "lambda.1se")
```

## Comparing Regularization Methods

Let's compare OLS, ridge, and lasso on the same data:

```{r}
# Fit models with optimal lambda
ridge_cv <- cv.glmnet(X, y, alpha = 0)
lasso_cv <- cv.glmnet(X, y, alpha = 1)

# Extract coefficients
coef_ols <- coef(lm(y ~ X))
coef_ridge <- coef(ridge_cv, s = "lambda.1se")
coef_lasso <- coef(lasso_cv, s = "lambda.1se")

# Compare (excluding intercept)
comparison <- data.frame(
  True = c(NA, true_beta),
  OLS = as.vector(coef_ols),
  Ridge = as.vector(coef_ridge),
  Lasso = as.vector(coef_lasso)
)
rownames(comparison) <- c("Intercept", paste0("X", 1:p))
round(comparison, 3)
```

Notice that:
- **OLS** estimates have high variance, especially for the zero-coefficient variables
- **Ridge** shrinks all coefficients toward zero but doesn't eliminate any
- **Lasso** correctly identifies many of the zero coefficients

## Visualizing the Comparison

```{r}
#| label: fig-method-comparison
#| fig-cap: "Comparison of coefficient estimates: true values, OLS, ridge, and lasso. Lasso successfully identifies zero coefficients while ridge shrinks but retains all."
#| fig-width: 9
#| fig-height: 5
# Plot comparison
var_names <- paste0("X", 1:p)
coef_df <- data.frame(
  Variable = rep(var_names, 4),
  Method = rep(c("True", "OLS", "Ridge", "Lasso"), each = p),
  Coefficient = c(true_beta,
                  as.vector(coef_ols)[-1],
                  as.vector(coef_ridge)[-1],
                  as.vector(coef_lasso)[-1])
)

ggplot(coef_df, aes(x = Variable, y = Coefficient, fill = Method)) +
  geom_bar(stat = "identity", position = "dodge") +
  scale_fill_manual(values = c("True" = "black", "OLS" = "gray",
                               "Ridge" = "steelblue", "Lasso" = "coral")) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  labs(title = "Coefficient Estimates by Method")
```

## Prediction Performance

```{r}
# Generate test data
set.seed(999)
X_test <- matrix(rnorm(50 * p), 50, p)
X_test[, 2] <- X_test[, 1] + rnorm(50, sd = 0.5)
X_test[, 3] <- X_test[, 1] + rnorm(50, sd = 0.5)
y_test <- X_test %*% true_beta + rnorm(50)

# Predictions
pred_ols <- predict(lm(y ~ X), newdata = data.frame(X_test))
pred_ridge <- predict(ridge_cv, newx = X_test, s = "lambda.1se")
pred_lasso <- predict(lasso_cv, newx = X_test, s = "lambda.1se")

# Test MSE
mse_ols <- mean((y_test - pred_ols)^2)
mse_ridge <- mean((y_test - pred_ridge)^2)
mse_lasso <- mean((y_test - pred_lasso)^2)

cat("Test MSE:\n")
cat("  OLS:", round(mse_ols, 3), "\n")
cat("  Ridge:", round(mse_ridge, 3), "\n")
cat("  Lasso:", round(mse_lasso, 3), "\n")
```

## When to Use Each Method

::: {.callout-tip}
## Choosing a Regularization Method

**Use Ridge when:**
- You believe all predictors are relevant
- Predictors are highly correlated
- You want stable predictions
- Interpretation of individual coefficients is less important

**Use Lasso when:**
- You want automatic variable selection
- A sparse, interpretable model is preferred
- You believe only a subset of predictors are truly relevant

**Use Elastic Net when:**
- Predictors are correlated and you want variable selection
- You want to select groups of correlated variables together
- Neither ridge nor lasso alone performs well

**Important**: Always standardize predictors before applying regularization, as the penalty treats all coefficients equally. The `glmnet` function does this automatically by default.
:::

## Regularization in Practice

### High-Dimensional Data Example

Regularization is especially valuable when $p$ approaches or exceeds $n$:

```{r}
#| label: fig-high-dim
#| fig-cap: "Regularization is essential for high-dimensional data where p ≈ n or p > n"
#| fig-width: 8
#| fig-height: 5
# High-dimensional example: p = 80, n = 100
set.seed(42)
n <- 100
p <- 80
X_hd <- matrix(rnorm(n * p), n, p)
# Sparse true model: only 5 predictors matter
true_beta_hd <- c(rep(2, 5), rep(0, p - 5))
y_hd <- X_hd %*% true_beta_hd + rnorm(n)

# OLS fails (numerically unstable or overfits severely)
# ols_hd <- lm(y_hd ~ X_hd)  # Would have issues

# Lasso works
cv_lasso_hd <- cv.glmnet(X_hd, y_hd, alpha = 1)

# How many non-zero coefficients?
lasso_coef <- coef(cv_lasso_hd, s = "lambda.1se")
n_nonzero <- sum(lasso_coef[-1] != 0)
cat("True non-zero coefficients:", 5, "\n")
cat("Lasso selected:", n_nonzero, "\n")

# Which variables selected?
selected <- which(lasso_coef[-1] != 0)
cat("Selected variables:", paste(selected, collapse = ", "), "\n")
```

### Regularized Logistic Regression

Regularization extends naturally to classification problems:

```{r}
# Binary classification example
set.seed(42)
y_binary <- rbinom(n, 1, plogis(X %*% c(1, -0.5, 0.3, rep(0, p - 3))))

# Regularized logistic regression
cv_logit <- cv.glmnet(X, y_binary, family = "binomial", alpha = 1)

# Coefficients
cat("Selected predictors for logistic regression:\n")
logit_coef <- coef(cv_logit, s = "lambda.1se")
which(logit_coef[-1] != 0)
```

## Exercises

::: {.callout-note}
### Exercise Reg.1: Ridge vs Lasso

1. Generate data with $n = 100$ observations and $p = 20$ predictors where:
   - Predictors 1-3 have coefficients 3, -2, 1
   - Predictors 4-6 are correlated with 1-3
   - Predictors 7-20 have coefficient 0

2. Fit OLS, ridge, and lasso. Compare the coefficient estimates to the true values.

3. Generate a test set and compare prediction performance.
:::

::: {.callout-note}
### Exercise Reg.2: Choosing Alpha

4. For the data above, use cross-validation to select the optimal mixing parameter $\alpha$ for elastic net. Try $\alpha \in \{0, 0.25, 0.5, 0.75, 1\}$.

5. Does the optimal $\alpha$ match what you would expect given the correlation structure?
:::

::: {.callout-note}
### Exercise Reg.3: High-Dimensional Classification

6. Load a gene expression dataset with many more genes than samples. Use regularized logistic regression to build a classifier.

7. Compare the number of genes selected by lasso at `lambda.min` vs `lambda.1se`. Which would you prefer in practice?
:::

## Summary

- **Regularization** adds penalty terms to prevent overfitting, especially with many predictors
- **Ridge regression** (L2 penalty) shrinks coefficients toward zero but never to exactly zero
  - Best when all predictors may be relevant
  - Handles multicollinearity well
- **Lasso regression** (L1 penalty) can shrink coefficients exactly to zero
  - Performs variable selection automatically
  - Produces sparse, interpretable models
- **Elastic net** combines L1 and L2 penalties
  - Select groups of correlated variables together
  - Often outperforms pure lasso with correlated predictors
- **Cross-validation** selects the optimal regularization strength ($\lambda$)
  - `lambda.min` minimizes CV error
  - `lambda.1se` (one standard error rule) gives more parsimonious models
- **Standardization** of predictors is essential before regularization
- Regularization extends to classification (regularized logistic regression) and other models

## Additional Resources

- @james2023islr - Detailed treatment of regularization methods
- @hastie2009elements - Theoretical foundations
- `glmnet` package vignette - Practical implementation details
- @tibshirani1996regression - Original lasso paper
