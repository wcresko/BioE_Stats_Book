# Nonparametric Tests {#sec-nonparametric}

```{r}
#| echo: false
#| message: false
library(tidyverse)
theme_set(theme_minimal())
```

## When Assumptions Fail

Parametric tests like the t-test make assumptions about the underlying data distributionâ€”typically that data are normally distributed with equal variances across groups. When these assumptions are violated, the tests may give misleading results. Nonparametric tests provide alternatives that make fewer assumptions about the data.

Nonparametric methods are sometimes called distribution-free methods because they do not assume a specific probability distribution. Instead, they typically work with ranks or signs of data rather than the raw values. This makes them robust to outliers and applicable to ordinal data where parametric methods would be inappropriate.

## The Mann-Whitney U Test

The Mann-Whitney U test (also called the Wilcoxon rank-sum test) is the nonparametric equivalent of the two-sample t-test. It tests whether two independent groups tend to have different values, based on comparing the ranks of observations rather than the observations themselves.

The null hypothesis is that the distributions of the two groups are identical. The alternative is that one group tends to have larger values than the other.

```{r}
# Generate data with non-normal distributions
set.seed(518)
group1 <- sample(rnorm(n = 10000, mean = 2, sd = 0.5), size = 100)
group2 <- sample(rnorm(n = 10000, mean = 5, sd = 1.5), size = 100)

# Mann-Whitney U test
wilcox.test(group1, group2)
```

The test works by combining all observations, ranking them, and comparing the sum of ranks in each group. If one group tends to have higher values, its rank sum will be larger than expected by chance.

## Wilcoxon Signed-Rank Test

For paired data, the Wilcoxon signed-rank test is the nonparametric alternative to the paired t-test. It tests whether the median difference between pairs is zero.

```{r}
# Paired data example
set.seed(123)
before <- rnorm(20, mean = 100, sd = 15)
after <- before + rexp(20, rate = 0.2)  # Skewed improvement

wilcox.test(after, before, paired = TRUE)
```

The test calculates the differences between pairs, ranks their absolute values, and considers the signs of the differences. Under the null hypothesis, positive and negative differences should be equally likely and of similar magnitude.

## Kruskal-Wallis Test

The Kruskal-Wallis test extends the Mann-Whitney U test to more than two groups, serving as a nonparametric alternative to one-way ANOVA. It tests whether at least one group tends to have different values from the others.

```{r}
# Example with three groups
set.seed(42)
data <- data.frame(
  value = c(rexp(30, 0.1), rexp(30, 0.15), rexp(30, 0.2)),
  group = factor(rep(c("A", "B", "C"), each = 30))
)

kruskal.test(value ~ group, data = data)
```

Like ANOVA, a significant Kruskal-Wallis test tells you that groups differ but not which specific groups differ from which others. Post-hoc pairwise comparisons can follow up on a significant result.

## Advantages and Limitations

**Advantages of nonparametric tests:**

Nonparametric tests do not require normally distributed data. They are robust to outliers since they work with ranks rather than raw values. They can be applied to ordinal data where the assumption of interval-level measurement would be violated. They often have good power relative to parametric tests even when parametric assumptions are met.

**Limitations:**

When parametric assumptions are met, nonparametric tests are slightly less powerful than their parametric counterparts. They test hypotheses about distributions or medians rather than means, which may not always align with research questions. They can be more difficult to extend to complex designs with multiple factors or covariates.

## Choosing Between Parametric and Nonparametric

The choice depends on your data and research question. If your data are reasonably normal (or your sample is large enough for the Central Limit Theorem to apply) and you care about means, parametric tests are appropriate and efficient. If your data are severely non-normal, contain outliers, or are ordinal in nature, nonparametric tests provide a safer alternative.

With large samples, the Central Limit Theorem ensures that parametric tests are robust to non-normality, so the choice matters less. With small samples, checking assumptions becomes more important.
